{"version":"v1","hackathon_url":"https://azurecosmosdb.devpost.com","generated_at":"2026-02-18T16:41:17.349418Z","result":{"hackathon":{"name":"Microsoft Developers AI Learning Hackathon","url":"https://azurecosmosdb.devpost.com","gallery_url":"https://azurecosmosdb.devpost.com/project-gallery","scanned_pages":8,"scanned_projects":171,"winner_count":13},"winners":[{"project_title":"Foodi Copilot","project_url":"https://devpost.com/software/foodi-copilot","tagline":"Foodi Copilot:Your AI-powered food assistant. Get personalized recipe suggestions, nutritional insights, and food product information using text or image queries. Powered by Azure OpenAI and Cosmos DB","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/933/632/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: 3rd Place"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azure-openai","url":null},{"name":"cosmosdb","url":null},{"name":"fastapi","url":null},{"name":"gpt4o","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"kartithackathon.azurewebsites.net","url":"https://kartithackathon.azurewebsites.net/"}],"description_sections":[{"heading":"The Spark","content":"I'm just a developer who loves food but struggles with health. Deciphering food labels and balancing my diet has always been a challenge, especially with my digestive issues. I can't count how many times my wife called asking where I was when she sent me for groceries - I'd be hopping between stores, desperately trying to find products that fit my health needs and calorie goals. It was during one of these marathon shopping trips that the idea for Foodi Copilot was born."},{"heading":"What I Hoped to Create","content":"I dreamed of an AI assistant that could:\n\nSuggest recipes tailored to individual dietary needs Break down nutritional info in a way that actually makes sense Analyze food products from text or images (because squinting at labels is no fun) Answer all those random food questions we all have"},{"heading":"The Build: A Learning Adventure","content":"Armed with more enthusiasm than expertise, I dove into Azure services:\n\nFrontend: Wrestled with React and TypeScript, somehow made it look decent with Fluent UI Backend: Python and FastAPI, containerized in Azure (sounds fancy, right?) AI Magic: Azure OpenAI Service (still amazed this works, big bow to gpt4o) Data Home: Azure Cosmos DB for MongoDB vCore (vector search was a game-changer) AI Conductor: Langchain (because one AI wasn't complex enough)"},{"heading":"The \"What Did I Get Myself Into\" Moments","content":"Past attempts felt like trying to build a spaceship with duct tape. Vector search finally made it possible. Integrating Azure services solo? Let's just say I have a love-hate relationship with documentation now. Training food recognition models made me question my life choices more than once. Balancing personalized recommendations with not accidentally poisoning someone was... fun. Solo development: Turns out, being a one-person team means a lot of talking to rubber ducks. Nights and weekends became my new \"work hours.\" My family started to wonder if I'd been replaced by a coding zombie. (Spoiler: I kind of was.)"},{"heading":"Small Wins That Kept Me Going","content":"The day vector search actually worked felt like winning the lottery. Building an interface my mom could use without calling me for help. Creating an AI system that doesn't just say \"I don't know\" to every question. Getting Azure Computer Vision to recognize a burrito (it was the little things). My cousin Al using an early version and not immediately giving up on it. That moment when my wife realized why I'd been glued to my computer every evening - and actually thought it was cool. (Don't worry, I'm as surprised as you are.)"},{"heading":"Lessons Learned (The Hard Way)","content":"Vector search isn't just a buzzword - it's a lifesaver for AI apps. AI agents, LLMs, and databases can work together... after a lot of coaxing. Cloud services are amazing, once you figure out how to use them without breaking the bank. Solo development requires the patience of a saint and the caffeine tolerance of a college student. Azure and Langchain are powerful, but they won't do your dishes or walk your dog."},{"heading":"The \"Maybe Someday\" List","content":"Expand the database to include foods my grandma would recognize. Integrate Azure Health Bot (because clearly, I enjoy complex integrations). Mobile app development - because who doesn't need more screen time? Add voice interactions, so I can argue with my phone about calories. Partner with health experts who can validate that I'm not completely off track. Build a community feature, turning my solo project into a group therapy session for health-conscious devs."},{"heading":"What's Next: The \"Definitely After Hackathon\" Plan","content":"Look, I'm not stopping just because the hackathon ends. This thing has become my digital baby, and I'm committed to helping it grow (unlike my actual muscles... yet). Here's what's cooking:\n\nAdding user authentication: Because apparently, not everyone wants to share their burrito cravings with the world. Implementing user preferences: So Foodi Copilot can remember that I've been trying to bulk up (spoiler: protein shakes are involved). Expanding diet profiles: My cousin's keto journey needs all the low-carb, high-fat support it can get. Weight loss tracking: My sister's on a mission to shed some pounds, and Foodi Copilot's going to be her new best friend (sorry, sis).\n\nI'm basically turning my family into guinea pigs for this app. They're thrilled. (Okay, they're tolerating it, but that's basically the same thing, right?)"},{"heading":"The Azure Toolkit That Made This Possible","content":"Azure Cosmos DB: Turns out, databases can be more than just boring tables. Azure OpenAI Service: Because sometimes, you need an AI smarter than you to figure things out. Langchain and LLM Agents: For when you want to feel like an AI orchestra conductor. Azure Computer Vision: Teaching machines to see food so I don't have to squint at labels. Vector Search: The unsung hero that turned this from a pipe dream into something almost useful.\n\nFoodi Copilot isn't just an app; it's my diary of late nights, coding frustrations, and small victories. It's far from perfect, but it's a step towards making healthy eating less of a puzzle. Every time I use it and don't immediately reach for takeout, I count it as a win.\n\nThis journey has been humbling, exhausting, and incredibly rewarding. I'm still learning, still improving, and still amazed when something actually works as intended. If this project helps even one person make healthier choices (besides me, my protein-obsessed self, my keto cousin Al, and my calorie-counting sister), I'll consider it a success. Who knows, maybe one day Foodi Copilot will help someone else navigate the nutritional maze without losing their mind in the cereal aisle. A developer can dream, right? And hey, maybe I'll even get my weekends back... eventually."},{"heading":"Built With","content":"azure azure-openai cosmosdb fastapi gpt4o python react typescript"},{"heading":"Try it out","content":"kartithackathon.azurewebsites.net"}]},{"project_title":"VISOUNDAY","project_url":"https://devpost.com/software/visounday","tagline":"Visounday is an AI video analysis and background music recommender for mini-vlog, perfect for documenting activities such as a beautiful Sunday.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/933/691/datas/medium.gif","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: 2nd Place"}],"team_members":[],"built_with":[{"name":"authentication","url":null},{"name":"azure-ai-vision","url":null},{"name":"azure-app-service","url":null},{"name":"azure-cosmos-db","url":null},{"name":"bing-search-api","url":"https://devpost.com/software/built-with/bing-search-api"},{"name":"canvas","url":"https://devpost.com/software/built-with/canvas"},{"name":"ci/cd","url":null},{"name":"cloudinary","url":"https://devpost.com/software/built-with/cloudinary"},{"name":"computer-vision","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"ffmpeg","url":"https://devpost.com/software/built-with/ffmpeg"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"flowbite","url":null},{"name":"gpt-4","url":null},{"name":"jsonwebtoken","url":null},{"name":"jwt","url":null},{"name":"media-query","url":null},{"name":"microsoft-authentication","url":null},{"name":"microsoft-azure-cosmos-db","url":null},{"name":"microsoft-azure-video-indexer","url":null},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"moongose","url":null},{"name":"multer","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"react-markdown-preview","url":null},{"name":"responsive","url":null},{"name":"social-login","url":null},{"name":"tailwind","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ayusudi/Visounday-client"},{"label":"github.com","url":"https://github.com/ayusudi/Visounday-server"},{"label":"github.com","url":"https://github.com/ayusudi/Visounday-playground-prototyping"}],"description_sections":[{"heading":"Inspiration","content":"VISOUNDAY is inspired by my personal frustration while editing videos of hanging out with friends on Saturday nights to post on IG stories. Unfortunately, spending too much time late at night searching for relevant songs and assets for editing meant the day ended, and I ended up uploading them in the sunday morning.\n\nSuitable for content creators and individuals who like documenting their daily lives."},{"heading":"What it does","content":"VISOUNDAY is an advanced AI on the web designed to evoke nostalgia for the feeling of a Sunday, enhancing everyday documentation and enjoyment through video and music recommendations.\n\nVISOUNDAY analyzes user videos by converting them into image frames and interpreting them using computer vision. Our server creates a cover image by collaging these frames with a canvas and refines the raw data obtained through computer vision and video indexing. The refined data is processed by GPT-4, producing comprehensive text markdown explanations. All data and chats are stored in Azure Cosmos MongoDB. The videos are processed through Azure Computer Vision and Video Indexer for detailed insights. Additionally, VISOUNDAY offers background music recommendations, retrieves relevant images from the Bing Search API, and provides tags from the Video Indexer. Users can interact with the AI through chats about their videos or engage in casual conversation, as VISOUNDAY's GPT-4 can understand a wide range of topics and languages. Additionally, users can chit-chat too."},{"heading":"How we built it","content":"Built with the initial attempt of uploading videos to ChatGPT using GPT-4, which can accept user inputs like documents, photos, and videos. It began with the creation of a prototype involving a single file (with .js and ipynb also tried on Postman for an API) to experiment with computer vision, video indexing, campus photos, Bing API, and finally, GPT-4 code.\n\nInitially, I tried this on Azure Open AI Studio, where we explored YouTube and discovered that I could make video input requests. However, since I hadn't received permission from the completed form to use GPT-4 vision on Azure Open AI Studio, I planned alternative methods that I had already tested, including using Python and JavaScript. But before that, let me share my prototyping code here: click here .\n\nArchitecture\n\nThe app is designed using a Client-Server architecture, ensuring robust performance and scalability.\n\nTechnologies Used\n\nGPT-4 for content generation and analysis. Azure Vision and Video Indexer for video and image processing. Microsoft Social Login for user authentication. Server with Express.js & Node.js with Multer, Canvas and Fmpeg to process file video and image. Client with React.js, Framework CSS Tailwind, and library React Markdown. Database with MongoDB from Azure Cosmos and ORM Mongoose. Stored Upload File with Cloudinary.\n\nProcess Flow\n\nUser Authentication : Users log in to the app using their Microsoft accounts through Social Login. Video Upload : Users upload a video ranging from 15 to 40 seconds. Cloudinary Integration : The server uploads the video to Cloudinary. Video Framing and Collage Creation : The server extracts frames from the video at 5-second intervals. A photo collage is created using a canvas to serve as the cover. Computer Vision Processing : The extracted frames are sent to Azure Computer Vision for analysis. AI Analysis and Recommendations : The results from Computer Vision and the cover photo are sent to GPT-4. GPT-4 analyzes the video to generate a Video Analysis, Video Title, and Video Backsound Song Recommendations. Video Indexing : The uploaded video is processed by Azure Video Indexer to generate a list of related tags. Related Image Search : Using the related tags, the Bing API searches for images that are related to the video's content. This feature is beneficial for users who want to conclude their video with an image that resonates with the content. Chat with GPT-4 : Users can interact with GPT-4 to discuss their content or any other queries they might have.\n\nThis app seamlessly integrates multiple AI and cloud-based technologies to provide users with a comprehensive tool for creating engaging video content. From video analysis to generating related images, the app leverages advanced AI capabilities to enhance the user's creative process."},{"heading":"Challenges we ran into","content":"Azure App Service does not support Node version 22.2.0, requiring us to learn Docker overnight with a friend. We configured Docker to deploy the server on Azure, addressing deprecated packages and dependencies that require Python and pip in the cloud environment. Created a tool to frame video at 5-second intervals using FFmpeg and generated a collage of the frame images as a cover using Node-canvas. I first learned about this hackathon on June 10, so my challenge was to learn and develop everything within 2 weeks, finishing on June 24, 2024. Learned that we can store content as markdown using react-markdown. I had previously thought I needed to manually convert the markdown output from GPT to HTML."},{"heading":"Accomplishments that we're proud of","content":"I am able to create a full environment code in Azure. I created a feature to process video; my last hackathon was focused on image data, and now I'm working with video. This is my first attempt at making an AI feature with computer vision. This is my first time entering Azure OpenAI Studio, deploying a model, and customizing it too."},{"heading":"What we learned","content":"Learned how to read subscription forecasts. I was shocked by my bills for playing on the cloud and deploying the model in 4 days, only to understand later that it was just a forecast. CI/CD and cloud services are fun. This hackathon taught me many new things that I had never been involved with before. Prototyping in one file and documenting in markdown is important."},{"heading":"What's next for VISOUNDAY","content":"I have a concern about switching from Cloudinary to handling BSON data and placing it in MongoDB. Create a better UI/UX design. My style has always been cyberpunk. Enhance the API response to be much faster, or maybe next, I'll learn gRPC. Explore integrating additional AI models to improve video and image processing capabilities. Implement a more robust testing framework to ensure the reliability and scalability of the application. Investigate other cloud providers to compare costs and performance. Improve the deployment pipeline for faster and more efficient updates. Develop a mobile-friendly version of the application to reach a broader audience. Conduct user testing and gather feedback to refine and optimize features based on real-world usage."},{"heading":"Source Code","content":"Client Web App Server Prototyping to be able create this feature Code Prototyping Learning Journey"},{"heading":"Built With","content":"authentication azure-ai-vision azure-app-service azure-cosmos-db bing-search-api canvas ci/cd cloudinary computer-vision docker express.js ffmpeg firebase flowbite gpt-4 jsonwebtoken jwt media-query microsoft-authentication microsoft-azure-cosmos-db microsoft-azure-video-indexer mongodb moongose multer node.js openai react react-markdown-preview responsive social-login tailwind"},{"heading":"Try it out","content":"github.com github.com github.com"}]},{"project_title":"UHIRED","project_url":"https://devpost.com/software/aitism","tagline":"Master your next job interview with UHIRED! Train skills, get tailored advice, and receive real-time feedback to ace every question and land your dream job!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/933/709/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: 1st Place"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"cosmos","url":null},{"name":"nextjs","url":null},{"name":"openai","url":null},{"name":"stt","url":null},{"name":"tailwind","url":null},{"name":"tts","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"uhired.app","url":"https://uhired.app/"}],"description_sections":[{"heading":"Inspiration","content":"In my career as a Software Engineer I have gone through various selection processes, and many times I couldn't properly communicate my skills. It wasn't due to a lack of technical competence, but because I wasn't prepared to answer questions clearly and concisely. I have also seen excellent professionals miss out on certain opportunities simply due to a lack of interview preparation. This was the inspiration to create UHIRED, a tool that helps you prepare for your interviews efficiently, giving you the chance to practice the perfect response to clearly and concisely communicate your expertise. You will have the opportunity to cover all the topics in your Job Description, receiving feedback on how to answer each question effectively based on your experiences and after answering it, you will receive detailed feedback on how to improve."},{"heading":"What it does","content":"Master Every Interview Question : Sharpen your answer to every question through our AI-powered interview. Practice and refine your answers to ensure you are ready for any scenario. Tailored Recommendations : Receive actionable tips on how to effectively showcase your skills and expertise for each specific question. Our recommendations are designed to help you stand out and make a lasting impression. Real-Time Feedback : Benefit from immediate, constructive feedback on your responses. Our real-time feedback mechanism helps you identify areas for improvement and provides practical tips to enhance your answers on the spot. Comprehensive Interview Results : Obtain a detailed summary of your interview performance, including strengths and areas for improvement."},{"heading":"How we built it","content":"To build UHIRED, I used a combination of technologies:\n\nBackend: Azure Web App : Hosts our web application, making sure it's always available and runs smoothly. Azure Cosmos DB : Stores and manages our interviews data. Azure OpenAI Services : GPT-4 : Generates smart text responses to help you prepare for interviews. Embeddings : Helps find the most relevant AI responses by understanding text better. Azure Speech Service : Text to Speech : Provides spoken feedback for your interview answers. Speech to Text : Lets you use your voice to interact with the app. Azure Blob Storage : Stores the audio files generated by Azure Speech Service Frontend: NextJS : Ensures our website loads fast and is easy to find on search engines. Tailwind CSS : Helps us quickly design a consistent and attractive user interface. Zustand : Manages the app's state efficiently, keeping everything running smoothly."},{"heading":"Challenges we ran into","content":"Creating JSON Prompts : Creating prompts that followed the JSON structure integrated with Azure OpenAI Services to cover all aspects of the app. Fields not being correctly populated : The issue was that using vague field names in the JSON structure often resulted in incorrect answers. By using highly descriptive and contextual field names, we achieved much better results. Integration Challenges : Coordinating various Azure services, the frontend, and the app's backend."},{"heading":"Accomplishments that we're proud of","content":"Seamless Azure Integration : We successfully integrated various Azure technologies, including Azure Web App, Azure Cosmos DB, and Azure OpenAI Services, to create a robust and scalable solution. Realistic Interview Experience : Our AI-powered tool provides realistic interview scenarios with high-quality recommendations and feedback, helping users to prepare effectively. I am really proud of that! Scalable and Easy Deployment : With Azure App Service, we ensured that our solution is easy to deploy and can scale when needed. Beautiful and User-Friendly UI : We designed a beautiful and user-friendly UI using NextJS and Tailwind CSS, ensuring a smooth and engaging user experience."},{"heading":"What we learned","content":"Leveraging New Technologies : Developing UHIRED was an exciting journey that pushed me to leverage new technologies to address my difficulties with interviews. Empowered by the Right Tools : I discovered that having the right tools at my disposal significantly enhances my ability to deliver high-quality projects efficiently. I am really happy to decided to join this competition! Rich Toolset from Azure : I discovered many solutions through Azure technologies that for sure added a rich tools belt to use in my future projects."},{"heading":"What's next for UHIRED","content":"Get Job Descriptions from Links : Allow users to easily import job descriptions directly from URLs. Implement User Accounts : Ensure that users can create accounts to keep their interview practices confidential and secure, providing a personalized experience. Introduce Multiple Voices and Language Support : Provide a variety of voice options and support for different languages. Enhance App Performance : Optimize the application to run more efficiently, ensuring a smoother and more responsive user experience. Integrate Vision Capabilities : Add features that utilize computer vision to further enhance the interview preparation process, offering innovative ways to practice and receive feedback."},{"heading":"Built With","content":"azure cosmos nextjs openai stt tailwind tts typescript"},{"heading":"Try it out","content":"uhired.app"}]},{"project_title":"ItinerAI","project_url":"https://devpost.com/software/itinerai","tagline":"Planning trips un-necessarily hard. Drowning in browser tabs, creating complicated itineraries? How is any trip supposed to make it out of the group chat. But its 2024, now there's an app for that.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/933/601/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azurecosmosdb","url":null},{"name":"azuredockercontainer","url":null},{"name":"azurefunctions","url":null},{"name":"azureopenai","url":null},{"name":"azurestaticwebapp","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"githubactions","url":null},{"name":"langchain","url":null},{"name":"rag","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"thankful-tree-0ba95a70f.5.azurestaticapps.net","url":"https://thankful-tree-0ba95a70f.5.azurestaticapps.net/"}],"description_sections":[{"heading":"Inspiration","content":"We often dream of creating the perfect itinerary for an upcoming trip, but the reality of planning can be overwhelming. The need to cross-reference various websites, compile findings, and share a cohesive itinerary with friends requires a significant time commitment. This process can be even more daunting when a minor change necessitates numerous updates, disrupting the entire schedule."},{"heading":"What it does","content":"Our app provides a seamless experience by allowing you to effortlessly view your itinerary items on an interactive map, complete with directions. With the help of our intelligent chatbot, you can receive personalized location suggestions and easily add new destinations to your itinerary. Additionally, you have the flexibility to manually rearrange and edit locations, ensuring your travel plans are perfectly tailored to your preferences. This integration makes your journey both enjoyable and efficient, giving you full control over your itinerary with ease."},{"heading":"How we built it","content":"We used REACT with typescript ad node JS as backend. The frontend is deployed using an Azure Static WebApp, which hosts this node JS server as an Azure Function App. The node server talks to our Container App, which hosts a LangChain agent connected to our Azure Cosmos Mongo DB and the AzureOpenAI agent. The node server is able to take all the information back, update the directions using the google maps API, and then serve the data to the front end.\n\nFor more details and a digram, see the end of the demo video!"},{"heading":"Challenges we ran into","content":"Handling data imports : Where we need to figure out what information are important and what are not for the model to produce the correct suggestions without introducing unnecessary info that might confuse the model.\n\nToo many tokens : Where responses from the chatbot often times contain way more information than we actually need. Hence causing us to easily exceed the token limit.\n\nFormatting the response : Trying to specify the exact format we want the chat bot to provide. Which often times does not work as any slight modifications will cause the json parsing to fail.\n\nUnderstanding before, after, start, end : Getting the chatbot to understand the relative order of the data, and insert itinerary locations appropriately."},{"heading":"Accomplishments that we're proud of","content":"Able to utilize LLM to interact with our database and have seamless integration with Google Maps. Getting the whole thing deployed and running on Azure was a challenge, but once it was working was super rewarding!"},{"heading":"What we learned","content":"CI/CD pipelines, creating resource groups on Azure, using RAG to connect database with chatbot. Azure OpenAI. We feel confident to integrate and create chatbots into any applications!"},{"heading":"What's next for ItinerAI","content":"Better memory system and context for the chatbot, more exact result, better user experience, i.e auto completing the prompts to add to the itinerary. More locations than Tokyo!"},{"heading":"Built With","content":"azure azurecosmosdb azuredockercontainer azurefunctions azureopenai azurestaticwebapp docker githubactions langchain rag react typescript"},{"heading":"Try it out","content":"thankful-tree-0ba95a70f.5.azurestaticapps.net"}]},{"project_title":"TourMate: Your Personal AI Travel Concierge","project_url":"https://devpost.com/software/tourmate-your-personal-ai-travel-concierge","tagline":"TourMate: Your AI-Powered Travel Planner for Unforgettable Journeys. Imagine having a personal travel concierge available 24/7, ready to tailor your dream vacation with just a few questions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/933/975/datas/medium.jpeg","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"angular.js","url":"https://devpost.com/software/built-with/angular-js"},{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"cosmosdb","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"tourmate.z13.web.core.windows.net","url":"https://tourmate.z13.web.core.windows.net/"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1k6lx0pNpS528YnRYFOO9RYtM339ru1TE9S9Vpa6giXA/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for TourMate came from our own experiences of sifting through countless travel websites and options, often ending up overwhelmed and unsure of our choices. We realized the need for a smarter, more personalized approach to travel planning—one that could streamline the process and ensure every trip is a perfect fit for the traveler."},{"heading":"What it does","content":"Introducing TourMate, an advanced AI chatbot designed to recommend the best tour packages based on your unique preferences. TourMate uses sophisticated AI algorithms and rich datasets to provide personalized travel suggestions. Let TourMate take the stress out of planning, so you can focus on creating memories."},{"heading":"What We Learned","content":"Through this project, we gained a good understanding of the hassle involved in travel planning and the potential of AI to simplify and enhance this process. We learned how to integrate various data sources, create meaningful embeddings, and leverage GPT versions to build a responsive and intelligent chatbot."},{"heading":"How We Built TourMate","content":"Our journey began with sourcing a comprehensive travel dataset from Kaggle, which we then structured using MongoDB to ensure efficient data management. We created embeddings to capture the nuances of user preferences and trained our chatbot using state-of-the-art GPT models. The chatbot was designed to handle various user queries, from budget constraints to specific interests in activities or accommodations, providing detailed and personalized travel packages."},{"heading":"Challenges Faced","content":"Building TourMate was not without its challenges. Integrating diverse data types, ensuring real-time performance, and maintaining high accuracy in recommendations were significant hurdles.\n\nWe also faced difficulties in training the AI starting from creating embeddings for large amounts of data, especially with proxy OpenAI, and keeping the cost factor always on mind. However, through persistent testing and iteration, we overcame these obstacles and refined TourMate into a reliable and efficient travel planning tool.\n\nBy combining our passion for travel with cutting-edge technology, we created TourMate to make the travel planning process enjoyable and hassle-free. Now, anyone can have a personal travel concierge at their fingertips, ready to craft the perfect vacation."},{"heading":"Built With","content":"angular.js azure cosmosdb docker mongodb openai python"},{"heading":"Try it out","content":"tourmate.z13.web.core.windows.net docs.google.com"}]},{"project_title":"Project X: The power of knowledge in the plam of your hands","project_url":"https://devpost.com/software/project-x-xgo16d","tagline":"Transforming Learning: AI-driven Podcasts, Quizzes, Dynamic References, and Document-based Chat for an Engaging Educational Experience.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/935/340/datas/medium.jpg","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azure-cosmos-db","url":null},{"name":"azure-open-ai","url":null},{"name":"azure-tts","url":null},{"name":"gpt-4","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"twilio","url":"https://devpost.com/software/built-with/twilio"}],"external_links":[{"label":"projectx-dev.nishithp.dev","url":"https://projectx-dev.nishithp.dev"},{"label":"github.com","url":"https://github.com/NishithP2004/projectx-frontend-dev"},{"label":"github.com","url":"https://github.com/NishithP2004/projectx-backend-dev"},{"label":"github.com","url":"https://github.com/NishithP2004/AI_Podcasts"},{"label":"youtu.be","url":"https://youtu.be/xKQOF48f_uE"},{"label":"dev.to","url":"https://dev.to/nishithp2004/ai-podcasts-powered-by-project-x-10kk"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Project X was derived from a deep belief in making learning engaging, accessible, and lifelong through the power of technology and AI.\n\nKey Motivations:\n\nDynamic Learning: Traditional methods often feel passive. An interactive experience that sparks curiosity and a love for learning was aimed to be created. Utilizing Resources: The internet is full of educational content, but finding relevant, engaging materials is tough. Bridging this gap using AI to connect students with the right resources was the goal. Lifelong Learning: In a rapidly changing world, continuous learning is essential. Project X was designed to empower individuals to explore new subjects, deepen their understanding, and stay ahead.\n\nPersonal Experiences:\n\nFinding Resources: As a student, the struggle to find relevant articles or videos, leading to frustration and wasted time, was often experienced. Discovery Moments: The joy of finding a resource that perfectly explains a complex concept has been felt, and this feeling of discovery was desired to be recreated for others. Tech Transformation: The transformative impact of technology in other areas was witnessed, inspiring the use of it to revolutionize education and make learning more engaging.\n\nProject X embodies the belief in the power of technology to democratize education and inspire a lifelong love of knowledge. A future is envisioned where learning is an exciting, integral part of everyday life.\n\nMoreover, the challenge of doom scrolling through YouTube reels without learning anything was also recognized, underscoring the need for more purposeful and enriching educational experiences."},{"heading":"What Project X Does: Revolutionizing Learning with AI","content":"Project X transforms the way we learn by turning static educational content into dynamic and engaging experiences. It empowers users to delve deeper into any topic by leveraging the power of AI and a vast library of online resources.\n\nHere's how Project X revolutionizes learning:\n\n1. Turns Documents into Interactive Learning Journeys:\n\nUpload & Analyze: Upload documents in various formats (notes, slides, research papers). Project X's AI extracts key concepts and identifies relevant information. Intelligent Chatbot: Engage in meaningful conversations with an AI chatbot that understands the uploaded document. Ask questions, clarify doubts, and receive accurate, context-aware answers. Dynamic Quiz Generation: Test your understanding with AI-generated quizzes tailored specifically to the content of your documents. Receive instant feedback and identify areas for improvement.\n\n2. Curates and Connects You with the Best Resources Online:\n\nAutomated Resource Curation: No more endless searching! Project X automatically curates a list of relevant articles, videos, tutorials, and more—all directly related to your uploaded document. Contextualized Exploration: The AI understands the context of your document, surfacing resources that specifically address key concepts, theories, or examples mentioned within it. Effortless Navigation: Say goodbye to irrelevant search results. Project X does the heavy lifting, presenting a curated selection of resources to deepen your understanding and engagement.\n\n3. Goes Beyond Traditional Learning with Interactive Podcasts & AI Discussions:\n\nCreate Engaging Podcasts: Craft captivating podcasts on any topic, featuring AI-powered voices of famous personalities. Share your knowledge and insights in an engaging audio format. Interactive AI Discussions: Engage in dynamic discussions with AI-powered personas through realistic voice conversations. Guide the discussion, explore different perspectives, and receive summaries and transcripts.\n\n4. Promotes Deeper Understanding and Knowledge Retention:\n\nBy combining AI-powered analysis, curated resources, and interactive learning tools, Project X creates a personalized and engaging learning experience that promotes:\n\nActive Learning: Move beyond passive consumption and actively engage with content through quizzes, chatbot interactions, and curated resources. Deeper Comprehension: Explore topics in-depth with contextually relevant materials, going beyond surface-level understanding. Improved Retention: Reinforce learning through interactive exercises, dynamic content formats, and personalized learning pathways.\n\nProject X is more than just a learning platform—it's your AI-powered learning companion, guiding you on a journey of continuous discovery and knowledge acquisition."},{"heading":"Building Project X: A Fusion of AI, Cloud Technologies, and Passion for Learning","content":"Building Project X was an exciting journey of fusing cutting-edge AI technologies with a user-centric approach to create a seamless and empowering learning experience. Here’s a glimpse into the development process:\n\n1. Foundation Laid:\n\nTechnology Stack: A robust and scalable technology stack was chosen: Frontend: React.js was selected for its flexibility, component-based architecture, and dynamic user interface capabilities. Backend: Node.js with Express.js was employed for its speed, efficiency, and extensive package ecosystem, facilitating seamless API development and server-side logic handling. Cloud Platform: Azure was our preferred cloud provider, offering a wide array of services ideally suited for the AI-powered platform.\n\n2. Intelligence Empowered by AI & Cognitive Services:\n\nAzure OpenAI (GPT-4): The intelligence at the core of Project X was embodied in GPT-4, enabling: Document Analysis: Key information, concepts, and relationships were extracted from uploaded documents using text-embedding-ada-002. Chatbot Interactions: Natural language understanding and generation powered intelligent, context-aware conversations. Quiz Generation: Challenging and diverse questions tailored to specific content were formulated. Podcast Script Creation: Engaging and informative scripts were generated based on user-selected topics and personalities. Azure Custom Search: Efficient retrieval of relevant web resources were facilitated by Bing Search based on search queries generated from the document content. Azure Document Intelligence: Document processing was streamlined through automated Optical Character Recognition (OCR) for various file formats. Azure Speech Services: Interactive podcasts were brought to life with high-quality text-to-speech capabilities, providing natural-sounding voices for AI personas.\n\n3. Data Managed & Stored:\n\nAzure Cosmos DB: The primary database provided: Flexible Data Model: Efficient storage of various data types (user information, course content, embeddings, etc.). Scalability & Performance: Seamless handling of large data volumes and user requests. Global Distribution: Ensured low latency and high availability worldwide. Azure Blob Storage: A secure and scalable solution was provided for storing user-uploaded documents.\n\n4. Integration & User Experience Seamlessly Handled:\n\nTwilio APIs: Interactive voice experiences for AI-powered podcasts were enabled, allowing dynamic discussions and WhatsApp summaries. YouTube API: Seamless integration provided access to a vast library of educational videos, curated and presented contextually. User Interface Design: A clean, intuitive, and user-friendly interface was prioritized with React.js, simplifying navigation, interaction, and maximizing the utility of Project X's features. Version Control: Git and GitHub were pivotal for collaborative coding, version control, and ensuring code quality.\n\n5. Streamlining Workflows with Azure Functions:\n\nEvent-Driven Architecture: A crucial role was played by Azure Functions in automating key processes. For instance, when a document is uploaded by a user, an Azure Function is triggered to handle the upload, store the document in Azure Blob Storage, and initiate the document analysis process with Azure Document Intelligence.\n\nSimplified Development: The development workflow was simplified by Azure Functions, allowing us to focus on writing code for specific tasks without the need to worry about server management or infrastructure.\n\n6. Leveraging Azure Container Apps for Scalability and Flexibility: Frontend & Podcast Hosting: Azure Container Apps was utilized to deploy and manage the React.js frontend and the service responsible for generating and serving podcasts.\n\nScalability on Demand: Container Apps' autoscaling capabilities ensured that the application could handle traffic spikes efficiently, automatically adjusting resources based on demand.\n\nSimplified Deployment: Containerization simplified the deployment process, allowing me to package the applications and their dependencies for consistent execution across different environments."},{"heading":"Technologies Powering Project X:","content":"Frontend: React.js Backend: Node.js, Express.js Cloud Platform: Azure AI & Cognitive Services: Azure OpenAI (GPT-4), Azure Cognitive Search, Azure Document Intelligence, Azure Speech Services Database: Azure Cosmos DB Storage: Azure Blob Storage Deployment: Azure Container Apps Communication: Twilio APIs Video Integration: YouTube API Version Control: Git, GitHub"},{"heading":"Usage Instructions","content":"Login:\n\nGo to the website projectx-dev.visibility.dev/login Click either Sign in with Google or Sign in with Microsoft\n\nCreating a Course:\n\nClick Create a Course on the left-side navigation bar. In the \"Course Name\" field, enter the desired course name. Click the \"Choose File\" button, and select the relevant file from your computer. (File size limit: 4MB, Allowed file types: Word, Excel, PPT, PDF, Image). Click Submit . You will get a pop-up message: \"Course creation initiated.\" Click Home on the left-side navigation bar to return to the main Courses page, where you will see the new course listed.\n\nUsing the Document Reader:\n\nClick View under the course name. On the Document Reader page, you will see the uploaded document content on the left and a chat box on the right. Type your message or question related to the document in the chat box and click the arrow button to send it. The AI will respond with relevant information based on the user uploaded documents. You can also copy and paste text into the chat box, for instance, a self-introduction.\n\nUsing the Document Quiz Generator:\n\nClick Quiz on the left-side navigation bar after clicking View under the course name. A quiz with 10 questions will be generated. Click on Regenerate Quiz to generate a new Quiz.\n\nAccessing References:\n\nClick References on the left-side navigation bar. This page lists various web references, including articles, videos, and tutorials related to the content of the current course. Click the arrow button next to a reference to open it in a new tab in the Interactive Browser .\n\nTaking a Quiz:\n\nClick Start Quiz on the right side of the References page. This will open a quiz related to the course content in a pop-up window. Select the answer(s) to the question and click Pick 1 for single-selection or Pick Multiple for multiple choice. After answering, click Next . Once you've answered all questions, a message will show: \"You have completed the quiz. You got [number] out of [total number] questions. You scored [score] out of [total score].\" You can then review the questions and your answers by clicking All .\n\nCreating a Podcast:\n\nClick Create a Podcast on the left-side navigation bar. In the Create a Podcast page, you can select a topic, choose characters (like Elon Musk or Sam Altman) and voices for the podcast. Click Submit to create the podcast.\n\nAccessing Profile and Information:\n\nClick on the profile icon on the top-right corner. You can choose Profile to view and edit your profile information. Select Feedback to give feedback on the application. Choose About to learn more about Project X. Click Sign Out to log out of the application.\n\nNote: Clicking Play under a course on the Podcasts page will play a podcast related to that course.\n\nThis guide provides a comprehensive walkthrough of Project X functionalities. Enjoy learning!"},{"heading":"Azure Functions","content":"The following functions are available as part of the Node JS backend Deployed on Azure Functions\n\nFunctions:\n\nbrowser: [GET] http://localhost:7071/api/browser courses-create: [GET,POST] http://localhost:7071/api/courses/create courses-delete: [DELETE] http://localhost:7071/api/courses/{id} courses-doc-content: [GET] http://localhost:7071/api/courses/documents/{id} courses-list: [GET] http://localhost:7071/api/courses/list podcasts-create: [POST] http://localhost:7071/api/podcasts/create podcasts-delete: [DELETE] http://localhost:7071/api/podcasts/{id} podcasts-list: [GET] http://localhost:7071/api/podcasts/list quiz: [GET,POST] http://localhost:7071/api/quiz reels: [GET] http://localhost:7071/api/reels search: [GET] http://localhost:7071/api/search search-queries: [GET,POST] http://localhost:7071/api/search-queries storageBlobTrigger1: blobTrigger"},{"heading":"Challenges Faced: Navigating the Frontiers of AI Development","content":"Building an ambitious project like Project X, pushing the boundaries of AI-powered learning, inevitably involved facing challenges that needed overcoming. Here are some of the hurdles we encountered and addressed:\n\n1. Managing LLM Token Limits and Costs:\n\nToken Limits: LLMs such as GPT-4 impose token limits for both input and output. This required me to carefully optimize prompts and responses, especially for handling lengthy documents and complex interactions. Techniques explored included: Text Chunking: Large documents were broken down into smaller chunks for processing, with results subsequently reassembled. Summarization: AI was employed to summarize lengthy texts before inputting them into the LLM, thereby reducing token consumption.\n\n2. Debugging and Troubleshooting Deployment Issues:\n\nContainerization Challenges: Deploying the frontend and podcast service using Azure Container Apps and the backend using Azure functions involved debugging issues related to containerization, environment variables, and network configurations. Integration Testing: Extensive integration testing and debugging were essential to ensure seamless communication and data flow between our frontend, backend, and various Azure services."},{"heading":"Accomplishments We're Proud Of: Building the Future of Learning","content":"Building Project X has been an incredibly rewarding journey. Here are some accomplishments that fill us with pride:\n\n1. Pushing the Boundaries of AI in Education:\n\nSeamless AI Integration: Multiple cutting-edge AI technologies from Azure OpenAI, Cognitive Search, Document Intelligence, and Speech Services were successfully integrated to create a truly intelligent learning experience. Novel Use of LLMs: Beyond typical LLM applications, they were utilized not just for text generation but also for complex tasks like document analysis, personalized resource curation, and interactive podcast generation.\n\n2. Creating a User-Centric Learning Experience:\n\nIntuitive Interface: An intuitive and user-friendly interface was designed that makes exploring complex topics and engaging with AI-powered features easy and enjoyable. Personalized Learning Journeys: Project X empowers users to learn at their own pace and in a way that aligns with their individual needs and preferences.\n\n3. Tackling Technical Challenges Head-On:\n\nOvercoming LLM Limitations: Innovative solutions were developed to manage token limits, optimize costs, and handle rate limits effectively, ensuring a smooth user experience. Mastering Cloud Deployment: Our application was successfully deployed using Azure Container Apps, leveraging the power of serverless architecture and containerization for scalability and flexibility. Building a Robust System: Comprehensive error handling, logging, and monitoring mechanisms were implemented to ensure system stability and reliability.\n\n4. Making a Real-World Impact:\n\nDemocratizing Education: Project X has the potential to make high-quality learning resources more accessible to a wider audience, empowering individuals to reach their full potential. Inspiring Lifelong Learning: By creating an engaging and personalized learning experience, a passion for learning is hoped to be ignited that extends far beyond the classroom.\n\nWhat I Learned: Key Takeaways from Project X\n\nBuilding Project X was a masterclass in pushing the limits of AI and building innovative learning experiences. Here's a snapshot of our key takeaways:\n\nCareful orchestration of LLMs: Balancing capabilities, token limits, and costs was found crucial for ensuring a smooth user experience.\n\nGame-changing impact of cloud technologies: Azure's services (Functions, Container Apps, AI, etc.) played a crucial role in the construction of a scalable and powerful platform.\n\nParamount importance of user-centric design: An intuitive interface and personalized learning pathways were identified as key elements for fostering engagement and achieving impact.\n\nFueling innovation through collaboration: Open communication and shared learning were essential for overcoming challenges and accomplishing our objectives.\n\nThe arrival of the future of learning: AI was recognized as having the potential to revolutionize how learning occurs, thereby making knowledge more accessible and engaging for everyone.\n\nI have learnt invaluable lessons from Project X, underscoring the importance of careful orchestration of LLMs, the game-changing impact of cloud technologies like Azure, the paramount importance of user-centric design, the fueling of innovation through collaboration, and the revolutionary potential of AI in learning."},{"heading":"What's Next for Project X: Expanding Horizons, Enhancing Experiences","content":"We're excited to continue evolving Project X, pushing the boundaries of AI-powered learning even further:\n\nElevated UI/UX: We'll be refining our interface based on user feedback, making it even more intuitive and enjoyable to use. Unleashing Larger Context Windows: Integrating models like GPT-4-Turbo-128k or GPT-4o will allow us to process more extensive documents and complex user queries with ease, overcoming previous token limitations. Expanding the Podcast Universe: Get ready for a wider variety of voices and personalities in our AI-powered podcasts, making learning even more engaging and entertaining. Learning Gamified : Enhancing the learning experience by introducing a personalised dashboard which displays the total points (xp) earned by a user by using the features provided by the app which included quizzes, interactive references, etc. The dashboard will also highlight the key areas where a user needs to work on and then use this information to improve the services provided, thereby personalising the learning experience as a whole. Study Jams, Collaborative learning and more ...\n\nProject X is on a mission to empower learners everywhere – stay tuned for exciting updates!"},{"heading":"Built With","content":"azure azure-cosmos-db azure-open-ai azure-tts gpt-4 node.js react twilio"},{"heading":"Try it out","content":"projectx-dev.nishithp.dev github.com github.com github.com youtu.be dev.to"}]},{"project_title":"Opezy","project_url":"https://devpost.com/software/memento-lo7wjy","tagline":"Empowering Small Businesses with Big Technology","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/934/188/datas/medium.gif","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azurecosmosdb","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"fastapi","url":null},{"name":"langchain","url":null},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"opezy.ankithreddy.me","url":"https://opezy.ankithreddy.me"},{"label":"dgip5pqdogpztvo-web.azurewebsites.net","url":"https://dgip5pqdogpztvo-web.azurewebsites.net/"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for creating Opezy stems from personally witnessing the incredible talents of small business owners, whether they're crafting gourmet dishes, creating unique retail experiences, or providing top-notch services. Many of these entrepreneurs excel in their craft but may not have extensive training or resources in business management. This realization sparked the idea for Opezy—an AI copilot designed to bridge the gap, allowing these skilled individuals to thrive not just in their craft but also in their business operations. Opezy is built to empower these entrepreneurs, providing them with the tools to make data-driven decisions and streamline tasks like sales analysis, inventory management, customer feedback, and marketing, ultimately enhancing their productivity and growth."},{"heading":"What it does","content":"Opezy is an AI platform that simplifies several key business operations for small businesses:\n\nSales Analysis: Identifies best-selling products and tracks sales trends to highlight underperforming items. Inventory Management: Uses GPT-4 vision to check inventory quality, ensuring that all products meet freshness standards. Customer Feedback: Analyzes feedback using vector databases to understand common complaints and areas for improvement. Demand Forecasting: Uses advanced statistical models and real-time data to predict product demand, optimizing inventory levels and reducing waste. Marketing: Automates the sending of targeted marketing emails and manages social media campaigns. Expense Management: Uses machine learning models like the Isolation Forest, along with vector embeddings of expense data, to detect financial anomalies and manage costs effectively. Voice Interaction : Incorporates a voice-enabled chatbot to provide real-time support and information, enhancing user experience and accessibility."},{"heading":"How it was built","content":"Opezy was developed using a range of cutting-edge technologies:\n\nData Storage: Azure Cosmos DB for MongoDB (v-core) is utilized for its flexible storage of database collections and vector embeddings, from sales data to customer feedback. AI Integration: Azure OpenAI services were integrated for tasks like natural language processing and The GPT-4 vision API is employed to visually inspect inventory and assess quality. Marketing Automation: The Gmail API and Twitter API facilitate the automatic sending of enriched marketing emails and tweets. Anomaly Detection: Machine learning models like the Isolation Forest are used to spot irregularities in expense records, combined with vector embeddings of expense data, to detect anomalies in respective category effectively. Conversational AI: Agent and tools capabilities from LangChain are implemented to interact with users and provide quick answers based on their queries. Cloud Deployment: The entire platform is deployed on Azure Cloud, ensuring scalability, reliability, and seamless integration across all services."},{"heading":"What's next for Opezy","content":"Scalability : Aiming to scale the platform to support a broader range of small businesses and onboard business data seamlessly. Enhanced Marketing Features: Plans include adding more advanced marketing tools like personalized recommendations and automated social media posts. Expanded AI Capabilities: Looking to broaden AI features to include more detailed predictive analytics . User Feedback: Continuously refining Opezy based on user feedback to ensure it meets their evolving needs."},{"heading":"Built With","content":"azure azurecosmosdb docker fastapi langchain openai react"},{"heading":"Try it out","content":"opezy.ankithreddy.me dgip5pqdogpztvo-web.azurewebsites.net"}]},{"project_title":"Advanced Semantic AI Platform (ASAP)","project_url":"https://devpost.com/software/aiwritingassistant","tagline":"ASAP is an AI-powered, all-in-one platform for code and documentation generation. It blends AI efficiency with the reliability of traditional, time-tested systems to deliver high-quality results.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/922/181/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"c#","url":"https://devpost.com/software/built-with/c--2"},{"name":"gpt-4","url":null},{"name":"gpt-4o","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"I was inspired to participate in the hackathon by the chance to dive deep into the data infrastructure behind applications like OpenAI's GPT. The innovative blend of AI’s speed with the reliability of traditional databases, as seen in Azure Cosmos DB, was incredibly exciting to explore.\n\nWith three decades of experience as a software engineer, I have firsthand knowledge of how overwhelming it can be to sift through vast databases for specific information. This process can take hours or even days, even for seasoned professionals. Allowing users to input queries in natural language streamlines search and retrieval, eliminating irrelevant data and saving time, while boosting productivity.\n\nInspired by the hackathon, I began developing a suite of AI-powered copilots under the name Advanced Semantic AI Platform (ASAP). These copilots streamline the tedious tasks of coding and generating technical documentation. The capability to conduct precise data searches using natural language, with data stored in a vectorized format in Cosmos DB, was exactly the solution I needed.\n\nWhat's more, during my hackathon project, I uncovered additional major benefits of Azure Cosmos DB technology, particularly in solving fundamental issues related to code and documentation generation using AI.\n\nThe main issue with code and documentation generators is the inconsistency and unreliability of generative AIs, which often introduce unexpected variations, sometimes even faulty ones, into the generated content. Before starting to work on the hackathon, I applied an intuitive idea of using minimalistic yet highly efficient data formats to produce consistent and uniform output and to maintain prompt compatibility across different versions of language models. The use of vector databases has significantly improved this process by more effectively managing and retrieving large-scale data, ensuring that the context and relevance of responses are maintained even with updated models.\n\nWhile exploring vectorization, I discovered the significance of integrating ontologies. These provide structured frameworks that enhance consistency, relevance, and adherence to industry standards in AI-generated documentation and code.\n\nI have developed a method called COSE (Cognitive Optimized Sparse Encoding), which leverages minimalistic yet efficient data formats. This approach aligns seamlessly with the inherently sparse and high-dimensional nature of vector databases. Integrating COSE into vector databases enhances data retrieval speeds and computational efficiency through cognitive optimization, facilitating adaptive and user-centric processing. This synergy not only boosts performance but also makes databases more intuitive, propelling advancements in AI and data science. The combination of the two technologies embodies a forward-thinking approach, ensuring optimal performance while maintaining user accessibility and ease of use.\n\nI began this project to automate my own tasks, and it has evolved into an extraordinary solution that I believe will greatly benefit other professionals in the IT community."},{"heading":"What It Does","content":"Imagine an AI that can generate code and documentation in minutes, find the information you need in seconds, and publish reports almost instantaneously! ASAP offers a suite of AI copilots transforming tech documentation and code creation using custom ontologies tailored to an organization's standards. Integrating advanced natural language search and automation within a unified workspace, ASAP combines the speed and efficiency of AI with the reliability of traditional systems.\n\nASAP's Suite Includes:\n\nOntologyGen: Creates custom ontologies aligned with organizational standards, terminologies, and data structures, ensuring adherence during the document and code generation processes. TechDocGen: Uses OntologyGen's customized ontologies to generate professional, consistent documentation. CodeGen: Deploys software ontologies to generate accurate code across multiple platforms, ensuring compatibility and optimal performance. ReportGen: Employs predefined ontologies to create comprehensive, standardized reports in various formats (PDF, diagrams, images). Natural Language Queries: Enables users to perform precise data searches using natural language, with data stored in a vectorized format in Cosmos DB. Unified Workspace: Designed for accessibility and ease of use, each AI copilot has its workspace with chat sessions stored for later retrieval using semantic search. Retrieval Augmented Generation (RAG): Enhances performance in knowledge-intensive tasks by retrieving and incorporating external information into prompts.\n\nEach copilot is a domain expert: TechDocGen for documentation, CodeGen for coding languages and platforms. ASAP's customizable settings, such as target audience and response attributes, enable replication of the original chat context. Lifelong Learning Agents, supported by Cosmos DB and vectorization, ensure continuous improvement in performance."},{"heading":"Problem","content":"Here are the key problems that ASAP addresses:\n\nTime Consumption: Coding and creating software documentation are extremely time-consuming. The vast amount of data developers need to sift through makes managing, organizing, and retrieving relevant information a challenge, leading to information overload and inefficiency. Lack of Consistency: Without a structured ontology, terms and processes might be used inconsistently across different documents, leading to confusion and misinterpretation. Collaboration across teams or departments can be hindered due to varying interpretations. Inefficiencies in Document and Code Management: Traditional methods of creating and retrieving documents and code are often time-consuming and labor-intensive. Limited Data Accessibility: Accessing and retrieving vast amounts of corporate data can be cumbersome and inefficient with conventional systems. Reduced Productivity: Manual document and code generation can detract from more critical tasks, lowering overall productivity. Lack of Expertise and Customization: Users often lack the tools to customize outputs or harness expert-level insights in their fields, particularly in non-technical domains. Static Performance: Many platforms do not adapt or improve over time, failing to learn from interactions or integrate new data effectively. Handling Complex Knowledge: Traditional platforms struggle with managing and utilizing complex, knowledge-intensive information efficiently.\n\nThese pain points collectively reduce development speed and cause frustration within teams, making the search for effective solutions a critical concern for software organizations."},{"heading":"Solution","content":"Efficiency in Document and Code Management\n\nBy integrating sophisticated natural language search and automation capabilities in a unified workspace, ASAP revolutionizes how documents and code are created and retrieved, reducing time and effort.\n\nImproved Accuracy and Relevance in Content Generation\n\nBy adopting custom ontologies, ASAP ensures that AI-generated content adheres to industry standards and semantics, addressing key pain points such as lack of uniformity, mismatched terminologies, and inconsistent quality in document and code generation.\n\nEnhanced Data Accessibility\n\nThe platform enables precise and efficient data retrieval through Microsoft Graph, storing information in a vectorized format in Cosmos DB for enhanced usability.\n\nProductivity Enhancement\n\nASAP automates essential aspects of document and code generation, allowing users to focus on more critical and strategic tasks, thereby boosting productivity.\n\nExpertise and Customization\n\nAI copilots like Code-Generation and Document-Generation Agents function as domain-specific experts and are tailored with settings like target audience and response attributes, enhancing operational efficiency and making advanced AI tools accessible to non-experts.\n\nContinuous Learning and Improvement\n\nLifelong Learning Agents ensure the platform continuously improves, adapting to new data and interactions.\n\nAdvanced Knowledge Integration\n\nLeveraging Retrieval Augmented Generation (RAG), ASAP excels in knowledge-intensive tasks, effectively managing and utilizing complex information.\n\nThese solutions position ASAP as a vital tool for enhancing efficiency, productivity, and adaptability in modern business environments."},{"heading":"Benefits","content":"Enhanced Efficiency in Document and Code Management\n\nASAP revolutionizes the creation and retrieval of documents and code by integrating sophisticated natural language search and automation capabilities within a unified workspace. This integration significantly reduces the time and effort required, streamlining workflows and operations.\n\nImproved Data Accessibility\n\nASAP enhances data usability by enabling precise and efficient retrieval through Microsoft Graph, coupled with storage in a vectorized format in Cosmos DB. This ensures that users can access and utilize data more effectively, improving decision-making and response times.\n\nIncreased Productivity\n\nBy automating key aspects of document and code generation, ASAP frees users to focus on more critical and strategic tasks. This automation not only speeds up routine processes but also boosts overall productivity.\n\nExpertise and Customization at Your Fingertips\n\nThe platform's AI copilots, such as the CodeGen and TechDocGen, offer expert-level guidance tailored to specific domains. These agents adapt to user preferences and needs, including target audience and response attributes, making advanced AI tools accessible even to non-experts and enhancing operational efficiency. OntologyGen facilitates the creation of custom ontologies, allowing businesses to define internal processes in a structured format. This customizability drives interaction with AI copilots, resulting in improved accuracy and efficiency in content generation.\n\nContinuous Performance Enhancements\n\nWith Lifelong Learning Agents, ASAP is designed to continuously learn and improve, adapting to new data and user interactions. This ongoing development ensures that the platform remains cutting-edge and increasingly effective over time.\n\nSuperior Handling of Complex Knowledge\n\nThrough Retrieval Augmented Generation (RAG), ASAP excels in knowledge-intensive tasks, managing and leveraging complex information efficiently. This capability makes it an indispensable tool for sectors that require detailed and sophisticated knowledge management.\n\nThese benefits underscore ASAP’s role as an essential tool for boosting efficiency, productivity, and adaptability in modern business environments, making it a strategic asset for any organization."},{"heading":"How We Built It","content":"ASAP’s development involved designing a Retrieval Augmented Generation (RAG) pattern, integrating MongoDB’s vector database capabilities with Azure OpenAI’s vector search and AI-assisted user interface. This architecture seamlessly transforms stored data into streamlined knowledge, optimizing the software development lifecycle."},{"heading":"Challenges We Ran Into","content":"Our developmental journey presented several challenges:\n\nOptimizing Data Management: Achieving a balance between speed and accuracy. User Experience: Crafting intuitive and user-friendly interfaces. Data Flow Integration: Ensuring smooth integration between traditional and vector databases. Scalability and Reliability: Maintaining performance and scalability. Natural Language Understanding: Developing a sophisticated chatbot experience."},{"heading":"Accomplishments That We're Proud Of","content":"We have achieved several milestones:\n\nPioneering AI Integration: Leading in AI solutions for software coding and documentation. Advanced Functionalities: Introducing a multi-copilot system and effective chat session management. Technical Innovations: Leveraging vCore-based Azure Cosmos DB and Azure OpenAI services. Enhanced User Interaction: Creating a sophisticated chatbot experience. One-Click Vectorization: Simplifying the process of vectorization."},{"heading":"What We Learned","content":"Optimizing Data for RAG\n\nOne valuable insight we gained while implementing the Retrieval Augmented Generation (RAG) was the necessity of refining our datasets for better model performance. Original documents, customer requirements, and similar sources often contain verbose and redundant information. This extra content not only increases token usage but also diminishes prediction accuracy.\n\nTo address this, we transitioned from using raw content to leveraging a new framework we invented during the hackathon:\n\nOptimizing Memory Organization in Large Language Models Using Cognitive Optimized Sparse Encoding (COSE)\n\nCognitive Optimized Sparse Encoding (COSE)\n\nCOSE enhances LLMs by emulating human memory for better data retrieval and memory organization.\n\nMimics Human Memory Efficiency: COSE models the way human memory works to achieve faster and more accurate recall with less computational effort. This efficiency allows LLMs to pull relevant information quickly, much like how the human brain recalls useful memories. Token Efficiency: By focusing on using fewer tokens, COSE effectively manages larger datasets within the confines of an LLM's context window. This is crucial in applications where memory and processing power are limited, ensuring efficient use of resources. Broad Applicability: COSE has wide-ranging applications in fields such as artificial intelligence, information management, and education. It enhances the performance of LLMs by improving their ability to learn, recall, and generate accurate information. Initial Bulk Training vs. In-Context Learning: There is a debate between the effectiveness of bulk training (training models on large amounts of initial data) versus in-context learning (learning dynamically from ongoing inputs). COSE brings a balance by improving adaptability, enabling LLMs to perform well in dynamic scenarios where data and context constantly change.\n\nSummary: COSE offers a human-like, token-efficient approach to improve LLMs' memory organization and data management. It effectively balances the stability of bulk training with the adaptability of in-context learning. Despite its complexities, COSE shows potential in diverse applications like AI and education.\n\nDetailed Explanation:\n\nLatent Space in LLMs:\n\nHigh-Dimensional Vector Space: Latent space represents a high-dimensional vector space that encodes linguistic, semantic, and syntactic features of the input data. This space allows for complex computations and representations that underpin the LLM's understanding and generation of language. Encoding and Decoding: Encoding: This process compresses the incoming data into a latent representation, effectively summarizing the essential features in a compact form. Decoding: The latent representation is then decoded back into human-readable text, which is coherent and contextually relevant.\n\nCore Concepts:\n\nHigh-Dimensional Space: Encodes multiple attributes of the data such as parts of speech, semantic roles, and syntactic structures. This dense encoding enables nuanced understanding and manipulation of language by the model. Encoding/Decoding Mechanisms: These mechanisms transform raw input into a latent form and vice versa, maintaining the integrity and meaning of the information through the process.\n\nApplications:\n\nSemantic Search: COSE enhances semantic search by retrieving documents based on a deep understanding of their content, not just keywords. This means users can find more relevant and contextually appropriate information. Text Generation: The model uses latent vectors to generate text that is not only contextually coherent but also stylistically appropriate. This results in more natural and human-like text output. Transfer Learning: COSE aids in adapting pre-trained models' knowledge to new tasks, making it easier for LLMs to perform well on tasks they were not explicitly trained for. This adaptability is crucial for applications that require continual learning and updating. Associative Learning: COSE enables LLMs to be 'primed' with new concepts and make connections between new and existing knowledge. This associative learning improves the model’s ability to understand and generate relevant output.\n\nCompression Concept:\n\nThe end result of COSE is akin to a form of data compression. It distills complex data into succinct, efficient representations (Compression). Later, this succinct information is expanded into a full, detailed form (Decompress) when needed, ensuring that the model can operate efficiently without losing the richness of information.\n\nCOSE thus represents a powerful approach to optimizing LLMs for real-world applications, balancing efficiency, complexity, and adaptability in a way that mirrors human cognitive processes."},{"heading":"What's Next for Advanced Semantic AI Platform (ASAP)","content":"We plan to enhance ASAP further by boosting its semantic AI and natural language processing capabilities. Future updates will focus on introducing more customization options, improving collaboration features, and ensuring continuous adaptation to user feedback and technological advancements.\n\nASAP is committed to revolutionizing software coding and documentation through ongoing improvement, making advanced AI tools accessible and user-friendly, thereby enhancing operational efficiency for all users.\n\nWith over thirty years of industry insight, we aim to deliver a product that exceeds expectations and drives success for IT professionals. The success of ASAP depends on factors including market demand, technological capability, competition, and execution strategy.\n\nIn recent years, the software industry, especially AI integration, has grown rapidly. Developers face increasing pressure to swiftly adapt to technological changes and deliver advanced solutions promptly. The demand for automation tools that streamline development workflows is rising. In 2023, 82% of IT leaders emphasized the standardization and efficiency of processes as their top priorities.\n\nCOSE proved to be valuable and we would develop it further.\n\nMarket Demand\n\nHigh Demand: There is a strong need for efficient software coding and documentation solutions. Target Audience: Developers, software engineers, and technical writers are eager to adopt robust documentation tools.\n\nCompetitive Advantage\n\nUnique Value Proposition: Differentiating features like integrated AI copilots within a single workspace, compatibility with Microsoft Graph, predefined templates, and tailored ontologies set ASAP apart. Industry Trends: Staying aligned with the latest trends in AI and software development ensures relevance and competitiveness.\n\nTechnological Capability\n\nAdvanced AI Integration: Leveraging semantic AI, structured ontologies, and cloud services positions ASAP as a cutting-edge solution. Scalability and Reliability: The platform’s ability to handle large datasets while maintaining high performance is crucial.\n\nCustomization and Flexibility\n\nProduct Development: Introducing more customization options for templates and workflows to meet diverse organizational needs. User Feedback: Responding to user feedback and quickly iterating on the product.\n\nCollaboration Features\n\nEnhanced Teamwork: Boosting collaboration features for more effective teamwork on documentation projects.\n\nASAP is dedicated to revolutionizing software coding and documentation by focusing on enhanced capabilities, improved collaboration, customization, and user support. By doing so, we are committed to helping organizations streamline their documentation processes and achieve greater productivity."},{"heading":"Case study","content":"Streamlining Workflow with Azure Cosmos DB and ASAP Tools\n\nConsultant’s Journey with ASAP: Optimizing Workflow with Azure Cosmos DB\n\nA seasoned Azure Cosmos DB consultant received an inquiry from a prospective customer about developing an application utilizing Cosmos DB. Following their conversation, she compiled the customer's requirements and sent them via email, both to the customer and as a record for herself.\n\nJuggling multiple clients, the consultant relies on ASAP to streamline her workflow. She opens ASAP and clicks on the Microsoft Graph tab. Using the advanced search capabilities, she locates her emails containing the requirements. By vectorizing these emails, she ensures that she won’t need to resubmit them to GPT every time; ASAP combines the system prompt with the custom prompt for seamless future reference.\n\nWith the requirements entered into the database and vectorized, the consultant turns her attention to documentation. She accesses her internal company documents via the OneDrive interface within the Microsoft Graph tab and opens an unstructured document. Using OntologyGen, she customizes settings to create a structured ontology, storing the generated documentation in Azure Cosmos DB in vectorized form for efficient retrieval through Semantic Search.\n\nNext, the consultant leverages DocGen to utilize the vectorized requirements and her custom ontology. The ontology follows a structured format of chapters and sections. Starting with one chapter, she configures DocGen to refine the output iteratively within a chat thread. Once satisfied with a section, she saves the final version.\n\nAfter completing each section and ensuring all chapters are ready, the consultant previews the entire document and makes necessary adjustments. Satisfied with the results, she uses ASAP to generate the final document as a PDF, fully optimized for her customer’s needs.\n\n1) Informal notes for custom project\n\nVector Search Implementation for ABC Corp. Subject: Project: Vector Search Implementation for ABC Corp. From: Constantine To: Constantine Received: 06/20/2024 18:08 Customer Notes. Project: Vector Search Implementation Consultant: John Doe Customer: ABC Corp Note 1 Date: 2024-06-06 The customer wants to implement Vector Search & AI Assistant for Azure Cosmos DB for MongoDB vCore. They need a guide on setting up the environment and integrating vector search capabilities. Note 2 Date: 2024-06-07 They also require detailed documentation on configuring AI services to assist with data queries and enhance search functionalities. Note 3 Date: 2024-06-08 The customer is interested in best practices for optimizing performance and ensuring scalability of the solution.\n\n2) Vectorization-COMPRESS\n\nUsing COSE Copilot with following settings:\n\nSettings:Create |OneShot |Technical |Distinguished |Large |Focused |Downward Synthesize\n\nNote: the Target Audience Level is Distinguished - e.g. the highest level because it is intended to be used from computer for communication on highest level of understanding.\n\n[COMPRESS-START] Vector Search Implementation for ABC Corp. Project Notes: Implement Vector Search & AI Assistant on Azure Cosmos DB for MongoDB vCore—setup and integration guide needed. (2024-06-06). Detailed documentation required for configuring AI services to support data queries and enhance search functionalities. (2024-06-07). Provide best practices for optimizing performance and ensuring scalability. (2024-06-08). [COMPRESS-END]\n\n3) Vectorization-DECOMPRESS\n\nSettings:Create |OneShot |Technical |Novice |Large |Focused |Downward Synthesize\n\nNote: the Target Audience Level is Novice - e.g. the lowest level because it is intended to be use from a person with all information required to start and finish the project.\n\n[DECOMPRESS-START] ABC Corp. Project: Vector Search Implementation 1. Implementing Vector Search & AI Assistant: - Platform: Azure Cosmos DB for MongoDB vCore. - Task: Setup and integration of vector search functionalities with AI Assistant. - Date: June 6, 2024. - Requirements: Comprehensive setup and integration guide. 2. Configure AI Services: - Focus: Supporting data queries. - Enhancement: Improve search functionalities. - Date: June 7, 2024. - Documentation Needs: Detailed configuration instructions for AI services. 3. Performance Optimization & Scalability: - Goal: Achieve high performance and scalability. - Recommendations: Best practices for optimization. - Date: June 8, 2024. [DECOMPRESS-END]\n\nLeveraging AI Copilot Tasks\n\nAfter Vectorization-COMPRESS, use Vectorization-DECOMPRESS as input to AI Copilot Tasks:\n\nTask #1 Title: Setup Guide for Vector Search Implementation on Azure Cosmos DB for MongoDB vCore\n\nCategory: Development\n\nPriority: High\n\nFramework Justification: Eisenhower Matrix - The task's importance is paramount due to its direct impact on the project's foundation, making it a high-priority task.\n\nDescription: Create a comprehensive guide on setting up the environment for Vector Search and AI Assistant for Azure Cosmos DB for MongoDB vCore. Include steps for initial setup, required configurations, and integration processes.\n\nDue Date: 2024-06-13 Assigned To: John Doe\n\nTask #2 Title: Document Configuration of AI Services for Enhanced Search Functionalities\n\nCategory: Documentation\n\nPriority: Medium\n\nFramework Justification: MoSCoW Method - Essential for the project’s completion but can be scheduled after initial setup, making it a 'Should have' task.\n\nDescription: Develop detailed documentation on how to configure AI services to assist with data queries and enhance search functionalities. Provide examples and step-by-step instructions to ensure clarity.\n\nDue Date: 2024-06-20\n\nAssigned To: John Doe\n\nTask #3 Title: Research and Document Best\n\nPractices for Performance Optimization\n\nCategory: Research/Documentation\n\nPriority: Medium\n\nFramework Justification: Pareto Principle - Identifying best practices will have a long-term impact on the project's success, focusing efforts on tasks that yield significant results.\n\nDescription: Research industry best practices for optimizing performance and ensuring the scalability of the implemented Vector Search solution. Document findings and create guidelines to be included in the overall project documentation.\n\nDue Date: 2024-06-18\n\nAssigned To: John Doe\n\nSelect an Ontology for creating documentation.\n\nComprehensive Guide to Azure Cosmos DB Documentation\n\nComprehensive Guide to Azure Cosmos DB Documentation Software Documentation for Azure Cosmos DB Ontologies 1. Requirements Documentation 1. Introduction Purpose: Explain the functionality, scalability, and performance of Azure Cosmos DB. Scope: Cover features, limitations, and use cases. Definitions, Acronyms, and Abbreviations: Clarify terms such as \"partitioning,\" \"throughput,\" and \"global distribution.\" References: Include links to Azure documentation, industry best practices, and regulatory standards. Overview: Summarize the database's multi-model capabilities, global distribution, and consistency models. 2. Functional Requirements User Needs: Describe the requirements for managing globally distributed databases. System Operations: Specify CRUD operations, indexing, and query capabilities. User Interface: Detail the Azure Portal and SDK interfaces. 3. Non-Functional Requirements Performance: Define latency targets and throughput requirements. Reliability: Describe SLAs, replication strategies, and failover mechanisms. Security: Outline encryption standards, access controls, and compliance with data protection laws. 4. Data Requirements Data Models: Detail support for document, graph, key-value, table, and column-family data models. Data Storage: Define partitioning schemes, consistency levels, and backup strategies. Data Manipulation: Specify methods for querying with SQL, Gremlin, MongoDB API, and Table API. ...\n\nStart generating section Introduction.\n\nThe input is for example:\n\nTask #2 Title: Document Configuration of AI Services for Enhanced Search Functionalities\n\nCategory: Documentation\n\nPriority: Medium\n\nFramework Justification: MoSCoW Method - Essential for the project’s completion but can be scheduled after initial setup, making it a 'Should have' task.\n\nDescription: Develop detailed documentation on how to configure AI services to assist with data queries and enhance search functionalities. Provide examples and step-by-step instructions to ensure clarity.\n\nDue Date: 2024-06-20\n\nAssigned To: John Doe\n\nit is combined with\n\nSegment #1:\n\nComprehensive Guide to Azure Cosmos DB Documentation 1. Introduction Purpose: Explain the functionality, scalability, and performance of Azure Cosmos DB. Scope: Cover features, limitations, and use cases. Definitions, Acronyms, and Abbreviations: Clarify terms such as \"partitioning,\" \"throughput,\" and \"global distribution.\" References: Include links to Azure documentation, industry best practices, and regulatory standards. Overview: Summarize the database's multi-model capabilities, global distribution, and consistency models.\n\nwith custom command: run it to produce:\n\nComprehensive Guide to Azure Cosmos DB Documentation\n\n1. Introduction\n\nPurpose: This guide aims to provide an in-depth understanding of Azure Cosmos DB, focusing on its functionality, scalability, and performance.\n\nScope: We'll cover the database's features, limitations, and use cases to give you a comprehensive overview of its capabilities.\n\nDefinitions, Acronyms, and Abbreviations:\n\nPartitioning: Method to distribute data across separate storage units for better scalability. Throughput: Measure of data flow rate; how much data can be processed per second. Global Distribution: Capability of distributing data across multiple geographic regions for improved access and redundancy.\n\nReferences:\n\nAzure Documentation: Azure Cosmos DB Documentation Industry Best Practices: Refer to published best practices for database management. Regulatory Standards: Links to GDPR and other relevant regulatory information.\n\nOverview: Azure Cosmos DB is a globally distributed, multi-model database service. It supports various data models and offers five consistency levels. Its global distribution capabilities ensure data is available and responsive worldwide while maintaining high performance and scalability.\n\nTask\n\nDocument Configuration of AI Services for Enhanced Search Functionalities\n\nCategory: Documentation\n\nPriority: Medium\n\nDescription: This section provides a step-by-step guide on configuring AI services to improve data query capabilities and enhance search functionalities.\n\nIntroduction to AI Services: Purpose: AI services can analyze and interpret complex data queries, providing more accurate search results. Scope: Covers configuration, integration, and optimization. Step-by-Step Configuration: Set Up AI Service: Navigate to the Azure Portal: Log in and select your subscription. Create a New AI Service: Choose the service suitable for enhanced search, like Azure Cognitive Search. Configure Basic Settings: Define your service name, location, and pricing tier. Integration with Cosmos DB: Connect Data Source: Link your Azure Cosmos DB account as the data source. Index Data: Set up indexing to ensure data is searchable. Define Search Index Schema: Create fields and specify data types to be analyzed by AI. Enhanced Search Features: Natural Language Processing (NLP): Enable NLP to improve query understanding. Synonym Mapping: Add synonyms to improve search result relevance. Custom Scoring Profiles: Create scoring profiles to prioritize certain data attributes. Testing and Optimization: Run Test Queries: Perform various searches to ensure configurations are correct. Adjust Parameters: Fine-tune index schema and scoring profiles based on test results. Monitor Performance: Use Azure Monitor to track usage and performance metrics. Conclusion: Ensuring AI services are correctly configured enhances the usability and relevance of search functionalities, driving better insights and data utilization.\n\nBy following this guide, users will be equipped to set up and optimize AI services with Azure Cosmos DB, thereby enhancing search functionalities and improving data retrieval efficiency."},{"heading":"Built With","content":"azure c# gpt-4 gpt-4o"}]},{"project_title":"AI4KBAgent™ : A ServiceNow KB CoPilot Powered by GenAI","project_url":"https://devpost.com/software/servicenow-kb-copilot","tagline":"A GenAI app for enterprises. Empower content developers to create, edit, and publish KB articles seamlessly in ServiceNow. Notify via Microsoft Teams and email. Viewers can chat and ask questions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/930/084/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"cosmosdb","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"langgraph","url":null},{"name":"openai-api","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"servicenow-api","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/dlyog/AI4KBAgent"},{"label":"dlyogkbagent.eastus.azurecontainer.io","url":"http://dlyogkbagent.eastus.azurecontainer.io:5011/"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for AI4KBAgent™ CoPilot came from the need to enhance enterprise knowledge management systems with the power of AI. Most enterprise service management systems use ServiceNow KB articles as the primary source of guidance for all employees and contractors. With Generative AI, we have the opportunity to augment the quality of KB articles and deliver them faster. However, KB content management presents a separate problem that requires a dedicated application to integrate with Generative AI and manage article content independently, delivering it to ServiceNow when appropriate. This app, ServiceNow CoPilot, is designed to meet this need."},{"heading":"What it does","content":"AI4KBAgent™ CoPilot uses AI to refine KB articles based on user inputs. Content developers can submit a title and any extra information they want to include. The app then creates the document using a multi-step agentic approach, as shown in the attached image. It starts by planning the article, creating a title, summary, category, and sections with high-level content. Then, iteratively for each section, it generates content and sends it to a critique agent. Based on the critique feedback, it revises the content. This loop can be configured; currently, it is set for three iterations. Once all sections are completed in this manner, it aggregates and saves the draft in CosmosDB. The user can further edit and set the state to approved or published. If approved, it becomes visible to content viewers in the app. If published, it triggers a ServiceNow API call to create a KB article and notifies teams via Microsoft Teams and email."},{"heading":"How we built it","content":"We built AI4KBAgent™ CoPilot by integrating several technologies:\n\nAzure OpenAI API: To generate and refine content based on user input. LangChain and LangGraph: To create an AI agent flow that plans and structures the article content. ServiceNow API: To create and manage KB articles in ServiceNow. Azure CosmosDB: To store and retrieve existing KB articles using vector search. Web Application: A user-friendly interface developed using a modern web framework to interact with the AI and manage KB articles."},{"heading":"Challenges we ran into","content":"One of the significant challenges was ensuring the AI-generated content met the quality standards for KB articles. We had to fine-tune the prompts iteratively, using self-reflection and review processes to achieve the desired output. Additionally, we faced challenges implementing Vector Search with CosmosDB NoSQL, as described in this article . Specifically, we encountered intermittent errors while creating and updating the Vector Policy. Although the policy saved successfully at one point, enabling vector search, subsequent steps resulted in errors. Therefore, we resorted to using standard CosmosDB queries and in-memory logic to identify relevant articles. We checked titles and summaries during article chat or question-answer sessions, and then used the full article for in-context learning. This approach worked well for smaller articles, providing accurate results. However, as articles grow in size, we plan to move to vector-based search using DiskANN for better performance. Integrating multiple APIs seamlessly and ensuring data consistency across platforms was another challenge."},{"heading":"Accomplishments that we're proud of","content":"We are proud of successfully integrating AI to create high-quality KB articles that meet user needs. The implementation of vector search in CosmosDB to suggest existing articles is a significant achievement. Our user-friendly web interface ensures a smooth and efficient experience for users. The real win is our successful integration with ServiceNow, allowing us to publish KB articles directly to ServiceNow, expanding the reach across the enterprise and making a meaningful impact. After an article is created, reviewed, and approved in the app, it is published in ServiceNow as a KB knowledge article. This triggers notifications via Microsoft Teams and email, providing the ServiceNow KB URL link like this KB Article for users to read and provide feedback."},{"heading":"What we learned","content":"Through this project, we learned the importance of balancing AI automation with human oversight to ensure content quality. We gained valuable experience in integrating multiple APIs and managing data across different platforms. Additionally, we learned how to optimize vector search to improve the relevance of suggestions. Although we faced challenges with CosmosDB's vector search feature, we found effective alternatives using standard queries and in-memory logic for smaller articles. Cosmos DB proved to be especially effective in AI chat, particularly in retrieval-augmented generation (RAG), and its preview vector search feature with DiskANN is a game-changer that we plan to explore further in the future【source】."},{"heading":"What's next for AI4KBAgent™ CoPilot","content":"The next steps for AI4KBAgent™ CoPilot include:\n\nEnhancing the AI model to better understand user input and generate more accurate content. Improving the vector search algorithm in CosmosDB for even more relevant suggestions. Adding more features to the web interface, such as advanced search filters and analytics to track KB article usage and effectiveness. Expanding integration with other knowledge management systems to broaden the tool's applicability. Implementing Graph RAG for better question-answer functionality. Scaling the app from a single container in Azure Container Instance to save costs, with future plans for larger scale deployment. Integrating with other systems like Slack or SharePoint to enhance enterprise connectivity. Checking first if a relevant article exists and suggesting it to the user before creating a new one, allowing the user to proceed only if they still want to create a new entry.\n\nFor more details on CosmosDB’s vector search capabilities, check out this YouTube link ."},{"heading":"Built With","content":"azure cosmosdb docker flask github langgraph openai-api python react servicenow-api"},{"heading":"Try it out","content":"github.com dlyogkbagent.eastus.azurecontainer.io"}]},{"project_title":"AgriHealth","project_url":"https://devpost.com/software/agrihealth-li5f2g","tagline":"Revolutionalize Agriculture leveraging Generative AI","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"streamlit","url":null}],"external_links":[{"label":"agrihealth.azurewebsites.net","url":"https://agrihealth.azurewebsites.net/"},{"label":"github.com","url":"https://github.com/oadeniran/AgriHealth"}],"description_sections":[{"heading":"Inspiration","content":"As a Nigerian, in the past few months we have seen a skyrocket of prices and inflation across all sphere of which food has not been left out. Food is one of the most essential things to man and prices soaring is not a good omen as more families plunge deeper into hunger and poverty, which is a direct opposite to SDGs 1 and 2 which are No poverty and Zero Hunger respectively. Our hope is that if our local food production (both livestock and crops) becomes well developed then we can attain sustainable development as a country and reduce our dependency of foreign imports which is the major reason why food has been getting more expensive in the worsening economy."},{"heading":"What it does","content":"The tool enables farmers or other agriculture stakeholders have a means to safeguard against agriculture produce crippling diseases. The tool can predict the status of the crops (and/or) livestock based on images sourced from the agricultural produce itself. Now GenAI has been applied with a azure powered GPT model that generates suggestions for farmers to mitigate the identified disease and do so in real time."},{"heading":"How we built it","content":"This is a project the combines numerous ML techniques including computer vision with deep learning, traditional ml models, and GenAI. It was built using python and various machine learning tools. A convolutional neural network was built with tensorflow, and the deep learning model was trained on respective datasets for each possible prediction. The models were then saved and deployed using streamlit. Streamlit handles the frontend where users can decide to upload pre-existing pictures or take a picture in real-time using mobile devices and the image is then run through the respective model and teh predictions of the model are returned to the users to make information powered decisions about the livestock or crop"},{"heading":"Challenges we ran into","content":"A major challenge we ran into is the dataset to be used for training. Kaggle is one of the best sources of dataset and we were able to find relatively few graphical and picture based datasets on livestock diseases. The datasets for crop diseases were also few and more relied on tabular form of data for predictions, therefore it will be imperative to gather and source data for extended purposes. Another challenge is the performance of the dataset. While we were able to achieve appreciable values of accuracy of 80% and above, higher accuracy values will still be required which can be gotten with more time and more training which was not done due to timeframe."},{"heading":"Accomplishments that we're proud of","content":"A major accomplishment we are proud of is being able to deploy the model for use publicly and in real time. Deploying deep model projects always serve as a challenge due to the heavy resources required"},{"heading":"What's next for AgriHealth","content":"There are a lot of things to be done to improve the project, however two of the most important of them are\n\nAddition of more livestock and crops Integration of chatbots (like openAI) to give Realtime feedbacks based on the prediction of the models"},{"heading":"Built With","content":"azure openai python streamlit"},{"heading":"Try it out","content":"agrihealth.azurewebsites.net github.com"}]},{"project_title":"Lego Robot AI","project_url":"https://devpost.com/software/lego-robot-ai","tagline":"Enriching STEM learning by controlling Lego robot using Python directly in web browser, plus Lego chatbot via speech, brick image search and live user chat!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/939/463/datas/medium.jpg","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"legorobot-web.azurewebsites.net","url":"https://legorobot-web.azurewebsites.net/"}],"description_sections":[{"heading":"Inspiration","content":"Our Year 6 girls' FIRST Lego League team embarked on an exciting journey to learn Python for Spike Prime Lego Robot last year in preparation for the FIRST Lego League competition. Transitioning from block coding to Python was a significant challenge, and while they faced struggles in getting their programs to run independently, their determination never wavered. One student sought help from ChatGPT, but encountered issues with legacy and third-party libraries.\n\nThese young, passionate learners are in need of an innovative approach to support their STEM education (science, technology, engineering and mathematics) that allows them to interact with objects in real-time, enriching their experience and fueling their curiosity."},{"heading":"What it does","content":"Lego Robot AI enriches learning experience by generating and explaining Python code using the Spike Prime 3 API specifications and knowledge base. Students can write and execute Python code directly on their Lego Robot through a web browser using a web serial port. This seamless integration helps young learners understand coding concepts while watching their robots come to life.\n\nThe app also features:\n\nLego Brick Image Library : Find similar bricks using an image search. Speech Conversation Support : Makes interaction easier for young kids. Live User Chat : Facilitates communication between kids and teachers. Image Generation/ Description : Help kids build their Lego robots and discover new ideas."},{"heading":"How we built it","content":"We did a prototype in March: a Python console program that can generate Python code using a fine-tuned GPT-3 model. However, this version was difficult to use due to:\n\ntext-based interface require extensive setup on the device the generated code often lacked accuracy\n\nNow, we have transformed the prototype into a fully web-based solution with a more capable LLM model and heaps of new features .\n\nFrontend : Built with React and TypeScript for a user-friendly web interface. API Endpoints : Proxied through Azure API Gateway to ensure seamless communication. Backend : Hosted on Azure utilizing Azure OpenAI services and Cosmos DB (MongoDB).\n\nWe employed a variety of Azure services including App Service, Function App, APIM, Container App, Cosmos DB, ACR, Speech Service, Computer vision, Translation Service, Key Vault, Azure Devops. This comprehensive setup has allowed us to create a powerful, user-friendly app that makes coding accessible and fun for young learners."},{"heading":"Challenges we ran into","content":"The knowledge base needs to be generated using raw Spike Prime API specs and user guides. Legacy versions and 3rd party libraries of APIs on the internet needs to be excluded The chatbot needs to generate accurate python code that is executable directly on the robot. Hard to integrate web serial port in the browser (compatibility issues) Need to build up Python code snippet libraries to perform various robot actions (for LLM) Feed useful information to RAG and does not slow down response time Use Cosmos DB (MongoDb) Vector search for both text and image Cosmos DB (MongoDb)'s change stream does not have 'delete' events Don't store/ leak API keys in React App Certain AI Services are expensive and needs to keep cost down Inconsistency between python library and node library"},{"heading":"Accomplishments that we're proud of","content":"ChatBot with Knowledge of Lego Spike Prime General Knowledge ChatBot with Knowledge of Spike Prime 3 Python API / Function / Execution Python code chat response syntax highlighting Web Serial Port integration to Lego Spike Prime Hub Execute python code directly from Browser Translate chat response to French Search similar Lego Brick via image upload Describe Lego brick image in text Generate Lego brick image by supplied description Receive Chat prompt via speech Read Chat responses by voice Live chat with other users via browser"},{"heading":"What we learned","content":"Build a React Typescript Frontend / Single Page Application Generate meaningful knowledge base for LLM Langchain RAG model with Azure OpenAI services (GPT-4o) Image Vector searching using Cosmos DB (MangoDB) & Computer vision Integrate with Azure Speach & Translation Service Cosmos DB (MangoDB) functionalities like change stream"},{"heading":"What's next for Lego Robot AI","content":"Receive feedback from the actual users: the kids Build up more code snippets and API specs in the knowledge base Improve Lego Brick image search (i.e. more images and 3D models) Use LLM to generate Robot competition strategies (e.g. best routes) Get rid of USB cable and use Bluetooth instead"},{"heading":"Built With","content":"azure mongodb node.js openai react"},{"heading":"Try it out","content":"legorobot-web.azurewebsites.net"}]},{"project_title":"Klerk","project_url":"https://devpost.com/software/klerk","tagline":"Klerk: Simplifying Government Services with a Multilingual AI Assistant for Accurate, Up-to-date, and Seamless Official Work Assistance.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/923/342/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"bootstrap","url":"https://devpost.com/software/built-with/bootstrap"},{"name":"cosmos","url":null},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"vscode","url":null}],"external_links":[{"label":"klerk.azurewebsites.net","url":"https://klerk.azurewebsites.net/"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Klerk stemmed from the need to simplify interactions with government services, especially in regions with high linguistic diversity. Recognizing the challenges posed by language barriers in accessing government and legal services, we aimed to create a solution that bridges these gaps, providing clear and accessible information to all citizens. Klerk is designed to make official work less daunting and more efficient for everyone, regardless of their language proficiency. Klerk 2.0 leverages Azure AI Speech Service to Text to Speech and Speech To Text , in order facilitate more seamless experience and quite helpful for someone who is not much literate with his writing skills."},{"heading":"What It Does","content":"Klerk is a multilingual assistant that facilitates seamless interactions with government services globally. Users can upload documents or input queries, select their target language, and receive translated responses. The application processes the input using Azure OpenAI, enhances it with relevant data from a Cosmos DB vector search, and provides accurate, context-aware responses in the desired language. Klerk supports a wide range of Indian regional languages and several international languages, making it a versatile tool for diverse linguistic needs."},{"heading":"Use Cases","content":"Bridging Regional Language Barriers\n\nLanguage barriers can significantly hinder people dealing with government or private institution paperwork if they do not understand the local language. This issue often discourages people from investing, purchasing land, or availing benefits of government schemes. Similarly, individuals who do not understand widely spoken languages like English but are fluent in a regional language face challenges. Klerk addresses this issue by translating documents into the user’s preferred language, making official paperwork accessible and understandable.\n\nAssisting Non-Regional Language Speakers\n\nImagine a person who only knows English, Hindi, and French visiting a government office in Karnataka, India, where all paperwork is in Kannada. This individual would struggle to understand the forms and documents. By inputting the document into Klerk and selecting the target language as English or another familiar language, they can easily understand the content and process.\n\nProviding Updated Government Information\n\nKlerk’s backend logic accesses an Azure Cosmos DB populated with the latest government rules, regulations, and policies for various departments, collected from official departmental websites. These documents are vectorized and indexed. When a user has a query related to a government scheme, Klerk provides updated, relevant, and specific details, ensuring the information is comprehensive and current. The current version focuses on documents related to \"Will\" processing, providing detailed and updated information for related queries.\n\nSpecific Information Retrieval\n\nKlerk's system and user prompt optimization facilitate quick and precise information retrieval. For example, if a user needs only the name and address from a sale deed document or types \"detailed information\" as a query, Klerk will display a response tailored to the user’s specific request. This system and user prompt optimization facilitate quick and precise information retrieval.\n\nFacilitating Government Office Work\n\nGovernment office work is often not organized enough for individuals to complete tasks independently. People typically seek help from official/unofficial clerks to initiate processes. Klerk assists by providing information about professional and knowledgeable clerks who can guide users through their intended work. Users can gather information about government departments, office locations, and clerks with just a few clicks, enabling them to communicate with clerks in advance and understand the requirements before visiting the departmental office. Klerk’s team maintains ratings for unofficial clerks ( associated with Klerk's team) based on past services, allowing users to choose the most suitable clerk for their needs."},{"heading":"Voice-Based Interaction in Klerk 2.0","content":"Klerk integrates Azure Speech Services to enable seamless voice-based interaction, ensuring accessibility for users with limited literacy. This voice-based interaction enhances user engagement by allowing them to speak instead of typing, making it easier to interact with government services, legal documents, and policy-related queries. The Klerk 2.0 has following additional feature.\n\nSpeech-to-Text (STT): Converts user speech into text using Azure Speech SDK. Supports multiple languages, allowing users to interact using their preferred language. Processes voice commands and queries to facilitate communication with Klerk.\n\nText-to-Speech (TTS): Converts the generated response into natural-sounding speech. Uses Azure Speech Synthesis with language-specific neural voices for enhanced user experience. Supports multiple regional languages, including Hindi, Kannada, Tamil, Telugu, Marathi, Urdu, and more.\n\nLanguage Adaptability: Klerk dynamically selects the appropriate speech code and synthesis voice based on the target language. This ensures that users receive responses in their native or preferred language, improving accessibility."},{"heading":"How We Built It","content":"Azure AI Cosmos DB for Vector Search: Employed Cosmos DB's vector search capabilities to augment user queries with relevant information from our knowledge base.\n\nAzure AI LLM Model GPT-4: to power natural language processing and response generation.\n\nAzure AI Embedding Model text-embedding-ada-002: has been used for vectorizing text queries for similarity searches.\n\nAzure AI Form Recognizer: to extracts key information from scanned government documents.\n\nAzure AI Document Intelligence Read Model: for OCR-based text extraction and Incorporated this model to process input files and return their content accurately.\n\nAzure AI Speech: to convert text responses into speech for text-to-speech (TTS) translation.\n\nAzure OpenAI Integration: Leveraged Azure OpenAI for processing and translating documents and queries, ensuring high accuracy in language processing.\n\nAzure WebApp Deployment: Deployed the application on Azure WebApp, ensuring reliable and scalable access.\n\nFrontend Development: Utilized HTML, CSS, and JavaScript to create a user-friendly interface, allowing users to upload documents, input queries, and select target languages.\n\nBackend Development: Implemented with Flask, our backend manages user requests, processes documents, and interfaces with AI and database services.\n\nCI/CD with GitHub: Integrated GitHub with Azure WebApp for continuous integration and deployment, facilitating seamless updates and maintenance. User Interface: Focused on crafting an intuitive interface that is accessible to users from diverse backgrounds, ensuring ease of use."},{"heading":"Challenges We Encountered","content":"Handling Multilingual Data: Ensuring accurate and contextually appropriate translations was challenging. We addressed this by refining our prompts and iterating on the AI's responses. Efficient Vector Searches: Initial vector searches were slow and less accurate. We optimized database queries and indexing strategies to improve performance. Technical Integration: Seamlessly integrating multiple technologies (Azure OpenAI, Cosmos DB, Flask, Azure AI Document Intelligence, Azure WebApp) posed challenges, particularly in data flow and response times."},{"heading":"Accomplishments We’re Proud Of","content":"Multilingual Support: Successfully enabling the application to provide accurate translations in multiple languages, breaking down significant communication barriers. Users can interact with Klerk in their preferred language, making government services more accessible to diverse communities.\n\nVoice-Based Interaction (Speech-to-Text & Text-to-Speech): Integrating Azure Speech Services to enable seamless voice-based communication. Users can speak their queries instead of typing, making it easier for those who are not comfortable with written text. Klerk responds with natural-sounding speech in the selected language, ensuring accessibility for users with limited literacy. This feature enhances user engagement and inclusivity.\n\nEfficient Query Augmentation: Implementing an advanced vector search mechanism in Cosmos DB to deliver highly relevant and precise information. By leveraging AI-powered embeddings, Klerk can retrieve contextual information efficiently, improving response accuracy.\n\nUser-Friendly Interface: Developing an intuitive and accessible user interface that caters to a diverse user base. The interface is designed to accommodate users of varying technical expertise, ensuring a smooth and engaging experience.\n\nSeamless Integration and Deployment: Achieving seamless integration of various Azure AI services, OpenAI models, and Cosmos DB to create a robust and scalable solution. Additionally, automated deployment using GitHub and Azure WebApp ensures a streamlined workflow, enabling rapid updates and improvements."},{"heading":"Lessons Learned","content":"Natural Language Processing (NLP): I gained more insights into leveraging Azure OpenAI for advanced natural language processing tasks. This involved understanding how to use pre-trained models for generating human-like text and incorporating these capabilities into the application.\n\nLanguage Translation and Summarization: I learned more to use Azure OpenAI for translating text between different languages and summarizing lengthy documents, making the application more user-friendly and versatile.\n\nAzure AI Document Intelligence: I explored how Azure AI Document Intelligence can automate the extraction of information from various types of documents. This included processing structured and unstructured data to streamline workflows. I learned how to integrate document intelligence capabilities into a web application, allowing for real-time document analysis and information retrieval.\n\nVector Databases: Learned the intricacies of using Cosmos DB for efficient and accurate vector searches. I learned to implement RAG using Azure services, ensuring that the application could retrieve relevant information from a large corpus of documents and generate coherent and contextually appropriate responses.\n\nUser Experience Design: I gained more insight in creating responsive web interfaces using HTML and CSS. This included designing user-friendly forms, implementing loading messages, and ensuring the application works well on different devices, use JavaScript for adding dynamic content and interactivity to the web application. This included handling form submissions, displaying real-time data, and providing a seamless user experience.\n\nCI/CD Practices: Enhanced our understanding and implementation of continuous integration and deployment practices using GitHub and Azure WebApp."},{"heading":"Adherence to Azure Responsible AI Principles","content":"Fairness: Klerk ensures fairness by providing translated document content detail and answer to query for users from diverse linguistic backgrounds. This reduces language barriers and ensures equal access to government services and legal information for all users, regardless of their native language.\n\nReliability and Safety: Klerk leverages Azure AI Document Intelligence Read Model and Azure OpenAI for reliable and accurate content processing. Rigorous testing and validation ensure that translations and information retrieval are dependable and safe for users, minimizing the risk of errors.\n\nPrivacy and Security: Klerk adheres to strict privacy guidelines by not storing or retaining any uploaded documents or it's content on its servers, ensuring that sensitive information remains secure and private.\n\nInclusiveness: Klerk supports multiple languages, including various Indian regional languages and several international languages, ensuring inclusiveness. The user-friendly interface is designed to be accessible to people from diverse backgrounds, including those with minimal understanding of source language to used with Klerk.\n\nTransparency: Klerk maintains transparency by clearly informing users about the application's functionalities, limitations, and data processing methods in the terms and conditions. Users are aware that the application is in the Beta version, and the translation accuracy may have minor imperfections.\n\nAccountability: Klerk provides a feedback loop where users can share their experiences and suggestions to Klerk's given email logistic given on terms and conditions page. This feedback mechanism allows for continuous improvement of the system, ensuring that the developers are accountable for the application's performance and user satisfaction."},{"heading":"What's Next for Klerk","content":"Expand Knowledge Base: Continuously add more departments and regions to provide comprehensive assistance.\n\nImprove AI Capabilities: Enhance the AI's understanding and response accuracy with more training data.\n\nUser Feedback Integration: Implement a feedback loop to improve the system based on user inputs and experiences.\n\nMobile Application: Develop a mobile version of Klerk to increase accessibility and convenience for users on the go.\n\nGlobal Reach: Expand language support and localization to cater to a global audience, making Klerk a universal tool for overcoming language barriers in government services."},{"heading":"Disclaimer:","content":"Office and Clerk details are not actual, including Name, Address, Mobile No, ratings. Retry Recommendation: Retry may help sometimes with desired results. Klerk aims to be a reliable and efficient assistant for anyone navigating the complexities of government services, breaking down language barriers and simplifying processes for a smoother experience."},{"heading":"Built With","content":"azure bootstrap cosmos css flask git html javascript openai python vscode"},{"heading":"Try it out","content":"klerk.azurewebsites.net"}]},{"project_title":"Meta-Store-AI","project_url":"https://devpost.com/software/my-super-cool-first-hackathon","tagline":"Meta Store AI combines 3D models with embedded animations and interactions with a backend helpful store clerk with the ability to execute the model actions and tell the customer about the products.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/922/307/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Developers AI Learning Hackathon","hackathon_url":"https://azurecosmosdb.devpost.com/","prize_name":"Phase 2 - Learn and Integrate: Honorable Mention"}],"team_members":[],"built_with":[{"name":"babylonjs","url":null},{"name":"cosmos","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/mattmacf98/Microsoft-AI-Hackathon"},{"label":"metastoreai.azurewebsites.net","url":"https://metastoreai.azurewebsites.net/"}],"description_sections":[{"heading":"Inspiration","content":"I work a lot with 3D models and 3D experiences. The Metaverse is cool but often times the amount of things a 3D viewer gives to customers is overwhelming and hard to navigate (with multiple buttons and sliders for doing things like color configuration, dimensions checking and animation playing. Paradoxically, the experiences are not specialized enough to take full advantage of having a digital twin of a product to show users.\n\nI really liked some of the examples of an AI agent incorporated into the Microsoft suite of tools, I thought it was really cool to see the agent do something that wasn't just text based but took action on some application. I wanted to emulate something like that in my project.\n\nI was also inspired by a project I previously worked on to embed code into 3D models which could be packaged up and executed client side. I thought doing this packaging and teaching an AI agent about the specifics of each loaded model would allow a user to...\n\n1. Cut through the complex UX and just use natural language to learn about the 3D model 2. Experience extremely specialized UXs per model without needing to learn the particulars of each one\n\nI wanted the interaction between the user and the AI agent to feel as though a real customer service rep was on the other side of the computer, and could take actions not just through chats but on the 3D viewer itself and respond to actions you have taken on the viewer."},{"heading":"Learned:","content":"I learned a lot about the Azure Cloud platform, I particularly liked the ease at which I could create a vector index on top of the MongoDB Cosmos DB I got to dive deeper into LangChain than I had previously, exploring an agent's ability to use tools was a very cool and fun project and gave me a lot to think about as to how backend AI APIs could function with a powerful agent acting as a facade into the core backend functionality of the app. I learned how difficult it is to get precise, consistent response back from the AI agent and figured out ways to address this headache – with re-enforced prompting and forgiving interfaces which the agent will communicate with"},{"heading":"How I Built It:","content":"I have documented the process here https://github.com/mattmacf98/Microsoft-AI-Hackathon . I built the 3D viewer and front end application using React and Babylon JS. The backend is Node JS which has two endpoints: 1 to talk to the agent and 1 to request a 3D model file from Azure.\n\nThe /ai endpoint will respond to every message using the format\n\n{message: result.output, functionToExecute: this.functionToExecute, productToLoad: this.productToLoad}\n\nThe function to execute and product to load are populated using tools exposed to the agent which it decides wether or not to use based on user input. These two functions are really nothing but some text. The front end will parse these fields to invoke actions on its side. When the front end loads a model, this information is passed on to the AI agent, which can then query the cosmos DB to get more information about the product such as\n\n{ \"id\": \"w-789-kjl\", \"name\": \"Adidas Shoe\", \"price\": 59.95, \"functions\": [\"look_at_insole\", \"show_next_variant\"], \"info\": \"This is a premium adidas blue shoe, it costs $59.95 and can be worn in any weather. It is guaranteed to make you run faster or your money back! There are an infinite amount of different variants you can see by invoking show_next_variant there is also a patented super comfy insole that should be highlighted that can be shown to the user by invoking look_at_insole, you can also toggle a bounding box using toggle_bounding_box\" }\n\nThe AI agent can then \"invoke\" model functions in the front end by populating the necessary field with a valid function."},{"heading":"Challenges","content":"My biggest challenge in this project was trying to get the AI agent to properly fill the structured content for the productToLoad and functionToExecute, it would often try to invoke a function like \"look_inner_sole\" when the actual name is \"look_at_insole\". To overcome this, I decided that whenever the AI agent populates this field, I will invoke something. I just had to decide what function in the model to invoke, I used Levenshtein distance (String edit distance) to find the nearest function that exists in the model that kindof looks like what the AI gave me. This works great for the functions I put in the model, in the future I might explore doing semantic distances instead of string edit so that something like kitten is closer to cat than it is kite.\n\nPrompting was definitely a big trial and error experience it was quite hard to debug since there is really no definitive guide book on prompting and you won't get a compilation error if your prompt is \"wrong\" like you would with code."},{"heading":"Built With","content":"babylonjs cosmos node.js react"},{"heading":"Try it out","content":"github.com metastoreai.azurewebsites.net"}]}],"generated_at":"2026-02-18T16:41:17.349418Z"}}