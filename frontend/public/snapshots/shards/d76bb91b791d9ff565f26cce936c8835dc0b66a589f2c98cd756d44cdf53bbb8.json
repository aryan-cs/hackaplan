{"version":"v1","hackathon_url":"https://nvidia-aws.devpost.com","generated_at":"2026-02-18T16:53:37.811158Z","result":{"hackathon":{"name":"Agentic AI Unleashed","url":"https://nvidia-aws.devpost.com","gallery_url":"https://nvidia-aws.devpost.com/project-gallery","scanned_pages":3,"scanned_projects":67,"winner_count":3},"winners":[{"project_title":"Skyrchitect","project_url":"https://devpost.com/software/skyrchitect-6d37yf","tagline":"An nemotron-nano-8B-v1 Model autonomously designs production-ready cloud architectures, generates infrastructure code, and calculates costs‚Äîturning ideas into deployable AWS infrastructure in seconds","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Agentic AI Unleashed: AWS & NVIDIA Hackathon","hackathon_url":"https://nvidia-aws.devpost.com/","prize_name":"Second Place"}],"team_members":[],"built_with":[{"name":"llama-3.1-nemotron-nano-8b-v1","url":null},{"name":"nvida","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/EimisPacheco/nvidia-aws-skychitect.git"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Skyrchitect came from my own frustration with cloud architecture design. I've seen countless developers struggle with translating their application ideas into production-ready cloud infrastructure. They know what they want to build, but get overwhelmed by the complexity of choosing the right services, understanding cost implications, ensuring security best practices, and writing infrastructure code. I realized that NVIDIA's Llama 3.1 Nemotron Nano 8B model, deployed on Amazon SageMaker with NVIDIA's Retrieval Embedding NIM, could autonomously solve this entire workflow‚Äîfrom understanding requirements to generating deployable infrastructure code. I wanted to create an AI agent that acts as a personal cloud architect, making enterprise-grade cloud design accessible to everyone.\n\nHow Skyrchitect Meets NVIDIA-AWS Hackathon Criteria:\n\nUses NVIDIA Llama 3.1 Nemotron Nano 8B as the core LLM for autonomous reasoning Integrates NVIDIA Retrieval Embedding NIM for RAG-enhanced architecture recommendations Deploys on Amazon SageMaker AI endpoints with NVIDIA GPU acceleration Solves a real-world problem that affects developers, startups, and enterprises daily Demonstrates innovative use of AI agents for complex, multi-step autonomous decision-making with retrieval augmentation Addresses the hackathon's goal of building practical AI solutions that create tangible value"},{"heading":"What it does","content":"Skyrchitect is an intelligent cloud architecture agent powered by NVIDIA NIMs on Amazon SageMaker that autonomously designs, visualizes, and deploys cloud infrastructures. Here's what I built:\n\nCore AI Agent Capabilities:\n\nAutonomous Architecture Design with RAG - The agent analyzes natural language requirements, uses NVIDIA's Retrieval Embedding NIM to find relevant cloud architecture patterns, then uses Llama 3.1 Nemotron to autonomously select appropriate AWS services, calculate costs, validate designs, and suggest optimizations Visual Diagram Generation - Automatically creates interactive architecture diagrams with proper node positioning, connections, and service relationships Multi-Modal Analysis - Analyzes uploaded architecture diagrams (images/PDFs) and extracts infrastructure components using vision capabilities Code Generation - Autonomously generates production-ready Terraform and CloudFormation infrastructure-as-code Cost Optimization with Semantic Search - Uses embedding NIM to find similar cost optimization patterns and proactively suggests alternative services\n\nHow It Meets NVIDIA-AWS Hackathon Requirements:\n\n‚úÖ NVIDIA Llama 3.1 Nemotron Nano 8B : Deployed on SageMaker endpoint ( nvidia-llama-nemotron-endpoint ) for complex reasoning across architecture design, service selection, and cost optimization ‚úÖ NVIDIA Retrieval Embedding NIM : Deployed on SageMaker endpoint ( nvidia-embedding-nim-endpoint ) for semantic search of architecture patterns, enabling RAG-enhanced recommendations ‚úÖ Amazon SageMaker Deployment : Both NVIDIA NIMs run on SageMaker ml.g5.xlarge instances with NVIDIA GPU acceleration ‚úÖ Autonomous Execution : The agent independently performs multi-step tasks‚Äîfrom retrieving relevant patterns to generating complete architectures‚Äîwithout human intervention ‚úÖ Tool Integration : Implements 5 custom tools (get_aws_service_info, calculate_architecture_cost, suggest_cost_optimization, get_service_alternatives, validate_architecture) that the agent calls autonomously using the Strands Agents framework\n\nRAG Workflow:\n\nThe agent uses a sophisticated retrieval-augmented generation approach:\n\nQuery Analysis : User provides architecture requirements Semantic Retrieval : NVIDIA Retrieval Embedding NIM finds top-3 relevant cloud patterns from knowledge base Context Enhancement : Retrieved patterns are added to the prompt sent to Llama Informed Generation : Llama 3.1 Nemotron generates architecture with context-aware recommendations Validation : Agent autonomously validates design using tools\n\nThis RAG approach ensures architectures follow proven patterns and best practices."},{"heading":"How I built it","content":"I architected Skyrchitect as a full-stack AI agent application with NVIDIA NIMs at its core:\n\nNVIDIA-AWS AI Agent Core (Backend):\n\nAmazon SageMaker Runtime : Direct integration with NVIDIA NIMs via SageMaker endpoints using boto3 NVIDIA Llama 3.1 Nemotron Nano 8B : Endpoint: nvidia-llama-nemotron-endpoint Instance: ml.g5.xlarge (NVIDIA GPU) Purpose: Architecture generation, reasoning, and code generation Invoked via: sagemaker-runtime.invoke_endpoint() NVIDIA Retrieval Embedding NIM : Endpoint: nvidia-embedding-nim-endpoint Instance: ml.g5.xlarge (NVIDIA GPU) Purpose: Semantic search for architecture patterns (RAG) Methods: embed_query() , embed_documents() , similarity_search() Strands Agents Framework : Built the architecture agent using the Strands framework, adapted to work with SageMaker endpoints instead of Bedrock Custom Tool Implementation : Created 5 specialized tools that the agent autonomously calls: get_aws_service_info - Retrieves service specifications and pricing calculate_architecture_cost - Computes total monthly costs suggest_cost_optimization - Finds cheaper alternatives using RAG get_service_alternatives - Compares services across providers validate_architecture - Checks for best practices and security FastAPI Backend : RESTful API with endpoints for architecture generation, optimization, validation, and code generation\n\nRAG Knowledge Base:\n\nI curated a knowledge base of cloud architecture patterns:\n\nHigh availability patterns (multi-AZ, load balancing) Microservices architectures (containers, orchestration) Serverless patterns (Lambda, API Gateway, DynamoDB) E-commerce architectures (CDN, auto-scaling, caching) Cost optimization strategies (spot instances, reserved capacity)\n\nThe NVIDIA Retrieval Embedding NIM converts these patterns into 384-dimensional vectors, enabling semantic similarity search.\n\nAgent Reasoning Flow:\n\nUser provides requirements ‚Üí Embedding NIM retrieves relevant patterns (0.90 similarity score achieved!) Enhanced prompt sent to Llama 3.1 Nemotron with retrieved context Llama autonomously calls tools to get service info and calculate costs Agent validates architecture and suggests optimizations Returns structured JSON with nodes, connections, and alternatives\n\nFrontend (React + TypeScript):\n\nInteractive architecture workflow with real-time diagram rendering using React Flow Pentagon-shaped cloud provider cards with dynamic theming Multi-step architecture creation process Cloud credentials management for deployment Code viewer with syntax highlighting for generated Terraform/CloudFormation\n\nKey Technical Achievements:\n\nSageMaker NIM Integration : Built custom adapter ( sagemaker_model.py ) to make NVIDIA NIMs compatible with Strands Agent framework Production RAG Implementation : Implemented complete retrieval pipeline with embedding generation, cosine similarity calculation, and top-k selection Dual Backend Support : Architected system to support both NVIDIA NIMs (SageMaker) and Claude (Bedrock) via environment variable switch Intelligent Fallback : Implemented hash-based fallback embeddings for development when SageMaker endpoints aren't deployed Zero UI Changes : Backend completely swappable between NVIDIA and AWS implementations without touching frontend code\n\nTech Stack:\n\nAI/ML : NVIDIA Llama 3.1 Nemotron Nano 8B, NVIDIA Retrieval Embedding NIM Infrastructure : Amazon SageMaker (ml.g5.xlarge instances) Framework : Strands Agents Framework Backend : Python FastAPI, Boto3 Frontend : React + TypeScript, React Flow Storage : AWS S3 (diagram backup)"},{"heading":"Challenges I ran into","content":"1. SageMaker Endpoint Integration with Strands\n\nThe biggest challenge was adapting the Strands Agents framework (designed for Bedrock) to work with SageMaker endpoints. Strands expects a specific model interface, but SageMaker uses a different API structure. I solved this by creating a SageMakerNIMModel adapter class that:\n\nImplements the same interface as Strands' BedrockModel Translates requests to SageMaker's invoke_endpoint format Handles NVIDIA NIM-specific request/response formatting Manages error handling and retries\n\nThis taught me about framework abstraction layers and API adaptation patterns.\n\n2. RAG Implementation with Embedding NIM\n\nImplementing retrieval-augmented generation required understanding embedding spaces and similarity metrics. Initially, I struggled with:\n\nEmbedding dimension : NVIDIA's embedding NIM produces 384-dimensional vectors Similarity calculation : Implemented cosine similarity from scratch using vector dot products and norms Top-k selection : Efficiently sorting similarities to find the most relevant patterns Context length management : Balancing retrieved context with prompt length limits\n\nI solved this by building a complete RetrievalEmbeddingNIM client with methods for query embeddings, document embeddings, and similarity search. The fallback hash-based embeddings (for development) surprisingly achieved 0.900 similarity scores!\n\n3. Endpoint Deployment Timing\n\nSageMaker endpoints take 10-15 minutes to deploy, which would have blocked rapid iteration. I solved this by implementing:\n\nFallback mode : Deterministic embeddings for development without deployed endpoints Graceful degradation : Code works in both demo mode and production mode Status checking : Automatic detection of endpoint availability Clear logging : Shows whether using real NIMs or fallback implementations\n\nThis enabled me to develop and test the entire application before deploying expensive GPU instances.\n\n4. Dual Hackathon Support\n\nI wanted to support both NVIDIA-AWS Hackathon (SageMaker NIMs) and AWS AI Agent Hackathon (Bedrock Claude). The challenge was:\n\nDifferent APIs : SageMaker vs Bedrock have completely different interfaces Different regions : us-west-2 for SageMaker, us-east-1 for Bedrock Different model formats : NVIDIA NIM JSON vs Bedrock's Anthropic format\n\nI solved this with:\n\nEnvironment variable switch : MODEL_TYPE=sagemaker or MODEL_TYPE=bedrock Separate agent implementations : Different files for each backend Common tool interface : Same 5 tools work with both agents Zero UI changes : Frontend completely agnostic to backend choice\n\n5. NVIDIA NIM Request Formatting\n\nNVIDIA NIMs expect specific request formats that differ from standard OpenAI-style APIs. I had to:\n\nStudy NVIDIA's NIM documentation Format messages correctly for Llama 3.1 Nemotron Handle embedding API differences (query vs passage encoding) Manage response parsing for different NIM types\n\nI created comprehensive adapters that handle all the formatting automatically."},{"heading":"Accomplishments that I'm proud of","content":"1. Production RAG with NVIDIA Embedding NIM\n\nI built a complete retrieval-augmented generation system using NVIDIA's Retrieval Embedding NIM. The semantic search achieves 0.900 similarity scores for matching user requirements to architecture patterns. This demonstrates real RAG implementation, not just a demo‚Äîwith measurable relevance metrics.\n\n2. NVIDIA NIMs on SageMaker\n\nSuccessfully deployed and integrated two NVIDIA NIMs (Llama LLM + Embedding) on Amazon SageMaker with GPU acceleration. This required understanding SageMaker endpoints, NVIDIA NIM containers, IAM roles, and request formatting. The integration is production-ready and scalable.\n\n3. Framework Adaptation\n\nI successfully adapted the Strands Agents framework (designed for Bedrock) to work with SageMaker endpoints by creating custom model adapters. This demonstrates deep understanding of both frameworks and ability to bridge different AWS services.\n\n4. Dual Hackathon Architecture\n\nBuilt a single codebase that works for both NVIDIA-AWS Hackathon (SageMaker + NIMs) and AWS AI Agent Hackathon (Bedrock + Claude). The backend is completely swappable via environment variable with:\n\nDifferent LLMs (Llama vs Claude) Different services (SageMaker vs Bedrock) Different regions (us-west-2 vs us-east-1) Zero frontend changes\n\n5. Intelligent Development Mode\n\nImplemented fallback embeddings that work without deployed SageMaker endpoints, enabling rapid development. The fallback mode still achieves 0.900 similarity scores using deterministic hash-based embeddings, proving the RAG architecture works before deploying expensive GPU instances.\n\n6. Clean Architecture\n\nCreated modular, well-documented code with:\n\nSeparate model adapters for different backends Clear separation of concerns (agent, tools, API, UI) Comprehensive logging showing agent decision-making Type hints and error handling throughout"},{"heading":"What I learned","content":"About NVIDIA NIMs:\n\nLlama 3.1 Nemotron Nano 8B : Learned this model's capabilities, optimal temperature settings (0.7), and how it compares to larger models for architecture reasoning tasks Retrieval Embedding NIM : Mastered embedding-based semantic search, understanding 384-dimensional vector spaces, cosine similarity calculations, and top-k retrieval strategies SageMaker Deployment : Learned how to deploy NVIDIA NIM containers on SageMaker, including endpoint configuration, instance types (ml.g5.xlarge), and GPU acceleration Request Formatting : Discovered NVIDIA NIM-specific API formats for both LLM inference and embedding generation\n\nAbout RAG (Retrieval Augmented Generation):\n\nWhen RAG Helps : Learned that RAG is most valuable for domain-specific knowledge (cloud patterns) that LLMs might not have in training data Embedding Quality : Discovered that good embedding models find semantically similar content even when exact keywords don't match Context Management : Learned to balance retrieved context length with prompt limits‚Äîtoo much context overwhelms the model, too little misses important patterns Similarity Thresholds : Found that scores above 0.70 indicate strong relevance for architecture pattern matching\n\nAbout AI Agents:\n\nFramework Abstraction : Learned that agent frameworks like Strands can work with different LLM backends if you create proper adapters Stateful vs Stateless : Discovered when to maintain conversation history (architecture design) vs when to use fresh context (code generation) Tool Design : Learned that effective agent tools should return structured data, not just text, enabling better agent decision-making\n\nAbout Amazon SageMaker:\n\nEndpoint Management : Mastered SageMaker endpoint lifecycle (creation, invocation, monitoring, deletion) Instance Types : Learned about ml.g5 instances for GPU-accelerated inference and cost vs performance trade-offs Runtime API : Deep dive into sagemaker-runtime.invoke_endpoint() , request/response formatting, and error handling Cost Optimization : Learned strategies to minimize costs ($2.82/hour for two endpoints) through smart instance sizing and endpoint lifecycle management\n\nAbout Multi-Backend Architecture:\n\nEnvironment-Based Configuration : Learned to use environment variables ( MODEL_TYPE ) for backend selection without code changes Adapter Pattern : Mastered creating adapters that make different APIs look the same to higher-level code Graceful Degradation : Learned to build systems that work in both full production mode and limited demo mode\n\nTechnical Skills:\n\nVector mathematics (cosine similarity, dot products, norms) SageMaker endpoint deployment and management NVIDIA NIM integration patterns Advanced prompt engineering for RAG systems Full-stack TypeScript/Python with multiple AI backends"},{"heading":"What's next for Skyrchitect","content":"NVIDIA NIM Enhancements:\n\nDeploy Larger Llama Models - Upgrade to Llama 3.1 70B or 405B on larger SageMaker instances for even better reasoning Advanced RAG - Implement vector database (FAISS or Pinecone) for larger knowledge bases with thousands of architecture patterns Fine-Tuned Embeddings - Train custom embedding models specifically for cloud architecture similarity Multi-Modal NIMs - Integrate NVIDIA's vision NIMs for analyzing architecture diagrams directly\n\nImmediate Enhancements:\n\nActual Deployment Execution - Connect to AWS CloudFormation/Terraform Cloud to actually deploy the generated architectures Real-Time Cost Tracking - Integrate with AWS Cost Explorer API to show actual costs of deployed resources Architecture Monitoring Agent - Create a second AI agent (using Llama) that monitors deployed architectures and suggests optimizations\n\nAdvanced Multi-Agent System:\n\nSpecialized NVIDIA NIM Agents : Security Agent : Llama-based security auditing and hardening Performance Agent : Analyzes metrics using embedding similarity Cost Agent : Continuous optimization with RAG-based recommendations Agent Collaboration - Multiple Llama instances working together on different aspects of architecture\n\nRAG System Evolution:\n\nDynamic Knowledge Base - Agent learns from successful deployments and adds new patterns to knowledge base User Feedback Loop - Users rate architecture suggestions, improving retrieval relevance over time Cross-Architecture Patterns - Use embedding NIM to find similarities between different architecture types\n\nEnterprise Features:\n\nTeam Collaboration - Multi-user architecture design with version control Compliance Templates - Pre-built architectures for HIPAA, SOC2, PCI-DSS with embedded compliance patterns Migration Planning - Agent analyzes existing infrastructure and plans cloud migration using RAG\n\nNVIDIA GPU Optimization:\n\nBatch Processing - Process multiple architecture requests in parallel on the same GPU instance Model Optimization - Use NVIDIA TensorRT for faster inference Mixed Precision - Leverage FP16 for faster embedding generation\n\nI envision Skyrchitect becoming the premier NVIDIA NIM-powered cloud architecture platform‚Äîdemonstrating how combining Llama's reasoning with specialized embedding models creates truly intelligent, context-aware AI agents."},{"heading":"How Skyrchitect Meets NVIDIA-AWS Hackathon Judging Criteria","content":"‚úÖ Technological Implementation (Primary Criteria)\n\nNVIDIA Llama 3.1 Nemotron Nano 8B : Core LLM deployed on SageMaker for autonomous reasoning NVIDIA Retrieval Embedding NIM : Semantic search engine for RAG, achieving 0.900+ similarity scores Amazon SageMaker Deployment : Both NIMs running on ml.g5.xlarge GPU instances Production-Ready : Complete SageMaker integration with proper error handling Strands Agents Framework : Full agent loop with autonomous tool calling Custom Tools : 5 domain-specific tools for cloud architecture decisions RAG Implementation : Complete retrieval pipeline from embedding to context enhancement\n\n‚úÖ Design\n\nClean Architecture : Modular backend with separate adapters for SageMaker NIMs Framework Abstraction : Strands Agent works with both Bedrock and SageMaker backends Dual-Mode Operation : Graceful degradation from production (real NIMs) to demo (fallback) Well-Documented : Comprehensive docs explaining NVIDIA NIM integration Type Safety : Full type hints in Python, TypeScript in frontend\n\n‚úÖ Potential Impact\n\nDemocratization : Makes enterprise cloud architecture accessible using open-source Llama Time Savings : RAG-enhanced recommendations are more accurate and faster Cost Reduction : NVIDIA GPU acceleration enables real-time architecture analysis Learning Tool : RAG system teaches users best practices through pattern matching Open Innovation : Demonstrates NVIDIA NIMs in production-scale application\n\n‚úÖ Quality of Idea\n\nNovel Application : First cloud architecture agent using NVIDIA NIMs with RAG Real Problem : Solves actual pain point in cloud development Technical Innovation : Unique combination of Llama reasoning + embedding-based retrieval Scalable Solution : Architecture supports thousands of patterns in knowledge base Future-Proof : Designed for larger Llama models and advanced NVIDIA NIMs"},{"heading":"NVIDIA-AWS Hackathon Compliance","content":"Required Components:\n\n‚úÖ Llama-3.1-Nemotron-Nano-8B-v1 : Deployed on SageMaker endpoint nvidia-llama-nemotron-endpoint\n\n‚úÖ Retrieval Embedding NIM : Deployed on SageMaker endpoint nvidia-embedding-nim-endpoint\n\n‚úÖ Amazon SageMaker : Both NIMs running on ml.g5.xlarge instances with NVIDIA GPUs\n\n‚úÖ Agentic AI Application : Autonomous multi-step reasoning with tool calling\n\n‚úÖ RAG Implementation : Complete retrieval-augmented generation workflow\n\nCode Evidence:\n\nbackend/models/sagemaker_model.py - Llama NIM integration backend/utils/embedding_nim.py - Embedding NIM with RAG backend/agents/architecture_agent_sagemaker.py - Agent using both NIMs deploy_sagemaker_endpoints.sh - SageMaker deployment automation test_sagemaker_integration.py - Integration test suite\n\nUnique Differentiators:\n\nDual Hackathon Support : One codebase works for both NVIDIA-AWS and AWS AI Agent hackathons Production RAG : Real semantic search with measurable relevance metrics (0.900 similarity) Framework Innovation : First Strands Agent implementation with SageMaker NIMs Zero UI Changes : Backend completely swappable‚Äîdemonstrates clean architecture Developer Experience : Fallback mode enables development without deploying expensive endpoints\n\nSkyrchitect demonstrates the power of NVIDIA NIMs on Amazon SageMaker for building intelligent, autonomous AI agents that solve real-world problems with retrieval-augmented generation.\n\nBuilt for NVIDIA-AWS AI Agent Global Hackathon 2025 Leveraging NVIDIA Llama 3.1 Nemotron Nano 8B + NVIDIA Retrieval Embedding NIM on Amazon SageMaker"},{"heading":"Built With","content":"llama-3.1-nemotron-nano-8b-v1 nvida"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Nomi","project_url":"https://devpost.com/software/nomi-aoim58","tagline":"A Multi-Sensor Network System for Seniors","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/934/958/datas/medium.png","prizes":[{"hackathon_name":"Agentic AI Unleashed: AWS & NVIDIA Hackathon","hackathon_url":"https://nvidia-aws.devpost.com/","prize_name":"Grand Prize"}],"team_members":[],"built_with":[{"name":"arduinoide","url":null},{"name":"awsapigateway","url":null},{"name":"awsdynamo","url":null},{"name":"awslambda","url":null},{"name":"awssagemaker","url":null},{"name":"boto3awssdk","url":null},{"name":"db","url":null},{"name":"esp32devboard","url":null},{"name":"fastapi","url":null},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"jupyternotebook","url":null},{"name":"mediapipe","url":null},{"name":"nodered","url":null},{"name":"nvidianim","url":null},{"name":"opencvimagerecognition","url":null},{"name":"pulsesensoramped","url":null},{"name":"pydantic","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"resistors","url":null},{"name":"thinpressurefilmsensor","url":null},{"name":"virtualenvironment","url":null}],"external_links":[{"label":"crustaly.github.io","url":"https://crustaly.github.io/nomi/"}],"description_sections":[{"heading":"Inspiration","content":"Last year, my grandmother in India slipped on the stairs while home alone. That moment exposed how fragile independence becomes with age, and how much we still rely on chance instead of technology for elder care. Her story isn‚Äôt unique. By 2050, more than 1 in 6 people worldwide will be over 65, totaling 1.6 billion seniors (UN, 2024). Yet, the global caregiver shortage is projected to reach 13.5 million unfilled roles by 2040 (WHO). In the U.S. alone, falls are the leading cause of injury-related death among seniors, costing over $50 billion annually (CDC, 2023). NOMI was created to change that ‚Äî combining sensor networks and Agentic AI to detect risks, summarize health insights, and alert caregivers in real time. Because no one‚Äôs grandmother should ever be left waiting for help."},{"heading":"What it does","content":"Nomi is an intelligent home health assistant designed to support elderly individuals and their caregivers through continuous, privacy-preserving monitoring. It tackles one of the biggest social challenges of our time ‚Äî aging in place safely ‚Äî by turning low-cost sensors into real-time insights powered by AI reasoning.\n\nWe built Nomi because millions of seniors live alone or rely on family members who can‚Äôt be present 24/7. Missed medications, unnoticed falls, or silent health declines can have life-changing consequences. Nomi bridges that gap ‚Äî quietly, respectfully, and intelligently.\n\nHere‚Äôs how each feature addresses a real-world problem:\n\nüïë Medication Detector\n\nThe problem: Many seniors forget or double-dose medication, especially when routines change. Our solution: A thin pressure film sensor under a pillbox detects when it‚Äôs opened. Combined with time tracking in DynamoDB, Nomi infers whether medication was taken ‚Äî or missed ‚Äî and includes this in its AI reasoning summary.\n\nüçΩÔ∏è Eating Activity Detector\n\nThe problem: Poor nutrition and irregular meals often go unnoticed until weight loss or fatigue appear. Our solution: Pressure sensors under utensils or trays detect eating events. Nomi tracks patterns across days, identifying skipped meals and alerting caregivers to subtle changes before they become health risks.\n\nü§ï Fall & Posture Detector\n\nThe problem: Falls are the leading cause of injury-related hospital visits among older adults, and many go unreported. Our solution: OpenCV + MediaPipe analyze posture locally on-device ‚Äî no video leaves the room. When a ‚Äúfallen‚Äù or ‚Äúinactive‚Äù posture is detected, Nomi triggers an immediate email alert to the caregiver.\n\n‚ù§Ô∏è Vital Signs Monitor\n\nThe problem: Caregivers often have no continuous view of heart rate, oxygen, or room conditions. Our solution: A Pulse Sensor Amped tracks heart rate and oxygen proxies, while temperature-humidity sensors monitor environment comfort. Data streams through AWS Lambda ‚Üí DynamoDB ‚Üí FastAPI ‚Üí React, updating live charts in the dashboard.\n\nüß† AI Health Insights\n\nThe problem: Even with data, caregivers struggle to interpret trends. Our solution: NVIDIA NIM (Llama-3.1-Nemotron-8B) interprets sensor data like a virtual health coach ‚Äî summarizing patterns (‚ÄúHeart rate spiked after standing‚Äù), spotting risks, and offering plain-language recommendations.\n\nüì© Caregiver Alerts\n\nThe problem: Emergencies often go unnoticed for hours. Our solution: When abnormal vitals or a fall are detected, AWS SNS or Gmail SMTP instantly emails caregivers with event time, vitals, and recommendations ‚Äî turning sensor data into timely action.\n\nüîí Privacy by Design\n\nThe problem: Constant monitoring can feel invasive. Our solution: All video-based detection happens locally using OpenCV and MediaPipe; only anonymized event labels reach the cloud. Nomi balances safety with dignity."},{"heading":"How we built it","content":"TL;DR (too long; didn‚Äôt read): We turned low-cost sensors on an ESP32 into caregiver-ready insights by streaming data to DynamoDB, fusing it in a FastAPI service, and asking NVIDIA‚Äôs Nemotron-based NIM to produce clear, structured summaries that render live in a React dashboard, with optional on-device OpenCV/MediaPipe fall detection for privacy.\n\nIn full detail: 1Ô∏è‚É£ Hardware ‚Üí Cloud We started with an ESP32 DevBoard, a pulse sensor, a thin pressure film, and a temperature-humidity sensor‚Äîbasically a mini hospital taped to our table. After days of debugging serial output at 3 AM (‚Äúwhy is my heartbeat 300 bpm?? oh right, short circuit‚Äù), the ESP32 finally sent clean data via HTTP to AWS API Gateway. From there, a Lambda function validated readings and stored them in DynamoDB, creating our realtime data pipeline.\n\n2Ô∏è‚É£ Backend & AI Reasoning Our FastAPI backend became the translator between sensors and sense-making. It pulls data from DynamoDB and asks NVIDIA NIM to summarize what‚Äôs going on‚Äîheart rate trends, posture changes, fall risks, and daily insights. The NVIDIA NIM uses Llama 3.1‚Äôs LLM Model to provide real-world context to data. We basically taught an AI to play doctor‚Ä¶ responsibly. Each record (heart rate, oxygen, posture, eating/meds, environment) is validated by Pydantic and stored with a resident/time key pattern in DynamoDB. We used Boto3 for all read/write ops.\n\n3Ô∏è‚É£ Frontend Dashboard The React + TailwindCSS dashboard came next. It shows live vitals, posture, and environment data in Recharts, with clean cards and color cues. The ‚ÄúInsights‚Äù card uses NIM‚Äôs output to explain what‚Äôs happening in plain English, no doctor‚Äôs degree required. It‚Äôs designed for clarity. We use large fonts, color cues, and an ‚Äúalerts‚Äù bar for quick caregiver attention.\n\n4Ô∏è‚É£ Alerts & Privacy Falls and abnormal vitals trigger AWS SNS or Gmail SMTP alerts that hit your inbox in seconds. For privacy, OpenCV + MediaPipe handle posture and fall detection locally, so your grandma‚Äôs living room doesn‚Äôt end up in the cloud. Only event labels reach the cloud.\n\n5Ô∏è‚É£ Development & Testing We tested everything in Jupyter Notebooks, built virtual environments that somehow always broke on someone‚Äôs laptop, and celebrated the first time a fall alert email actually arrived. That moment felt like magic! The entire workflow runs locally with FastAPI + React, then scales seamlessly to AWS for deployment."},{"heading":"Challenges we ran into","content":"AWS SageMaker deployment permissions unexpectedly blocked our NIM endpoint. The challenge mandated deployment on AWS SageMaker or EKS, and we successfully designed our pipeline for it. We initially deployed and ran the Llama-3.1-Nemotron-Nano-8B-v1 NIM on a SageMaker endpoint. However, mid-development, the provided IAM role permissions were modified by the environment administrator, leading to a cascade of fatal errors: we hit multiple AccessDeniedException and InvalidClientTokenId errors on crucial commands like sagemaker:CreateEndpointConfig and for GPU instances like ml.g5.xlarge. Our architectural pipeline was intact, but the infrastructure lock-out forced us to pivot from a managed SageMaker deployment to an API-only local proxy. We want to emphasize that our architecture is fully compatible with, and was originally deployed on, the required AWS services.\n\nConnecting multiple Arduino-based sensors to a centralized AWS DynamoDB was way harder than it sounded. We had heart rate, pressure, and temperature readings all coming in at different frequencies ‚Äî sometimes spiking, sometimes silent ‚Äî and Dynamo really doesn‚Äôt like inconsistent payloads. We spent hours debugging JSON formats, timestamp mismatches, and authentication from the ESP32 before finally creating a Lambda middle layer to normalize everything. The moment the first clean DynamoDB table populated felt like watching the data universe align.\n\nTesting OpenCV + MediaPipe posture detection on live humans was surprisingly tricky. Lighting, camera angle, and motion blur all confused the fall-detection logic. We learned the hard way that ‚Äúpretending to fall‚Äù isn‚Äôt easy or safe ‚Äî and that a laptop webcam has no mercy for bad posture.\n\nObviously, no one wants to keep a 24-hour camera and sensor feed running during judging. So we had to rethink our testing strategy, and built a Dynamo DB Sample data file, to help us show the entire reasoning and alert system without needing a live person constantly standing, eating, or ‚Äúfalling.‚Äù"},{"heading":"Accomplishments that we're proud of","content":"We designed and 3D-printed the Nomi camera stand ourselves using CAD, then connected all the sensors to a centralized AWS DynamoDB system‚Äîa tough but rewarding challenge. This was our first time building an Agentic AI project that combined both physical hardware and intelligent reasoning, giving Nomi real-world impact rather than being just another software demo. The system is intentionally low-cost and accessible.\n\nA major breakthrough was learning how to directly query and process data from DynamoDB, parsing live sensor inputs seamlessly into meaningful insights. Using Pydantic models, we structured Nomi‚Äôs data pipeline and reasoning flow so that database retrieval and AI output remain tightly integrated."},{"heading":"What we learned","content":"1Ô∏è‚É£ Structuring reasoning between LLMs and real data We learned how to connect the dots between raw DynamoDB records and NVIDIA NIM‚Äôs language reasoning. It‚Äôs not just ‚Äúthrow the data at the model‚Äù ‚Äî you have to decide what context matters, normalize units, and prompt the LLM to reason in a structured way. That design step turned out to be as important as the model itself.\n\n2Ô∏è‚É£ Hardware debugging builds character We learned that ‚Äúsimple‚Äù sensors can produce wildly unpredictable signals. Force sensors drift, humidity sensors lie, and pulse sensors sometimes detect your Wi-Fi instead of your heartbeat. Patience and calibration became part of our skill set.\n\n3Ô∏è‚É£ Schema discipline saves lives (and data) Working across microcontrollers ‚Üí Lambda ‚Üí DynamoDB ‚Üí FastAPI ‚Üí React taught us one golden rule: consistent data schema or nothing works. Pydantic became our peacekeeper, enforcing structure when our sensors wanted chaos.\n\n4Ô∏è‚É£ Creative problem-solving beats permissions In the hackathon sandbox, SageMaker endpoints were blocked, so we built a local NIM proxy that mimicked AWS inference behavior. It taught us that limits often force the most inventive solutions ‚Äî and sometimes the fake endpoint works just as well as the real one. However, our project still runs on SageMaker, but it requires setup on the user end.\n\n5Ô∏è‚É£ Frontend matters more than we expected You can have the smartest backend in the world, but if the dashboard isn‚Äôt clear, the caregiver won‚Äôt trust it. Designing simple, readable insights turned out to be a real test of empathy and UI clarity."},{"heading":"What's next for Nomi","content":"Real-time streaming with MQTT: Right now, our ESP32 sends data in batches. Next, we‚Äôll switch to an MQTT-based pipeline for true real-time updates. This will let caregivers see vitals instantly, not just in short refresh intervals, and enable smoother integration with AWS IoT Core for better scalability.\n\nExpanded sensor ecosystem: Beyond pulse, pressure, and posture, we‚Äôre prototyping new modules for blood oxygen (SpO‚ÇÇ) and activity tracking. Our goal is modular plug-and-play ‚Äî any sensor, any data source, one unified dashboard."},{"heading":"Built With","content":"arduinoide awsapigateway awsdynamo awslambda awssagemaker boto3awssdk db esp32devboard fastapi json jupyternotebook mediapipe nodered nvidianim opencvimagerecognition pulsesensoramped pydantic react resistors thinpressurefilmsensor virtualenvironment"},{"heading":"Try it out","content":"crustaly.github.io"}]},{"project_title":"Agent Builder","project_url":"https://devpost.com/software/agent-builder","tagline":"Build powerful AI agent workflows through natural conversation. Simply describe what you need, and watch as intelligent agents, tools, and connections come together‚Äîno coding required.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/935/984/datas/medium.png","prizes":[{"hackathon_name":"Agentic AI Unleashed: AWS & NVIDIA Hackathon","hackathon_url":"https://nvidia-aws.devpost.com/","prize_name":"Third Place"}],"team_members":[],"built_with":[{"name":"agent","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"workflow","url":null}],"external_links":[],"description_sections":[{"heading":"üéØ Inspiration","content":"The inspiration for Agent Builder came from a simple observation: building AI agent workflows is incredibly powerful but unnecessarily complex. Data scientists and developers spend countless hours writing boilerplate code, configuring integrations, and manually wiring together agents and tools‚Äîtime that could be better spent on solving actual business problems. We envisioned a world where anyone could describe what they want in plain English and watch a production-ready AI workflow materialize before their eyes.\n\nWe were particularly inspired by the democratization potential of NVIDIA's NIM ecosystem. The combination of powerful reasoning models like llama-3.1-nemotron-nano-8B-v1 and efficient retrieval embeddings like nv-embedqa-e5-v5 opened up possibilities for truly intelligent workflow generation‚Äînot just template filling, but genuine understanding of user intent and intelligent component selection. We wanted to prove that with the right architecture, AI could not only understand what you want to build but actually build it for you."},{"heading":"üí° What it does","content":"Agent Builder is a conversational AI workflow platform that transforms natural language descriptions into fully deployable agent workflows. Instead of dragging, dropping, and configuring nodes manually, users simply chat with an AI assistant that understands their requirements and generates complete workflows automatically.\n\nCore Features:\n\nAI Chat Interface : Describe workflows in natural language (e.g., \"Create a code review workflow with Git integration\") Intelligent Workflow Generation : Automatically selects appropriate agents, tools, and connections based on semantic understanding Visual Flow Builder : Drag-and-drop canvas for manual workflow creation and AI-generated workflow refinement Smart Catalog Management : Semantic search over agents and tools using natural language queries One-Click Deployment : Deploy workflows as containerized microservices to AWS ECS/Lambda Real-time Testing : Test workflows with live input and see step-by-step execution Dual Mode Operation : Works in dummy mode for demos or connects to backend services for production\n\nExample Use Case : A user types \"I want to analyze customer feedback and generate reports.\" The AI Chat immediately generates a workflow with a Sentiment Analyzer agent, Data Analyzer agent, and Text Summarizer, all properly connected with the appropriate tools (Database Connector, PDF Generator) attached‚Äîready to deploy in seconds."},{"heading":"üî® How we built it","content":"Architecture Overview\n\nWe built Agent Builder using a multi-service microarchitecture designed for scalability and intelligent automation:\n\nFrontend Layer (React + Vite + TypeScript)\n\nVisual flow canvas powered by ReactFlow AI Chat interface with streaming responses Zustand for state management Real-time workflow visualization\n\nOrchestration Service (Node.js + Express)\n\nRESTful API for flow management WebSocket support for deployment updates PostgreSQL for persistent storage Redis for caching and session management\n\nLLM Service (Python + FastAPI)\n\nllama-3.1-nemotron-nano-8B-v1 NIM for workflow reasoning and JSON generation nvidia/nv-embedqa-e5-v5 for semantic retrieval and catalog search LangChain for prompt engineering and schema validation Vector database (Pinecone/Weaviate) for agent/tool indexing\n\nDeployment Service (Node.js + AWS SDK)\n\nDocker containerization for generated agents ECS and Lambda deployment orchestration S3 for artifact storage\n\nNVIDIA NIM Integration\n\nReasoning with llama-3.1-nemotron-nano-8B-v1:\n\n# Augmented prompt with retrieved context system_prompt = \"\"\"You are an AI workflow architect. Generate valid workflow JSON with agents, tools, and connections. Available components: {retrieved_components}\"\"\" response = llm_nim.invoke({ \"model\": \"llama-3.1-nemotron-nano-8B-v1\", \"messages\": [ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_intent} ], \"json_schema\": flow_graph_schema # Constrained decoding })\n\nRetrieval with nv-embedqa-e5-v5:\n\n# Semantic search for relevant components query_embedding = embedding_nim.embed( model=\"nvidia/nv-embedqa-e5-v5\", input=\"code review with git integration\" ) relevant_agents = vector_db.search( query_embedding, collection=\"catalog_agents\", top_k=5 )\n\nKey Technical Decisions\n\nConstrained JSON Generation : Used schema-guided decoding to ensure llama-3.1-nemotron outputs valid workflow structures Pre-computed Embeddings : All catalog items indexed offline for sub-millisecond retrieval Streaming Responses : LLM outputs streamed to frontend for better UX Marker-based Edge Resolution : Used __INPUT__ and __OUTPUT__ markers to connect generated nodes to existing flow boundaries Handle-based Tool Connections : Implemented specific connection handles (e.g., tool-bottom ) for proper agent-to-tool relationships"},{"heading":"üöß Challenges we ran into","content":"1. JSON Schema Reliability Getting llama-3.1-nemotron to consistently generate valid workflow JSON was initially challenging. The model would sometimes hallucinate node IDs or create invalid edge connections. We solved this by implementing constrained decoding with strict JSON schemas and adding post-validation layers that catch and repair common structural errors.\n\n2. Edge Connection Complexity Managing different types of connections (main flow vs. tool attachments) required careful design. We implemented a handle-based system where agents have multiple connection points ( tool-top , tool-bottom ) and used sourceHandle / targetHandle fields in edges to specify exact connection points. This required updates to our type definitions and careful mapping in the FlowCanvas component.\n\n3. Real-time Synchronization Keeping the visual canvas in sync with the underlying flow state when AI Chat generates new nodes proved tricky. ReactFlow's state management conflicted with our Zustand store updates. We solved this by implementing dual useEffect hooks that watch for changes in both nodes and edges separately, with careful change detection to avoid infinite update loops.\n\n4. LocalStorage Cache Invalidation When we added new default items (like the Git Tool), they wouldn't appear because localStorage was overwriting the defaults. We implemented a merge strategy that combines stored data with new defaults, ensuring backwards compatibility while allowing new features to appear automatically.\n\n5. Context Window Management Balancing the amount of context (conversation history + catalog data) sent to llama-3.1-nemotron within token limits required careful prompt engineering. We implemented a sliding window approach for conversation history and used the embedding model to retrieve only the most relevant catalog items rather than sending the entire catalog."},{"heading":"üèÜ Accomplishments that we're proud of","content":"1. True Natural Language to Deployment We achieved our core vision: users can literally speak a workflow into existence and deploy it to production infrastructure‚Äîall in under 30 seconds. The combination of llama-3.1-nemotron's reasoning and nv-embedqa-e5-v5's retrieval creates genuinely intelligent workflow generation, not just template filling.\n\n2. Semantic Intelligence The embedding-based catalog search is remarkably intuitive. Users can search for \"something to validate data\" and get Data Analyzer, Schema Validator, and Content Moderator‚Äîsemantically related results that would never match with keyword search.\n\n3. Visual + Conversational UX We successfully merged two interaction paradigms: traditional drag-and-drop flow building and conversational AI. Users can start with AI Chat to get 80% of the way there, then fine-tune manually on the canvas‚Äîthe best of both worlds.\n\n4. Production-Ready Architecture This isn't just a demo‚Äîthe architecture supports real deployments with Docker containerization, automatic scaling, and proper error handling. Generated workflows become independent microservices with their own resource allocation.\n\n5. Dual-Mode Flexibility The dummy mode allows instant demos without backend dependencies, while real mode provides full production capabilities. This made development faster and demos more reliable."},{"heading":"üìö What we learned","content":"Technical Learnings:\n\nConstrained Decoding is Essential : Free-form LLM output is too unreliable for structured generation. Schema-guided decoding dramatically improved consistency. Embeddings Enable Magic : Semantic search with nv-embedqa-e5-v5 creates surprisingly intelligent behavior‚Äîthe model \"understands\" user intent in ways keyword matching never could. Context is King : The quality of llama-3.1-nemotron's reasoning heavily depends on the retrieved context. Good retrieval makes good generation. Handle Complexity Matters : ReactFlow's handle system is powerful but requires careful state management and type definitions to work correctly with dynamic graphs.\n\nAI/ML Insights:\n\nSmall Models Can Be Powerful : llama-3.1-nemotron-nano-8B-v1 proved that you don't need massive models for structured reasoning tasks‚Äîthe \"nano\" variant handles JSON generation beautifully with much lower latency. Retrieval-Augmented Generation Works : Combining embedding-based retrieval with LLM reasoning is incredibly effective‚Äîthe model performs better with 5 retrieved examples than with 100 examples in the prompt. Streaming Improves UX : Even though total generation time is the same, streaming tokens make the system feel 3x faster to users.\n\nProduct Insights:\n\nConversation as Interface : Users intuitively understand how to describe workflows‚Äîno training needed. Natural language truly democratizes AI development. Visual Feedback is Critical : Seeing nodes appear on the canvas in real-time creates a magical experience that text-based tools can't match. Fallbacks Build Trust : Having pattern-matching fallbacks when the LLM is unavailable ensures the system always responds, building user confidence."},{"heading":"üöÄ What's next for Agent Builder","content":"1. Enhanced LLM Reasoning\n\nMulti-turn workflow refinement: \"Actually, add validation before the analyzer\" Automatic error handling node insertion based on failure patterns Cost optimization suggestions: \"This could be 30% cheaper with batching\"\n\n2. Expanded Component Library\n\nIntegration with NVIDIA NIM catalog for automatic agent discovery Community marketplace for sharing custom agents and workflows Pre-built templates for common patterns (ETL, customer service, content moderation)\n\n3. Advanced Retrieval\n\nHybrid search combining embeddings + metadata filters Workflow similarity search: \"Find workflows like this one\" Automatic component recommendations based on flow context"},{"heading":"üé¨ Conclusion","content":"Agent Builder represents a fundamental shift in how we build AI systems: from code-first to conversation-first. By leveraging NVIDIA's llama-3.1-nemotron-nano-8B-v1 for intelligent reasoning and nv-embedqa-e5-v5 for semantic understanding, we've created a platform where anyone can build production-grade AI workflows through natural conversation. This is just the beginning‚Äîwe're excited to see how the community uses this foundation to democratize AI development and unlock new possibilities we haven't even imagined yet.\n\nTalk to Build. Deploy to Scale. üöÄ"},{"heading":"Built With","content":"agent node.js python react workflow"}]}],"generated_at":"2026-02-18T16:53:37.811158Z"}}