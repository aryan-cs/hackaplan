{"version":"v1","hackathon_url":"https://treehacks-2025.devpost.com","generated_at":"2026-02-18T16:39:41.589429Z","result":{"hackathon":{"name":"TreeHacks 2025","url":"https://treehacks-2025.devpost.com","gallery_url":"https://treehacks-2025.devpost.com/project-gallery","scanned_pages":11,"scanned_projects":257,"winner_count":70},"winners":[{"project_title":"Hearti","project_url":"https://devpost.com/software/hearti","tagline":"Simplifying congenital heart disease diagnosing, and treatment planning, and medical explanations for doctors and patients using computer vision, machine learning, VLMs, and more.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/000/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Most Impactful Hack (JBL Partybox 110 per team member)"}],"team_members":[],"built_with":[{"name":"elevenlabs","url":null},{"name":"fastapi","url":null},{"name":"lumaai","url":null},{"name":"meta-quest","url":null},{"name":"next.js","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"nvidia","url":"https://devpost.com/software/built-with/nvidia"},{"name":"plotly","url":"https://devpost.com/software/built-with/plotly"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react.js","url":null},{"name":"tailwindcss","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vespaai","url":null}],"external_links":[{"label":"heart-ai-seven.vercel.app","url":"https://heart-ai-seven.vercel.app"},{"label":"github.com","url":"https://github.com/ethan-tam33/heartAI"}],"description_sections":[{"heading":"Inspiration","content":"Congenital heart disease (CHD) is the most common birth defect in the United States, and thanks to advances in medicine, more children with CHD are surviving into adulthood. But with this improved survival comes a lifelong need for careful monitoring - as symptoms can change over time. Recognizing this challenge, we set out to build an AI-powered web application that helps both doctors and patients navigate the complexities of CHD. Our vision is simple: provide a tool that offers clear, reliable insights for global healthcare professionals and their patients while ensuring that patients receive a smooth transition into their medical journey."},{"heading":"What it does","content":"Interactive 3D model of the input 2D heart. Instead of relying on flat 2D images, users can explore a fully interactive 3D representation of the heart. This provides doctors and patients with a clearer, more intuitive understanding of the condition. Multi-classification of CHD categories. The model identifies the most likely congenital heart disease (CHD) categories based on the uploaded scan, helping users understand potential diagnoses. Severity Classification. Users can also view how serious their CHD symptoms are. Model Confidence . We understand that AI may not always be correct, so a confidence score is included in the web application to guide a doctor’s conclusion with the AI findings. Feedback Form. For doctors, we have a space for them to accept or reject the model’s findings and explain their reasonings. This will help better fine-tune the model and ensure continuous improvement in its accuracy and reliability for diagnosing CHD conditions. AI Voice Chat. Guided by the previous datapoints, the AI voice chat is ready to discuss with the user any questions they have about the model's output and reasoning. Custom Heart Segmentation Model Currently, CMRI segmentation is performed manually to ensure accuracy. To automate this process, we developed a custom segmentation model trained on an NVIDIA A100 GPU using Brev Compute. While the model requires additional data and refinement for full accuracy, it successfully identified regions of interest despite the limited training dataset. Our architecture is specifically designed for cardiac imaging, labeling 3D volumes into eight major heart structures (four chambers and four arteries/veins). With further data and training, we aim to eliminate the need for manual segmentation in the pipeline."},{"heading":"How we built it","content":"Surface Intensity and Segmented Heart Models\n\nCombining density and resonance information from the CMRI and the segmentation boundaries, we built an algorithm that can map and shade the volume of the heart based on the intensity of the MRI scan. This algorithm removes the background, carefully maps MRI data to volumes of the heart. This ensures that we do not impart any unwanted artifacts or information, an integral safety feature for this application. In addition to mapping MRI intensity, we also implemented an algorithm that generates smooth meshes using image processing techniques to provide an easy-to-read interpretation of the heart’s many shapes and volumes, without focusing too much on the color of the MRI data. We realized that all of this data can take quite a bit of memory to render, so we created optimized models that prioritize surfaces over volume, use sparse views to provide a lightweight model, and have the ability to select which specific major volumes of the heart you would like to view at a time. Embedding similarity using VespaAI We converted our data into vector embeddings paired with CHD labels and leveraged VespaAI to run precise, rapid queries. By using an angular metric, we account for both the direction and magnitude of our high-dimensional data. Web App Tech Stack For the frontend and backend, we used FastAPI, React.js, Next.js, Typescript, and TailwindCSS."},{"heading":"Challenges we ran into","content":"While the dataset we found online was extremely useful and new, it was very limited (59 hearts only). To train a truly accurate model, we need a much larger sample size."},{"heading":"Accomplishments that we're proud of","content":"-We successfully built and rendered 3D representations of 2D heart CMRI scans. Besides looking extremely cool, it is both new and valuable for the ways in which we treat people with CHD . - Optimized the models to prioritize surface pixels, smooth representations, and segment-specific views to make them more accessible across devices and browsers. -We built a web-based AI agent that can not only predict a user’s CHD conditions but can also act as a source of medical information for decades . If inputted with enough patient data, it can even more effectively provide accurate information that doctors can approve."},{"heading":"What we learned","content":"We had a lot of ideas for this project, so we needed to quickly define what was most relevant and useful for our target audience (not what sounded the coolest). We moved quickly from talking with each other frequently and truly as a team. Some technical highlights: learned how to process research/medical data into more intricate + interpretable visualizations that capture features of the heart while minimizing artifacts, learned about different metrics to assess how correlated 2 embeddings are, and learned how to learn quickly using VespaAI’s querying functionality."},{"heading":"What's next for Hearti","content":"While building Hearti, we considering incorporating AI-style transfer to turn the MRI data insights into realistic textures of real heart tissue. While exciting, this has a lot of challenges that have to be approached carefully because hallucinations can have consequences. Allowing Hearti to take advantage of a greater range of medical imaging techniques and scans. It would be great if we could upload a full-body or chest MRI and have Hearti extract the important regions and use our model to create stunning visuals of the heart. Incorporating time series data would be interesting because it would allow us to make use of our optimized models to create 4D reconstructions of the heart. We aim to expand our custom segmentation model’s capabilities by processing more heart scans directly and enhancing automation in cardiac imaging."},{"heading":"Built With","content":"elevenlabs fastapi lumaai meta-quest next.js numpy nvidia plotly python pytorch react.js tailwindcss typescript vespaai"},{"heading":"Try it out","content":"heart-ai-seven.vercel.app github.com"}]},{"project_title":"OmNom","project_url":"https://devpost.com/software/omnom-hg16v3","tagline":"OmNom is your late night food savior, autonomously navigating a variety of outdoor and indoor environments across campus to bring back your cravings just in time for you to finish your PSET.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/322/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Most Creative Hack (Pioneer DJ DDJ-FLX4 Pack per team member)"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"clip","url":null},{"name":"dpt","url":null},{"name":"fast-api","url":null},{"name":"google-maps-places-api","url":null},{"name":"jetson-nano-c++","url":null},{"name":"lasercutting","url":null},{"name":"matplot-lib-v0.dev","url":null},{"name":"metal","url":null},{"name":"next.js","url":null},{"name":"open-cv","url":null},{"name":"open-route-service","url":null},{"name":"openai-api","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"roboclaw","url":null},{"name":"soldering","url":null},{"name":"tailwindcss","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null},{"name":"vit","url":null},{"name":"vite.js","url":null},{"name":"websockets-api","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/NilsIrl/ActuallyAHuman/tree/main"},{"label":"om-nom-ordering.vercel.app","url":"https://om-nom-ordering.vercel.app/"}],"description_sections":[{"heading":"Mechanical design","content":"Fabrication\n\nOm nom was built from scratch, with no pre existing assemblies or electronics used. A lot of Om nom is built from laser cut sheet metal, and we made 10 different sheet metal lasercut parts (22 total on the robot).\n\nOm nom's drivetrain uses 6\" wheels and repurposes a moving dolly. It has single loop chain wraps on each side\n\n5 degree of freedom arm\n\nOm nom uses a kevlar rope driven lift for vertical lift, and a horizontal lift with a servo and a 2 bar linkage. Om nom has a pan and tilt for more fine alignment, and a 250mm extension driven by a 2 bar linkage with a stylus on the end to be able to interact with touchscreens. On the extension is a wide angle camera, to be able to more precisely align with screens and center the stylus on what to order."},{"heading":"Rope","content":"Om nom holds food by tying it to a rope attached at the top of the lift. The user has to \"cut the rope\" in order to get their food."},{"heading":"Electrical System","content":"Electronic modules\n\nOm nom's electrical system is the integration of a variety of modules: Jetson Nano Orin - Om nom's brains Arduino Mega - PWM Servo Control Custom servo power injector board Arduino UNO - IMU interface 2x Roboclaw 2x15A - Motor controllers 10 sensors\n\nWe opted to have mostly everything interface with the Jetson Nano Orin via USB - enabling easier development and testing of interfaces on a laptop. For the cases where USB wasn't possible, we gave Om nom a dedicated microcontroller for each interface, and programmed the microcontroller to convert the interface to USB.\n\nSensors\n\nWe have 10 sensors onboard: -3 quadrature motor encoders (left drive, right drive, lift) -3 current sensors (one per motor) -1 IMU (BNO055) -1 GPS with RTK Dead Reckoning (Ublox Zed F9R) -1 Stereoscopic camera (spatial recognition) -1 wide angle camera (ordering and interaction)\n\npower management\n\nMaking a safe power management solution for a robot of this size was a significant challenge. The motors and Jetson board run off of 12 volts, and we needed to be able to supply ~30 amps to be able to run the drive motors and lift concurrently. We settled on using 3 sealed lead acid batteries in parallel - allowing a nominal max peak current draw of 30 amps, and being very safe. In addition, each of our servos can draw up to 2 amps each (4 total servos). Initially we made a custom servo power distribution board using linear regulators (what the PRL had available) to step down 12V to 5V, with a max current rating of 7 amps. This ended up dissipating too much heat (since it was burning ~50W into heat),and melted its own solder, so we were forced to scrap the board. We decided to use AA batteries as an alternative with a different custom servo power distribution board."},{"heading":"Software","content":"We opted against using an existing robot control scheme like ROS, and instead wrote every driver for every interface and electronic module. This gave us more control over each module, but also added significant complexity and development time - since each module has its own quirks which weren't represented in the datasheets. A bulk of our time was spent just writing, validating, and testing these interfaces to get the electronics talking to each other.\n\nrobot control\n\nWe chose to build our own robot controls from scratch, writing and tuning our own closed loop control. We use motor encoders, IMU, and GPS to localize the robot. For outdoor navigation, we use the GPS and IMU to follow waypoints, and indoors our robot reacts more responsively to the environment through a stereoscopic camera used to simulatneously measure depth, and wide angle camera on the arm of OmNom.\n\noutdoor path finding\n\nWorkflow :\n\nUser inputs order Parsed by open ai -> start and end points, order information extracted Waypoints generated from start and end points (open route service api) Waypoints sent to the Jetson Nano.\n\nindoor navigation\n\nStereoCamera: Frame Capture Object Detection: YOLOv8 Similarity Mapping & Image Embedding: CLIP (contrastive language image pretraining) Depth Estimation + Mapping: DPT (Dense Prediction Transformer) Semantic Search: sentence transformers + convex database\n\nScene Analysis: ViT GPT\n\nStereoCamera captures frames DPT performs depth estimation on each frame Generates normalized depth map Correlates ROIs in depth map with bounding boxes Computes average depth for each bounding box Classifies bounding box as Far , Mid , or Close Signals direction to turn or move based on classification User enters target prompt CLIP generates vector embedding of the text prompt StereoCamera captures frames YOLOv8 detects objects in each frame DPT performs depth estimation, producing normalized depth map Semantic search : Compares CLIP embeddings of detected objects with text prompt embedding Ranks/determines closest related object to target prompt If present, recognizes/tracks target object\n\nipad interaction\n\nTo work with general ordering interfaces, we created a state machine.\n\nIt works by taking frames from the wide angle camera, overlaying them with a grid, and then feeding them to gpt-4 with a detailed prompt of the order. We use gpt-4 to determine the state of our interation (in progress vs. complete), and where / whether we should click the screen.\n\nsimulated ordering app\n\nTo test our state machine, we created a prototype ordering app. Fully AI generated using v0.dev and Windsurf . Deployed using Vercel . Coded this thing in english lol. Simple app ordering interface to represent common interfaces.\n\nfrontend website\n\nBuilt with Vite.js + React + Typescript + TailwindCSS . Communicates with OmNom via Websockets API and Fast API . Stream's OmNom's live location to you via Google Maps API so you can rest assured OmNom is on his way."},{"heading":"Challenges we ran into","content":"Motor controllers: Our decision to use the roboclaw motor controllers and do a custom interface made our work quite hard - the USB interface made it a challenge to do simple things like set a power to a single motor. We had quite an ordeal just getting the motors spinning.\n\nGNSS Drift: The RTK module we used advertised precision to within 1 foot, but we found it drifted quite a bit. This was one of the key things that made our navigation unreliable outside - preventing us from having a reliable demo which did the whole sequence of events.\n\nPower Management: Making a custom 12 volt to 5 volt converter was a time sink which proved to be fruitless - since it was too inefficient it melted its own solder. We ended up having to switch to AA batteries for our 5V servo power after enough attempts to get it to work all off of our 12V batteries.\n\nLangchain\n\nDeepSort\n\nDepth Estimation"},{"heading":"Accomplishments that we're proud of","content":"Taking on this project was extremely ambitions. Building a robot of this complexity, and programming it to effectively interface with the world in only 36 hours was an uncertain task, and at many points we weren't sure if we were even going to have a moving robot.\n\nThe hardware. Learning new hardware platforms and tuning PID loops for drivetrain and stable movement of the pulley took an extensive amount of effort and time. Coming up with creative solutions on the software to overcome hardware blockages was quite rewarding as well - we originally were planning to implement SLAM and generate a 3D visual ming of the environment but had to pivot our indoor navigation solution after realizing SLAM was not performant due to vibrations. After some research and talking with other hackers, we came up with a solution completely based on stereovision and depth perception."},{"heading":"What we learned","content":"Arjun: I learned about mechanical design in creating this type of robot. Lessons I learned include the age old lesson of measure twice cut once, as one wrong measurement in my CAD resulted in half an hour of rework on a metal plate.\n\nNils: I learned a lot about distributed systems while integrating the different sensors together on the robot. I didn't have much experience in robotics before, so I also learned a lot about PID loops and the long and arduous process of tuning them.\n\nRobert: Coming in I wanted to do something blending both hardware and software for a more interactive experience. Only after our team spent the first 14 hours building the robot did I realize the vast challenges of learning new frameworks and integrating different hardware together. While hardware definitely needs to be quality and tuned for effective use, I see significantly more impact and potential of software enabling hardware than I did before. TreeHacks this year has inspired me to pursue more experiences across both hardware and software to build incredible software that has real impact through effective hardware.\n\nJohnathan: I learned a lot about the mechanical side this year, which I didn't expect, especially coming in as a software person. Working on the web app taught me a lot about websockets and RESTful APIs. Finally, designing the ipad interaction state machine was a good lesson on thinking step-by-step, as well as implementing python image libraries."},{"heading":"What's next for OmNom","content":"OmNom is just getting started. We were focused on food delivery during TreeHacks - trying to solve a relatable problem for Stanford students. This technology has applications beyond just food delivery for students - most directly connected is delivery for peaople who can't go to restauraunts, but the technology test bed and algorithms we've implemented will work for other applications that require a physical presence - being able to interface with the world almost as well as a human.\n\nIn the future, OmNom will have improved hardware for more diverse interactions and much better navigation capabilities. By reinforcing our linear rail system and building a stable vertical gain tree, we will install an IMU and LiDAR enabling SLAM 3D mapping and navigating novel environments. We also would like to work more with VLMs and vision models to generate robot navigational instructions based on user navigation prompt and retrieval of destination floorplan images. We would also combine tracking, recognition, and localization and mapping to map objects to specific locations within buildings for detailed object retrieval and interaction. Say you forgot your charger in a classroom, OmNom could remember where you left it and retrieve it for you, then deliver a Valentine's message on its way back to you."},{"heading":"Built With","content":"arduino clip dpt fast-api google-maps-places-api jetson-nano-c++ lasercutting matplot-lib-v0.dev metal next.js open-cv open-route-service openai-api python react roboclaw soldering tailwindcss typescript vercel vit vite.js websockets-api"},{"heading":"Try it out","content":"github.com om-nom-ordering.vercel.app"}]},{"project_title":"HawkWatch","project_url":"https://devpost.com/software/hawkwatch","tagline":"Transform any camera into a smart security system with HawkWatch. Our AI VLM instantly detects and alerts security to potential threats, giving businesses of all sizes a peace of mind.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/015/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"TreeHacks Grand Prize | 1st Place ($11k Cash)"}],"team_members":[],"built_with":[{"name":"chatgpt","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"mp4","url":null},{"name":"nextjs","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"resend","url":null},{"name":"supabase","url":null},{"name":"tensorflow","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vlm","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Grace-Shao/Treehacks2025"}],"description_sections":[{"heading":"Inspiration","content":"In an era where security cameras are everywhere but meaningful surveillance is scarce, we saw an opportunity to transform passive recording systems into intelligent security guardians. Our inspiration came from real-world incidents where crucial moments were missed despite having camera coverage, and the overwhelming challenge security personnel face in monitoring multiple video feeds simultaneously. We wanted to create a solution that doesn't just record but understands, analyzes, and acts, whether it's for local businesses like grocery markets to bigger organizations like hospitals and shopping malls."},{"heading":"What it does","content":"HawkWatch is an intelligent video surveillance platform that detects crime, suspicious activities and life threatening events such as fainting and choking and sends phone alerts to alert security of the issue. Our intelligent model generates time-stamped incident reports with video evidence. It has 4 main features:\n\nReal-time detection of dangerous activity is done by sending audio, video, and Tensorflow's body position data to Google's Gemini Visual Language Model, and sending email notification if needed An upload feature allows existing mp4 files to be analyzed A library of saved livestream footage and mp4 uploads, with detailed security analysis complete with timeline and information which is saved with each entry Statistics page which offers an AI summary, chart analysis, and the option to export situation data to CSV.\n\nAdditional features\n\nSends instant alerts to security through email/phone notifications set Provides an intuitive dashboard for monitoring multiple cameras, with option to call security Ability to download archive footage to MP4 Offers an OpenAI powered assistant that provides contextual support. The bot is fed real-time information about the ongoing event and can respond to user queries, such as \"What should I do in this situation\" if someone has passed out, helping with quick context-aware advice Offers both real-time streaming and uploaded video analysis"},{"heading":"How we built it","content":"Our tech stack combines modern tools for a robust, scalable solution:\n\nFrontend : The UI is built with Next.js 13+ and TypeScript, paired with Tailwind CSS for a sleek, responsive design. This ensures a seamless experience for users across different devices. Backend : We use Supabase for secure user authentication and database management, allowing for easy access control and efficient data handling. AI Processing : HawkWatch uses Google's Gemini Visual Language Model (VLM) for real-time video analysis and TensorFlow.js for processing video streams on the client side. These models enable accurate event detection, ranging from criminal activity to health-related emergencies. Email/Phone Service : Resend API powers our email and phone notification system, ensuring that alerts are sent in real-time with minimal delays. Real-time Updates : We leverage the Canvas API for live updates, ensuring that HawkWatch’s real-time analysis is fast and accurate, even as it processes multiple video streams. Contextual Assistance : OpenAI’s language models are integrated to power our assistant bot, which helps security teams with situational guidance. The bot uses context from the most recent events to offer real-time advice, improving the decision-making process during critical moments."},{"heading":"Challenges we ran into","content":"Performance Optimization : Balancing real-time video processing with browser performance and Gemini rate limits AI Model Accuracy : Fine-tuning detection algorithms to minimize false positives Video Stream Handling : Managing multiple video streams without overwhelming the system"},{"heading":"Accomplishments that we're proud of","content":"Created a fully functional AI surveillance system in 36 hours Achieved real-time processing with minimal latency Implemented a beautiful, intuitive user interface Built a scalable architecture that can handle multiple cameras Developed a system that's accessible through any modern browser"},{"heading":"What we learned","content":"Advanced video processing techniques in the browser Real-time data handling with WebSocket connections to handle real-time updates effectively AI model optimization for edge cases Complex state management in React applications, especially when dealing with large datasets Integration of multiple third-party services The importance of user experience in security applications"},{"heading":"What's next for HawkWatch","content":"Future enhancements we're planning:\n\n1. Advanced AI Features\n\nPerson identification and recognition Object tracking across multiple cameras Behavioral pattern analysis\n\n2. Enhanced Security\n\nEnd-to-end encryption GDPR compliance tools Advanced access control\n\n3. Smart Home Integration\n\nIntegration with popular smart home platforms Automated response actions Voice assistant compatibility\n\nOur vision is to make HawkWatch the go-to platform for intelligent video surveillance, making security monitoring more efficient and effective for everyone."},{"heading":"Built With","content":"chatgpt gemini mp4 nextjs react resend supabase tensorflow typescript vlm"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Edith","project_url":"https://devpost.com/software/edith-bvh014","tagline":"Solving the accessibility crisis, one second pair of eyes at a time.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/273/964/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Most Technically Complex Hack (Meta Quest 3 per team member)"}],"team_members":[],"built_with":[{"name":"adafruit","url":"https://devpost.com/software/built-with/adafruit"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/jotalis/Edith"}],"description_sections":[{"heading":"Software","content":"With our software stack, we introduce a powerful any-3D-object segmentation architecture backed by several SoTA machine learning models. Many object segmentation models are trained on highly specific datasets such as COCO and hence lack generalizability, whereas VLMs like ChatGPT-4o lack a rich understanding of the 3D world. By parallelizing AI inference on 4 NVIDIA A100s, we computed and streamed accurate, realtime depth maps, segmentation maps, and point clouds to contextualize the richness of the 3D world for those visually impaired. With the live Gemini multimodal API, we prompt Gemini with the object heading and depth, from which it is able to obtain the precise location of any desired object from where the user is.\n\nThe low latency communication between devices was a huge part of getting this to work in essentially real time. Our initial networking setup was built from a series of complicated websocket servers and clients on our various devices, including a Raspberry Pi, our laptops, and an NVIDIA A100 server. This setup enabled more granular communication and flow of data between devices, allowing us to pursue more ambitious ideas with our glasses. However, we ultimately decided on a much simpler architecture involving porting forward files and video streams across the network and simple HTTPs API routes.\n\nOn the embedded software side, we wrote several scripts to program LEDs, and vibration motors and feed the camera stream to other devices. We programmed an IMU on the glasses to calculate the yaw, pitch, and roll to detect when someone has fallen. To elevate the user’s sense of spatial awareness, by coordinating our IMU with the vibration motors, we programmed a haptic feedback algorithm using complementary filtering to get the motors to vibrate with a decaying amplitude the closer they approach the desired object of interest, indicating to them when they are facing the object."},{"heading":"Hardware","content":"To address the complex challenges faced by visually impaired individuals in navigating their environments safely and autonomously, we developed Edith—an actionable, adaptable, accessible assistant translating the visually rich world into forms that the blind can experience. Edith facilitates spatial awareness and object recognition, enabling users to interact with their surroundings with greater ease and confidence. Given the system’s technical demands, including real-time object detection, haptic feedback, and low-latency data processing, a robust hardware framework was imperative. The decision to implement a smart glasses-based system, rather than handheld or mobile alternatives, was driven by the need for an intuitive, ergonomic, and non-intrusive user experience. Smart glasses provide a seamless integration into daily life, avoiding the conspicuousness and inconvenience of handheld devices such as smartphones or external cameras."},{"heading":"Engineering and Design Considerations","content":"Our primary engineering objective was to optimize the functionality and efficiency of the system while maintaining a lightweight and comfortable form factor. The hardware architecture centers around a Raspberry Pi Zero 2W, supplemented by an inertial measurement unit (IMU) for motion tracking, a high-resolution camera for real-time video processing, and haptic feedback motors to provide tactile sensory input. Additionally, an LED strip was incorporated to enhance night-time usability by acting as an auxiliary illumination source. To ensure prolonged operation, the glasses integrate lithium-polymer (Li-Po) batteries, managed by step-up voltage boost converters to maintain stable power delivery across all components. Modular Framework and Material Selection To facilitate rapid prototyping, adaptability, and scalability, we employed a modular design paradigm for the hardware assembly. The structural framework consists of 3D-printed components, allowing iterative design refinements and seamless modifications. The integration of heat-treated brass elements enhances durability and structural integrity, ensuring the device remains lightweight while offering sufficient rigidity to support embedded electronics. The modular design also enables the repositioning of different hardware modules, allowing iterative optimization of weight distribution and system balance. Through multiple design iterations, we identified the optimal layout for component placement, ensuring both ergonomic comfort and efficient power management."},{"heading":"Custom Designed Circuits","content":"In addition to the CAD models used to develop and implement Edith, custom-built PCBs were designed to support the embedded hardware. Using perf boards, we developed small, compact tactile circuit boards that effectively integrate haptic feedback motors while efficiently utilizing the limited space available within our compact glasses. An example of the PCB we designed and implemented is presented below. While similar designs exist, our approach is particularly exciting as it was developed to be as small and compact as possible using only resources available at a university hackathon or makerspace."},{"heading":"Our takeaways","content":"We entered this competition as four driven individuals with one goal in mind: to walk out with the greatest creation we could build in 36 hours. But beyond the project itself, what made this experience truly special was being surrounded by our closest friends—watching each other work with enthusiasm, dedication, and relentless determination was nothing short of inspiring. For 36 hours, we were confined within the walls of the David Packard Electrical Engineering Building, yet in that space, we found countless moments of growth. Each of us came into this hackathon with different backgrounds—whether in embedded systems, cutting-edge machine learning, or CAD design—but together, we played to each other’s strengths and, more importantly, pushed ourselves to learn. The challenges we faced were often outside our comfort zones, leaving us no choice but to adapt and grow. From interfacing with a Raspberry Pi for the first time to soldering custom PCBs on a perf board, every obstacle became an opportunity to expand our skill sets. By the end, each of us walked away not only with new technical expertise but also with memories that will last a lifetime. Oh, and about five empty Monster cans each!"},{"heading":"Built With","content":"adafruit gemini numpy pytorch raspberry-pi react tailwind typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"BrailleBot: Make Every Document Accessible!","project_url":"https://devpost.com/software/readable-ai-braille-printer","tagline":"World's first braille printer under $15","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/270/209/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Best Hardware Hack (DJI Mini 3 Drone with Remote Control per team member)"}],"team_members":[],"built_with":[{"name":"cad","url":null},{"name":"chakra","url":null},{"name":"claude","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"llama","url":null},{"name":"nextjs","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/cathyzbn/braille-printer"}],"description_sections":[{"heading":"Why build this?","content":"During the start of Treehacks, we chatted with a fellow builder who described corresponding with her visually-impaired pen pal: She types her message on her computer, and then manually punches the braille, letter by letter, on a sheet of paper to send over.\n\nWhy does she do this? Braille printers are outrageously expensive due to the high degree of precision required and the niche market. Due to the high cost, many who seek to print braille documents instead manually punch each letter with a specialized hole puncher.\n\nThis struggle extends far beyond personal correspondence—it highlights a critical gap in education for the visually impaired. A low-cost Braille printer could empower teachers and students, making learning materials accessible to those in low-income communities, individuals who are both visually and audibly impaired, and those who rely on long-lasting, tangible print to acquire knowledge and information.\n\nThat's when we had our breakthrough. Modern 3D printers offer incredible precision at a fraction of the cost of specialized equipment. By retrofitting a basic 3D printer with our custom embossing mechanism, we created a braille printer that maintains professional-grade accuracy while cutting costs by nearly 99%. We didn't stop at making braille printing affordable. Today, only 3% of the internet is accessible to visually impaired users, largely because most content creators don't include text descriptions for images and visuals. We realized that we can use AI to describe images and convert any document to an accessible format.\n\nWe believe that document format or visual content should never be a barrier to access, and with BrailleBot, sharing ideas can be as simple as pressing 'print' for everyone."},{"heading":"How we built it","content":"Hardware: The “punch” mechanism is comprised of a series of 3D printed parts and powered by the printer’s extruder motor, making the modification incredibly fast and inexpensive. A snap action mechanism converts a quarter rotation of the extruder motor into a load and then rapid punch motion. A screw at the top allows the user to adjust the spring’s compression, thereby changing the depth of the punch. The stock extruder can create up to 10 punches a second.\n\nSoftware On the backend, we leverage the Groq inference engine and Zoom API, using Llama3-8B and Llama3-11B Vision to transcribe documents, generate text descriptions of images, and summarize Zoom lecture transcripts with low latency. Then, we use our Python Flask backend to convert the ASCII PDF representations into braille letters and GCode instructions for the Marlin firmware (used by most 3D printers). Commands are sent to the printer through USB serial. We use a JS canvas to display a live realtime preview of the print. The frontend is built with Next.js, Typescript, Chakra, and React."},{"heading":"Challenges we ran into","content":"Finding the right combination of punch geometry, travel distance, and impact material was a challenge on both the software and hardware side because of the limited resources on hand and criteria that the product be cheap and easy to retrofit. We tried a variety of foams, cloths, and papers, eventually landing on a cork sheet which can be purchased at hobby stores for a few dollars. This had the right amount of compliance to make a clear mark without punching through the paper. We also created a mechanism to iteratively test spring compressions and travel distances by having an adjustable spring and head offset, allowing us to efficiently explore a large design space. We 3D-printed multiple punch heads to ensure the best tactile experience."},{"heading":"Reflection","content":"It was so fun to build the braille printer together, combining each of our unique focus areas. Scott is an EE, Lawton is a MechE, Cathy specializes in AI, and Jason in front-end, resulting in a full-stack team. Building parts of the project and handing them off to each other felt like handing off the baton in a relay race. We’re proud that the resultant product works with a high degree of accuracy, and we were surprised at how low we were able to make the total cost of materials. On the software front, we're proud to be able to make all document formats accessible, from PDFs and word documents to scanned images using OCR. Above all, we're most proud of creating something that could make a real difference in people's lives. Building an affordable, reliable braille printer that can handle multiple document types means more visually impaired people can have access to printed materials in their preferred format."},{"heading":"What's next for BrailleBot: Make Every Document Accessible!","content":"We plan to open-source our work, so anyone seeking to build a braille printer can do so at low cost. We will include detailed documentation, files, and backend code logic, 3D printed prototypes, such that anyone can replicate our build."},{"heading":"Built With","content":"cad chakra claude groq llama nextjs openai python typescript vercel"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"BlinkAI","project_url":"https://devpost.com/software/blinkai","tagline":"Blink AI gives a voice to those who cannot type or speak. It translates blinks into Morse code, converts them into text, and sends prompts to AI—unlocking seamless, hands-free communication instantly.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/825/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Best Beginner Hack (Keychron K8 Pro QMK/VIA Wireless Mechanical Keyboard per team member)"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"mediapipe","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/FuzionEcho/BlinkAI"}],"description_sections":[{"heading":"Inspiration","content":"For millions of people living with Parkinson’s, ALS, and other mobility challenges, something as simple as sending a message can feel overwhelming. Most digital assistants rely on voice or typing, but what if those barriers didn’t exist at all?\n\nThat’s why we created Blink AI—not just another assistant, but an intelligent system powered by a groundbreaking translation algorithm that converts blinks into text, opening a world of possibilities for those who need it most.\n\nBecause communication should never be limited by physical ability. Everyone deserves a way to express their thoughts, ask questions, and stay connected—and now, they can do it with just a blink."},{"heading":"What it does","content":"Blink AI detects intentional blinks, measures their duration and sequence, translates them into Morse code, converts that Morse code into text, and then sends the final text to an AI assistant to generate a response.\n\nHow It Works:\n\nBlink Detection: The Blink AI algorithm tracks intentional blinks, filtering out natural blinking to avoid false inputs. Blink Interpretation: It analyzes the duration and frequency of each blink, translating them into dots and dashes in Morse code. Morse Code Conversion: The Morse code sequence is instantly decoded into readable text, forming a structured message. AI Processing: The translated text is sent to an external AI assistant, which processes it like a standard input and generates a response. Response Delivery: The AI assistant’s response is sent back to the user, enabling a smooth, natural exchange of information.\n\nWhat It Unlocks:\n\nAsk Questions & Get Answers – Interact with AI assistants and receive real-time responses. Send Messages with a Blink – Communicate effortlessly without touch or voice. Control Smart Devices – Operate technology hands-free with precise blink commands. Express Thoughts Clearly – A powerful tool for individuals with limited mobility, turning intention into action.\n\nBlink AI doesn’t just detect blinks—it translates them into a language AI can understand, creating a seamless bridge between human expression and technology.\n\nNo voice. No keyboard. Just a blink."},{"heading":"Challenges we ran into","content":"Implementing the Morse Code Algorithm Accurately – Converting blinks into Morse code required precision. The algorithm had to reliably distinguish between short and long blinks, recognize intentional sequences, and filter out involuntary blinks. Fine-tuning timing and detection thresholds was crucial to prevent errors and misinterpretations. Integrating the AI Assistant – Once Morse code was translated into text, we needed to ensure the AI assistant could process the input accurately and return relevant responses. This required optimizing how text was formatted, transmitted, and made compatible with various AI models to provide fast, meaningful, and user-friendly interactions. Balancing Speed and Accuracy– The system needed to be both fast and precise. If it processed blinks too quickly, it risked misreading input; too slowly, and it would feel unresponsive. Achieving the right balance required extensive testing, adjustments, and optimizations."},{"heading":"Accomplishments that we're proud of","content":"We developed a fully functional prototype that accurately translates blinks into real-time AI-powered communication. We created one of the first Morse-code-based blink detection systems, turning blinks into a seamless way to interact with AI. We empowered individuals with a new way to communicate, proving that technology can be for everyone."},{"heading":"What we learned","content":"Technology should work for people—not the other way around. If it’s not intuitive, it won’t be used. Small improvements make a big difference. Even a 0.1-second boost in response time completely changed the user experience. Good technology isn’t about making things complicated—it’s about making life easier. The best solutions take away barriers, not add them. What's next for BlinkAI Making Blink Detection Even Smarter – Improving accuracy so it recognizes blinks more naturally, refines Morse code translation, and responds even faster. Helping Blink AI Learn From Users – Fine-tuning the system so it adapts to each person’s unique blinking style, making interactions feel effortless. Expanding AI Compatibility – Ensuring Blink AI works smoothly with different AI assistants and platforms, giving users more choices. Bringing Blink AI to More People – Optimizing it for more devices so anyone who needs it can use it, no matter their setup. Real-World Testing & Feedback – Working directly with users, accessibility experts, and healthcare providers to keep improving Blink AI where it matters most."},{"heading":"Built With","content":"css flask html mediapipe opencv python react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"PUSHPA - Pyro UAV System for High-risk Patrol and Alerting","project_url":"https://devpost.com/software/pushpa-pyro-uav-system-for-high-risk-patrol-and-alerting","tagline":"PUSHPA gives firefighters eyes on the ground and in the air. Smart glasses steer drones for real-time wildfire mitigation w optical & UAV input + x20 kernel-optimized edge compute on custom SDKs.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/770/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Edge AI Prize: Operator Optimization Champion ($5k Cash)"}],"team_members":[],"built_with":[{"name":"executorch","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"langchain","url":null},{"name":"llama.cpp","url":null},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"vercel","url":null}],"external_links":[{"label":"pushpafire.vercel.app","url":"https://pushpafire.vercel.app/"},{"label":"github.com","url":"https://github.com/chandrasuda/nextgen-firewatcher"}],"description_sections":[{"heading":"Built With","content":"executorch gemini langchain llama.cpp pytorch vercel"},{"heading":"Try it out","content":"pushpafire.vercel.app github.com"}]},{"project_title":"MediLedger","project_url":"https://devpost.com/software/mediledger","tagline":"A privacy-preserving drug availability search powered by EigenDA and ZK-PTLS","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/763/datas/medium.jpeg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Web3 Prize: Best AI Agents ($8k Cash + 8k EIGEN [1st] & $7k Cash + 7k EIGEN [2nd])"}],"team_members":[],"built_with":[{"name":"data","url":null},{"name":"eigenda","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"flutterflow","url":null},{"name":"merkle-proofs","url":null},{"name":"merkle-trees","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"sha256","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"web3","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/shayaansultan/treehacks-final"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/project/exchange-crypto-app-template-u-i-kit-jlnp6p"},{"label":"docs.google.com","url":"https://docs.google.com/document/d/1T2YmsUn6v2e3QweyVUE5JKmtD1eC_nNfvqhIwLeXnAo/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Hospital stockouts—the unavailability of essential medications and medical supplies—present a major crisis in healthcare. Patients suffer delays in critical treatments, emergency rooms lack life-saving drugs, and hospitals are forced to ration care. Despite medical advancements, hospitals still rely on outdated, fragmented inventory systems, leading to supply shortages that compromise patient outcomes.\n\nThe biggest challenge? Hospitals don’t want to share their entire inventory with competitors. Right now, checking stock requires manual calls between hospitals, which is slow, inefficient, and unscalable. The result? Wasted time that directly exacerbates preventable patient harm.\n\nOur project is uniquely ambitious in the way we are applying Web3 to a deeply centralized system like healthcare, but we want to reinvent how cybersecurity works in this important industry. We care about using blockchain for social good, so a detailed problem statement highlighting the inspiration for our product is linked below."},{"heading":"What it does","content":"MediLedger is all about collaboration: providing a decentralized platform on which hospitals can more securely and confidently transmit sensitive data. Our system enables hospitals to securely check drug availability at other hospitals without exposing full inventory details on both ends.\n\nInstead of storing a simple list of drugs, each hospital stores its own inventory in a Merkle tree, where each leaf node contains a cryptographic hash of a drug name and quantity. The Merkle root (a compact summary of the entire inventory) is then submitted to EigenDA, a decentralized data availability layer. EigenDA allows hospitals to store inventory commitments on-chain without revealing individual stock details and ensures data integrity—any change in inventory changes the Merkle root, which prevents tampering.\n\nWhen a hospital needs a drug, the AI agent first checks the local hospital’s inventory. If the drug is out of stock or the supply is trending low, the AI agent fetches Merkle roots of nearby hospitals from EigenDA using Zero-Knowledge Proofs (zkProof) and the provided Merkle trees. The hospitals that have the requested drug generate a Merkle proof, which extracts the leaf node (hashed drug and quantity) from the Merkle tree, includes all necessary sibling hashes to allow recomputation of the Merkle root, and ensures the proof matches the stored Merkle root on EigenDA. The system then outputs a list of hospitals whose Merkle proofs have been validated, thus proving they have the drug in stock."},{"heading":"How we built it","content":"The backend server was built with Express.js, Node.js, and TypeScript. For the hashing algorithm, we used SHA256. While configuring the adapter for EigenDA, we used EigenLayer’s extensible agent framework for verifiable AI capabilities and data availability logging. The frontend was built with FlutterFlow, with an initial POC frontend built using Next.js."},{"heading":"Challenges we ran into","content":"The biggest challenge was integration—the backend deployment was not Much of what we worked on also involved repositories and tools with limited documentation, where we underwent educated debugging in a field we had little prior experience in. We initially had two ideas (this one and another more fintech focused on price manipulation) that we pivoted twice between on Friday, so defining the initial feature set for both with a blockchain focus took some time, especially with the crunch time of 36 hours. The biggest challenge was integration as well with FlutterFlow."},{"heading":"Accomplishments that we're proud of","content":"For most of the team, TreeHacks 2025 is our first hackathon ever. We could have played it safe using familiar Web2 tech stacks from classes, we decided to challenge ourselves by diving headfirst into Web3 and cryptography. In a span of 36 hours, we overcame the steep learning curve of understanding and implementing Merkle proofs and trees, which are notoriously complex but incredibly powerful for verification.\n\nDespite occasional roadblocks, we were determined to not give up on our ambitious ideas (where else has blockchain been integrated with healthcare?). Our team wore multiple hats—from testing backend deployment to graphic design and marketing. In the end, we are proud of how we built a fully decentralized inventory system for hospitals to use."},{"heading":"What we learned","content":"One of our largest takeaways from this weekend is how accessible hacking can be—even to students with limited technical exposure. With the right background and a willingness to learn new tools, we became a determined team that collaborated well on an interesting idea. Blockchain can be used for security and social good, and we hope we proved that through intersecting these technologies with healthcare.\n\nFor a more technical side, we learned how hash-based data structures allow for selective proof generation. Storing data off-chain in EigenDA was a major shift from Web2 thinking, and now we understand that different decentralization techniques come with trade-offs in performance."},{"heading":"What's next for MediLedger","content":"If we had more time, we would love to implement more features related to Web3:\n\nRight now, our proof verification happens off-chain in our backend. A great addition would be to store Merkle roots & zkProofs on Ethereum or a Layer 2 (e.g., Polygon, Arbitrum). This would allow on-chain smart contracts to verify zkProofs without a centralized backend. Currently, hospitals only react when a drug is out of stock or if supply has a diminishing trend. We would love to use our AI agent to predict drug shortages before they happen: analyzing historical hospital inventory data to forecast demand surges and sending alerts to hospitals before a stockout occurs. Currently, hospitals authenticate via API keys. Instead, we could use Web3 wallets (e.g., MetaMask) for hospital authentication, where hospitals could digitally sign zkProofs before submitting them. This would prevent unauthorized hospitals from accessing inventory data."},{"heading":"Built With","content":"data eigenda express.js flutterflow merkle-proofs merkle-trees node.js sha256 typescript web3"},{"heading":"Try it out","content":"github.com app.flutterflow.io docs.google.com"}]},{"project_title":"Iris","project_url":"https://devpost.com/software/iris-vlnzp0","tagline":"Real-time vision for the blind.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/272/786/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Edge AI Prize: Most Creative On-Device AI Deployment ($10k Cash)"}],"team_members":[],"built_with":[{"name":"cuda","url":"https://devpost.com/software/built-with/cuda"},{"name":"expo.io","url":"https://devpost.com/software/built-with/expo-io"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"nvidia","url":"https://devpost.com/software/built-with/nvidia"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"}],"external_links":[{"label":"github.com","url":"https://github.com/RoyceAroc/iris"}],"description_sections":[{"heading":"Inspiration 🤔","content":"Have you ever wondered how blind people cross the road? How imminent hazards like a \"wet floor sign\" or a pothole to their right might inflict harm? Here are some scary statistics:\n\nA survey of 300 blind people found that about 40% experienced head height collisions at least once a year. source 68% of people with visual impairment had been directly exposed to at least one serious life event. source There is a 46% increased risk of road traffic crash among those with visual impairment. source\n\nThis hackathon we decided to build Iris."},{"heading":"What it does + How we built it 🛠️","content":"Iris is a sleek headset built for the blind and visually impaired, designed to provide real-time guidance through the world. By attaching an iPhone as a camera to an affordable 3D-printed headset, users can navigate their surroundings with confidence. Iris doesn’t just describe the environment – it actively guides the wearer through A.I. agent specialists. Using real-time models such as captioning, classification, image segmentation agents, and depth estimation agents, Iris detects hazards in under 100 milliseconds. When a danger is identified, it instantly provides a haptic buzz, signaling the user of danger. From there, Iris delivers step-by-step spoken instructions via TTS, directing the wearer on how to safely navigate around obstacles, cross streets, or reorient themselves in complex environments.\n\nTaking a VLM model, we were able to tune it for real-time workloads via aggressive PyTorch compiler optimization, cudNN utilization, specific matmul precision, and usage of optimal attention kernels like flash / paged attention. To achieve high network throughput, we developed a custom userspace protocol to stream tokens to clients, and to reduce agent latency, we quantized models to architecture-specific dtypes.\n\nBuilt using Expo, the mobile app is compatible on both Android and iOS devices. Iris is also able to take STT commands and decode it into agentic workflows such as hazard detection, scene similarity scoring, and object detection (via Groq LPU APIs) which are carried out in real-time. Want to know when the pedestrian sign turns white for you to walk across? Want to know where your water bottle is? Iris has your back."},{"heading":"Challenges we ran into 😿","content":"Even the slightest delay in detecting a hazard could mean the difference between safe navigation and serious injury for a blind user. Latency was a critical factor, so we spent a significant amount of time tuning our models to minimize Time to Detection (what we call TTD). We implemented attention optimizations, leveraged cuDNN for accelerated inference, and fine-tuned every aspect of our pipeline. But no amount of optimization could make up for the fact that we were hitting a hard wall – our compute resources simply weren’t enough. That’s when we reached out to Brev.dev, which gave us access to the compute power we needed to push our models further, drastically improving both training and inference speed.\n\nCompute wasn’t our only bottleneck. Network bandwidth became a serious constraint. Our initial protocol relied on JSON, which, while easy to work with, was bloated – filled with unnecessary delimiters and whitespace that slowed down transmission. Every millisecond mattered. To eliminate this overhead, we designed a custom datagram protocol, stripping out redundant characters and ensuring that every byte carried meaningful information. The result was a lean, high-speed data stream that helped us shave off critical delays.\n\nFinally, there was the challenge of hardware. One of our core goals was affordability – if our solution wasn’t accessible to blind users, it wasn’t a solution at all. Off-the-shelf headsets were too expensive, so we designed our own. We developed a custom STL model for 3D printing a headset that could securely hold an iPhone, ensuring a low-cost but effective hardware setup. This way, users could leverage the sensors and processing power of their existing devices without needing expensive proprietary hardware."},{"heading":"Accomplishments that we're proud of 💯","content":"Iris started as a simple idea: an intelligent headset that could actively guide blind users through the world. But for us, it was also personal. One of our team members has a blind relative, and hearing firsthand about the daily challenges of navigating city streets without reliable assistance pushed us to make Iris as fast and reliable as possible. What we ended up with was a fully functional, multimodal A.I. system capable of real-time hazard detection and step-by-step navigation. Here are some of Iris' capabilties:\n\nIris can chain together vision-language models, depth estimation, and object detection to dynamically respond to user queries in real time. We designed and 3D-printed an affordable, durable headset that transforms an iPhone into an AI-powered navigation assistant Users can issue STT commands like “Where’s the door?” or “Can I cross the street?” and receive responses near-instantly."},{"heading":"What we learned 💭","content":"This project pushed us deep into the weeds of HPC optimization, hardware, and networking in exciting ways. One of the biggest lessons came from wrestling with the PyTorch compiler. We thought we understood it – until we started hitting obscure bottlenecks that weren’t obvious at first glance. Some layers weren’t fusing correctly, some optimizations were actually slowing things down, and cuDNN didn’t always behave the way we assumed it would. We spent hours profiling CUDA graphs, attempting optimal Tensor core and register usage, tweaking VRAM memory layouts, and rewriting parts of our model just to squeeze out those last few milliseconds.\n\nOn the hardware side, we learned that designing an STL file isn’t just about making a model that looks right – it has to print right. Our first 3D print warped mid-print. Through trial and error, we figured out how to design for material constraints, adjust print settings for strength, and optimize for minimal waste. Eventually, we had a sleek, durable headset that could securely hold an iPhone – something that felt incredibly rewarding after our early failures."},{"heading":"What's next for Iris 🚀","content":"Iris has proven its ability to provide real-time hazard detection and navigation, but we’re just getting started. Here’s what’s next:\n\nRight now, Iris warns of hazards through a single vibration alert. We plan to implement a more advanced haptic feedback system that provides directional guidance such as buzzing stronger on the left or right to indicate safer paths or signaling when the user should stop. Beyond hazard detection, Iris will integrate more specialized agents like sign language interpretation and personalized object recognition (i.e., users can “teach” Iris what their personal items look like, so it can help locate them.) We want to put Iris in the hands of real users. Our next step is testing with more visually impaired individuals to refine the experience, gather feedback, and iterate on both software and hardware. We truly see this as something that changes the lives of blind individuals for the better."},{"heading":"Built With","content":"cuda expo.io groq nvidia python pytorch"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SurviveX","project_url":"https://devpost.com/software/survivex","tagline":"Your hands-free conversational survival assistant. Powered by on device LLM inference, offering step-by-step instructions to keep you smarter and safer anytime, anywhere.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/272/515/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Edge AI Prize: Most Creative On-Device AI Deployment ($10k Cash)"}],"team_members":[],"built_with":[{"name":"executetorch","url":null},{"name":"finetuning","url":null},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"llama","url":null},{"name":"mixedreality","url":null},{"name":"nvidia-brev","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"torchtune","url":null},{"name":"vision","url":null},{"name":"visionos","url":null},{"name":"visionpro","url":null},{"name":"vscode","url":null},{"name":"xcode","url":"https://devpost.com/software/built-with/xcode"}],"external_links":[{"label":"github.com","url":"https://github.com/rockuutree/SurviveX-Treehacks/tree/main"},{"label":"drive.google.com","url":"https://drive.google.com/drive/folders/1Pntxog05MBqzbzW5uoAEGDyOuOXghBv9?usp=sharing"}],"description_sections":[{"heading":"The Problem 💭","content":"30% of people who get lost in the wilderness never find their way back. Imagine yourself lost and alone in the middle of nowhere, no internet access to call for help or look up survival knowledge and time is running out .\n\nWhether it's a hiker with a broken leg, a soldier in combat, a responder during a natural disaster, or someone lost at sea, the lack of internet connectivity and immediate expert guidance can mean the difference between life or death ."},{"heading":"Our Solution ⚙️","content":"SurviveX is an offline embedded AI assistant that combines Edge AI on a hands free device like the Apple Vision Pro along with health monitoring and voice guidance to provide real-time survival assistance .\n\nOur assistant provides step-by-step guidance , offering stressed users fast responses and ample opportunity for clarification throughout the process. Tracking vital signs through Terra's API , using them to augment our responses. Our machine learning model is designed to run on a hands free device like the Apple Vision Pro enabling survivors, first responders and soldiers to communicate via speech so your hands can focus on what matters : survival . Fine tuning our model to provide a user specific solution governed by environment indicative of the stress of the situation.\n\nWhether you need to treat an injury, start a fire with flint, fix up your broken car on the side of the highway, or aimlessly wonder and be navigated by the stars. SurviveX acts as your personal survival expert, adapting its guidance based on your speech and environmental conditions."},{"heading":"How we built it","content":"We used ExecuTorch for on device inference for our Edge AI solution. The model we decided on was Meta's Llama-3.2-1B-Instruct since it was small and the most practical for on device inference. To fine tune our model we used torchtune - a PyTorch library for fine tuning our Llama Model on an instance of Nvidia's H-100 using Brev.dev. We used SwiftUI for the interface on the Vision Pro and linked in binaries from ExecuTorch for implementing the on device Llama model. We used data from Terra API to simulate tracking of heartbeats and vitals which our model used to adjust its tone and focus."},{"heading":"Fine-tuning our Model","content":"Our Llama3.2-1B-Instruct model claimed to be fine tuned but we were a little worried when our model started generating bogus output when asked how to start a fire. We realized we needed to fine tune with 1000s of lines of code of particular scenarios our users may be in. We fine-tuned using LORA and then our custom dataset using tunetorch. We prepared them into a specific prompt/completion format using GPT-4:\n\n{ \"from\": \"human\", \"value\": \"How do I start a fire without matches? Can you please in small and concise steps explain to me how can I go about this task?\" }, { \"from\": \"assistant\", \"value\": \"1. Okay. Let us start by gathering dry tinder, such as leaves, grass, or bark. Let me know once you have got it.\" }\n\nFine tuning our Llama3.2-1B model gave us much more promising results. Moving up to 3B tokens, caused our computer to crash which made us use Nvidia's Brev to fine tune with its H100 GPU."},{"heading":"What's next for SurviveX","content":"Using iPhone's/Vision pro's external cameras' & sensors' ability to extract video input to provide personalized feedback based off our user's point of view. Higher GPU power would possibly enable us to use a larger model on device to extend beyond the Llama3.2-1B."},{"heading":"Challenges we ran into","content":"Programming for the Vision Pro had limited resources and none of us had done it before. All of our team members wear glasses so they couldn't actually use the Vision Pro without switching off into contacts 😦 ExecuteTorch was only able to be run on an Intel Processor so generation of .pte files could only be done on one device. We had to stick to a smaller model such as Llama's 1B tokens because Llama3.2 with 3B slowed down on device inference. Fine tuning our model also increased the size which ran fine on MacOS and iOS but led to certain memory limits on the Vision Pro. Linking ExecuTorch and Swift within the Apple Ecosystem proved to be harder than we expected. The lack of Macbooks with an Apple Silicon chip in our team meant less hands on Swift."},{"heading":"What we learned","content":"You don't need an internet connection to generate a response from an ML model! There are a lot of different survival scenarios that the wilderness can lead you to. Striking a good balance between speech-to-text and touch when developing for a hands free environment. With half of the team spending their time in the terminal we learned how useful linux commands can be."},{"heading":"Accomplishments that we're proud of","content":"We have never worked with on device inference or Edge AI before so it was really fulfilling to get ExecuTorch working, and generating our pte files. Fine tuning our model on our local machines proved difficult and we pivoted with Nvidia's new platform, Brev, which helped us fine tune the Llama model with higher context windows. Once we had fine tuned the model, we were able to change context length depending on the device and its capabilities. Changing the context length parameter increased the quality of the model on both the iPhone as well as the Apple Vision Pro. None of us had really built for a hands free environment like the Vision Pro before so being able to leverage Swift knowledge and connect our on device AI inference along with speech to text was a huge learning experience."},{"heading":"Built With","content":"executetorch finetuning github llama mixedreality nvidia-brev python pytorch swift torchtune vision visionos visionpro vscode xcode"},{"heading":"Try it out","content":"github.com drive.google.com"}]},{"project_title":"HiveMind","project_url":"https://devpost.com/software/hivemind-18cula","tagline":"HiveMind: Your Second Brain for Smarter Learning.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/272/627/datas/medium.jpeg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Education Grand Prize ($1k Cash + Meta Ray Ban Smart Glasses x4 + Marine Layer Zoom Hoodie x4)"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"chromadb","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"intersystems","url":null},{"name":"nextjs","url":null},{"name":"perplexity","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sonar","url":null},{"name":"streamlit","url":null},{"name":"superbase","url":null},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vector-embedding","url":null},{"name":"vespa","url":null},{"name":"zoom","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/rajashekarcs2023/HiveMind"},{"label":"github.com","url":"https://github.com/rajashekarcs2023/knowledge-balance-network"},{"label":"github.com","url":"https://github.com/AahilAliCodes/Hivemind2"}],"description_sections":[{"heading":"Problem","content":"Education is one of the most powerful tools for personal and professional growth, yet the way knowledge is imparted remains largely standardized and inflexible. Many students struggle with course material because traditional learning methods fail to cater to individual learning needs. Some students grasp concepts quickly, while others require additional time and explanation, leading to gaps in comprehension that affect academic performance and long-term retention. Moreover, collaboration and peer learning opportunities are often limited, leaving students to navigate their academic struggles alone. Some students grasp concepts effortlessly and can help their peers, but there is no structured way to connect them. Our goal is to change this by creating an intelligent, AI-driven educational platform that adapts to each student’s needs, making learning more effective, engaging, and inclusive."},{"heading":"Inspiration","content":"The idea for HiveMind was born from observing the disparity in student learning experiences, and have a framework based system to collectively improve everyone in the class week by week, leaving no one behind. We created a system that fosters collaborative learning environment, collective growth and collective improvement until everyone in the class is on same level of understanding and comprehension."},{"heading":"What it does","content":"HiveMind is an AI-powered educational platform that enhances student learning through continuous collective growth through peer learning sessions. By seamlessly integrating with online learning environments, the platform analyzes student responses and categorizes them into different levels of understanding. This classification enables the system to provide tailored support, ensuring that students who need extra help receive it, while those who have mastered concepts can reinforce their knowledge by guiding others.\n\nThe platform assesses student performance through AI-driven quizzes and assignments, determining their comprehension levels. Based on this analysis, students are placed into one of four learning hubs: those requiring foundational support, those needing additional clarification, those who have a strong grasp of concepts, and those who have achieved mastery. This hub is made up of a right balance of 4 students and this matching is done by the custom algorithm. The system then facilitates peer matching, allowing students with higher scores to assist those in lower scores. Additionally, Its seamless integration with existing learning platforms ensures that students receive real-time feedback and targeted support without disrupting their educational workflow."},{"heading":"How we built it","content":"HiveMind was developed using a combination of advanced web technologies, machine learning frameworks, and 3D visualization tools. Our backend, built with Python, manages data processing, user authentication, and AI-driven assessments. We implemented custom algorithms to evaluate student performance, generating meaningful insights that adapt to their learning needs. The frontend, developed with TypeScript, Three.js, and Next.js, provides an intuitive interface where students can track their progress through an interactive 3D brain visualization.\n\nTo enable real-time peer matching and vector-based assessments, we incorporated vector embeddings using IRIS Vector database, and Perplexity Sonar API reasoning that analyze student responses and categorize them accordingly. These embeddings are stored efficiently to facilitate quick retrieval, ensuring smooth transitions between different learning levels. Once the peers are matched, they will go through a Zoom session which is monitored by an AI agent which evaluates the progress of the student as well as the hub. Throughout the development process, we optimized the platform for scalability, ensuring that HiveMind could accommodate a growing number of users without performance bottlenecks. The integration of AI, real-time analytics, and peer-to-peer networking allowed us to create a seamless experience that enhances student engagement."},{"heading":"Challenges we ran into","content":"One of the biggest challenges we faced was merging the 3D dashboard into our existing system, which already included vector embeddings and a peer-to-peer network. This led to numerous Git merge conflicts, causing unexpected issues that disrupted our workflow. With time running out, we had to make a critical decision—to start fresh with a new GitHub repository in the final hours of development. This shift required us to carefully migrate existing functionalities while ensuring that all components remained fully functional.\n\nRendering the 3D brain visualization was another major hurdle. Achieving a balance between performance and visual appeal proved difficult, as real-time interactions required optimized rendering techniques. We experimented with various configurations in Three.js, adjusting parameters to ensure smooth animations and intuitive navigation. Additionally, maintaining compatibility across different devices added another layer of complexity, as rendering performance varied based on system specifications.\n\nDespite these challenges, our team remained persistent, debugging issues, restructuring code, and finding creative solutions to ensure that HiveMind functioned as intended. Overcoming these obstacles reinforced our teamwork and problem-solving skills, making the final product even more rewarding."},{"heading":"Accomplishments that we're proud of","content":"Building HiveMind was an ambitious challenge, and we are proud of what we have achieved so far. One of our biggest accomplishments is successfully implementing an AI-driven assessment system that evaluates student comprehension in real-time. Our machine learning models accurately categorize students based on their understanding, enabling tailored learning experiences that adapt dynamically to individual needs. We also developed a structured peer-matching system that facilitates collaboration between students at different knowledge levels, fostering a strong sense of community and support within the platform.\n\nAnother major achievement is our intuitive , which provides students with a clear and interactive representation of their learning progress. By integrating AI-driven insights with visually appealing analytics, we have transformed the way students perceive their academic journey. Additionally, our backend infrastructure is optimized for scalability, ensuring that HiveMind can handle a growing number of users without compromising performance. Successfully integrating the platform with online learning tools while maintaining a seamless user experience was a significant milestone that highlights our technical expertise and problem-solving abilities."},{"heading":"What we learned","content":"Throughout the development of HiveMind, we encountered numerous challenges that pushed us to think critically and innovate. One of the most valuable lessons we learned was the importance of real-time processing in educational platforms. Immediate feedback is crucial for student engagement, and optimizing our AI models to deliver quick yet accurate assessments requires extensive experimentation and fine-tuning.\n\nWe also gained a deeper understanding of the complexities involved in integrating AI-driven tools with existing educational platforms. Ensuring compatibility with learning management systems, designing efficient data pipelines, and maintaining user privacy were all critical considerations that shaped our development process. Furthermore, we realized that while AI can enhance learning, human interaction remains an essential element of education. This insight reinforced our belief in combining AI-driven recommendations with peer-based learning to create a balanced and effective system.\n\nFrom a user experience perspective, we learned the significance of designing interfaces that encourage participation without overwhelming students. Keeping the UI simple yet powerful was a challenge that required iterative testing and feedback. Understanding the psychology of learning and motivation helped us refine our platform to make it more engaging and beneficial for students."},{"heading":"What's next for HiveMind","content":"HiveMind is only at the beginning of its journey, and we have exciting plans for its future. One of our key next steps is integrating the platform with Zoom, enabling real-time transcription and AI-powered suggestions during class discussions. This feature will enhance virtual learning by providing students with relevant questions, summaries, and recommendations, making online education more interactive and insightful.\n\nWe also aim to enhance our adaptive learning models to provide even more personalized learning paths. By refining our AI algorithms, we can ensure that each student receives targeted resources and exercises tailored to their specific needs. Another major focus is incorporating gamification elements, such as achievement badges, leaderboard rankings, and interactive challenges, to keep students motivated and engaged throughout their learning journey.\n\nIn the long term, we plan to expand HiveMind’s reach by forming partnerships with universities, online learning platforms, and EdTech companies. By integrating our system into large-scale educational environments, we can impact a wider audience and help bridge learning gaps across diverse student populations. Our vision is to make education more personalized, collaborative, and accessible, ensuring that every student has the tools they need to succeed."},{"heading":"Built With","content":"api chromadb flask gemini intersystems nextjs perplexity python sonar streamlit superbase three.js typescript vector-embedding vespa zoom"},{"heading":"Try it out","content":"github.com github.com github.com"}]},{"project_title":"VisionGuard: smart vision for smarter driving","project_url":"https://devpost.com/software/visionguard-smart-vision-for-smarter-driving","tagline":"Our AI-powered system analyzes in-car video footage to transcribe and interpret driver behavior in real time. By detecting safe/unsafe actions, we provide actionable insights to improve road safety.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/298/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Tesla: Excellence Prize ($2k Tesla Store Gift Card [1st], $1k Gift Card [2nd], $1k Gift Card [3rd])"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Autonomy Grand Prize (6-months use of a Tesla Model 3 or Y w/ Supervised FSD)"}],"team_members":[],"built_with":[{"name":"apis","url":null},{"name":"colab","url":null},{"name":"gpt","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reinforcement","url":null},{"name":"vscode","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/JayYeung/tesla_kaggle"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1I6-hDtITWCSTrmS5IM57R79F7E6Rf0M57GbeiiAiLEo/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Driving in extreme conditions like icy roads and blizzards often leads to accidents due to poor visibility or slippery surfaces . Having personally experienced car accidents caused by vehicles losing control on ice, it’s clear how critical it is to ensure that drivers are cautious and aware. Similarly, my teammates from the crowded Bay Area face challenges with complex road systems and traffic , where crashes are just as frequent.\n\nThis inspired us to develop VisionGuard , a system that leverages AI to analyze driver behavior and road conditions , ultimately helping drivers make safer decisions. Our goal is to build smarter driving tools that reduce accidents and save lives ."},{"heading":"What It Does","content":"VisionGuard leverages cutting-edge Vision-Language Models (VLMs) to analyze in-car video footage , transcribing and interpreting driver behavior with high precision. By detecting both safe and unsafe actions , VisionGuard generates actionable insights to encourage safer driving practices.\n\nOur system identifies critical situations such as: ✅ Distracted driving (e.g., phone use, drowsiness). ✅ Reckless maneuvers (e.g., sudden lane changes, aggressive driving). ✅ Hazardous environmental conditions (e.g., low visibility, icy roads).\n\nWhile we aimed for real-time inference , we found that current VLM models are too computationally expensive for low-latency performance . However, through optimizations, we significantly improved processing efficiency and benchmark accuracy , making this approach feasible for future deployment .\n\nBeyond individual drivers, VisionGuard’s insights can benefit:\n\nInsurance companies – Enhancing risk assessments and reducing fraud. Fleet managers – Monitoring driver behavior for safety compliance. Autonomous vehicle systems – Providing explainability layers for AI-driven decisions.\n\nOur AI-driven approach aims to make roads safer at scale ."},{"heading":"How We Built It","content":"Computer Vision Framework\n\nWe leveraged state-of-the-art Vision-Language Models (VLMs) , including Gemini Pro 2.0 and Pixtral (13B) , optimizing them with inference-time techniques such as:\n\nChain of Thought (CoT) Prompting: Inspired by OpenEMMA , guiding multi-step reasoning for complex driving scenarios. Ensembling Methods: Majority Voting (best-performing approach). Weighted Confidence Scores to prioritize high-certainty outputs. Self-Consistency Decoding (multiple runs, selecting the most consistent answer). Bias Mitigation for QA Tasks: Detected a bias toward first/last answer choices (A/D) . Shuffled response orders and aggregated multiple generations to improve reliability. Temperature Annealing: Dynamically adjusting sampling temperature based on uncertainty. Logit Smoothing: Preventing overconfident but incorrect predictions. Mixture of Depth (MoD): Reducing computation on low-complexity frames to optimize performance.\n\nAlthough these optimizations reduced inference time , achieving true real-time performance on large VLMs remains a challenge .\n\nAI-Powered Reasoning\n\nWe integrated OpenAI’s GPT-4 API to enhance natural language reasoning . VisionGuard translates raw detections into human-readable feedback , such as:\n\n\"You seem distracted—keep your eyes on the road.\" \"Road conditions are hazardous—reduce your speed.\"\n\nAdditionally, we implemented causal reasoning chains , allowing VisionGuard to explain why alerts were triggered , increasing transparency and user trust.\n\nReinforcement Learning for VLM Fine-Tuning\n\nWe experimented with GRPO (Generalized Reinforcement Policy Optimization) to fine-tune the Qwen VL-2B model, leveraging 4× A100 NVIDIA GPUs .\n\nImplementation Details: Used Hugging Face TRL library for RL training. Tensor sharding across GPUs to fit the model in memory. Trained on the NuScenes-QA dataset , similar to Tesla-provided data . Implemented reward shaping for deeper reasoning ability.\n\nDespite our efforts, batch size constraints (1) and long training times (6+ hours) prevented meaningful improvements over API-based VLMs , reinforcing the importance of scaling laws for multimodal AI.\n\nInference Pipeline & Bottlenecks\n\nWe developed a high-throughput processing pipeline , but achieving real-time inference on large VLMs was infeasible due to:\n\nHigh computational cost per frame (especially for complex scenes). Memory bandwidth limitations when streaming video into large models. Latency bottlenecks in API-based reasoning (due to network-dependent processing).\n\nTo improve performance, we explored:\n\nFrame Sampling & Preprocessing: Used event-based sampling to prioritize critical driving moments . Applied contrastive normalization for better video clarity. Parallelized Inference: Asynchronous execution of multiple VLM instances. ONNX Runtime + TensorRT optimizations for acceleration. Edge Deployment Optimization: Investigated distillation-based lightweight VLMs for future on-device inference. Implemented multi-threaded execution to minimize bottlenecks."},{"heading":"Challenges We Ran Into","content":"1. Real-Time Inference Limitations\n\nWe originally aimed for real-time inference but found that current VLM architectures are computationally too expensive to process full-resolution video at interactive speeds. Even with optimizations like tensor parallelism and ONNX acceleration , the latency was too high for real-time driver feedback.\n\n2. GPU Memory Constraints\n\nFrequent OOM (Out-Of-Memory) errors when fine-tuning large VLMs locally. Used DeepSpeed, FP16 precision, and tensor sharding to fit models within 4× A100 GPUs .\n\n3. API Integration Challenges\n\nRate-limiting issues when interfacing with OpenAI and Google Gemini. Latency bottlenecks with cloud-based reasoning models."},{"heading":"Accomplishments That We're Proud Of","content":"✅ Functional Prototype: Built a working system that analyzes driving behavior and environmental hazards . ✅ VLM Fine-Tuning Exploration: Pushed the limits of reinforcement learning for multimodal AI . ✅ Pipeline Optimization: Developed low-latency video processing techniques , bringing VLM-powered driving intelligence closer to real-time feasibility . ✅ Cross-Disciplinary Collaboration: Our team bridged AI, reinforcement learning, and computer vision , tackling one of the hardest challenges in multimodal AI ."},{"heading":"What’s Next for VisionGuard","content":"🚗 Improving Real-Time Feasibility – Exploring smaller, distilled VLMs for on-device deployment . 📊 Data Partnerships – Collaborating with insurance & fleet management to enhance risk prediction. 👁 Multimodal Expansion – Integrating LiDAR, GPS, and radar for enhanced environmental awareness. ⚡ Edge Optimization Research – Experimenting with efficient quantization strategies for mobile and embedded systems ."},{"heading":"Tech Stack","content":"🔹 Vision-Language Models: Google Gemini, Mixtral AI (Pixtral 13B), OpenAI 🔹 Deep Learning Frameworks: Hugging Face Transformers, DeepSpeed, Torch 🔹 GPU Acceleration: CUDA, ONNX Runtime, TensorRT 🔹 Computer Vision: OpenCV"},{"heading":"Built With","content":"apis colab gpt python reinforcement vscode"},{"heading":"Try it out","content":"github.com docs.google.com"}]},{"project_title":"ECGo","project_url":"https://devpost.com/software/ecgo","tagline":"Mobile ECG with Wi-Fi capabilities that makes AI-powered diagnoses and stores data on a remote server.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/274/110/datas/medium.JPG","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Web3 Prize: Most Crazy Idea ($7k Cash + 7k EIGEN)"}],"team_members":[],"built_with":[{"name":"c","url":"https://devpost.com/software/built-with/c"},{"name":"esp32","url":null},{"name":"influxdb","url":null},{"name":"opacity","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"streamlit","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/fightingj305/ecgo"}],"description_sections":[{"heading":"Inspiration","content":"Both our team members come from families working in the medical device industry, and we know firsthand how important critical medical devices are for saving lives around the world. Our project aims to solve the lack of portable ECG machines on hand in developing regions or crisis-stricken areas, where such devices (and the doctors needed to operate them) are incredibly expensive or simply unavailable."},{"heading":"What it does","content":"ECGo takes a three second sample of your heartbeat using attached electrodes. It then forwards this sample to a remotely hosted database, from which an AI model is able to run an arrhythmia diagnosis on the test data. This information is then sent back to the user, along with a sample ID they can use to access their data in the future from our web application."},{"heading":"How we built it","content":"The heart (get it?) of the hardware on this project consists of an ESP32 microcontroller, which uses its built-in ADC to read values from an AD8232 heartbeat filtering IC. This data is then displayed on two 0.96\" OLED screens controlled over I2C. The user can also use a joystick to pan and zoom the ECG waveform displayed on the screen. For safety, both the microcontroller and IC are powered by AA batteries to avoid any potentially dangerous power connections.\n\nOn the software side, the ESP32 must be connected via Wi-Fi to some network which allows it to make HTTP requests. We use these requests to write data to an InfluxDB database, and also read the AI model's diagnosis out from the same database. Each sample is randomly assigned an ID from 0-10000 for use with the web interface.\n\nThe front-end software is a simple streamlit app, which uses Python to query the database for user data and displays the resulting data.\n\nThe server uses a CNN Transformer which we pretrained on open-source ECG data. It also interfaces with the InfluxDB database by checking it periodically for a new series of data, and writes its inference to the database when it finds new data."},{"heading":"Challenges we ran into","content":"One early challenge with the hardware bringup was the fact that my OLED screens were manufactured with a single I2C address, meaning that there would be no way to use two screens to display different data. To fix this we had to desolder a small surface mounted resistor and shift it to a different pin, which was quite a challenge. Otherwise, noise was (and is) a big issue with the project; we do our best to avoid being in areas with tons of electromagnetic interference when taking detailed samples. Testing the model was also a challenge, since we don’t have a good way of measuring an irregular heartbeat with our electrodes. The model did report a positive diagnosis of arrhythmia for incredibly noisy environments, which have more irregular signals, and a negative diagnosis for a regular heartbeat.\n\nWe explored the web3 track and experimented with placing an AI-powered agent on the application also assists with interpreting their test results and giving medical suggestions. However, this was rather difficult to integrate in the limited time we had left and so in the final product we left it off."},{"heading":"Accomplishments that we're proud of","content":"As a team of only two (one hardware/firmware and one software/AI), we are very proud of having a successful final product with so many different features and complexities. We’re very happy with the way the hardware fabrication went, from prototyping to the final PCB soldering without any major hiccups. Furthermore, the entire data flow of chip -> HTTP -> Database -> Model -> Database -> HTTP -> chip is surprisingly reliable and effective."},{"heading":"What we learned","content":"This project was a new experience for both of us when it comes to integrating our areas of technical expertise with each other, as interactions between embedded systems and AI on such a close level are less common in our normal work. Technically, we both learned a lot about working with time-series data and InfluxDB. Working through the aforementioned integration gave us both a better understanding of how to work with diverse inputs and outputs that we don’t usually see."},{"heading":"What's next for ECGo","content":"ECGo is not just about arrhythmia diagnosis - this paradigm of an inexpensive, mobile device for use in the field in conjunction with more powerful AI diagnostics and support in a remote server has great potential to us in bringing healthcare to areas around the world with less access to technology or medical personnel. The next future steps are to further develop the diagnostic capabilities and improve the ECG filtering, and then adapt the device to be even more technology-free by moving the patient query onto the ESP32 as well, making the relatively inexpensive hardware component the only item we need on-hand to perform the whole suite of diagnostic, data storage, and recommendation."},{"heading":"Built With","content":"c esp32 influxdb opacity python pytorch streamlit"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"AgentZero","project_url":"https://devpost.com/software/agentzero","tagline":"Our autonomous and verifiable AI agent removes human inconsistency in making trading decisions, enabling data-driven trades with real-time analysis, customization, and safety.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/345/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Web3 Prize: Best AI Agents ($8k Cash + 8k EIGEN [1st] & $7k Cash + 7k EIGEN [2nd])"}],"team_members":[],"built_with":[{"name":"eigenlayer","url":null},{"name":"gnark","url":null},{"name":"go","url":"https://devpost.com/software/built-with/go"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/2pir2/TreeHack2025"}],"description_sections":[{"heading":"Inspiration","content":"As a stock-market investor, I deeply understand emotions are one of the greatest obstacles to success in financial markets. Humans have all felt the fear of uncertainty and loss, the temptation of impulsive choices, and the long period to react and make decisions. But what if we could remove human fragility and the reaction time from the equation? By using a fully automated , verifiable AI agent, we utilize technology to make precise, unstoppable, and data-driven decisions. Our fully autonomous agent is more than just a tool—it is a revolution in investing. With real-time analysis, user-defined customization, and adaptive intelligence, it ensures that trades are executed efficiently, rationally, strategically, and safely. The future of finance is not dictated by emotions, but by innovation . It is time to embrace the next generation of investment intelligence."},{"heading":"What it does","content":"Our system consists of two specialized machine learning models:\n\nPrice Model: This model is trained on historical Ethereum (ETH) price data, identifying patterns, trends, and volatility to make accurate price predictions. News Model: This model processes real-time news from Eigenlayer and other sources, analyzing sentiment, keywords, and market-moving narratives to gauge the impact of external events on ETH prices using the finBERT large language model.\n\nWe combine these models by allowing users to assign custom weightings to each—giving them control over how much influence price trends versus news sentiment have on the agent’s final decision-making. Once the weighted predictions are generated, our fully automated AI agent can execute trades and market reactions in real-time, ensuring optimal strategies based on user preferences and live data. This level of customization and automation ensures a strategic, data-driven approach that removes emotional bias and maximizes efficiency in ETH trading."},{"heading":"How we built it","content":"We built our system by integrating EigenLayer for fetching real-time news data and Dune for pulling Ethereum price data.\n\nNews Processing: The news data is retrieved via EigenLayer and analyzed using an LLM (finBERT), which assigns a sentiment score from -1 to 1 continuously to each news event, where score 1 indicates most positive, -1 indicates most negative, and 0 represents neutral. We then computed the average score for all of related the news. Price Prediction: The price model is trained using a Random Forest Regression model on historical price data from Dune, identifying market trends and fluctuations. User Interaction & Agent Customization: Users can set custom weightings for the influence of news sentiment versus price prediction on decision-making. Users also define the execution frequency, determining how often the automated agent executes actions based on the weighted results.\n\nBy combining these elements, our AI-driven system provides a fully customizable and verifiable trading agent, allowing users to optimize their strategies with real-time, data-driven decisions while eliminating emotional bias."},{"heading":"Challenges we ran into","content":"At the beginning, we had limited knowledge of blockchain technology, making it challenging to deploy our agent on-chain. However, by learning from online resources and extensive research through Google, we gradually gained the coding skills needed to integrate agent.\n\nAnother major challenge was designing a verification process to ensure that the model was trained on the intended dataset. Verifying the authenticity of training data in a decentralized environment is inherently complex. We addressed this by reducing the model's size, making it more efficient while maintaining transparency in the training process.\n\nThrough persistence and problem-solving, we overcame these obstacles to build a trustworthy, on-chain AI trading agent that is both verifiable and efficient ."},{"heading":"Accomplishments that we're proud of","content":"We started from scratch and converted our innovative and crazy idea into an applicable real-world project. Through effective collaboration, extensive problem-solving, and actively seeking help from sponsors, our team built an impactful and inspiring AI agent that can own and manage on-chain data (current price), integrate verifiable off-chain services and Web3 APIs (Dune and Infura testnet faucet), and enforce policies or rules that guarantee safety, trust, and transparency."},{"heading":"What we learned","content":"New knowledge in web3 and blockchain: We learned new knowledge in web3 and blockchain, which is a hot and intriguing area but brand new to all 3 of our team members, including the fundamental concepts like zero-knowledge proof and zkTLS, and TestNet. The fact that the agent can automatically generate proofs that provide users with an easy and efficient way of verification is intriguing to us. We also learned about the impactful companies in this area like EigenLayer with its Verifiable Agents and Taisu Venture, which helped us verify the source of the data we retrieved from OpenAI. As we developed our verification of machine learning datasets, we also came up with an interesting algorithm for proofing the dataset used. Technical skills: We also developed our coding skills by building up machine learning and LLM models and atomizing the whole process, and tested experiments with APIs. Soft and interpersonal skills: Moreover, as John Hennessy mentioned in the workshop, we should be humble and authentic and always do something new. We kept these values in mind through the whole process of TreeHacks, and always asking questions, sharing our thoughts, and thinking outside of the box to be innovative. Our team worked together from not knowing each other to close teammates and friends. Our interpersonal skills like teamwork and communication improved, and we knew how to work with people from different backgrounds and skillsets."},{"heading":"What's next for AgentZero","content":"Although we have successfully deployed AgentZero on-chain, there is still room for improvement to enhance its performance and accuracy.\n\nImproved News Analysis: We aim to refine the news analysis logistics to better interpret and categorize news events with greater contextual understanding. Enhancing sentiment detection and incorporating entity recognition could improve decision-making accuracy. Higher-Resolution Data: Currently, due to the lack of a pro version, we are limited to hourly data rather than minute-by-minute or second-level updates. Upgrading our data pipeline to process more granular real-time data would enable faster, more responsive trading actions. AgentZero is also applicable to other coins in the future. Exploring Advanced Deep Learning Models: While Random Forest Regression has served us well, we plan to experiment with more advanced deep learning architectures such as LSTMs, Transformers, or Reinforcement Learning to improve price prediction accuracy. By implementing these improvements, AgentZero will continue evolving into a smarter, faster, and more precise AI-powered trading agent."},{"heading":"Built With","content":"eigenlayer gnark go python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ERWIN: Enhanced Rock Weathering Impact Navigator","project_url":"https://devpost.com/software/erwin-enhanced-rock-weathering-impact-navigator","tagline":"An accessible carbon removal assessment platform for Enhanced Rock Weathering (ERW) projects. Democratizing geochemistry science to inform CO2 removal research and carbon credit investments.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/272/268/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Sustainability Prize: Best solves the user's pain point of a provided challenge ($800 Cash)"}],"team_members":[],"built_with":[{"name":"beautifulsoup4","url":null},{"name":"crunchflow","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"flask-(python)","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mapbox-api","url":null},{"name":"open-meteo","url":null},{"name":"qiskit-(python)","url":null},{"name":"selenium","url":"https://devpost.com/software/built-with/selenium"},{"name":"soilgrid-api","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Jackal-Studios/rock-thing"},{"label":"www.canva.com","url":"https://www.canva.com/design/DAGfPeW-cJw/De_TRIO2zEPGTqJQVsqDMw/view?utm_content=DAGfPeW-cJw&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h2678110d5b"}],"description_sections":[{"heading":"Inspiration","content":"Our TreeHacks team formed due to common career motivations in tackling climate change. We’re particularly interested in how technology can shape global finance toward climate solutions, hence our interest in the Carbon Removal Assessment challenge.\n\nWe spoke with Stanford Earth Systems Professor Kate Maher to learn about pain points in Enhanced Rock Weathering (ERW), an increasingly popular method of carbon removal, where certain species of rock with high CO2 sequestration rates are ground up and distributed to maximize reaction rates via surface area.\n\nForecasting the carbon removal potential of an ERW project is an esoteric process with a steep learning curve - estimates used by investors and project planners in carbon credit markets have vast uncertainty, and more scientific approaches require PhD-level knowledge and access to poorly-documented methods of accessing computational models running on Fortran and text files. There are few people in the world with the resources and experience to scientifically forecast carbon removal rates for ERW projects."},{"heading":"What it does","content":"We’ve developed a visual platform democratizing ERW carbon removal assessment for any user, focusing on carbon credit markets and ERW researchers.\n\nNow, users can select a target region for an ERW project and input simple parameters of feedstock (rock) type project area, feedstock surface density, and time series length for the output. After clicking ‘Run’, the user is presented with graphs of the CO2 removal rate per meter squared, and the pH, over time in years. The user is also presented with an estimate of the total CO2 removed over the total project area over the inputted period of years.\n\nAccommodating our two main categories of stakeholders, we have a ‘Basic’ and ‘Advanced’ mode on the User Interface. Those less familiar with the science of ERW, such as potential investors, can choose the Basic mode to be provided with the simple data needed to observe Carbon Removal rates in selected regions. For those who are better-versed in ERW, the Advanced mode serves to provide more options and detailed information regarding the science of ERW and our calculations."},{"heading":"How we built it","content":"After the user clicks ‘Run,’ ERWIN automatically retrieves a comprehensive set of soil and environmental data for the selected location and integrates it with user-provided inputs to generate and refine the data required for underlying computational algorithms. To do this, we took the latitude and longitude of the center of the user’s region of interest and used soil and weather APIs to fetch data relevant to soil carbon exchange geochemistry, such as soil composition, cation exchange rate, and mean annual precipitation, to name a few.\n\nAs an additional functionality, before the user selects 'Run,' they have the option to explore quarries recommended for their bounded location. After the user chooses a type of rock and a selected area, the toggle switch on the top left outputs n (set to 50) points with the best quarry options to obtain that rock type. This data comes from the National Mine Map Repository, pulled using Selenium and BeautifulSoup4, and stored in CSV files on a public GitHub repository. These spreadsheet-like files are named with the latitude, longitude, and rock type that they represent, and are accessed as such within the script in JavaScript.\n\nOne of the greatest challenges of carbon removal projects lies within random sampling and generating probability distributions across vast datasets. Powered by Qiskit in Python, our quantum computing system leverages quantum superposition and entanglement to significantly enhance sampling efficiency and accuracy. Likewise, our quantum processes can run on quantum computers in real-time with IBM Quantum Cloud in the present day, and these processes will become more scalable over time.\n\nIdentifying the relevant time series outputs across the many output files, based on CO2 removal relevance and scientific interest, we generated distribution curves of these prioritized datasets (ion concentration and soil pH time series), used stoichiometry to convert chemical data to CO2 removal rates, and graphed the mean curves for CO2 removal rates and pH over time with one standard deviation of uncertainty. By integrating the data under the curve, we found the total CO2 removed per m^2 across the time series and multiplied this by the project area to get the total CO2 removed over the provided project timeframe.\n\nOverall, our key tasks were:\n\nUnderstanding ERW and carbon removal assessment needs and barriers, for carbon credits markets and researchers, with input from Professor Kate Maher and private sector stakeholders (ERW startup affiliates). Designing and iterating an accessible interface simplifying user input and output. Parsing expected input and output files to identify data types to fetch from the web/APIs, and necessary calculations/conversions to meet the required input template and output needs. Developing an easy-to-use user interface for users with all levels of experience with ERW. Building a Docker container and workflow to standardize operations across operating systems and devices."},{"heading":"Challenges:","content":"The data fetched and inputted does not correspond directly to the inputs required by the computational algorithms used, and given an example input text file, it took us many hours to reverse engineer the syntax, meanings of different parameters, and units. The lack of documentation served as a frustrating barrier, but also a valuable firsthand insight into the pain point of our intended users - this lack of clarity is the obstacle faced by anyone approaching ERW carbon removal forecasting, from investors to researchers, and the problem we aim to solve. Reading up extensively on soil geochemistry formulas and methodologies and consulting Professor Maher, we were able to parse what the data meant, and the calculations necessary to fit our data to the required inputs. The same process applied to parsing the output files."},{"heading":"Accomplishments we’re proud of","content":"This was our team’s first hackathon, and we’re incredibly proud of having built a solution to a real-world problem that can drive measurable climate impact, by increasing confidence in carbon removal projects through science-backed forecasts."},{"heading":"What we learned","content":"We’re particularly proud of learning carbon removal geochemistry on the fly, and teaching ourselves to understand complex academic methodologies to the extent that we were able to build a simplified workflow around the necessary computations - to build a simplified model, we had to understand not only inputs A and outputs B, but also what calculations are required to get from A -> B. Within 36 hours, we built scalable digital infrastructure, a robust UI interface, and a backend interfacing with numerous APIs to fetch soil and climate data, while learning ERW science, formulas, and data representations, and meeting user needs."},{"heading":"What's next for ERWIN: Enhanced Rock Weathering Impact Navigator","content":"We seek to continue our collaboration with Professor Maher and other ERW stakeholders in academia and the private sector, to receive more user feedback and better serve stakeholder needs. We also want to expand our offering of recommending ERW project locations for maximizing carbon sequestration and minimizing distance from feedstock quarries, likely supplemented with better feedstock quarry spatial data through computer vision analysis of satellite imagery.\n\nThe carbon removal project market, and particularly the ERW sector, is rapidly growing, and we see this tool having great potential for the basis of a venture accelerating a zero GHG future."},{"heading":"Built With","content":"beautifulsoup4 crunchflow docker flask-(python) javascript mapbox-api open-meteo qiskit-(python) selenium soilgrid-api"},{"heading":"Try it out","content":"github.com www.canva.com"}]},{"project_title":"Project AVIA","project_url":"https://devpost.com/software/project-avia-autonomous-vigilance-insights-for-aviation","tagline":"Leveraging healthcare data to improve the performance of fighter jet pilots","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/304/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Healthcare Grand Prize (Meta Ray Ban Smart Glasses x4) and [2nd place]"}],"team_members":[],"built_with":[{"name":"cors","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"mistral","url":null},{"name":"ngrok","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"terra","url":null},{"name":"vercel","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Harsh-Karia/TreeHacks"}],"description_sections":[{"heading":"Built With","content":"cors express.js flask mistral ngrok python react tailwind terra vercel"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Therms - Thermoregulation Wearables with AI-powered Insights","project_url":"https://devpost.com/software/therms-thermoregulation-wearables-with-ai-powered-insights","tagline":"A non-invasive wearable using novel thermoregulation research to provide precise heating/cooling to maintain core temperature. Built to enhance physical performance and battle geriatric disorders.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/273/153/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Healthcare Grand Prize (Meta Ray Ban Smart Glasses x4) and [2nd place]"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Scrapybara: Capy Prize (Japan offsite + $1k credits + Scrapybara Pro 6 months [1st] & $1k credits + Scrapybara Pro 3 months [2nd] & $100 credits + Scrapybara Pro 1 month [3rd])"}],"team_members":[],"built_with":[{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"nextjs","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"scrapybara","url":null},{"name":"tailwindcss","url":null},{"name":"terraapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/arnavnc/therms-platform"},{"label":"pubmed.ncbi.nlm.nih.gov","url":"https://pubmed.ncbi.nlm.nih.gov/9804564/"},{"label":"pubmed.ncbi.nlm.nih.gov","url":"https://pubmed.ncbi.nlm.nih.gov/22076097/"},{"label":"pubmed.ncbi.nlm.nih.gov","url":"https://pubmed.ncbi.nlm.nih.gov/19640130/"},{"label":"pubmed.ncbi.nlm.nih.gov","url":"https://pubmed.ncbi.nlm.nih.gov/15879169/"}],"description_sections":[{"heading":"Inspiration","content":"Our project is inspired by the fascinating thermoregulation research conducted by Professors Craig Heller and Dennis Grahn at Stanford University. Their work captivated us (papers linked below), and we had the opportunity to take Professor Heller’s class, where we directly experienced how temperature affects physical performance, health, and disease. For some scientific context, our body's glabrous skin (hairless areas rich in specialized blood vessels called arteriovenous anastomoses, or AVAs) plays a critical role in heat exchange. The soles of our feet and the palms (made of glabrous skin) of our hands act as thermal windows, regulating heat dissipation and conservation. When the body needs to cool down, AVAs dilate, increasing blood flow to the skin’s surface for heat loss. Conversely, when warmth is needed, AVAs constrict, reducing heat loss.\n\nIn Professor Heller’s class, we were actually able to test this research by experimenting on ourselves. We pushed our aerobic and anaerobic limits with and without a cooling stimulus applied to our palms. The results were mind-boggling: we could do more reps and sets of strength exercises, and we nearly doubled our running time in a 100°F room with cooling applied. Experiencing this research firsthand fueled our interest in its potential as an incredibly useful healthcare-engineering project.\n\nHowever, the cooling setup we used in class had some serious drawbacks. It relied on a liquid cooling pump attached to tubes which were bulky, messy, and far from portable. Wearing large cooling gloves made using our hands impossible, which was impractical for real-world applications. Accordingly, we set out to solve this problem by shifting the cooling mechanism to the soles of the feet and designing a compact, self-contained system capable of both heating and cooling. Our goal was to create a wearable thermoregulation device that eliminates the need for a liquid cooling pump while maintaining efficiency, portability, and usability. We also saw an opportunity to integrate healthcare analytics, leveraging data from the device to provide valuable insights for both medical and performance-based applications. In short, we had to create a miniaturized HVAC system that seamlessly fits in the sole of a shoe without the bulk, without the mess, and without interfering with daily activities."},{"heading":"What it does","content":"Therms is a wearable thermoregulation device designed to actively regulate body temperature by targeting the glabrous skin on the soles of the feet. Therms uses thermoelectric Peltier modules to provide rapid, reversible heating and cooling. Our design uses a reversed-polarity model to switch the system between heating and cooling states. An embedded sensor continuously monitors foot temperature, ensuring the system responds in real time to thermal needs. AI-powered analyses of temperature data alongside health metrics (e.g., activity levels, medical conditions) help predict and preemptively adjust stimuli responses , optimizing comfort and performance. Therms also connects to APIs (Terra, Scrapybara, OpenAI) to incorporate additional physiological data, such as heart rate, sleep patterns, and chronic conditions. This enables personalized thermoregulation strategies and early detection of anomalies. For example, if a diabetic user’s foot temperature rises abnormally, Therms can cool the area to prevent tissue damage and AUTOMATICALLY book an appointment with a healthcare professional.\n\nTherms has medical applications, supporting individuals with thermoregulation dysfunction, such as those with spinal cord injuries, diabetes, or neurodegenerative disorders, by stabilizing foot temperature and preventing complications. Therms is also focused on performance-based applications to enhance recovery and endurance by optimizing thermal regulation during and after intense activity (soldiers, athletes, manual laborers). Finally, Therms has immense geriatric applications by protecting elderly users, who are particularly vulnerable to hypothermia and hyperthermia, with preemptive adjustments based on real-time data.\n\nOverall, Therms combines advanced thermoelectric technology with a deep understanding of human physiology to create a wearable system that actively supports thermal homeostasis. By targeting the glabrous skin on the feet, Therms achieves efficient, non-invasive thermoregulation while integrating AI-powered insights to improve health outcomes and performance."},{"heading":"How we built it","content":"After hours of whiteboard sessions and squabbling with one another, we designed a modular wearable system that efficiently manages heat transfer while leveraging AI-powered analytics for personalized thermal regulation.\n\nMiniaturizing an HVAC-like system into a shoe sole required replacing traditional liquid cooling with Peltier boards for precise heating and cooling. These boards were placed on top of a heat sink that were connected with a copper foil so that any heat accumulated on the Peltier after the current was turned off was dissipated away. A motor controller dynamically adjusts polarity and current flow to switch between heating and cooling states. A copper foil was also placed on the top of the frame to connect the Peltier boards and distribute heat transfer evenly among the surface area of the foot, while a temperature sensor was placed next to the top copper layer, but insulated from it, to provide real-time thermal feedback. A microcontroller coordinates these components and processes sensor data, while an IMU sensor tracks movement to provide additional activity insights. The entire system is housed in a modular sole frame (durability, extendability, and ease of integration into different footwear).\n\nThe software ecosystem enables real-time data processing and automation. Firebase handles seamless communication between the device and the web application, while Terra API integrates external physiological data from other wearables. ChatGPT processes the integrated thermal and health data, identifying anomalies and optimizing thermoregulation strategies. Scrapybara automates tasks such as booking medical appointments and managing prescriptions based on detected trends. A Next.js frontend and Node.js backend ensure a responsive and scalable platform for users to monitor and personalize their Therms experience."},{"heading":"Challenges we ran into","content":"A major challenge came from Peltier technology, which can prove to be incredibly inefficient energy-wise and temperature-wise. When turned off, the boards retained and radiated heat on both sides, causing unwanted warming immediately after cooling. To counter this, we created an optimized heat sink system with our modular sole frame that captured as much heat as possible and dissipated it through a copper layer; this significantly reduced the risk of overheating.\n\nThe modular design we keep talking about was one of the most challenging parts. We had to design a system that allowed for easy upgrades and customization and incorporate several different hardware modules in a very small space. After multiple iterations of our 3D models, we refined the design to maximize space efficiency while ensuring easy assembly and part replacement.\n\nOn the software side, we built a pipeline incorporating three different data sources and three third-party APIs: shoe data + user data + Terra API activity data → OpenAI insights → Scrapybara automation. Our streamlined process looked great in theory but proved far more complex in execution. Ensuring data consistency across different technologies was a tedious task, with unexpected bugs that took considerable time and patience to debug."},{"heading":"Accomplishments that we're proud of","content":"We are incredibly proud to have successfully created one of the first completely pump-less, battery-powered heating and cooling systems in a wearable format. Several reports said Peltier technology is inefficient and that using a liquid cooling/heating system that wasn’t mobile was the only way to effectively thermoregulate. We also had no design blueprints to reference, so it was exciting and also gratifying to see our hacky model come to life. Getting the entire design, code, and actual working device done within 36 hours was incredibly rewarding and helped us each hone and develop new skill sets that will serve us for life.\n\nBeyond hardware, we integrated incredibly varied technologies like Terra API, Scrapybara, and OpenAI and created a fully working web application portal that can pair shoes, collect user data, provide insights, and display professional data visuals. We are also proud of the research-first approach we took, where we used mathematical and biological models to quantify stimuli responses, healthcare insights, and have a solid grasp of why this technology is so important to human performance and health."},{"heading":"What we learned","content":"Throughout this project, we gained hands-on experience in developing scalable software and hardware. Our software required multiple APIs and a plethora of data being transmitted to different endpoints, which acted as a forcing function to make our application lightweight and scalable. We can expand the number of shoes, user metrics, wearables integrated through Terra, and much more without any latency using our Firebase + NextJS architecture. We refined our skills in CAD modeling and 3D printing, iterating through multiple designs to achieve a compact and efficient wearable.\n\nOn the hardware side, we learned how to integrate super niche hardware modules, from thermoelectric modules to motor controllers, while balancing thermal physics, power efficiency, and real-world usability. Additionally, we explored IMU sensor modeling, developing a system that extracts motion data to enhance user tracking and inform advanced healthcare analytics. Finally, working with Terra API and Scrapybara taught us how to streamline data integration across multiple platforms, ensuring seamless communication between hardware, software, and AI-driven automation."},{"heading":"What's next for Therms - Thermoregulation Wearables with AI-powered Insights","content":"We plan to refine and optimize Therms with the goal of commercializing it for users who need to condition/train heavily (athletes and soldiers) or users who have thermoregulation dysfunction symptoms. Our next steps include enhancing energy efficiency, ensuring that heating and cooling functions operate with minimal waste, and redesigning the form factor so the entire system fits nicely within the shoe.\n\nFrom this business and research perspective, we aim to scale Therms into a consumer-ready product that enhances human performance and recovery in athletes, workers, and everyday users by reducing fatigue and optimizing thermal regulation. Additionally, Therms has critical healthcare applications, particularly for senior citizens, providing a continuous, non-invasive way to monitor body temperature and mobility, reducing the risk of temperature-related health issues.\n\nBy integrating advanced AI-driven insights, real-time health tracking, and an ergonomic design, we envision Therms becoming an essential, wearable thermoregulation solution for both performance enhancement and preventive healthcare."},{"heading":"Built With","content":"firebase nextjs node.js openai scrapybara tailwindcss terraapi"},{"heading":"Try it out","content":"github.com pubmed.ncbi.nlm.nih.gov pubmed.ncbi.nlm.nih.gov pubmed.ncbi.nlm.nih.gov pubmed.ncbi.nlm.nih.gov"}]},{"project_title":"Ranger","project_url":"https://devpost.com/software/ranger-6jkv5s","tagline":"Audio is a luxury we take for granted, that deaf folks don't get. Ranger is a wearable AR solution aiming to fix that, visualizing audio around them, and bringing them into the world of *sound*.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/498/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"NVIDIA: Best Accelerated Compute (5090s per team member - 1 GPU JHH signed [1st] & 5080s per team member [2nd])"}],"team_members":[],"built_with":[{"name":"adb","url":null},{"name":"edge","url":null},{"name":"jetson-nano","url":"https://devpost.com/software/built-with/jetson-nano"},{"name":"linux","url":"https://devpost.com/software/built-with/linux"},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"unity","url":"https://devpost.com/software/built-with/unity"}],"external_links":[{"label":"github.com","url":"https://github.com/YuanSamuel/Treehacks2025/"}],"description_sections":[{"heading":"💭 Inspiration","content":"Sometimes helpful ideas come from the silliest places. One day, as our team was playing Fortnite instead of doing our homework, we realized how helpful it was to have a \"sound ring\" in game. Noises like enemy footsteps or gunshots are displayed with a visual indicator to show what direction they came from. Luckily, we aren't trapped in Moisty Mire shotgunning ChugJugs, but we did realize people hard of hearing could benefit from this in real life. Both innocuous situations (dropping your wallet in the street and a parent calling you down to dinner) and intense situations (scary noises that would make anyone want to pick up the pace a bit walking home at night, an ambulance screaming by) are much more difficult to navigate for those who can't process auditory input.\n\nVisualizing sound helps you enter and understand conversations easily, avoid dangerous situations, and generally be aware of important goings-on in your surroundings. We wanted to build portable, wearable technology that would bring our vision from our colorful video game screen to the real world. We see our product as a proof-of-concept for something that could truly - cheaply, portably, fashionably - transform the lives of deaf people."},{"heading":"🖥 What it does","content":"Ranger is an edge-based AR solution for audio visualization that uses a Meta Quest frontend to indicate to the wearer where sounds are coming from. The user wears a hat, which has a microphone array to capture omnidirectional audio and send it to our processing unit, a Jetson Nano. Ranger classifies all different sorts of real-world sounds using a classifier model and displays them as icons on a circular grid, placing markers according to distance and direction. For speech, we live-transcribe conversations using Whisper which allows those hard of hearing to immediately parse what's happening, even if speech comes from behind them.\n\nThe sound visualization does not interfere with your real-world view, only enhancing the information already available. It's a real-life HUD, enriching the wearer's experience and using edge computing to bring them into the wonderful world of sound."},{"heading":"🛠 How we built it","content":"Hardware :\n\n1 Meta Quest 3 1 Jetson Orin Nano Mics (ReSpeaker 4-Mic Circular Microphone Array, Boya Bluetooth TX/RX) (Most importantly) A giant cowboy hat\n\nSoftware :\n\nPython Unity ML Models (Yamnet, Whisper)\n\nRanger runs completely on the edge! All heavy computation is done on a Jetson Orin Nano; all communication is done with direct wired USB-C connections. For a TL;DR: using the Jetson Orin Nano, we developed an audio processing software that takes in a 4-channel microphone input, triangulates the direction, amplitude, and type of the loudest sound occurring at any time step, transcribes any detected speech, and sends all this information to the Meta Quest 3 using network-over-USB.\n\nFor those interested in a more in-depth overview:\n\nReal-time Voice Transcription We use Whisper Mini running on the Jetson Nano for real-time voice transcription. To accomplish real-time voice transcription, we capture the last 10 seconds of a user’s audio and process it immediately. Although the Jetson Nano has the ability to run larger Whisper versions (up to the recently released turbo one with 800M parameters), our priority was reducing latency.\n\nSound triangulation To determine where sound comes from relative to the user, we use a ReSpeaker microphone array. We set it up to triangulate the audio channels to pinpoint where audio comes from (an example of this technique below). This approach gave us a vector with angle and volume, which allows us to position our classified sounds as relatively positioned icons in our 3D scene.\n\nSound classification Sound classification is done using a convolutional neural network (CNN). The model runs on the Jetson and classifies audio in a probability distribution of up to 500 candidate labels. Using this class along with the latest source of noise from the sound triangulation, we’re able to accurately pinpoint what a noise is and where it comes from relative to the user!"},{"heading":"🛑 Challenges we ran into","content":"This was our first time doing a hardware-based project, and our inexperience showed up immediately . We found ourselves sifting through mountains of cables, walking back and forth to the hardware booth every 20 minutes, and flipping between \"ITSSOOVER\" and \"WERESOBACK\" faster than the GPU fan on our Jetson Nano. We all loved Operating Systems, Concurrency, and Computer Architecture in school, but building a project completely from scratch, with very little electric or audio engineering knowledge, through largely un-trekked territory was an uphill battle. We had several significant challenges:\n\n1) Parsing raw input data intelligently We planned this out without a pretty cursory understanding of auditory science, so we had to spend a lot of time understanding hardware synchronization concerns, channel mixing, and in-built audio driver configurations. Spending a lot of time thinking and diagramming though, rather than just spurting out poorly written code, was incredibly helpful later during integration.\n\n2) Interfacing between backend and frontend We originally wanted to use bluetooth to communicate between the Jetson Nano and Meta Quest but ran into a ton of issues with trying to get low fidelity bluetooth communication schemes working. After a lot of tinkering, we decided to connect them together with a USB-C cable and used Android Debug Bridge to treat the wire as a network connection through a server socket.\n\n3) Developing a fully on-the-edge system The Jetson Nano relies on a DC power supply, which we tried to get around with a USB-A adapter and a power bank. Our lack of electric engineering know-how showed here, though, as we didn’t realize the USB-A adapter was inherently capping our voltage. We want to give a huge thank you to Mr. Chitoku Yato at Nvidia for saving us on this with a custom USB-C to DC cord."},{"heading":"🏆 Accomplishments that we're proud of","content":"The thing we’re all the most proud of is that we actually built what we set out to build! For our first hardware hack, with three different, distinct devices, across different frameworks, operating systems, and modalities of energy, this was an incredible feat. In the trenches of every challenge - Unity running slower than molasses on our rundown Intel Macs, bugs with multithreaded audio device access, bizarre audio sampling configurations provoking questions that not a soul on StackOverflow seemed to ask - we pushed past it and got things working. Each of us had a different moment we started jumping for joy:\n\n“I started going crazy when we first saw the visualization of the DOA (direction of audio)“ - Tyler\n\n“When I got to see a radar - like white dots on the circle - and the dots started to move when I did, I almost teared up“ - Samuel\n\n“After I spent 4 hours straight on our second Jetson Mini getting the Whisper model working with Cuda“ - Charan\n\n“I will never again feel happiness like I did when I saw the Android Debug logs on the Meta Quest print the first packet we sent“ - Sarvesh"},{"heading":"🧠 What we learned","content":"Hardware is called hardware because it’s hard and you can wear it. Getting through our first hardware hack gave us a lot of confidence both for building on this idea and pursuing new ones.\n\nHardware : Nuances of DC power conversion and portability Mechanics of audio input processing Remote usage and sharing of graphic/audio drivers Software : Supporting machine learning for edge devices Cabled network communication Unity scripting and scene visualization"},{"heading":"✏️ What's next for Ranger","content":"The Meta Quest is the best affordable AR wearable right now, but eventually, we'd want the lightest-weight solution so people would be happy to use Ranger for long periods. We fiddled around with some AR glasses, but many smaller companies focus on treating the glasses as an external monitor, and the Meta RayBans do not have a display. This year, though, the new Meta RayBans will have a visual display, so we could easily swap our (lovably) bulky Quest for a sleek, non-invasive pair of shades.\n\nOur team has been chewing on this idea for a long time, and we want to develop this beyond our 36-hour sprint here at TreeHacks. During our development, we thought of a million insanely cool stretch goals to upgrade our current version, and each could be a project on its own. Special audio software that can perform Single Source Separation would let us transcribe multiple voices at once. A more advanced beam-forming location and tracking algorithm would let us intelligently classify objects over time. Porting over multilingual Voice Language Models to our brave little Jetson could expand this project globally, unlocking a new world of interaction for deaf people. This idea has a remarkable depth that we only scratched the surface of, and our intention in the future is to dive deep ."},{"heading":"Built With","content":"adb edge jetson-nano linux machine-learning python unity"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"BAS Climate Action Matcher","project_url":"https://devpost.com/software/bas-climate-action-matcher","tagline":"A tool to match companies to relevant climate actions. Embedding search to find climate initiatives and agentic workflows+tool usage to discover corporate climate actions from sustainability reports","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/272/015/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Sustainability Prize: Best prototyping process ($800 Cash)"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Intersystems: Best Use of GenAI using InterSystems IRIS Vector Search ($2k Cash [1st] & $1.5k Cash [2nd] & 1k Cash [3rd])"}],"team_members":[],"built_with":[{"name":"dain","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"intersystems-db","url":null},{"name":"langchain","url":null},{"name":"nvidia-llama","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Suzehva/bas_labs"}],"description_sections":[{"heading":"Inspiration","content":"Mitigating climate change is only possible through a patchwork of collective action. The future of our planet will be determined by our ability to change previous destructive corporate processes radically. However, companies often struggle to find concrete solutions and actions to reduce their carbon footprint while advancing their business goals.\n\nWe partner with Race to Zero to create a climate action matcher . Through a user-friendly interface and an agentic workflow grounded in tool usage, we match THE USER COMPANY with relevant UN-catalogued Cooperative Climate Initiatives and provide them with sustainability reports from similar companies and peer corporate actions to inspire concrete action.\n\nUltimately, we want to transform how we approach climate change – from apathy to collective action. By connecting companies and their initiatives, we want to show that action is possible, popular, and influential – especially when done together as an industry, nation, and planet."},{"heading":"What it does","content":"Our tool matches companies to relevant climate initiatives. We provide an agentic system with tools like RAG embedding search on a custom database of company sustainability reports, and UN-cataloged Cooperative Climate Initiatives, web scraping on websites like https://zerotracker.net/ and https://nzdpu.com/home , and more to discover corporate climate actions."},{"heading":"How we built it","content":"DAIN Butterfly : We use the DAIN Butterfly agentic workflow with tool usage as our central orchestrator for user interface interactions. We built custom tools that find companies similar to THE USER COMPANY based on industry sector and country, match companies to UN-catalogued Cooperative Climate Initiatives, and find relevant climate actions from hundreds of sustainability reports from a custom database we built. We provide the agent with the initial context of its foal (e.g., it is trying to write a report that should reference its sources). Still, the agent can choose which tools to use and autonomously decide its tool strategy depending on the outcomes of previous actions and details specified by THE USER COMPANY. To ensure responsible usage, we instruct the agent to include sources to its information (which it can do as our tools return the href links they got their information from) in its findings, allowing THE USER COMPANY to confirm and dive deeper into the sources. We used the DAIN UI components to format the responses engagingly and professionally.\n\nInterSystems Embedding Database : We use InterSystems as our database. We collected and embedded 172 UN-catalogued Cooperative Climate Initiatives with descriptions and over 17,000 paragraphs from scraped sustainability reports.\n\nNVIDIA Llama Embeddings : We use Llama-3.2-nv-embedqa-1b-v2 embeddings for our embedding database and query embedding in our RAG vector search.\n\nLangChain : We use LangChain to load sustainability PDF reports directly from the web and recursively split the text for subsequent chunk embeddings.\n\nGoogle Gemini Scoring and Classification : We implement company sector classification using Gemini Flash Experimental 2.0. Moreover, we use Gemini to score corporate actions based on their reproducibility and return on investment for action ranking and matching.\n\nScrapybara : We implement an agent to find concrete PDF links on corporate websites that may be deep in the link structure of the page.\n\nSelenium Web Browser : We implement web scraping using Selenium."},{"heading":"Challenges we ran into","content":"Finding relevant climate actions first proved tricky since sustainability reports can be pretty vague, and embedding similarity search works best if we try to match the target report structure as closely as possible. We solved the problem by having the DAIN agent brainstorm climate initiatives the company could be doing and then verify these ideas by finding actual climate actions by companies in their sustainability reports.\n\nAnother challenge was to have the agent perform enough actions to take advantage of all our tools. We ended up spending some time on prompt engineering and writing clearer tool descriptions which had a clear boost in performance."},{"heading":"Accomplishments that we're proud of","content":"Created an end-to-end pipeline to match companies with sustainability efforts. Created an embedding vector database with hundreds of sustainability reports to be open-sourced to the broader community after the event. Developed core technical skills in web scraping, database manipulation, embedding models, document parsing, and tool creation. Built our understanding of sustainability reporting and found many avenues for continued work."},{"heading":"What We Learned","content":"We learned a ton during the hackathon! On the technical side, we learned web scraping, document embeddings, how to work with Docker containers, and connecting Python and Typescript! On the environmental side, we opened the door to the vast world of sustainability reporting and tracking. Seeing all the initiatives already underway was inspiring, and we are incredibly excited to keep pushing for more action."},{"heading":"What's next for BAS Climate Action Matcher","content":"Extend the initiatives into a dynamic knowledge graph to track the impacts of climate actions. Extend scoring to include nature-based solutions, collaborations, estimated impact, and cost. Create a dashboard to standardize climate reporting for easier comparison."},{"heading":"Built With","content":"dain gemini google intersystems-db langchain nvidia-llama"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"WiiWork","project_url":"https://devpost.com/software/wiiwork-8u1x62","tagline":"Everyone is hyped about AI agents that can interact with the world. Problem: Websites weren't built with LLMs in mind What would it look like if a website was built ground up with Agent usage first?","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/924/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"NVIDIA: Best Accelerated Compute (5090s per team member - 1 GPU JHH signed [1st] & 5080s per team member [2nd])"}],"team_members":[],"built_with":[{"name":"anthropic","url":null},{"name":"convex","url":null},{"name":"deepseek","url":null},{"name":"elevenlabs","url":null},{"name":"framer-motion","url":null},{"name":"nvidia","url":"https://devpost.com/software/built-with/nvidia"},{"name":"openai","url":null},{"name":"porkbun","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"shadcn","url":null},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null},{"name":"whisper","url":null},{"name":"zustand","url":null}],"external_links":[{"label":"wiiwork.dev","url":"https://wiiwork.dev"},{"label":"github.com","url":"https://github.com/Alezander9/WiiWork"}],"description_sections":[{"heading":"Why:","content":"Everyone's excited about AI agents interacting with the world. Companies are pouring billions into development. But there's a problem: websites weren't built with AI in mind. Over this weekend, I sought to answer: what would it look like to build a website from the ground up to be AI-native?"},{"heading":"How","content":"I built WiiWork solo over the weekend, drawing inspiration from web accessibility patterns. Just like screen readers use ARIA labels, I created a context-rich system where AI agents can understand and interact with web elements naturally.\n\nThe core innovation is a Higher Order Component (HOC) system that wraps around UI elements and communicates their purpose, appearance, and location to the agent. This \"Agentic Web Component\" system is paired with a comprehensive global state manager (using Zustand) that synchronizes user and agent interactions in real-time and prevents errors from getting out of control."},{"heading":"Agent","content":"The agent system is especially interesting. Instead of using a one-size-fits-all approach, I built a multi-model router that classifies user requests into categories:\n\nSimple tasks → GPT-4-mini (fast & efficient) Medium → Claude 3.5 Sonnet (more thoughtful) Complex Logic → NVIDIA DeepSeek (specialized reasoning) Conversational → GPT-4 (natural interaction) Malicious messages get flagged and a AI alignment model interferes\n\nEach category has its own carefully engineered system prompt, optimizing for both performance and cost. I prompt engineered all the models using Theo, and prompt engineering tool I built over winter break: https://trytheo.dev"},{"heading":"Cool Features","content":"Real-time voice interaction using ElevenLabs for speech synthesis and Open AI whisper for speech regognition Mobile-desktop pairing system for voice input Virtual cursor system with smooth animations Tool calling framework using regex parsing Reading list demo app Oh yeah, and the whole Wii thing. I was just feeling nostalgic. ## Technical Challenges The biggest challenge was maintaining state synchronicity between user and agent interactions. I solved this by creating a central state management system that handles both input streams consistently. Another tricky part was getting the voice interaction just right - dealing with audio streams and buffers, especially managing faulty recording features on mobile phone browsers, was a learning experience for me. ## What I Learned This project pushed my full-stack skills to the limit. I got deep experience with: Building complex AI orchestration systems Swapping between multiple LLM providers and maintaining chat history Real-time audio processing Low level web component code Accessibility-inspired design patterns"},{"heading":"What's Next","content":"I believe this pattern of AI-native web development is the future. In a few years, I expect many websites will use similar patterns to make themselves agent-friendly. WiiWork is just the beginning, a proof of concept showing what's possible when we make websites with Agents in mind from the start.\n\nBuilt solo at TreeHacks 2025 using React, TypeScript, Convex, Zustand, and working about 25/36 possible hours. Check it out at https://WiiWork.dev"},{"heading":"Built With","content":"anthropic convex deepseek elevenlabs framer-motion nvidia openai porkbun react shadcn tailwind typescript vercel whisper zustand"},{"heading":"Try it out","content":"wiiwork.dev github.com"}]},{"project_title":"Lemon","project_url":"https://devpost.com/software/lemon-7gn5hq","tagline":"A simple, transparent ecommerce platform for bargain produce that connects consumers and businesses with local farmers to reduce food waste and fight climate change","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/268/614/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Sustainability Prize: Best incorporation of broader context to enhance venture viability ($800 Cash)"}],"team_members":[],"built_with":[{"name":"dart","url":null},{"name":"flutter","url":"https://devpost.com/software/built-with/flutter"},{"name":"supabase","url":null}],"external_links":[{"label":"lemon.flutterflow.app","url":"https://lemon.flutterflow.app"},{"label":"github.com","url":"https://github.com/twosodium/lemon"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/run/DdNU9oq49bbWYsMZY3xf"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/project/thackssss-jvfor7"}],"description_sections":[{"heading":"Inspiration","content":"Reducing food waste is one of the most effective ways to combat climate change. In fact, food waste has an environmental footprint 248x bigger than plastic! 🌍 As we researched the root causes of food waste, we discovered that some 1.2 billion tonnes of fruit and vegetables are discarded annually at farms before they ever reach the market. But why? In the U.S., over 50% of produce is thrown away because it’s considered \"too ugly\" to sell well — that’s about 60 million tons of perfectly edible food gone to waste. One of our team members has seen firsthand how healthy and edible produce gets discarded just because it doesn’t look perfect. This issue is REAL and needs to be addressed immediately."},{"heading":"What it does","content":"To solve this problem, we built Lemon. 🍋🌱 Lemon is an app that connects farmers, businesses and individual customers to reduce food waste and fight climate change. By signing up as a customer, you can access discounted, surplus produce from local farmers that would otherwise go to waste. Lemon offers a fully transparent shopping experience; each product is listed with details about its source, and the user has full control over when and what products they buy, like traditional online grocery stores. Producers (farmers) can create listings for their surplus or \"wonky\" produce, allowing them to sell items that are perfectly good but not deemed aesthetically pleasing enough for traditional markets. Businesses can also shop for discounted produce like customers, but Lemon offers business accounts additional support by enabling them to request in bulk, advertise to nearby users and and form long-term partnerships with local farmers. Lemon’s goal is to raise awareness about the impact of food waste, making it easier for everyone to make sustainable shopping choices and contribute to a greener, more connected world.\n\nPainpoints ❓Many consumers want to make sustainable choices but don’t know how. Lemon makes it easy by offering access to eco-friendly food at discounted prices. 🍏 Farmers with surplus & 'imperfect' produce get to sell their produce, creating a new source of income and increasing their positive impact on the environment. 👉 Businesses get to access locally-produced crops at much lower prices, search for and partner with producers and benefit from advertising perks, like sustainability spotlights on the app's main page that are shown to customers in their area.\n\nBroader context Lemon’s focus on reducing food waste aligns with environmental regulations. The U.S. government has set a goal to cut food waste in half by 2030 through various initiatives and consumer education. Some states are even passing legislation to restrict the amount of food waste going to landfills. Lemon has strong word-of-mouth potential, driven by eco-conscious consumers as well as farmers who are profiting out of Lemon. We also incorporated share buttons in the app so that users can share information about this initiative with their friends.\n\nMarket & Impact With 60 million tons of wasted produce every year in the U.S., the opportunity for a sustainable solution is enormous. The demand for eco-friendly food options is growing rapidly.\n\nStakeholder Incentives Farmers: sell produce that would otherwise go to waste; a new source of income. Customers: get fresh, discounted produce; contribute to sustainability. Businesses: get fresh, discounted produce in bulk; get recognition for sustainability efforts."},{"heading":"How we built it","content":"We implemented our vision for Lemon using Flutterflow. After drawing a lo-fi prototype, taking user experiences into account, we started familiarizing ourselves with Flutterflow and got to building. We used Supabase/PostgreSQL as our database and also added customized functions to our Flutter app. Using Flutterflow's sponsored special access plan, we were able to collaborate real-time on the app and distribute tasks between team members."},{"heading":"Challenges we ran into","content":"Familiarizing ourselves with a completely new environment, Flutterflow, was challenging at first. This is the first hackathon experience of all team members, making the process extremely exciting but challenging at the same time. Some things on which we spent more time than we probably needed to were 🙃\n\nSetting up the database and user authentication Product listings Shopping cart"},{"heading":"Accomplishments that we're proud of","content":"We are super proud of all the progress we made! Starting with only an interest in contributing to sustainability to landing on an idea to building it out with Flutterflow, we accomplished so much that we are proud to showcase."},{"heading":"What we learned","content":"We are amazed to see how fast we were able to pick up how Flutterflow works and our ability to familiarize ourselves with a new tool showed us the importance of transferable skills, like logic. It also demonstrated that 'hackers' should be quick to adapt and fast learners."},{"heading":"What's next for Lemon","content":"We are looking forward to getting feedback on our app, its design, and functionality. We hope to make iterations based on the feedback. We will have access to Flutterflow for two more months and who knows---maybe we'll even get to publish our app!\n\n[NOTE: Only MOBILE view]"},{"heading":"Built With","content":"dart flutter supabase"},{"heading":"Try it out","content":"lemon.flutterflow.app github.com app.flutterflow.io app.flutterflow.io"}]},{"project_title":"HorizonX","project_url":"https://devpost.com/software/horizonx","tagline":"HorizonX is a dual-purpose platform using !!on-device!! VLMs to empower blind people with real-time haptic guider while crowdsourcing anonymized data to help governments improve public safety.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/310/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Tesla: Excellence Prize ($2k Tesla Store Gift Card [1st], $1k Gift Card [2nd], $1k Gift Card [3rd])"}],"team_members":[],"built_with":[{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"nextjs","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"webgpu","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/bsflll/treehack2025"}],"description_sections":[{"heading":"Inspiration","content":"GPT 4o sucks!! It is built for the visually impaired, but only with wifi!! Inspired by the potential of Video Language Models (VLMs) , we hope to bridge accessibility gaps. Over 2.2 billion people globally live with vision impairments, we built HorizonX an AI camera, a privacy-first real-time \"sight partner\" on mobile phone that help the blind individuals navigate crowded places. At the same time, our AI allows people to report hazards (potholes, broken signage) to government simultaneously improving public spaces for everyone, which facilitate government's reconstruction efficiency and transparency."},{"heading":"What it does","content":"HorizonX is a two-part ecosystem:\n\nFor User - Mobile App for Visually Impaired Users:\n\nNarrates surroundings in real time via camera (e.g., “A cyclist approaching from your left”). Guides navigation with obstacle detection and step-by-step audio cues. Reads text aloud (menus, signs, currency) and identifies products.\n\nFor Governments - Centralized Platform for Public Safety\n\nAggregates anonymized data from users to map hazards (potholes, broken signage) and crowds. Provides dashboards for infrastructure planning and emergency response."},{"heading":"How we built it","content":"Running a VLM on a mobile device is a hefty task. After trying multiple approaches with MLX and WebGPU, we failed over and over to overcome the bandwidth and memory limits of our phones. We found a way to run a Linux emulator on the Galaxy S24 and mount it to local images, then installing core pytorch and tokenization libraries in a python environment to host a streaming API. We have a local endpoint hosted on the network to send the anonymized reported descriptions on a live map. This map also has information sourced from Sonar Research Pro calls to source live citizen complaints and create a centralized view. All of this is brought together with our locally hosted app, which—without connection—can navigate individuals around complex areas."},{"heading":"Challenges we ran into","content":"Extreamly difficulty system integration\n\nMLX development of moondream in swift running moondream library: couldn’t install onnxruntime on Termux on Android finding smaller vlms: Moondream2B is by far the smallest model (other than Moondream0.5B) tried translating Moondream0.5B from custom ONNXVL wrapper to ONNX to Torch to serve directly from local safetensor, but would have to rewrite KV caching twice and serve custom KV caching also tried webGPU or running from browser, but don’t have enough RAM on Safari to do inference"},{"heading":"Accomplishments that we're proud of","content":"Getting a multi-billion sized VLM to run on a mobile device! This is a significant milestone in local-first and privacy-focused AI inference. Since mostly it's impossible to run a 2B model on mobile phones."},{"heading":"What we learned","content":"Vim on a phone is easier than on a laptop"},{"heading":"What's next for HorizonX","content":"Speed up model on edge devices by further quantization or better integration with the current software. And provide more humanistic service to the special community and better verification experience for government."},{"heading":"Tesla Challenge","content":"Also, we participated in Tesla Challenge, here's our statement: 1) inspiration Combine LLMs and CV could combine to provide richer insights than traditional image-processing pipelines alone. By leveraging GPT-based models, we hoped to produce human-like, context-aware answers to driving-related questions, going beyond standard object detection or classification tasks. 2) code https://github.com/bsflll/treehack2025/tree/main/tesla-real-world-video-q-a 3) methodology Data: We sample frames at fixed intervals from short driving videos—here, five frames from a nominal 5-second clip. This approach provides a concise but representative snapshot of each scenario. Prompt: Descriptive Prompt by GPT-4o: Generates high-level contextual awareness of each video. Expert Opinions (Model “o1”): Requests multiple “expert answers” in parallel (e.g., five completions) to encourage diverse perspectives. Consolidation (Model “o1” / Final Step): Summarizes or decides on the best single answer. Minimal Answer Extraction: Ensures only the final letter (choice) is returned for submission or scoring. Asynchronous Execution Failure check at the end:\n\n4) kaggle links Submission by Zian Shi"},{"heading":"Built With","content":"docker javascript nextjs python pytorch react webgpu"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CyberDrive","project_url":"https://devpost.com/software/cyberdrive","tagline":"CyberDrive - Transforming driving footage into intelligent insights.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Tesla: Excellence Prize ($2k Tesla Store Gift Card [1st], $1k Gift Card [2nd], $1k Gift Card [3rd])"}],"team_members":[],"built_with":[{"name":"amazon","url":"https://devpost.com/software/built-with/amazon"},{"name":"async","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"openai","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"}],"external_links":[{"label":"github.com","url":"https://github.com/KoaChang/TeslaMCQTreehacks"}],"description_sections":[{"heading":"Built With","content":"amazon async gemini google openai opencv python pytorch"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"WingNote","project_url":"https://devpost.com/software/wingnote","tagline":"WingNote – Smart hardware meets AI-powered documentation to listen, structure, and simplify care for nurses and patients.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/805/datas/medium.JPG","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Google Cloud: Best Use of Gemini (Google Pixel Watches x4)"}],"team_members":[],"built_with":[{"name":"arduino-nano","url":null},{"name":"deepgram","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"li-po-battery","url":null},{"name":"mistral-ai","url":null},{"name":"next.js","url":null},{"name":"openai","url":null},{"name":"perplexity-api","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rfid-reader","url":null},{"name":"supabase","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null},{"name":"vercel-v0","url":null},{"name":"xiao-esp32","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/bencullenn/wing-note"},{"label":"gamma.app","url":"https://gamma.app/docs/WingNote-Smart-Hardware-for-Seamless-Nurse-Documentation-ggxl8whe2cff0lg"}],"description_sections":[{"heading":"Inspiration","content":"Nurses spend over 4 hours per shift on documentation, leading to burnout, high turnover rates, and reduced patient care time. At the same time, most patients don’t understand their own medical notes, creating a gap in healthcare accessibility.\n\nWe set out to build WingNote —a fully integrated hardware + AI-powered solution that automates nurse documentation while ensuring patients receive clear, personalized summaries in their preferred language and at their reading level.\n\nWith a tap of an RFID badge , a nurse can start recording a patient interaction, and AI does the rest—transcribing, analyzing, structuring, and presenting the information in an EHR-ready format , while also generating a patient-friendly summary ."},{"heading":"What it does","content":"WingNote is a smart badge & AI-powered software system that automates clinical documentation for nurses and improves patient understanding.\n\nHardware\n\nRFID-based patient identification – Scans patient wristbands to log patient ID. Power efficient audio & video syncing – Captures speech & patient behavior for every interaction throughout the day. Xiao ESP32-S3 smart badge – Microphone, camera, and Wi-Fi enables wireless transmission of real-time data. Custom 3D-printed housing – Encases ESP32, RFID reader, and patient wristbands for hospital use.\n\nAI-powered software\n\nSpeech recognition (Deepgram API) – Converts nurse-patient dialogue into detailed, diatorized text. Vision analysis (Gemini 2.0 Flash) – Identifies helpful details about the patient and room to enhance notes. EHR structuring (Mistral AI) – Transforms raw conversation into Epic-compatible notes. Doctor’s review portal (Next.js/Vercel v0) – Physicians edit & approve AI-generated documentation. Patient summary app (Next.js/Vercel v0) – Provides multilingual, easy-to-read summaries of doctor's notes. Patient Q&A (Perplexity Sonar API & Postgres SQL) – Patients can receive reliable medication answers grounded in their doctor's notes and web data.\n\nWingNote fully automates the documentation pipeline, improving workflow efficiency and patient care."},{"heading":"How we built it","content":"Hardware stack\n\nXiao ESP32-S3 Sense – Microcontroller with Wi-Fi, camera, and microphone. RC522 RFID reader – Used for patient wristband scanning. Arduino Nano (I2C bridge) – Converts SPI to I2C to allow RFID to work over a retractable wire. Custom 3D-printed cases for: Smart badge (ESP32-S3 with mic & camera). RFID reader & Arduino Nano (compact with retractable cable). Patient wristband (RFID tag enclosed for easy scanning).\n\nSoftware stack\n\nDeepgram API – Speech-to-text transcription with enhanced medical-grade accuracy and speaker identification. Gemini 2.0 Flash API – Patient behavior tracking. Mistral AI – Converts conversation data into structured EHR notes. Next.js/Vercel v0 – Doctor's portal → EHR-ready notes for review & approval. Patient’s portal → Multilingual, age-adapted health summaries. Perplexity API – Provides LLM responses grounded in web data Supabase - Postgres SQL database and cloud storage for notes and artifacts\n\nFrom hardware to AI-powered documentation, we built WingNote from the ground up."},{"heading":"Challenges we ran into","content":"RFID integration with ESP32-S3 The RC522 RFID module requires 6 wires , but the retractable cable we used only had 4 wires . Solution: Introduced an Arduino Nano as an I2C bridge , converting: RFID Reader (SPI, 6-wire) → Arduino Nano (SPI to I2C) → ESP32-S3 (4-wire I2C). Dual serial communication on Xiao ESP32-S3 Initially, we tried using two UART ports simultaneously, but the ESP32-S3 couldn’t support dual UART for both the computer and the RFID unit . Solution: Switched RFID communication to I2C and kept UART free for debugging. Power efficient data transmission over Wi-Fi Issue: ESP32-S3 couldn’t stream high-resolution video & audio simultaneously and it takes a lot of battery to do so continuously . Solution: Store audio and video locally on the device while filming. Transfer audio and video between appointments to save on battery and improve quality. Reliably transferring data between the hardware and software remains a significant challenge.\n\nOvercoming these challenges led to a fully functional, AI-powered clinical documentation system."},{"heading":"Accomplishments that we're proud of","content":"Built a complete hardware + AI solution from scratch . Seamless RFID scanning + multimodal AI processing (speech + video). Fully functioning real-time documentation workflow (nurse → AI → doctor → patient). Deployed an AI-powered patient portal that enhances healthcare accessibility. Integrated Deepgram, Gemini 2.0, Perplexity, and Mistral AI to process medical documentation.\n\nWingNote is a fully operational, AI-enhanced documentation system that works end-to-end."},{"heading":"What we learned","content":"Embedded systems development – Learned how to interface ESP32-S3, RFID modules, and I2C communication. AI workflow optimization – Successfully merged audio & vision AI into a seamless pipeline. Real-time AI integration – Implemented low-latency STT + vision analysis with structured data formatting. Hardware design & 3D printing – Designed & printed custom enclosures for smart badges, RFID readers, and wristbands.\n\nThis project pushed us to solve real-world hardware & AI integration challenges."},{"heading":"What's next for WingNote?","content":"Direct EHR integration (Epic, Cerner, etc.) Currently, doctors review AI-generated notes in our portal, but future versions will push data directly into EHRs. Real-time transcription for nurse efficiency Currently, transcription happens after the visit, but we plan to add live transcription during conversations . Wider language support Expanding multilingual AI summaries & patient Q&A support for even more languages.\n\nWingNote is just getting started—our vision is to make documentation completely seamless."},{"heading":"Built With","content":"arduino-nano deepgram gemini li-po-battery mistral-ai next.js openai perplexity-api python rfid-reader supabase typescript vercel vercel-v0 xiao-esp32"},{"heading":"Try it out","content":"github.com gamma.app"}]},{"project_title":"Portable Braille","project_url":"https://devpost.com/software/portable-braille-cdq8v5","tagline":"Educational device that encourages young visually impaired people to learn and practice Braille in order to increase chances for future employment and expand general literacy level of the community.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/064/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"NVIDIA: Robo Prize (Jetson Dev kit per team member)"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"TreeHacks Grand Prize | 3rd Place (iPhone 16 Pro per team member)"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"bluetooth","url":"https://devpost.com/software/built-with/bluetooth"},{"name":"braille","url":null},{"name":"c","url":"https://devpost.com/software/built-with/c"},{"name":"esp32","url":null},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"rayban|meta","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Adil-Jussupov/PortableBraille"}],"description_sections":[{"heading":"Inspiration","content":"Literacy level of Blind and Visually impaired community even in United States is lower than 10%. And the reason is simple - Braille (tactile text) is not affordable and not accessible today. I follow Haben Girma - the first deaf-blind person graduated Harvard Law school. She is a brilliant person, wrote books, influencer and advocates for people with disabilities rights. I saw how efficient she is in communications with people. However, she needs a special person who assists her by typing speech of her interlocutors, so it can be converted to Braille format and Haben can read it in tactile format. I believe that she could do even more if there is a device that could do handle it, keeping her communication private and without relying on assistance."},{"heading":"What it does","content":"For visually impaired people the device allows users to read and write text on the phone privately, texting, surfing web, swiping Instagram, and of course learn and practice reading and writing. It is the first device that encourages blind users to improve their reading skills in most delightful ways - communication with loved ones and friends. Also device allows to \"read\" face expressions while talking to people, by face expression recognition using RayBan|Meta glasses and doing computing on the Jetson Orin Nano. It is the only solution today that doesn't require connection to internet and work even faster than BeMyEyes (powered by OpenAI). For deaf-blind users convert all speeches to text, and then converts it to tactile Braille and render in on the device, so user doesn't need an assistant."},{"heading":"How we built it","content":"We 3d-printed an enclosure of the device on the 3d printer. Soldered Li-Po battery to battery-management-System module for correct charging/discharging and powered the microcontroller and Step-up boost DC-DC converter to power up braille module, which needs 200V for operating. Braille module gets data from microcontroller. There is a keypad of at least 6 buttons for typing braille combinations. Keypad also connected to the microcontroller. Microcontroller syncs with a phone over Bluetooth Low Energy. We hacked RayBan|Meta glasses in analog way to connect in to Jetson Orin Nano."},{"heading":"Challenges we ran into","content":"Education Open"},{"heading":"Accomplishments that we're proud of","content":"First really portable braille display that fits a pocket It encourages to practice reading Connected different platforms, even closed, to each other Allows blind and visually impaired individuals to receive and follow contemporary AI trends: not to listen but to read audio messages."},{"heading":"What we learned","content":"how to power the hardware device with AI connect different platforms altogether still learning..."},{"heading":"What's next for Portable Braille","content":"test the device with more users to collect more feedback explore if this project can transform to a Product"},{"heading":"Built With","content":"arduino bluetooth braille c esp32 ios rayban|meta"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ZoneOut","project_url":"https://devpost.com/software/zoneout-atz5pe","tagline":"Zoned out during a lecture? Your very own lecture buddy is here to save you! With real-time Q&A based on audio, text & visual context from the lectures, you'll be acing your classes again in no time!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/487/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Zoom: Best Use of Zoom APIs ($250 Git Card + Herschel Duffle Bags [1st] & $100 Git Card + Hoodies [2nd] & $50 Git Card + Bottles [3rd])"}],"team_members":[],"built_with":[{"name":"chromadb","url":null},{"name":"colpali","url":null},{"name":"openai","url":null},{"name":"rtms","url":null},{"name":"vlm","url":null},{"name":"zoom-api","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/NxtGenLegend/TreeHacks"}],"description_sections":[{"heading":"Inspiration","content":"We've all been in classes or meetings where we ZoneOut, even if for a few seconds, and came back to see that the cure to cancer has been invented! Inspired by attending online lectures this Friday (we're dedicated students), we found out that this isn't as uncommon as you'd think. As a matter of fact, the average retention rate of a student after just 45 mins of online learning is just 61% , which is HIGHER than the average person. Moreover, the average drop in engagement is 87% (scaling exponentially!) on average, in meetings with larger sets of participants."},{"heading":"What it does","content":"ZoneOut utilizes textual, visual & audio contexts from meetings, enabling our AI assistant to teach, revise & explain any concept, In-Depth & in Real-Time , to users to maintain higher levels of retention, productivity & engagement."},{"heading":"How we built it","content":"We built ZoneOut with a complex, yet execution directed architecture, developed completely on Windsurf . We used the Zoom API to connect the client to a Real-Time Media Server (RTMS) via a Handshake protocol, and consequently sampled data at microintervals as well as based on when a sentence/section of an idea being discussed finished. We collected textual data from the chat, audio data via live transcripts and visual screensharing/camera data with help from Zoom's API. We then used OpenAI for embedding the text & images with Chain-Of-Thought (CoT) Reasoning to keep context well-fitted and connected, independent of context window sizes and to keep images & text associated with one another. We also used parallel computation to allow us to index this data using Chroma concurrently, associating images with concepts in both audio & textual formats in different timestamps. Finally, we used a similarity search RAG system with ChromaDB for the audio/transcript & textual data, and a vision-based RAG system on ColPali (VLM) , which we accelerated using a caching system that we developed, allowing us to use it without reloading it into memory again & again. The outputs of both RAG systems are then passed through OpenAI's API to format it nicely. We also optimized sampling parameters to avoid hallucinations caused by excess external information or misunderstanding contextual information. We then send this data back to the client, who's now back in the loop of everything that's happening!"},{"heading":"Challenges we ran into","content":"Originally, the RTMS faced issues with streaming audio & video. After a lot of debugging & troubleshooting, we found & cured the error by handling edgecases through intensive vision programming back & forth, sending our sample code to the Zoom team so they can debug other teams. Then, our VLM workflow turned out to be too slow as the VLM was being loaded into memory repeatedly. So again, after coding a lot of reacharounds, we finally implemented our own caching system to supercharge our VLM, which now works with various forms of handwriting effectively. We also faced hallucinations wherein the model knew information it should not, and misinterpreted information it had. We cured this using indexing & CoT, to reach the product we have today!"},{"heading":"Accomplishments that we're proud of","content":"This hackathon has been a proud technical moment for all 3 of us. Our achievements stem from our challenges. We very quickly figured out the edge case of professors writing on whiteboards, both virtual & real, instead of explaining things like equations. So we developed a multi-language model workflow to work around. Another proud accomplishment was improving the Zoom RTMS repo, as we were the first people that figured it out, turning our curiosity into open source contributions in Zoom's repos. Next was integrating a complex parallel workflow to interpret & contextualize images, text & audio data altogether, particularly because of how LLMs & VLMs can be very funky sometimes. After that, was when we implemented our own caching system to boost our VLM system, after having faced a barrage of vision problems. Finally, was our creative use of prompt engineering, context windows & frontend-backend structuring for Windsurf to swap between entirely different frontend frameworks (HTML/CSS & React) & even simple backend worflows without breaking the frontend or the backend, letting us build very quickly, despite initial samples & software not being completely compatible, causing issues in the webSockets & handshake protocol, amongst other incompatibility issues."},{"heading":"Built With","content":"chromadb colpali openai rtms vlm zoom-api"},{"heading":"Try it out","content":"github.com"}]},{"project_title":" Noteworthy: Visualize, Capture, Remember.","project_url":"https://devpost.com/software/noteworthy-visualize-capture-remember","tagline":"AI-powered lecture companion: Transforms speech into real-time visualizations, incorporates notes, and enhances learning with summaries, resources, and chatbot integration—effortless and efficient.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/764/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Zoom: Best Use of Zoom APIs ($250 Git Card + Herschel Duffle Bags [1st] & $100 Git Card + Hoodies [2nd] & $50 Git Card + Bottles [3rd])"}],"team_members":[],"built_with":[{"name":"bash","url":"https://devpost.com/software/built-with/bash"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"html/css","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"ngrok","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"p5.js","url":null},{"name":"perplexity","url":null},{"name":"playwrightapi","url":null},{"name":"postman","url":"https://devpost.com/software/built-with/postman"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"zoomapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/arushisharmaa/stanford-tree-hacks-"}],"description_sections":[{"heading":"Inspiration and What it Does 🌟","content":"We've all been in lectures where the professor quickly sketches a vague diagram, expecting instant understanding, then erases the board while moving onto a completely new topic. Most students struggle to keep up, especially in technical subjects where visualizations are crucial . While we have had many text-to-speech and text-to-image integrations in educational resources, we rarely see speech-to-image transformations . Our team wanted to redefine this learning experience by building an AI-powered lecture companion that dynamically generates real-time visuals based on lectures. This tool also provides students with a comprehensive learning platform where their notes are efficiently stored and organized, as well as allowing students to access summaries, Q&As, chatbots, and other useful resources."},{"heading":"What We Learned 💡","content":"Throughout the development process, we discovered several key insights about multimodal communication, efficient information transmission, and integration of AI models:\n\nReading APIs and understanding the unique offerings of each technology to develop innovative applications. Extensive verbal information could be represented with concise illustrations. Vertical integration of AI-based technologies requires a clear formal definition of expected inputs and outputs. Visual Learning Matters as students absorb information faster when concepts are accompanied by meaningful visuals. Personalized Learning Improves Retention as students benefit from a tool adaptive to their pace and note-taking style. Collaboration Enhances Education as the ability to share AI-generated notes and visuals fosters deeper discussions and understanding."},{"heading":"How We Built It 🛠️","content":"Our project utilizes cutting-edge AI technologies to create an intelligent, real-time assistive educational application. The development process unfolded through these innovative stages:\n\n🔹Speech-to-Text Processing\n\nWe engineered a speech recognition system by:\n\nLeveraging Zoom's powerful servers to capture and process audio in real-time . Utilizing data generated from Zoom’s WebSocket Technology for text extraction. Crafting a custom JSON-to-text converter that dynamically updates transcripts as lectures progress.\n\n🔹AI-Generated Visuals\n\nWe created lecture visualizations by:\n\nAnalyzing speech transcripts to categorize word functions and highlight key concepts. Integrating Large Language Models (LLMs) to transform extracted data into structured visual prompts. Engineering prompts to receive strictly structured outputs redirected into further scripts. Standardizing the style of visual representations of concepts. Interfacing with OpenAI's API to generate P5.js code , enabling real-time sketch creation . Utilized client-server communication to switch between JS and Python. Implementing a dynamic rendering system that evolves visuals as the lecture unfolds.\n\n🔹Intelligent Note-Taking\n\nWe implemented a versatile digital workspace by:\n\nDesigning an intuitive interface for seamless image insertion and manipulation. Creating a powerful multi-source merger capable of integrating diverse media types (chatbots, PDFs, etc.).\n\n🔹AI Summaries & Resource Suggestions\n\nWe enhanced learning opportunity through:\n\nIntegrating Perplexity AI API to generate comprehensive summaries and power an intelligent Q&A system. Developing a sophisticated resource recommendation engine using Perplexity AI API for curated video, book, and article suggestions. Incorporating Gemini AI API to provide instant, context-aware answers to student queries.\n\n🔹Collaboration, Sharing, and Organized Access\n\nWe created a collaborative learning environment by:\n\nDesigning an intelligent storage system that categorizes notes by course, lecture, and date. Seamlessly integrating Zoom's Calendar API to auto-populate the app with students’ calendar data. Developing a versatile note management system allowing anytime access, editing, printing, and sharing."},{"heading":"Challenges We Faced 🚧","content":"Building an advanced AI-powered tool in a limited timeframe posed several challenges:\n\nEvaluating the capabilities and limitations of each cutting-edge AI model and leveraging sample sets to make a final decision on an image-generation tool. Ensuring Visual Accuracy – Mapping abstract concepts to the right AI-generated diagrams was complex. Real-Time Syncing – Keeping students and professors on the same page without lags. Balancing Automation & Control – Allowing AI to generate content while keeping pace with professors’ lectures."},{"heading":"Project Impact 🌙","content":"Education should be accessible, engaging, and tailored to every student’s needs . Our tool:\n\nSupports Different Learning Styles – Visual, textual, and interactive elements enhance comprehension. Bridges the Gap for Struggling Students – Helps those who have difficulty keeping up with fast-paced lectures. Promotes Inclusive Learning – Works across devices, even for students without premium note-taking tools. Encourages Deeper Engagement – AI-generated questions and insights stimulate critical thinking, leveraging the best technologies available for student growth."},{"heading":"Why It Stands Out 🎉","content":"Our AI-powered lecture companion aligns perfectly with Zoom’s Education Grand Prize criteria :\n\nCreativity – A groundbreaking way to transform passive lectures into interactive learning experiences . One of the very few real-time speech-to-image education tools on the market! Technical Complexity – Integrates real-time AI speech processing, APIs, visual generation, chatbot integrations and multi-source syncing . Social Impact – Empowers students worldwide by making education more accessible, personalized, and engaging . It helps all students learn comfortably at their own pace."},{"heading":"Future Directions 🔮🔜","content":"Rendering Video and Images on the Same Platform Deploy the app as an extension in platforms like Zoom Apps and Microsoft Apps to seamlessly integrate video and image rendering during lessons and presentations. Utilizing Additional Tools for Diverse Illustration Styles Leverage tools like Mermaid AI to create structured flowcharts and diagrams, while integrating advanced models to visualize more complex topics, enhancing the learning experience. Implementing the React 360 Framework Explore how this platform can fit into a VR space by implementing the React 360 framework , providing a more immersive and interactive learning environment."},{"heading":"The Future of Learning Starts Now 🚀","content":"This is just the beginning. Our AI-powered lecture companion redefines how we learn, connect, and grow —ensuring that every student, no matter their learning style or background, has the tools they need to succeed in the classroom and beyond ."},{"heading":"Try Us Out! 📱","content":"📹 GitHub: https://github.com/arushisharmaa/stanford-tree-hacks- 📹 Youtube Video: https://youtu.be/m_1f7-KvGmQ"},{"heading":"Built With","content":"bash gemini html/css javascript ngrok node.js openai p5.js perplexity playwrightapi postman python react zoomapi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"FlowPilot: Your Agentic Co-Pilot","project_url":"https://devpost.com/software/flowpilot-zoom-agent-that-automates-your-workflows","tagline":"FlowPilot listens, understands, and executes, transforming Zoom meetings into Slide decks and Notion workflows. No more lost action items - just approve and watch it execute.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/129/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"DAIN Labs: AI Agent Excellence & Innovation Awards ($5k Cash [1st] & $2.5K Cash [2nd] & $500 Cash [3rd] + Agent Launchpad Invitation)"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Zoom: Best Use of Zoom APIs ($250 Git Card + Herschel Duffle Bags [1st] & $100 Git Card + Hoodies [2nd] & $50 Git Card + Bottles [3rd])"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Perplexity: Hacking With Perplexity Award ($50 Gift Card per team member)"}],"team_members":[],"built_with":[{"name":"dain","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"rtms","url":null},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"},{"name":"zoom","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/cyu60/dain-agent"},{"label":"github.com","url":"https://github.com/CarterCote/flow-pilot"},{"label":"flowpilot.us","url":"https://flowpilot.us/"},{"label":"v0.dev","url":"https://v0.dev/chat/fork-of-presentation-app-PJSbvsry9Gc"},{"label":"ai-generated-presentation.vercel.app","url":"https://ai-generated-presentation.vercel.app/?id=83dc8131-c0a6-4863-a2b2-c9147c1d3588"}],"description_sections":[{"heading":"Inspiration","content":"FlowPilot listens, understands, and executes, transforming Zoom meetings into Slide decks and Notion workflows. No more lost action items - just approve and watch it execute. FlowPilot provides amazing collaboration opportunities for sales, engineering, and product teams alike.\n\nHaving been apart of workflows at fast-growing startups and large corporations, our team grew increasingly frustrated with the significant friction caused by meetings. This includes the amount of action items discussed in meetings that often get forgotten, tediously written, or executed.\n\nWe noticed that while tools like Notion excel at documentation and Google Workspace handles scheduling, there wasn't a seamless way to transform meeting conversations into automated workflows. FlowPilot bridges this gap by creating an intelligent layer that turns verbal commitments into automated actions."},{"heading":"What it does","content":"FlowPilot transforms Zoom meetings into an autonomous productivity engine. By leveraging DAIN's advanced agent framework, our system:\n\nCreates comprehensive Notion documentation including meeting notes, workflow templates, and task assignments Automatically generates follow-up slide decks for educational sessions and team presentations Handles calendar scheduling and email communications through Google Workspace Maintains contextual awareness across multiple meetings to track long-term projects and commitments"},{"heading":"Technical Architecture","content":"1. Meeting Intelligence Layer\n\nZoom RTMS Integration : Implements WebSocket connections for real-time audio stream processing Verifies local server client for the Zoom application via Ngrok\n\n2. DAIN Agent Framework Implementation\n\nAgent Orchestration : Custom-built context router that manages state between multiple specialized agents Implements DAIN's memory system for maintaining conversation context and user preferences Uses structured JSON formats for inter-agent communication and task delegation Specialized Agents : Documentation Agent: Handles Notion to add tasks and todo Calendar Agent: Manages Google Calendar scheduling and reminders Communication Agent: Handles Gmail operations and meeting follow-ups, and supports audio email attachments via Eleven Labs text-to-speech Presentation Agent: Automates slide deck creation and updates using a custom renderer from v0.dev Outbound Phone calls: Make outbound phone calls via DAIN client\n\n3. Custom Agent Integration through Orchestrator\n\nData Processing Pipeline : Orchestrator agent built using Magic Loops' schema system Real-time event processing for immediate action execution\n\n4. Execution Framework with DAIN\n\nTunneling Service Architecture : Secure WebSocket tunneling for real-time bi-directional communication Load-balanced request routing for scalable action execution OAuth token management for secure service authentication Service Integration Layer : Notion API integration for document/task creation and updates Google Workspace API integration for calendar and email management Custom templating engine for slide deck generation with v0.dev interface Send email with audio transcript generated with ElevenLabs"},{"heading":"Implementation Challenges Solved","content":"Our journey in building FlowPilot presented several complex technical hurdles that required innovative solutions. The first major challenge was optimizing real-time processing of audio streams. We solved this by implementing a sophisticated buffering system that uses chunked transfer encoding, allowing us to process meeting data with minimal latency while maintaining data integrity. Context management proved to be another significant challenge, particularly when handling multiple concurrent meetings and maintaining conversation state. By leveraging DAIN's memory system, we developed a robust solution that maintains contextual awareness across sessions while efficiently managing system resources.\n\nAction reliability was perhaps our most critical challenge, given the importance of executing tasks accurately. We addressed this by building a resilient tunneling service with intelligent retry mechanisms and transaction management, ensuring that every action - from creating Notion pages to scheduling calendar events - completes successfully or fails gracefully with proper error handling. The final hurdle was integrating multiple third-party services seamlessly. We overcame this by developing a unified authentication layer that efficiently manages various OAuth flows while maintaining secure token storage and rotation.\n\nAnother major hurdle was building out a custom presentation renderer. Developing it took a long time and was difficult to get running properly. With v0.dev, however, we were able to quickly build a custom frontend that reliably renders our presentations. Finally, integrating multiple third-party services seamlessly was challenging, so we developed a unified authentication layer that efficiently manages various OAuth flows while maintaining secure token storage and rotation."},{"heading":"Future Technical Roadmap","content":"Our vision for FlowPilot's technical evolution focuses on two key areas: advancing our agent intelligence and scaling the platform infrastructure. We're working on implementing DAIN's advanced memory models to significantly enhance our agents' context retention capabilities, allowing for more nuanced understanding of long-term projects and recurring meetings. This will enable our agents to make more intelligent decisions about task creation and workflow automation while maintaining context across multiple sessions.\n\nOn the infrastructure side, we're architecting a distributed processing system that will handle concurrent meetings more efficiently, implementing an enhanced caching layer for improved performance, and moving towards a microservices architecture for our agent components. This architectural evolution will allow us to scale horizontally while maintaining the reliability and speed our users expect. We're particularly excited about the potential to expand our agent ecosystem, creating specialized agents that cater to specific industries and use cases while maintaining our core focus on seamless automation and intelligent workflow management."},{"heading":"Built With","content":"dain docker express.js node.js rtms websockets zoom"},{"heading":"Try it out","content":"github.com github.com flowpilot.us v0.dev ai-generated-presentation.vercel.app"}]},{"project_title":"Lumora","project_url":"https://devpost.com/software/lumora","tagline":"Lumora is a Personal Assistant Marketplace. No need to manually set meetings or write to-do lists any longer! Automatically generate quizzes and flashcards based on your online zoom class!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/277/806/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Vercel: Most Creative Use of Vercel/v0 in Edge AI track ($1.2k in Vercel Pro Credits)"}],"team_members":[],"built_with":[{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"next.js","url":null},{"name":"perplexity-api","url":null},{"name":"supabase","url":null},{"name":"tailwindcss","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"v0.dev","url":null},{"name":"zoom-api","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ShaneYokota72/Lumora"}],"description_sections":[{"heading":"✨ What is Lumora?","content":"Lumora is a Personal Assistant Marketplace that transforms your Zoom meetings and chats into actionable workflows. By integrating with Zoom's powerful APIs, Lumora orchestrates intelligent workflows to automate tasks, boost productivity, and enhance collaboration—whether you're in education, enterprise, or beyond."},{"heading":"💡 Why the Name Lumora?","content":"Lumora represents illuminating collaboration—bringing clarity and energy to your workflows."},{"heading":"🎯 What Inspired Us","content":"In our day-to-day lives, we constantly encounter small tasks that add up to consume significant time—scheduling meetings, creating study materials, or researching topics. When these tasks accumulate, they can overwhelm our productivity. Lumora was born from the vision to automate these routine tasks, allowing people to focus on what truly matters."},{"heading":"🚀 What We Built","content":"Within 36 hours of hacking, we created a powerful collaboration hub featuring:\n\nThe Lumora Library\n\nWe've built 7 specialized tools across education, productivity, and social media:\n\nEducation Suite : Quiz Generator, Flashcard Creator, Research Assistant Productivity Tools : Meeting Scheduler, Todo List Manager, Timeline Manager Social Media : Tweet Drafter\n\nKey Features\n\nComprehensive Zoom Integration : Processes both meeting transcripts and chat messages in real-time Automatically triggers relevant workflows based on conversation context Works seamlessly with the world's leading meeting platform Dynamic Marketplace : Modular architecture enables unlimited tool creation Future potential for thousands of community-created tools Future plans to share and discover tools built by other users Customizable Workspace : Select and activate only the tools you need Mix and match tools for your specific workflow Easily add new tools as your needs evolve Interactive Dashboard : Track completed tasks and take instant actions: Schedule meetings with one click View generated quizzes Access research findings with useful source links Post tweets directly to Twitter/X"},{"heading":"🔮 What's Next","content":"Live AI Tutor/Assistand : Video and voice comes together to help users when they're stuck! Screensharing included! No-Code Tool Creator : Enabling users to build custom workflow tools Expanded Library : Adding more specialized tools for different industries Enhanced Integrations : Supporting more platforms beyond Zoom"},{"heading":"🛠️ Tech Stack","content":"APIs : Zoom API, Groq API, Perplexity APIs, Calendar API LLM : Groq for orchestration, function calling, and lightning speed inference. Perplexity for deep up-to-date research Frontend : Next.js, Tailwind CSS Backend : Node.js Database : Supabase"},{"heading":"📜 License","content":"Lumora is open-source under the MIT License .\n\nMade with ❤️ by the Lumora Team | Revolutionizing the way we work, one task at a time ✨"},{"heading":"Built With","content":"groq next.js perplexity-api supabase tailwindcss typescript v0.dev zoom-api"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Cipher Shield","project_url":"https://devpost.com/software/cipher-shield","tagline":"Making discrimination mathematically impossible via the blockchain & homomorphic encryption","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/612/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Taisu Ventures: Next-Gen Web3 Security Award (4x Deeper Network Air + 1 NGRAVE cold wallet)"}],"team_members":[],"built_with":[{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openfhe","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"}],"external_links":[{"label":"github.com","url":"https://github.com/zoraizmohammad/treehacks-2025"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for our project came from a problem based approach to ideation. We as students fill out countless job applications, and each one asks for demographic data. Most simply opt out as there seems to be no tangible benefit. In fact, there seems to only exist a potential downside: that it may be illegally misused to negatively affect your application.\n\nAnd yet, demographic collection does serve a large social purpose for society, specifically for detecting trends that may expose discrimination. In numerous cases, including employment, healthcare, loans, and credit cards, collecting demographic data is actually mandated by the federal government. We figured that in the era of decentralization and advanced encryption schemes, there is no reason for misuse of this information to even be mathematically possible ."},{"heading":"What it does","content":"Cipher Shield acts as a trusted third party, providing completely secure data aggregation services via the blockchain. We empower companies to extract aggregate statistical insights from homomorphically encrypted data, thus making it impossible to profile individuals based on their protected data."},{"heading":"How we built it","content":"Blockchain: The chain is a critical component of our architecture and business model since it accelerates trust in our services. Every aggregation request is stored on the chain, creating an immutable, public record. Before any homomorphic computation is initiated, our smart contract performs critical on-chain validation - a transparent verification that ensures enough records exist to make reverse engineering mathematically impossible. If there aren't enough data points to maintain statistical anonymity, the contract automatically rejects the request. Once the request is validated and the computation is complete, the final aggregated result is also stored on chain, creating an audit trail of our services that companies can trust - we can't nullify data because every step is permanently and publicly verifiable on the blockchain.\n\nDealing with advanced encryption schemes and blockchain operations, our business logic was written entirely in C++ and solidity. We leveraged OpenFHE homomorphic encryption operations and key generation.\n\nAs a B2B product, our demo needed to include an entire demo business to use our product, which required a whole other backend+frontend pair."},{"heading":"Challenges we ran into","content":"We ran into a number of challenges while building the product: Implementing our design of the homomorphic encryption in OpenFHE library, specifically with the dual private key setup which did not come as a out of the box solution. Writing the implementation for the backend responsible for the data aggregation in C++ and facilitating the key exchange with correct serialization, deserialization. Managing the large payloads of ciphertext in the communications between company and server and storing ciphertext."},{"heading":"Accomplishments that we're proud of","content":"We started off as 4 complete strangers with incredibly varying interests and skillsets and we were able to collaborate seamlessly. Together, we created the first ever implementation of a split key homomorphic encryption scheme that’s also verified through blockchain technology. We learned a lot about homorphic encryption as well as blockchain. We were able to intergrate the full stack of the application together and make all of the parts of the system working locally, with all of the Customer Data, Company/Organization, and Authority."},{"heading":"What we learned","content":"We learned about how to implement Homomorphic Encryption libraries and packages and alter their fundamentals to be able to work with split keys. We learned about the different ways that data security and insurance is important in B2B SaaS solutions, as well as potential business market ideas that could benefit both business and consumer at the same time. We also learned about the integration of Web-2 with Web-3 technology as well as integrating visualizations on the full-stack end despite coming in with very limited knowledge about blockchain. Moreover, we also learned about creating front-end web applications as well as a balance of creativity, inspiration, and feasibility when implementing a large scale and ambitious project. We also learned about the importance of Web-3 in web-application security."},{"heading":"What's next for Cipher Shield","content":"We believe Cipher Shield should mediate the demographic collection and protected data collection processes of every company in America, and we believe this will have transformative societal outcomes in mitigating discrimination.\n\nThe data aggregation capabilities of our scheme extend far beyond single company aggregation, but can even be used to securely combine hospital datasets.\n\nTo encourage companies to adopt our tech, we would also want to create an easy to use SDK for organizations, companies, and industries less knowledgeable on the prospect and understanding of Blockchain and encryption. Furthermore, the ability to directly create a form or application directly on Cipher Shield and embed it into any company or organizations’ website directly through a Web Embedding that CipherShield uses would be essential to seamless adoption."},{"heading":"Built With","content":"c++ node.js openfhe react solidity"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"BEF","project_url":"https://devpost.com/software/beef-xk5pif","tagline":"For leaders in high-performance environments, we aggregate real-time wearable data from teams, providing LLM-driven insights, detecting arrhythmias, and reconstructing ECGs—no medical expertise needed","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/879/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Terra API: Health Command Center (Ultrahuman Air Ring per team member)"}],"team_members":[],"built_with":[{"name":"cuda","url":"https://devpost.com/software/built-with/cuda"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"terraapi","url":null},{"name":"vercel","url":null},{"name":"vercelv0","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/shafink11/ppg_project"}],"description_sections":[{"heading":"EDGE-DEVICE Implementation","content":"The Mistral AI 7B model is a 7.3-billion-parameter large language model that we chose as the core AI engine for the Health Command Center hosted on a singular GPU in Rayan's poor overworked GPU. This model was selected because it offers strong performance relative to its size, is open-source, and is designed for conversational, instruct-tuned tasks.\n\nIntegration and Pipeline\n\nOur system aim to use a Retrieval-Augmented Generation (RAG) approach by integrating the Mistral 7B model with our Terra dataset stored in a Pinecone vector database. Here’s a breakdown of the process:\n\nData Indexing in Pinecone: We would convert collected health data (e.g., \"Date: 2025-02-10 – Steps: 12,000; Avg. Heart Rate: 75 bpm; Sleep: 7.5 hours\") into textual summaries. These summaries would be transformed into high-dimensional vectors (embeddings) using an embedding model. The resulting vectors are stored in the Pinecone vector database, creating an index of the user’s health data that supports fast similarity searches. Query Embedding: When a user asks a question (e.g., \"Did I meet my step goal this week?\"), the query is embedded into the same high-dimensional space. This ensures that semantically similar queries and data entries have vector representations that are close together. Retrieval from Pinecone: The query embedding is used to perform a similarity search within the Pinecone database. The top matching entries (typically the most relevant data points) are retrieved to serve as context for the model’s response. Response Generation with Mistral 7B: The retrieved contextual data is combined with the original query and fed into the Mistral 7B model. Mistral 7B generates a response grounded in the specific health data. For instance, it might respond: \"Yes, you met your step goal on 5 out of 7 days this week, with an average of 11,000 steps per day, exceeding your goal of 10,000 steps.\" This approach ensures that the AI’s answers are factual and based on the actual data rather than on generalized assumptions."},{"heading":"ECG Reconstruction and AFIB Classification Inference","content":"Our system addresses two primary goals:\n\nClassify arrhythmic events directly from PPG data. Reconstruct a single-lead ECG signal from PPG, enabling physicians to interpret events more reliably.\n\n1. Arrhythmia Detection with a Lightweight 1D CNN\n\nWe trained a 1D CNN on a PPG dataset (70% split for training) to determine whether a given PPG segment indicates an abnormal event—such as frequent premature ventricular contractions (PVCs) or atrial fibrillation. Key highlights:\n\nArchitecture : Two convolution layers (16 then 32 channels), global max pooling, and a final fully connected layer for a binary classification logit. Performance : Despite its simplicity, this CNN outperformed more complex models like LSTMs and classical machine-learning approaches (SVM, random forests). It also converged more reliably, making it ideal for on-edge inference. Training : We used a combination of SGD with momentum and BCEWithLogitsLoss , leveraging mixed precision ( torch.cuda.amp ) for efficiency.\n\nOnce this model flags an abnormal segment, the system triggers the reconstruction step described below.\n\n2. Single-Lead ECG Reconstruction via a BiLSTM\n\nWhen the CNN identifies a suspicious arrhythmia, we reconstruct a single-lead ECG from the same time-aligned PPG segment, offering a more familiar signal for clinical review:\n\nWorkflow : During the user’s initial setup, they can record short bursts of simultaneous ECG (e.g., from an Apple Watch) and PPG data for personalized calibration. This paired data trains a small BiLSTM network to map from PPG signals to the corresponding ECG waveform. Once deployed, whenever the PPG classifier detects an arrhythmic event, the trained BiLSTM model reconstructs a single-lead ECG snippet. Architecture : A two-layer bidirectional LSTM (hidden size = 64) with a linear layer outputting a single channel at each time step. Loss Function : Mean Squared Error (MSE) between the reconstructed ECG and the actual ECG ground truth. Efficiency : Early stopping and Adam optimization keep training fast and prevent overfitting, making it practical to fine-tune per user on consumer-grade hardware. Note - paired-PPG-ECG database sourced from here\n\nSignificance\n\nBy combining arrhythmia detection (1D CNN) with on-the-fly ECG reconstruction (BiLSTM), our approach ensures:\n\nTimely Alerts for non-experts (e.g., coaches or team leaders) when something is amiss. Clinically Interpretable Data (ECG-like signals) for physicians to review remotely. Low Resource Requirements , making real-time monitoring feasible at the edge without specialized medical devices or extensive GPU infrastructure.\n\nWorkflow\n\nWhen an anomalous event is detected in the health data from Terra—such as irregular heart rhythm patterns—the system immediately initiates an in-depth analysis by reconstructing the ECG signal. This process involves several critical steps:\n\nEvent Detection and Trigger The system continuously monitors incoming wearable data for any irregularities or predefined warning signs. Once an event is detected, it triggers a signal that activates further analysis. Dedicated GPU-Powered Endpoint An endpoint hosted on Rayan's desktop, equipped with an NVIDIA GPU, is designated to handle the computationally intensive tasks like Fortnite. This setup ensures that the ECG reconstruction and subsequent classification are performed quickly and efficiently. AFIB Classification Inference Advanced machine learning algorithms (ground-up made deep neural networks optimized for time-series data) analyze the signal. These algorithms are specifically trained to identify patterns associated with atrial fibrillation (AFIB) as well as other cardiac anomalies. The outcome is a classification that indicates whether the detected patterns correspond to AFIB or another condition. ECG Reconstruction Instead of relying solely on raw wearable sensor data—which may be incomplete—the system reconstructs a high-fidelity ECG signal when an alert is processed. This process integrates multiple data streams to generate a detailed view of the heart's electrical activity, providing a clearer understanding of potential abnormalities. Actionable Insights With a high-quality ECG and a corresponding classification result, the system is able to offer actionable insights. If AFIB or another serious cardiac condition is detected, the system generates alerts for further review by medical professionals, ensuring timely intervention.\n\nThis integrated, GPU-accelerated approach enhances the overall reliability and depth of our health monitoring system, transforming raw sensor data into critical, actionable medical insights."},{"heading":"Technical Challenges","content":"Throughout the development of the B.E.F., we encountered several significant technical challenges, which led to valuable learning experiences and informed our future directions.\n\nData Synchronization and Freshness\n\nChallenge: Ensuring that the health data remained up-to-date was difficult due to reliance on periodic data fetching rather than true continuous streaming.\n\nReal-Time Streaming Difficulties\n\nChallenge: While Terra offers real-time streaming via WebSocket connections and BLE, implementing a stable, continuous data stream was complex."},{"heading":"User Interface and Experience","content":"The design of the user interface (UI) aimed to create an intuitive, conversational experience where users could query their health data as if talking to a personal health assistant.\n\nInteraction Flow\n\nChat-Style Interface: Users interact via a simple chat window, where they type natural language questions. Immediate Feedback: Upon query submission, the system processes the query using the RAG pipeline and responds within a few seconds with a detailed, conversational answer.\n\nSample Queries and Responses\n\nQuery: \"What was their average heart rate this week?\" Response: \"This week, their average resting heart rate was about 72 bpm. It remained stable throughout the week, ranging from 70 to 74 bpm, which is consistent with their usual readings.\" Query: \"Did they meet my step goals recently?\" Response: \"Over the past 7 days, they met or exceeded your daily step goal on 5 days. Their best performance was on Thursday with 12,345 steps, while two days fell slightly short at around 8,000 steps.\" Query: \"How has the teams sleep been in the last month?\" Response: \"In the last month, they averaged approximately 6 hours 45 minutes of sleep per night. There were about 10 nights where they achieved the recommended 7–8 hours, though a few nights dropped to around 5 hours. This suggests a trend of slightly below-target sleep on many days.\" Query: \"Give me a summary of the teams workout performance.\" Response: \"In the past two weeks, they completed 5 workout sessions. They ran a total of 15 miles with an average pace of 9 minutes per mile, burning roughly 2,300 calories. Notably, their longest run was 5 miles on Tuesday, and their pace has improved by about 30 seconds per mile compared to two weeks ago.\""},{"heading":"Future Enhancements","content":"Enhanced User Interface and Visualizations\n\nDynamic Visuals: Future iterations should incorporate interactive charts and graphs that dynamically update as users query different aspects of their health. Mobile and Voice-Enabled Interfaces: Developing dedicated mobile apps and voice-controlled interfaces would enhance accessibility and enable seamless integration into daily routines. User-Centric Customization: Allowing users to customize their dashboard and the types of alerts or insights they receive would create a more personalized experience.\n\nhttps://www.mdpi.com/1424-8220/21/1/156"},{"heading":"Built With","content":"cuda python pytorch react-native terraapi vercel vercelv0"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"WatchDog AI","project_url":"https://devpost.com/software/watchdog-ai","tagline":"Anti-Rug AI agent for memecoins by frontrunning + sandwich attacks. Longer version: - Memecoin risk evaluator - Auto trade bot - Stops hard and soft rug by removing financial incentives","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/091/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Taisu Ventures: Most Promising Web3 Project (4x Deeper Network Air)"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/BdNWoG/watchdog-ai-final"}],"description_sections":[{"heading":"Inspiration","content":"Crypto AI - especially AI agents - has been a hot topic within the industry with many players trying to identify new ways to incorporate AI agents into their builds. However, it seems that the most promising fits are still within that of memecoins - another hot topic this cycle.\n\nMemecoins are fun. But rugs are not. Yet, it seems like there is no way to stop rugs from happening since it is financially advantageous to do so... or is there?\n\nps another inspiration is the web3 track \"prompt\" to use AI agents + AVS to do something crazy\n\nSo why is this crazy. Because this essentially is a way for a person to do good, make money and stop memecoin rugs at the same time. You get to make money of the rugger (be it hard rug - liquidity removal or soft rug - sell offs). In this project, we align fun, profitability and security."},{"heading":"What it does","content":"WatchDog AI in its essence is a memecoin anti-rugger, solving the problem by removing the financial incentives. The AI agent is designed to sit on top of a potential AVS, obtaining information from 1) DEX to analyse the riskiness and the possibility of rugs, 2) mempools for transactions and 3) operates as an MEV trading bot.\n\n1) Memecoin evaluator The AI agent itself possesses the ability to evaluate the potential of future rugs based on past transactions and other data about the memecoin. It provides an evaluation on a risk scale. If the memecoin is deemed to be \"risky\" WatchDog AI would start listening to the mempool for potential malicious rugs.\n\n2) Mempool listener WatchDog taps into the mempools of coins to identify suspicious transactions in real time. It could be able to detect rug attempts and react instantly by sacrificing gas fees to frontrun the rug attempt.\n\n3) MEV attack Autotrader Identifying the rug attempt (for instance a large sell off), the agent could decide to frontrun the transaction with a large short and buy back after the the rug attempt has taken place. (Or even worse, find a way to obtain a flash loan to create a further leverage on the trade. ) This would greatly reduce the monetary gains of rugger. Else if the rugger attempts to remove liquidity, the bot could remove liquidity ahead of the rugger such that there are insufficient liquidity for the rugger to remove resulting in their loss before adding liquidity back into the pool."},{"heading":"Technicals","content":"To implement this project the following were developped,\n\nA simulated DEX, A risk evaluating AI agent A mempool listener A frontrunning + sandwich attack trading bot.\n\nThe latter few kind of combines to become a simulated AVS/can be easily moved on top of an AVS.\n\nThe following demo will be conducted on a simulated DEX (since repeatedly rug testing a real dex would probably not be wise). As such, the technicals of the price mechanisms and all (simple) transaction modes were built out from scratch together with the associated mempool (bootstrap version).\n\nThe AI agent is able to analyse the coin based on robust data and autonomously make trading decisions to profit and stop rugs. This part is the more fun part and probably the part that really steps out of current boundaries. For the functional demo refer to the media attached."},{"heading":"Potential Future Steps","content":"This project framework can be deployed in an on-chain environment to become a fully verifiable trustless AI. A potential framework could be DEX -> dexscreener, AI agent -> WatchDog, AVS -> Eigen AVS, transactions -> Flashbots"},{"heading":"Closing/What can WatchDog potentially Do","content":"WatchDog AI is a real-time, trust-minimized DeFi security solution that uses AI agents detect malicious transactions. It directly addresses the 40%+ of crypto scams that come from rug pulls and malicious contract actions by preemptively intercepting them in the mempool. By leveraging frontrun and sandwich attacks, it proactively rescues user funds instead of merely labeling risky tokens. Unlike profit-driven MEV bots, WatchDog AI focuses on protecting users and recovering stolen funds, thereby establishing a new standard for real-time, trust-minimized security and greatly reducing rug pulls and malicious contract scams in DeFi—making rug pulls a thing of the past."},{"heading":"Built With","content":"python solidity typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"The Giving Tree","project_url":"https://devpost.com/software/the-giving-tree-x0a5hk","tagline":"Intelligent, Efficient, Crypto Donation Platform powered by AI agents","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/274/192/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Taisu Ventures: Web3 for Good Award (4x Deeper Network Air)"}],"team_members":[],"built_with":[{"name":"beautiful-soup","url":"https://devpost.com/software/built-with/beautiful-soup"},{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"chromadb","url":null},{"name":"flutter","url":"https://devpost.com/software/built-with/flutter"},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rss","url":null},{"name":"web3","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/emilyjsun/treehacks25/tree/main"}],"description_sections":[{"heading":"Inspiration","content":"As natural disasters and social crises fill our news feeds today, the need for humanitarian aid is at an all time high. With charitable donations sometimes being the only thing many charities rely on to combat their respective issues, for causes such as natural disasters or international conflicts, timely donations make a real time impact. However, not all types of donations are helpful. Without knowing what a disaster-stricken community might need, donations of material goods can quickly cause problems, classified as the “Second Disaster”.\n\nOur team discussed what allows rapid monetary transactions between a donor and a charity, through borders and roadblocks that material or cash donations could face, cryptocurrency. Cryptocurrency, once sent from one wallet to another is immediately a liquid asset one can use. Cryptocurrency is essential in areas where it is currently impossible to physically access banks and their services. With its speed, for charitable causes that require immediate reaction, crypto donations could contribute in helping hundreds to thousands and even millions of people. For example, just within seven days after the horrid Earthquakes in Turkey and Syria, AHBAP, a local non-profit, managed to raise 4.9 million dollars through crypto donations. It is also reported that around 20% of non-state aid to Ukraine was done in crypto currency.\n\nBut nobody has time to constantly check the news, find a charity related to a topic, and donate. We wanted to come up with the most efficient way to assist users with crypto charity donations while being responsive to urgent news that relates to causes that they wish to donate to."},{"heading":"What it does","content":"The Giving Tree is the most efficient and intelligent charitable donation portfolio manager. It takes the user’s mission statement (topics that they are passionate about) and creates their category portfolio. With live RSS feeds from news websites, our app tracks every article that it deems relevant to charity. It then sorts these articles into categories of impact. With these articles, we find charities that we could donate to and update our user’s charitable donation portfolio in real time to ensure that the user is making the most efficient and intelligent donations."},{"heading":"How we built it","content":"The Giving Tree is built with a Flutter front end, Python FastAPI backend scripts, a database layer including both Postgres and ChromaDB cloud databases, as well as a series of AI agents to help us cater a collection of charities to most effectively handle user’s crypto. Our chroma databases use similarity search with the embeddings to provide the framework in sorting and filtering out news articles and charities.\n\nWe scraped charity data from a series of websites, gathering wallet addresses, names, and descriptions to help determine donation portfolios. Additionally, we are constantly scraping RSS feeds for new articles, which are filtered by relevance and further processed through a similarity search to find the most relevant charities. This information, along with user data, is passed to smart contract functions to execute and record transactions on the block chain."},{"heading":"Challenges we ran into","content":"Coming into the project, none of us had any experience developing with Flutter before nor have tried Web3 on Flutter. Because of this, there was a relatively steep learning curve when it came to our frontend, especially when integrating with AppKit, a package that was relatively unused in this context. We struggled with authentication of crypto wallets due to the limited documentation and community suggestions as well as generating and tuning prompts for the AI agents that would lead for us to get the responses that we thought were “correct”.\n\nBecause our project had so many different parts to it, it was also a struggle trying to piece them all together. While we had mapped out our system layout, it was still a struggle bringing the full integration to life and we worked close to the deadline to connect everything."},{"heading":"Accomplishments that we're proud of","content":"Having never really done Web3 development in the past, creating an application involving blockchain and integrating it with a frontend that none of us have worked with before was a difficult yet fulfilling challenge. We also had a relatively complex workflow involving our data processing and filtering, and being able to not only scrape everything but smoothly integrate our results with AI agents and vector databases was something we were proud to accomplish. Trying to wrap our heads around how everything ultimately tied together as well as highlighting the necessary relationships and interactions required collaborative system design effort from everyone, and we had very productive conversations trying to optimize our structures."},{"heading":"What we learned","content":"As a team, we learned about developments in Flutter and creating our own API endpoints to support integration of backend development with Flutter. We learned how to utilize software IDE’s that integrated AI flawlessly to power our development process. We experimented and challenged ourselves to learn more about agentic workflow, letting AI make its own decision with tool calling, and classifications evaluations in order to fine tune our prompt generation. We also had the opportunity to dive deeper into Web3 and blockchain development, taking time to experiment with smart contracts and crypto wallets."},{"heading":"What's next for The Giving Tree","content":"We want to continue integrating other cryptos into our app so that crypto donations become accessible to anyone who has ever owned a crypto wallet. We also want to expand the number of charities that will accept crypto donations, maybe once they find out how effective and powerful crypto donations could be, they’ll join us for our mission!"},{"heading":"Built With","content":"beautiful-soup chroma chromadb flutter openai perplexity postgresql python rss web3"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Vigilante","project_url":"https://devpost.com/software/vigilante-xkf7s9","tagline":"Combat misinformation through flagged Tweets and grounded sources, and reference a realtime dashboard that visualizes trends on a global scale.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/662/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Otsuka Valuenex: VALUENEX Big Data Visualization Award ($1k Amazon Gift Card)"}],"team_members":[],"built_with":[{"name":"d3.js","url":"https://devpost.com/software/built-with/d3-js"},{"name":"fastapi","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"next","url":null},{"name":"perplexity","url":null},{"name":"plasmo","url":null},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"supabase","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/dav1dbai/Vigilante"},{"label":"www.usevigilante.com","url":"https://www.usevigilante.com/"}],"description_sections":[{"heading":"Inspiration","content":"Today, misinformation spreads quicker than you can say “retweet.” Staying informed has never been more crucial—or more challenging. Social media platforms can be powerful tools for connection, but they’ve also become battlegrounds for accuracy, where misleading claims and manipulated information thrive. We built Vigilante to change that.\n\nIn addition to being driven towards working on projects that solve real-world problems, each of our teammates are incredibly passionate about human-computer interaction—how technology and innovation overlaps with the human experience. We came across a paper published in the journal Science where researchers leveraged AI chatbots to combat belief perseverance (DOI: 10.1126/science.adq1814). As solution-centric engineers, we saw endeavors like these as validation that AI can be a powerful tool to fight entrenched misinformation and cognitive biases. We wanted to bring that power to the masses, leveraging AI to combat misinformation and improve media literacy in doing so. We chose to focus on misinformation on social media because it’s one of the defining challenges of our lives. Social media has the power to shape public discourse and individual belief systems. When placed in the wrong hands, it can be used as a tool for propaganda, sowing division and manipulating public opinion. For a demographic like ours, these points feel especially poignant and relevant. We were motivated by the idea of building a tool that could not only combat misinformation but also help foster media literacy—empowering users to think critically about what they read and engage with online."},{"heading":"What it does","content":"Vigilante provides users with a real-time fact-checking workflow that allows them to effortlessly identify potential misinformation . The user’s entire Twitter feed is anonymously ingested and, using agentic AI workflows, verifies in real-time every single post. Posts are stripped of evocative or emotional language and are broken down into their individual claims; these claims are then verified using Perplexity's Sonar API to ground our analysis in up-to-date sources.\n\nWe do not flag every tweet you see as misinformation—rather, we carefully filter out opinions from statements , satire from misinformation . We also highlight when posts (and even specific claims) are, in fact, accurate and supported by evidence.\n\nUsing Vigilante, in addition to staying informed through your feed, you also help contribute to a crowdsourced database of tweets, their metadata, and analyses. This data lives on a dashboard that visualizes ongoing trends in misinformation on social media using data science and NLP techniques. This publicly available dashboard provides its audience with insights into what topics to be wary of online, all the while helping to tune your media literacy and stay on top of misinformation."},{"heading":"How we built it","content":"Vigilante exists in 3 parts: a Chrome extension, the fact-checking engine, and a data visualization dashboard. The fact-checking engine, as well as routes for interfacing with our database stored in Supabase, is exposed via a FastAPI backend. The FastAPI backend supports data queries and analysis for the dashboard, and the model routing and plumbing for our verifier. We deploy and expose our backend for production on Koyeb.\n\nThe Chrome Extension is built using Plasmo as a framework, with React/Tailwind for frontend. We implemented details like caching tweets client-side and using Plasmo messaging to bypass standard Chrome popup styling. The dashboard uses Next/React/Tailwing as scaffolding, and D3 for data visualizations. We also support real-time updates on the dashboard using a combination of technologies. For regular updates to overall statistics (i.e. number of tweets and claims analyzed), we leverage the Stale-While-Refresh technique through the useSWR hook in React. For live updates on new tweets flagged by our system, we loop in Supabase real-time messaging and listen for table insertions in the database.\n\nThe fact-checking engine is built on top of multiple AI technologies. To ensure the best UX as possible, it was imperative the system exhibited the lowest amount of latency as possible. To accomplish this goal, we leveraged Groq’s blazing fast inference API. We use 3 different language models across the system, each of which excelled in different tasks: gemma2-9b-it , ​​llama-3.1-8b-instant , and llama-3.2-11b-vision-preview . We specifically use the vision model to endow the system with multimodal capabilities, allowing it to handle posts with text, images, and/or video, since each of these modalities are critical to a post’s core meaning. To evaluate a post’s claims, we leverage the new Perplexity Sonar Pro model to both search the web for evidence and provide an explanation for whether a claim is accurate or misleading. This process is also completely parallelized for each claim embedded within the text of a post, allowing for as fast of a system as possible."},{"heading":"Challenges we ran into","content":"Our team is all super passionate about this problem space, but the initial hours of our work on this project was somewhat fragmented as we weren’t aligned on specific details. We went on a variety of fruitless goose chases, and didn’t settle on a specific vision for our project until we were deep in the weeds of it half-way through. (For instance, we toyed around with the idea of incorporating datasets of tweets from past election seasons and had even built out some visualizations, but ultimately chose not to pursue that path.) Ultimately though, we were able to rally and refocus on our vision—moving away from disorganized prototyping to a specific actionable gameplan.\n\nOne specific issue we ran into was brainstorming visualizations for the dashboard. We all had different ideas for how we envisioned the dashboard, and we initially found ourselves working independently on designing visualizations and developing data processing pipelines. However, when we synced up, we realized our visions had all largely diverged from each others’. Though it isn’t inherently bad to have competing ideas, it was difficult for us to reconcile a common vision once we had all individually put so much thought into our own. Once we put our heads together, however, we developed a cohesive plan for not only this dashboard, but our project altogether. This taught us the importance of early alignment and continuous collaboration—syncing frequently and creating a shared vision early on can save time and reduce duplicated or divergent efforts.\n\nA more technical issue we encountered was not being overly skeptical of (what we thought were) good results from our system. The fact-checking system went through numerous rounds of iterations (archived as a Python notebook in our repo) and we had a working prototype that we implemented into our FastAPI backend. However, after performing a refactoring for additional features, we accidentally removed the call to the Perplexity API, a critical functionality, without us noticing. We took for granted that the system was still flagging tweets as “misleading” without actually checking to see whether its explanations were up to our standards. Thankfully we were able to catch this issue and resolve it in time before it got buried even further. We learned to not take surface level results for granted and to look deeper behind the scenes—which is quite fitting for this project altogether."},{"heading":"Accomplishments that we're proud of","content":"One of our biggest accomplishments was our ability to iterate rapidly and improve upon multiple aspects of our system within a short timeframe. We took on significant technical challenges and addressed performance bottlenecks in our backend with creative, scalable solutions. Each iteration brought meaningful improvements, and we’re proud of how we managed to adapt and refine the system as we encountered new challenges.\n\nAnother accomplishment were particularly proud of is that we were able to incorporate multimodal fact-checking capabilities. “An image is worth a thousand words” is particularly true in the context of social media, so it was important that our system wasn’t restricted to only text input. However, this was not a simple challenge to solve. We chose to integrate llama-3.2-11b-vision-preview into our system to analyze claims that extend beyond simple textual information. This is a feat we consider a major leap forward in combating misinformation in multimedia-driven social platforms. On another note of personal fulfillment, we’re proud of how we built a project that aims to address an incredibly pressing issue in today’s world. Navigating misinformation is shaping up to be one of the defining challenges of our lives; as social media platforms explore stripping away existing safeguards to prevent the spread of this sort of content, individuals have to be extremely vigilant with what they engage with online. Vigilante was built with this in mind, and we’re proud of how effective of an initial solution we’ve come up with to this problem."},{"heading":"What's next for Vigilante","content":"After we recover from our sleep-deprived past 36 hours, we plan to publish the Vigilante Chrome extension to the Chrome Webstore. We also aim to continue to maintain the crowdsourced dashboard to effectively present the anonymous data we collect to the general public.\n\nBeyond what we currently have built out, we plan to extend Vigilante’s functionality to mobile platforms. Apps for iOS and Android would provide fact-checking on-the-go and bring our technology to another form factor. Moreover, many users solely engage with social media on their phones, so it is important we can reach this audience as well.\n\nWe are also very interested in how we can better leverage the data our users provide for personal and community benefit. Meta and others have begun to cancel their fact-checking programs, and users often report negative experiences with features like “community notes” on X/Twitter. We suspect that we can leverage the data we’re collecting to reveal larger trends in misinformation and hopefully begin to prevent it before it spreads.\n\nAltogether, we envision Vigilante as a tool that not only combats misinformation but also fosters a culture of critical thinking and informed engagement online. We’re excited to continue building after Treehacks, refining and scaling this platform to make a meaningful impact in the fight against misinformation."},{"heading":"Built With","content":"d3.js fastapi groq next perplexity plasmo postgresql python react supabase"},{"heading":"Try it out","content":"github.com www.usevigilante.com"}]},{"project_title":"Conductor AI","project_url":"https://devpost.com/software/conductor-ai","tagline":"Agentic routing at scale for 66x cheaper and 4x faster — Powering the Agentic Future","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/188/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Codegen: Best Code Generation Application ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"ai-agents","url":null},{"name":"contrastive-learning","url":null},{"name":"knn","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"recommendation-systems","url":null}],"external_links":[{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1SJoZzRauls_bHquv1L-HxrW5x9swVYLvnOsvtVu0Tj8/edit?usp=sharing"},{"label":"github.com","url":"https://github.com/shloknatarajan/ariadne-routing"},{"label":"github.com","url":"https://github.com/codegen-sh/codegen-sdk/pull/521"}],"description_sections":[{"heading":"Agentic Routing At Scale: The Cornerstone of an Agentic Future","content":"Enterprises run a large number of agents, and being able to choose which agent to run a query on is a complex and expensive task. Routers exist today that can handle O(10) agents, but what about O(1000). Salesforce has O(10000) agents and — in a future where agents seem poised to replace many jobs — organizations may come to rely on the orchestration of O(1,000,000) agents with O(100,000,000) prompts/day .\n\nIf we want a future driven by agents, with companies formed of agents, and to maximize performance/$ for inference, we need to solve the large-scale routing problem. The routing space is large and continuing to explode, and there exist no systems that can scale to the size that is needed.\n\nWe develop a novel recommendation-system based algorithm for prompt routing, drawing inspiration from the TikTok algorithm\n\nFortunately, our team (check below) is adept at handling matching problems at scale. Taking inspiration from the recommendation system at TikTok, ConductorAI matches queries and routers through a two-stage embedding approach, learning prompt and agent embeddings from exploring the interaction between the two and without manual encoding or intensive processing of either. Check out the technical details in our slides!\n\nConductorAI: 66x cost savings, 4x latency reduction, better than any single AI model , and exponentially better performance with many agents\n\nConductorAI provides unheard of speed and cost reduction at reasonable accuracy for an agent-driven future. Conductor implicitly learns a mapping between a semantic-embedding space and an agentic-embedding space, where coordinates correspond to features such as problem difficulty, tools required, and context that may be relative when deciding between agents. Additionally, adding agents to the system requires no hard-coded rules or descriptions, Conductor can naturally learn agent embeddings that exceed human performance.\n\nWith A agents and P prompts, a traditionally LLM based router has inference scale on the order of O(PA), as each agent needs to be referenced in context, a classification-based router scales on the order of O(PA), and the theoretical perfect router scales at O(P). We scale at O(P log A) amortized, being the only neural-network based approach to do so. With the Intersystems vector search system, this O(log A) term is practically unnoticed."},{"heading":"Conductor Composer","content":"To showcase the power of ConductorAI routing on practical problems, we have orchestrated an Agentic Suite around ConductorAI, incorporating agents for:\n\nPerplexity Search Code Generation Customer Service Database Management Executive Assistant HR Questions Legal Advice Software QA Web Automation Calendar Agent\n\nHere is our github repo containing our router, completely open-source:\n\nhttps://github.com/shloknatarajan/ariadne-routing"},{"heading":"Codegen Developer Tool: SWE-Bench Agent Harness & Evaluator","content":"For our code generation service, we extended it to provide a dev tool for Codegen users to run SWE-Bench on. We've made it very easy to run, test, and evaluate on SWE-Bench using Codegen's SDK, and included our own Codegen Agent that works on SWE-Bench.\n\nHere is the pull request containing the addition:\n\nhttps://github.com/codegen-sh/codegen-sdk/pull/521"},{"heading":"Team","content":"Shlok Natarajan - Stanford University, Routing Research with Prof. Azalia Mirhoseini and Prof. Roxana Daneshjou Devan Shah - Princeton, Recommendation Systems at TikTok Advay Goel - MIT, Building @ Prod Victor Cheng - vly.ai , a Y Combinator company for Coding Agents"},{"heading":"Built With","content":"agents ai-agents contrastive-learning knn python pytorch recommendation-systems"},{"heading":"Try it out","content":"docs.google.com github.com github.com"}]},{"project_title":"Moods and Metrics ","project_url":"https://devpost.com/software/moodmap-ai","tagline":"Hyper-analyze your mood with a simple click of a button. See your emotions clearly through audio, video, and transcription sentiment analysis to take control of your mental well-being!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/023/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Otsuka Valuenex: ekkomi® Advanced Technologies for User Feedback Award ($500 Amazon + $100 ekkomi® matcha gift card per team member)"}],"team_members":[],"built_with":[{"name":"d3.js","url":"https://devpost.com/software/built-with/d3-js"},{"name":"fastapi","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"huggingface","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"}],"external_links":[{"label":"github.com","url":"https://github.com/corey-shen/stress-level-audio-analysis"}],"description_sections":[{"heading":"Inspiration","content":"In a world where stress and anxiety are increasingly prevalent, we were inspired to create a tool that not only helps individuals understand their emotional states but also empowers them to take control of their mental well-being. Mood and Metrics was born out of a desire to bridge the gap between technology and emotional health, offering a way for people to easily visualize and comprehend their emotional levels through AI-driven insights."},{"heading":"What it does","content":"Mood and Metrics offers three analysis modalities:\n\nAudio Analysis: Utilizes a locally deployed audio sentiment analysis model to capture arousal, valence, and dominance values to map them onto a stress score scale Video Analysis: Sends a video clip to Gemini, and evaluates calm/stress levels and in-depth text reasoning analysis through facial expression data Transcription Analysis: Performs sentiment analysis on video transcription data, evaluating calm/stress levels and in-depth text reasoning analysis For all modalities, data is visualized in 2D or 3D"},{"heading":"How we built it","content":"Utilized a state-of-the-art fine-tuned wav2vec2 transformer architecture ( https://arxiv.org/abs/2203.07378 ) for audio sentiment analysis via Hugging Face and Pytorch Leveraged three.js to create an interactable 3D graph to map valence, dominance, and arousal Mapped emotion using a theoretical approach based on a tri-dimensional model of core affect and emotion concepts ( https://www.redalyc.org/pdf/3111/311126297005.pdf ) Integrated Google’s Gemini AI API for video-based and transcription-based mood analysis. Developed a React frontend with an interactive UI"},{"heading":"Challenges we ran into","content":"Locally installing transformer model via Hugging Face and Pytorch, and allowing GPU cuda acceleration Extrapolating accurate stress score from arousal, valence, and dominance values Getting the axis and rotation from the camera perspective to match for 3D visualizations Constructing a pipeline to send video footage to Gemini via an API call Creating a clean frontend to visualize graphs using data from the backend server"},{"heading":"Accomplishments that we're proud of","content":"Successfully integrating AI-driven audio, video, and transcription sentiment analysis Creating an engaging and informational visualization of emotional/mood states Achieving reliable stress detection for educational, healthcare, and meditational use"},{"heading":"What we learned","content":"The nuances of valence, dominance, and arousal in audio sentiment analysis Balancing technicality and simplicity in data visualization"},{"heading":"What's next for Moods and Weights","content":"Enhancing AI models with more modalities such as heartbeat, EKG signals, etc. Collecting accurate breathing audio to stress score data and training the model from scratch Adding real-time video analysis without needing pre-recorded clips Expanding features with personalized stress relief suggestions Deploying a mobile version for on-the-go, fully local emotional tracking"},{"heading":"Built With","content":"d3.js fastapi gemini huggingface python pytorch react tailwind three.js"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SynapseAI","project_url":"https://devpost.com/software/synapseai","tagline":"Transforming Thoughts into Visual Expression - Empowering Minds Beyond Words","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/885/802/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Chroma: Best Chroma Cloud AI Application ($200 per team member)"}],"team_members":[],"built_with":[{"name":"chromadb","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"langchain","url":null},{"name":"luma-ai","url":null},{"name":"muse-sdk","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"openai-api","url":null},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"scipy","url":"https://devpost.com/software/built-with/scipy"},{"name":"tensorflow","url":null},{"name":"terra-api","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vue.js","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/angad-the-coder/synapse"}],"description_sections":[{"heading":"Inspiration","content":"Driven by a shared passion for healthcare, our team sought to tackle this track. When further research was conducted, we had the shared consensus that there was a disproportionate level of emphasis on soldier’s productivity during service, but rarely on catalyzing their transition back to civilian life.\n\nOur veterans are twice as likely to develop PTSD compared to the rest of us, with 87.5% experiencing at least one traumatic event during their service. To make matters worse, nearly 20% have suffered traumatic brain injuries in the past two decades. These invisible wounds often leave them struggling to communicate, manage stress, or reintegrate into everyday life.\n\nCurrent rehabilitation methods have proven to be ineffective. Up to two-thirds of veterans still meet the diagnostic of PTSD post-treatment, and dropout from treatment ranges from 25 to 48%. A recurring pain point for those undergoing rehab is the inability to express themselves, which can lead to frustration, avoidance, and disengagement from rehab. Heightened stress levels is further exacerbated by cognitive impairments from conditions like Aphasia that make verbal communication difficult.\n\nThis inspired us to innovate in the intersection of empowerment and expression. A tool that both empowers and catalyzes veterans to better express themselves and regulate their stress levels.\n\nIntroducing, SynapseAI"},{"heading":"What it does","content":"SynapseAI transforms brain waves and biometric data into vivid visual representations in real time, enabling non-verbal communication and emotional expression for veterans with PTSD, TBI, or aphasia. Here's how it works:\n\nPre-Flow:\n\nVeterans are equipped with a Muse EEG headband and a Terra-enabled wearable that sends stress level data. Brainwave patterns and biometric data are captured and transformed into embeddings using machine learning. These embeddings are stored in a vector database as \"thought categories.\"\n\nUser Flow:\n\nIn real time, SynapseAI captures brainwave activity and stress levels. The system uses cosine similarity to match current brainwave patterns with stored embeddings. Based on the match, a multi-agent AI system (LangChain) expands, refines, and synthesizes a prompt, generating a video via Luma AI, dynamically modulated by the user’s stress levels.\n\nThe result? Veterans can express thoughts and emotions visually, enabling communication and emotional regulation even under high stress."},{"heading":"How we built it","content":"EEG Signal Processing → Processed EEG Brainwaves and extracted features using FFT, PSD, and Wavelet Transforms.\n\nThought Embedding Model → Trained a CNN-based triplet loss model to create embeddings for thoughts based on 5 main categories.\n\nDoctor Helps Patient - Thoughts of care, empathy, concern; Sister argues with brother - Thoughts of conflict, frustration; Fire burns house - Thoughts of emergency, fear, rapid reaction; Child cries for mother - Thoughts of sadness and distress; Null - To account for no thoughts\n\nMuse Headband Testing → We collected 50 10-second brainwave readings, changing between the different emotion clusters\n\nVector Search Database → Stored thought embeddings in ChromaDB for real-time retrieval.\n\nMulti-Agent Thought Processing (LangChain): Expansion Agent → Adds vivid details to enhance thought clarity. Reasoning Agent → Breaks down complex thoughts into a logical sequence. Emotional Agent → Adds emotional layers to thought. Adapts thought representation based on stress and biometric feedback from Terra API using the Garmin Watch.\n\nLLM-Powered Thought Refinement → Uses OpenAI-based agents to structure thoughts before video generation. Adapts thought representation based on stress feedback from Terra API using the Garmin Watch.\n\nAI Video Generation → Converts structured thought output into an AI-generated cinematic experience using Luma AI"},{"heading":"Challenges we ran into","content":"EEG Data is Noisy → Required extensive filtering and preprocessing to extract meaningful patterns.\n\nMapping Thoughts to Meaningful Outputs → Training an EEG-to-thought model was difficult due to limited labeled datasets and the complex/unstructured nature of EEG data. We had to collect data on our own using the Muse Headset and train on that.\n\nVector Search Optimization → Fine-tuned cosine similarity in ChromaDB to improve thought-matching accuracy.\n\nBiometric Adaptation Complexity → Integrating real-time stress data into the thought expansion process required careful balancing. We did extensive research on how stress can affect characteristics of thought, like its imagery.\n\nLLM Constraints & Rate Limits → Had to optimize API calls to avoid delays and costs.\n\nDespite these challenges, we successfully built a working pipeline that converts raw EEG signals into structured AI-generated videos."},{"heading":"Accomplishments that we're proud of","content":"We are proud of bridging research in neuroscience, AI, and social impact. This bridge has spurred an innovation that will empower affected veterans to more effectively express their chain of thought, allowing healthcare professionals to better engage with each individual's unique situation. This ultimately catalyzes the transition back to civilian life for post-service veterans.\n\nAdditionally, we are likewise proud of hacking and building moving pieces that we have never worked on before like training a model"},{"heading":"What we learned","content":"We learned a lot in this hack. For the majority of us, this is our first hackathon. We primarily learned how to integrate different components of a large project, from the ML model, to the pipeline, and the frontend. Additionally, collecting data from the EEG was substantially tricky, and we learned that EEG-based communication is promising but complex."},{"heading":"What's next for SynapseAI","content":"We’re excited to bring our project to life! We plan to talk to our Ideal Customer Profile(s) (ICP) to run pilot studies, assessing product-market fit.\n\nAdditionally, we want to: Expand EEG Model Training → Collect more data to refine thought embeddings and improve accuracy.\n\nFine-Tune Real-Time Thought Adaptation → Enhance biometric-driven AI reasoning for better personalization. Incorporate more biometric variability like heart-rate or perspiration to our agent pipeline.\n\nEnable Continuous EEG Streaming → Allow real-time, uninterrupted thought-to-video processing.\n\nOptimize AI Video Generation → Improve coherence, storytelling, and realism in generated videos."},{"heading":"Built With","content":"chromadb google-cloud langchain luma-ai muse-sdk numpy openai-api pandas python pytorch scipy tensorflow terra-api typescript vue.js"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Superclassroom","project_url":"https://devpost.com/software/superclassroom","tagline":"Adaptive Learning Content Generation for Students with Disabilities","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/274/213/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Intersystems: Best Use of GenAI using InterSystems IRIS Vector Search ($2k Cash [1st] & $1.5k Cash [2nd] & 1k Cash [3rd])"}],"team_members":[],"built_with":[{"name":"intersystems","url":null},{"name":"iris","url":null},{"name":"mistral","url":null},{"name":"rag","url":null},{"name":"vectors","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/airjlee/superclassroom"}],"description_sections":[{"heading":"Inspiration","content":"Every since I could remember, I've happily dedicated countless hours to helping my brother, Zac, digest content and prepare for his exams. Zac was very naturally smart, but always struggled with learning the way that the US education system wanted him to. As it turns out, he wasn't alone -- an overwhelming 74% of students nationwide are dissatisfied with means in which they are provided to learn and absorb concepts, and are proven to perform upwards of 200% better when introduced to more adaptive ways of learning.\n\nBut it gets bigger than that -- Zac was also diagnosed with dyslexia, making his journey towards academic success an even steeper climb. The support for dyslexia in the US remains a frightening concern, as over 95% of diagnosed dyslexics claim that they do not receive the appropriate accommodations in school, amongst the rigid education system.\n\nInspired by my brother's struggles and the infinite curiosity that lie in the different ways that human beings love to learn -- I wish to create a platform that is able to adapt to each student's individual needs."},{"heading":"What it does","content":"Superclassroom is a student-adaptive exam preparation and learning material generation tool designed to adapt to each student's needs. Students using the platform will upload course material relevant to their upcoming exam or topics that they want to study, establishing a personalized knowledgebase for each course the student is taking.\n\nSuperclassroom then uses this information in a RAG pipeline to generate highly-relevant course study material, such as comprehensive notes, topic summaries, adaptive flashcards, and practice exams.\n\nThere is also a space to leave notes about the student's profile, which is used in the RAG pipeline to generate more personalized results and accommodate disability."},{"heading":"How I built it","content":"Superclassroom is built with MCP servers and RAG architecture with Intersystem's Iris vectorized database and vector search to store multimodal sources of course content. Superclassroom then uses Mistral's LLM along with the relevant information to create highly accurate and relevant results based on the documents that the student provides.\n\nSuperclassroom uses a dataset of over 1200 relevant documents in the pipeline related to disability, accessibility, and research on how to best provide a positive learning experience in order to generate adaptive and considerate learning material."},{"heading":"Challenges I ran into","content":"The hardest challenge for me was definitely finding a way to accommodate all kinds of learners. Incorporating notes about the student's learning styles into the RAG pipeline was a great way to work around it and develop a platform that can truly adapt on its feet."},{"heading":"Accomplishments that we're proud of","content":"Getting a tool that is functional and something that my younger brother can actually use is a huge accomplishment for me."},{"heading":"What's next for Superclassroom","content":"Keep building and eventually develop a platform that will change the lives of many students worldwide!"},{"heading":"Built With","content":"intersystems iris mistral rag vectors"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Verify","project_url":"https://devpost.com/software/factchex","tagline":"Instantly verify tweets with our seamless web extension, ensuring understanding in a world riddled with misinformation. From political stories to fake sports trades, know the truth automatically","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/794/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Perplexity: Best Search Hack ($250 Gift Card per team member + HQ visit)"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"perplexity","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/jcub-gold/TreeHacks_25_Project"}],"description_sections":[{"heading":"Inspiration","content":"As young, first-time voters, almost all of our political awareness and understanding comes from our engagement with social media. Although information access is easier than ever, attaining unbiased information has become nearly impossible in today's highly divisive political environment. From charged language that misrepresents reality to outright lies, a solution that could provide an accurate and unbiased assessment of online content could remove this misinformation fog currently clouding social media."},{"heading":"What it does","content":"Verify is a Chrome web extension that can analyze a tweet with just a click of a button, presenting an unbiased assessment of the post's subject matter, while citing key sources and presenting an accuracy score. Although intended primarily for political awareness, the tool can be used to assess the accuracy of any tweet, from debunking false sports trades to fact-checking history fun facts. We were motivated to add this on X specifically, as the platform only has community notes as a check against misinformation, which is not available on most tweets."},{"heading":"How we built it","content":"Our team built Verify by building a Chrome-integrated UI with HTML and CSS, and a JavaScript backend that connected to Perplexity's API for accurate searching and tweet language processing. Our team segmented work between front and back-end development based on our personal interests and experience. At a high level, Connor built our front end, learning and implementing key web design principles on the fly. Meanwhile, Jacob built out our back end in JavaScript, ensuring accurate tweet processing and API calls. Jonas handled complexities related to API calls and ensured well-formatted, balanced, and customized prompt responses with Perplexity. Kevin focused on connecting all the pieces, and integrating the back and front end while supporting UI/UX development."},{"heading":"Challenges we ran into","content":"The biggest challenge we faced was developing the backend, particularly the messaging between service workers and Chrome extensions. To resolve this problem, we all got hands on deck to resolve it and pushed through several hours of debugging line-by-line, learning a ton about threading and multiprocessing."},{"heading":"Accomplishments that we're proud of","content":"We're incredibly proud that we were able to come up with and implement a tool that we would personally use in our day-to-day lives. As first-time hackers, we loved seeing our work turn from just an idea into a product that we believe can make a genuine impact on misinformation. We're also very satisfied with the technical progress we achieved."},{"heading":"What we learned","content":"We all gained significant technical experience, specifically improving our ability to develop in a full-stack environment and to integrate API calls to leverage the immense impact of LLMs. We also learned how severe the misinformation gap is on social media while implementing our product. When testing our tweets on popular creators on both sides of the political spectrum and even on history fun fact pages, we realized how many social media users digest misinformation without thinking deeply about whether the information they're reading is verified."},{"heading":"What's next for Verify","content":"We are all-in on this project idea and its impact. We will expand upon Verify to integrate the product on other social media platforms like Reddit, Instagram, and YouTube. We believe the biggest opportunity will be to analyze and summarize video content on platforms like YouTube, reducing both misinformation and time spent. Ideally, we would also be able to implement Verify on iOS and Android to further expand its reach by integrating it within apps."},{"heading":"Built With","content":"css html javascript json perplexity"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"traider","project_url":"https://devpost.com/software/traider","tagline":"Trade smarter not harder! Upgrade your financial literacy with traider","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/870/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Context: Best AI Employee Workflow ($2.5k Cash + $2.5k Context API Credits)"}],"team_members":[],"built_with":[{"name":"fast-api","url":null},{"name":"figma","url":null},{"name":"google-search-api","url":null},{"name":"nvidia","url":"https://devpost.com/software/built-with/nvidia"},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"v0","url":null},{"name":"vercel","url":null}],"external_links":[{"label":"traider-omega.vercel.app","url":"https://traider-omega.vercel.app/"},{"label":"github.com","url":"https://github.com/ajagtapdev/traider"},{"label":"drive.google.com","url":"https://drive.google.com/file/d/1jR0J82edGx1O9fPfPDKvBWEco9wBvq2L/view?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Finance isn’t a subject commonly taught thoroughly in schools, yet it plays a crucial role in everyone’s lives. Many people, especially students, grow up without exposure to financial concepts, making it challenging for them to navigate real-world financial decisions. This lack of financial knowledge can lead to overwhelming debt, poor savings habits, and financial stress. In our lives, we have seen that these knowledge gaps in finance can cause enormous hurdles when entering college, internships, or the corporate world as young adults struggle to manage their growing debt and earnings."},{"heading":"What it does","content":"traider is an educational platform that provides beginner investors with mock trading simulations and instantaneous AI-powered feedback for their trades. Scraping REAL historical and current stock data from Yahoo Finance, traders can engage with accurate market data and place their trades over a user-specified simulation duration beginning at a specified start date. Unlike models trained on outdated datasets, our product offers real, relevant market and economic data based on the stock ticker and current date in the simulation, which we then synthesize with NVIDIA to supplement young traders with more robust information to fuel their financial decision-making. This allows users to make informed trades using accurate and up-to-date market information.\n\nOur product stands out from other trading simulations in that it allows students to pick any timeframe in the past to put their trading skills to the test. Further, our supplemental market events information provides a more detailed painting of the financial landscape during the simulation, encouraging informed investing. Our dynamic and colorful data insights allow students to examine takeaways, while our AI-powered feedback for each executed trade promotes healthy learning and detailed mistake analysis. The leaderboard and financial calculator features offer a social and educational playground for students to explore the world of trading while gamifying finance like never before."},{"heading":"How we built it","content":"Frontend:\n\nNext.js Tailwind Vercel/v0: We ran multiple versions of v0 to fine-tune and perfect out front end while asking for abstract animations for the background and generated complex containers for the simulator page Shadcn: Awesome built-in Next.js UI library which provided interactive graphic cards for buttons and pop-outs Figma Canva Radix UI Lucid React\n\nBackend:\n\nNVIDIA: We used NVIDIA and NVIDIA Cloud Compute to test the model. We used NVIDIA bev dev on a T4 GPU Cloud Instance to run a Jupyter notebook through Ubuntu Linux to fine-tune Lama 70b on financial prediction data Vercel Hosting Fast API Perplexity: We used it for the AI stock trading feedback simulator OpenAI: Used NVIDIA llama-70B to generate responsive and instantaneous feedback on stock exchanges. Additionally, provided a section for students to ask questions about the stock market and other financial questions Google Search API: Scrape relevant market and economic events given a stock ticker and a date. Once we scanned financial news websites for economic data, we passed it through NVIDIA using llama 70B to get the best possible market consensus on a stock ticker and synthesize that information to display to users. This enhanced their decision-making process as they decided on how to place trades."},{"heading":"Challenges we ran into","content":"Finding an accurate LLM to provide users feedback on their trades Providing a layout for the various data visualizations in an aesthetically pleasing and non-overwhelming manner Handling massive amounts of stock data from Yahoo Finance and parsing through it to calculate daily portfolio values with complex mathematical calculations Picking up new technology and integrating with databases with which we have no prior experience, such as Convex Handling the dynamic animations on the homepage Combining the v0 code with preexisting code and integrating the frontend with the backend Real-time data handling: stock and trading information datasets led to rate limits we faced when making our API calls Finding a logo that would encompass our product. We spent a lot of time ideating different designs on Figma"},{"heading":"Accomplishments that we're proud of","content":"Backend:\n\nWe were excited to be able to integrate and leverage AI models, such as Perplexity and Llama 70B, NVIDIA Brev, and cloud computing, for numerous features of our trading simulation. These included providing young investors instantaneous feedback on their trades, parsing through and summarizing market data with Google Search API and Llama 70B, and offering an AI-powered chatbot that answers users' questions and concerns regarding finance and trading.\n\nWe were also proud to utilize a database technology that was new to us, Convex, to store user information. Through Clerk, we could also set up secure user authentication through Google sign-in. Additionally, we leveraged the Yahoo! Finance API to consolidate and display financial stock data for any time frame the user needed, allowing the user to see detailed stock information, such as high, low, open, close, volume, and more. These complex calls and technology integrations allowed for a seamless user experience and analytical trading dashboard.\n\nFrontend:\n\nWe are proud of our dynamic homepage UI and the detailed and analytical simulator page UI. Their seamless and intuitive interfaces make them easy to use for our young target audience. We paid close attention to the color scheme, ensuring that it aligns with our brand identity and creates a visually cohesive and inviting interface. The result is a user-friendly design that is both functional and appealing, providing an enjoyable experience for users of all experience levels.\n\nAdditionally, the components we are most proud of are the Homepage for its nifty abstract animations, which make the page more exciting. The home page’s color palette is also bright, giving it a more vibrant feel since it is marketed towards students to encourage them to learn more about finance. Another page we are proud of is the trading simulation and portfolio analytics page. The UI is attractive and enhances the theme. Many of the boxes and the pop-out form offer interactive and dynamic components.\n\nWe were also proud to be able to offer various features on our page in just 36 hours of coding, from financial calculators to a leaderboard to data visualizations regarding portfolio analytics. We thoroughly enjoyed building this engaging product and integrating the technical backend with an appealing front end."},{"heading":"What we learned","content":"Through developing this project, we learned about the complexity of designing scalable backend architectures, handling real-time data processing, and optimizing system performance. We learned the importance of efficient database structuring to maintain data integrity while ensuring fast retrieval. Additionally, we also deepened our understanding of financial market mechanics, from order execution to portfolio management, and the challenges involved in simulating a fair and realistic trading environment. Most importantly, we gained valuable experience working collaboratively and iterating design choices to build the most reliable user-centric platform."},{"heading":"What's next for traider","content":"Next mission for traider is to bring it to the real market with users of all age groups who are students to try our product out. We also plan on implementing more robust features, such as fine-tuning an LLM on trades and historical stock data to provide more tailored insights to users, developing a more social aspect of the platform to allow young investors to share their accomplishments with their networks, and integrating voice-based AI-powered chat support for our younger client base. We would also aim to build AI-generated weekly quizzes and maintain performance metrics and streaks to keep our users rewarded and engaged on a daily. Additionally, we would love to implement cryptocurrency in our platform."},{"heading":"Built With","content":"fast-api figma google-search-api nvidia openai perplexity v0 vercel"},{"heading":"Try it out","content":"traider-omega.vercel.app github.com drive.google.com"}]},{"project_title":"Swarm AI","project_url":"https://devpost.com/software/swarm-ai-s6lhd0","tagline":"Swarm AI empowers AI companies to test thousands of speech-to-speech conversations in parallel, measuring latency, coherence, memory, & engagement. We turn weeks of QA & model evaluation into minutes.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/272/134/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Codegen: Best Developer Tool ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"next.js","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"render","url":null},{"name":"supabase","url":null},{"name":"vercel","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/phombal/swarm-backend-new"},{"label":"github.com","url":"https://github.com/sidjavvaji/swarm.git"}],"description_sections":[{"heading":"Inspiration","content":"Picture this: there we were, running our AI restaurant waiter, feeling confident about our automated system taking phone orders. But then came the testing phase, and reality quickly set in. Our only option was to manually call our AI repeatedly to validate each conversation flow – a process that’s extremely bottlenecked by a major limiting factor. Not exactly scalable when you're trying to ensure your AI can handle a million different things, ranging from simple reservations to complex menu customizations. We quickly realized we weren't alone in this challenge. From customer service to healthcare, everyone adopting voice AI was facing the same bottleneck. These AI agents needed to be thoroughly tested before deployment, but the testing tools hadn't caught up with the technology. In an age of automation, we were still relying on manual testing processes. And that didn’t sit well with us. That's when the idea for Swarm AI clicked: what if we could just create a platform that could spawn thousands of virtual callers, each with their own characteristics, accents, and conversation patterns? Just as load testing transformed web development from guesswork into a science, we believed voice AI testing needed its own revolution. By enabling developers to uncover edge cases and identify potential issues before they reach real customers, we could help ensure more reliable AI interactions across every industry."},{"heading":"How it works","content":"At the core of Swarm AI is a system designed to run thousands of test calls at once. Users start by accessing our dashboard, where they can create and configure testing jobs through an intuitive interface. The dashboard lets them specify exactly how they want their test agents to behave – from setting specific conversation flows to selecting different accents, latency/network conditions, background noise, etc. for thorough testing coverage. Our backend, built with FastAPI, manages these testing requests through a smart batching system. Instead of starting all calls at once, we spread them out to keep everything running smoothly. Each batch runs independently, letting us handle many calls while keeping the system stable. For each call, we first save it to our database and then connect through Twilio. Once connected, calls flow through our real-time processing system. We use WebSockets to handle two-way audio, letting our AI agents listen and talk naturally. The audio is processed instantly using OpenAI for transcription, while our AI engine creates responses based on the test settings. The dashboard provides real-time visibility into ongoing tests through a live analytics panel. As calls progress, users can see key metrics updating in real-time – success rates, average call duration, and completion status for each test agent. The analytics interface pulls directly from our database, showing both aggregate statistics and detailed breakdowns of individual call performance. Our AI agents follow test settings that control their behavior, including accent, talking speed, and tone. These settings are loaded when each call starts, letting us test many different scenarios to find potential issues in the target voice AI system. Users can also access a detailed transcript view for any completed call, allowing them to analyze specific interactions or troubleshoot issues that arose during testing. After test completion, our platform generates comprehensive reports that highlight patterns, anomalies, and potential improvements for the voice AI system being tested. This data-driven approach helps users quickly identify and fix issues before they impact real customers."},{"heading":"Challenges we ran into","content":"Our biggest technical hurdles centered around real-time communication and scalability. The WebSocket connection, important for maintaining live conversations between AI agents, proved particularly tricky – every disconnection meant a failed test and lost data. We overcame this by implementing robust connection handling and retry mechanisms. Call management also presented unique challenges. What seemed straightforward – ending a call – became complex when dealing with thousands of concurrent conversations. We had to carefully orchestrate call termination to ensure clean exits and proper resource cleanup. Our batching system underwent several iterations before we found the right balance between system load and testing throughput. One of our most interesting challenges was running simultaneous speech-to-text and speech-to-speech processing. This required careful stream management and precise timing to prevent feedback loops or processing delays. Figuring out how to transfer speech-based audio chunks efficiently and quickly proved was a major obstacle as well. After numerous debugging sessions and architecture revisions, we developed a stable solution that could handle both streams efficiently. Finally, our database architecture evolved significantly throughout development as we better understood our data needs. What started as a simple call logging system grew into a complex but efficient structure handling test configurations, real-time analytics, and detailed conversation transcripts."},{"heading":"Accomplishments that we're proud of","content":"The both of us poured our expertise into crafting a robust testing platform that exceeded our initial vision. The clean, beautiful interface we designed masks the complex orchestration happening behind the scenes – something we take pride in. We're especially proud of our system's reliability. Through persistent debugging and optimization, we created a platform that can handle many concurrent test calls while maintaining stable performance. But beyond the technical achievements, what stands out is how well we worked together. The both of us brought our strengths to the table and stepped up when needed, allowing us to build something substantial in such a short timeframe."},{"heading":"What We Learned","content":"It’s incredibly difficult to just pick a few, but here are our major takeaways. First and foremost, a well-designed system architecture will always outperform spontaneous solutions (no matter how quickly we think we can move). We also dove deep into how computers listen to and process phone audio, and we explored various approaches to optimize real-time communication. Additionally, mastering WebSockets and WebRTC allowed us to handle two-way audio in a hyper-efficient manner, ensuring smooth interactions even at scale. Along the way, we gained a solid understanding of parallel computing and batch processing, applying principles from our systems classes to balance performance and compute resources effectively. We also learned the importance of having great company (and food + boba) when building."},{"heading":"What's next for Swarm AI","content":"While we initially set out to build a voice AI for restaurant ordering, developing this testing platform opened our eyes to a much bigger opportunity in the voice AI ecosystem. We're now pivoting our startup to focus on Swarm AI as a comprehensive testing platform for voice AI developers, with plans to expand our testing capabilities and add features like custom scenario builders, advanced analytics, and integration with popular voice AI development frameworks."},{"heading":"Funny Moment","content":"Watching the AI agents making Dad jokes with one another (“What type of nut goes to space?” Answer: “An Astro-Nut”)"},{"heading":"Built With","content":"fastapi javascript next.js openai python render supabase vercel"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"TheraMind","project_url":"https://devpost.com/software/theramind","tagline":"Less paperwork, more patient care: the AI therapist's assistant.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/786/datas/medium.jpeg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"DAIN Labs: AI Agent Excellence & Innovation Awards ($5k Cash [1st] & $2.5K Cash [2nd] & $500 Cash [3rd] + Agent Launchpad Invitation)"}],"team_members":[],"built_with":[{"name":"dain","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"scrapybara","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/cherylwu834/treehacks25"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration for TheraMind stems from the growing mental health crisis and the challenges therapists face in meeting patient demand.\n\n1 in 5 US adults experiences mental illness annually. Source — National Institute of Mental Health NIMH 46% of psychologists couldn't meet patient demand in 2022. Source — American Psychological Association (APA) 2022 COVID-19 Practitioner Impact Survey via APA Report 52% of therapists reported experiencing burnout in 2023. Source — SimplePractice 2023 Therapist Well-Being Report via Fierce Healthcare Burnout-related turnover costs healthcare organizations millions. Source — American Medical Association (AMA)via AMA Report Administrative burdens are a top burnout factor for 55% of therapists. Source — SimplePractice 2023 Therapist Well-Being Report via Fierce Healthcare\n\nOur guiding question, \"How do we help therapists help their patients?\" , emphasizes the dual focus of TheraMind: empowering therapists while enhancing patient care. By addressing the challenges therapists face—such as administrative burdens, burnout, and time constraints—TheraMind enables them to dedicate more energy to their patients. Through tools like data visualization, sentiment analysis, and streamlined meeting preparation, TheraMind bridges the gap between therapist efficiency and patient outcomes."},{"heading":"What TheraMind Does","content":"Our AI workflow automator is designed to streamline therapists' daily tasks, reducing the cognitive load of managing appointments, patient communication, and progress tracking. With a simple prompt, therapists can instantly retrieve their upcoming appointments, ensuring they stay organized without sifting through calendars. The system also automates patient reminders, sending timely emails to reduce no-shows and keep clients engaged. Additionally, our tool provides a comprehensive summary of patient progress by analyzing survey responses and visualizing historical data, offering valuable insights at a glance. By handling these essential but time-consuming tasks, our solution allows therapists to focus on what truly matters—providing quality care to their patients."},{"heading":"How We Built TheraMind","content":"DAIN — We used DAIN as our framework to tie together all the components into an integrated workflow. DAIN helped trigger various service tools, such as checking the therapist's appointment database, automatically sending reminder emails, and retrieving detailed patient information. By streamlining these tasks, our DAIN service allows for seamless data management and communication between systems. This automation greatly reduces the cognitive load on therapists, allowing them to easily track patient progress, manage contexts between different patients, and quickly identify necessary treatments by analyzing large amounts of data efficiently. This implementation not only improves efficiency but also contributes to a more organized and manageable workflow for therapists.\n\nData Driven Insights with Scrapybara — We used Scrapybara as an AI agent to automate data scraping from therapists' Google Drive, specifically extracting patient survey files. By leveraging summarization and sentiment analysis, it helped therapists sift through large amounts of journal entries and survey responses, identifying key emotional patterns and helpful data-driven insights. This allows for more efficient workflow management and improved mental health support by highlighting trends in patient well-being without the therapist ever having to touch a line of code to deduce those insights.\n\nSentiment Analysis — We applied sentiment analysis to patient journal entries by processing text data to detect emotional tone and classify sentiment. Using natural language processing (NLP) techniques, we extracted key mood indicators, assigning numerical mood scores based on word choice, context, and emotional intensity. These scores were aggregated to generate quantitative metrics, allowing therapists to track fluctuations in patient mood in between sessions. The data was then visualized in the DAIN framework through intuitive graphs and dashboards, enabling therapists to quickly identify trends, detect potential concerns, and make informed decisions about patient care.\n\nPerplexity — We conducted extensive background research on mental health to better understand the challenges faced by both patients and therapists. This research, which we conducted largely with Perplexity, highlighted the growing demand for mental health services, particularly in the face of rising stress, burnout, and other mental health issues among healthcare professionals. We explored existing mental health frameworks and tools, identifying gaps in support, especially in the way therapists manage patient data and track progress. Additionally, we investigated the needs of therapists, including the increasing pressure to manage large volumes of patient information and the need for effective tools to monitor patient well-being in real-time. Our findings reinforced the importance of creating solutions that can automate time-consuming tasks, reduce mental load, and provide personalized, actionable insights for both therapists and patients. This background research streamlined by Perplexity’s efficient search methods quickly got us up to speed with the problem space and laid the foundation for our work in integrating AI agents to streamline workflows and enhance mental health."},{"heading":"Challenges We Faced","content":"One of the main challenges we encountered was slow performance, which would require further optimization to run efficiently in real time. Additionally, integrating the AI agent, understanding its capabilities, and interfacing with different systems proved to be complex, as we wanted to ensure seamless communication and data flow across various platforms. These challenges highlighted our focus on improved optimization strategies of AI agents and better interoperability between systems to achieve smoother and faster execution in the future."},{"heading":"Accomplishments that We're Proud Of","content":"We’re proud of creating an intuitive and user-friendly interface that makes mental health support accessible. Successfully implementing AI-driven sentiment analysis and personalized journaling features was a major achievement. Seeing our idea come to life and knowing it could positively impact people’s well-being made the late-night debugging session worth it."},{"heading":"What We Learned","content":"Through our exploration of AI agents in automated workflows applied to healthcare, we discovered the potential to significantly enhance efficiency by automating qualitative and quantitative data analysis tasks and supporting human collaboration. These agents can optimize decision-making and provide real-time insights, improving both productivity and well-being. We learned that AI can assist professions prone to burnout that greatly impact a wide range of patients, ultimately creating more accessible, inclusive, and balanced work environments. This integration of AI into workflows is transforming healthcare and workplaces at large, having a profound social impact on well-being by enabling smarter, more efficient processes that support professionals and improve lives."},{"heading":"What's next for TheraMind","content":"Our vision for TheraMind is to expand its reach and impact by catering to new user groups who experience high levels of stress and emotional burnout. In the near future, we aim to tailor our AI-driven mental health support for medical professionals, project managers, educators, and more. By refining our AI’s ability to provide industry-specific emotional support and wellness insights, we can help these professionals navigate their daily lives more effectively and allow them to achieve peace of mind."},{"heading":"Built With","content":"dain python scrapybara typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CodeInsight","project_url":"https://devpost.com/software/code-insight","tagline":"Help developers understand codebases faster","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/256/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Codegen: Best Code Analysis Application ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mistral","url":null},{"name":"perplexity","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/evanng07/ej-treehacks"}],"description_sections":[{"heading":"Inspiration","content":"Both of us have worked at big corporate companies where we have had to be onboarded to a certain application very fast. It is often hard as an intern to automatically come in and understand the code base just like that. So we decided to create a tool that allows you to analyze a code base and see the function dependency which is nicely visualized and also chat to mistral AI to learn more about the code base. In terms of UI, we had some inspiration from Obsidian."},{"heading":"What it does","content":"CodeInsight is an interactive tool that analyzes a codebase to generate a dynamic visualization of function dependencies. It extracts function definitions and their call relationships from a selected repository and displays them as an interactive network graph. Users can click on a node to view the corresponding source code in a draggable, resizable panel, with animated arrows that show the direct call flow—both outgoing (caller to callee) and incoming (callee from caller). In addition, CodeInsight integrates a chatbot powered by an AI model to answer questions about the codebase, helping developers quickly gain an understanding of complex systems."},{"heading":"How we built it","content":"We built CodeInsight using a combination of Python and JavaScript. On the back end, Python’s AST module performs static analysis to extract functions and their call dependencies from a codebase. This data is then served as JSON via a Flask API. For the front end, we used D3.js to render an interactive, force-directed graph of the function dependencies, adding features like zooming and dragging. We integrated Interact.js to allow code panels (that display the source code for each function) to be draggable and resizable, and Highlight.js provides syntax highlighting. To further enrich the user experience, we integrated a chatbot powered by Groq’s Mistral model (accessed via our configured API client) that lets users ask questions about the codebase. Finally, we packaged the whole thing into a standalone application using pywebview so users can simply select a repository without needing any command-line interaction."},{"heading":"Challenges we ran into","content":"One of our biggest challenges was ensuring accurate static analysis in a way that filtered out built-in or external functions while capturing the true internal dependencies of a complex codebase. We also had to balance performance with interactivity—making sure our visualization remained responsive even for large repositories. Integrating multiple libraries (D3.js for visualization, Interact.js for UI controls, and Highlight.js for code formatting) and ensuring they all worked harmoniously presented additional hurdles. Finally, deploying the app in a consistent manner across different environments and handling subtle differences in runtime behavior was a significant challenge."},{"heading":"Accomplishments that we're proud of","content":"We’re proud that CodeInsight not only generates a clear and interactive visualization of function dependencies but also provides an intuitive interface that helps users explore code in a dynamic way. The integration of real-time animated call flows (with directional arrows and moving dots) gives users an immediate sense of the underlying code structure. Additionally, the ability to open detailed code panels and interact with an AI chatbot for further insights makes Code Insight a comprehensive onboarding and analysis tool. Seeing our tool help new developers understand complex codebases quickly is a real win for us."},{"heading":"What we learned","content":"Working on Code Insight taught us a great deal about static code analysis and the nuances of parsing complex codebases. We gained valuable experience integrating diverse technologies—ranging from Python’s AST for analysis, to D3.js and Interact.js for building a rich, interactive UI, and even leveraging AI via Groq’s Mixtral model. We learned how to manage cross-environment deployments and how subtle differences in runtime or browser behavior can affect the end-user experience. Most importantly, we discovered the critical importance of effective visualizations in speeding up developer onboarding and code comprehension."},{"heading":"What's next for CodeInsight","content":"Looking ahead, we plan to extend CodeInsight with dynamic analysis features that capture runtime call flows, further enhancing our static analysis results. We’d like to integrate advanced filtering and search options—such as grouping functions by modules or highlighting high-complexity areas—to provide deeper insights into the codebase. Additionally, we aim to expand the chatbot’s capabilities so that it can answer even more nuanced questions about the code. Ultimately, our goal is to create a comprehensive tool that not only accelerates onboarding but also aids in ongoing code maintenance and refactoring, making it an indispensable resource for developers."},{"heading":"Built With","content":"flask groq html javascript mistral perplexity python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"PULSE.AI","project_url":"https://devpost.com/software/pulse-ai-0oafm5","tagline":"An AI code agent tool that integrates live browser session context for smarter coding assistance. It analyzes tabs, runs extra searches, and uses RAG to provide Codeium’s Cascade with extra context.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/007/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Codeium: Best Use of Windsurf ($1k Cash + 1-Year Windsurf + Codeium Subscription)"}],"team_members":[],"built_with":[{"name":"cascade","url":null},{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"codeium","url":null},{"name":"css3","url":"https://devpost.com/software/built-with/css3"},{"name":"fastapi","url":null},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mistral","url":null},{"name":"mpc","url":null},{"name":"perplexity","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rag","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sonar","url":null},{"name":"structuredoutputs","url":null},{"name":"windsurf","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ekang7/PulseAI"}],"description_sections":[{"heading":"Inspiration","content":"Developers constantly search the internet for solutions while coding—whether it's project planning documents, API documentation, recent research papers, or Stack Overflow discussions. We wanted to seamlessly integrate browser tab context into an AI code editor (Windsurf), making coding more efficient by surfacing relevant information automatically."},{"heading":"What it does","content":"PulseAI captures browser context (screenshots, URLs, and titles), performs extra searches for adjacent topics via Perplexity, and stores & retrieves semantically relevant data using ChromaDB and Retrieval Augmented Generation (RAG) to bolster AI-assisted coding on Codeium's Windsurf-Cascade editor. We have two main workflows, a Passive Workflow for building up the browser context vector embedding ChromaDB and an Active Workflow for handling real-time user queries to the AI Code Editor.\n\nPassive Workflow Steps:\n\nThe Chrome extension extracts raw browser context (a screenshot of the webpage along with its title and url) when the user decides to click “Add this”. Text is extracted using OCR and Pixtral (a multimodal Mistral model) describes any images in the raw data. Processed browser data is stored in ChromaDB for retrieval. Perplexity's Sonar API runs searches based on aforementioned browser data and user queries to preemptively retrieve other relevant information. Namely, we get the topic of the summarized context, generate similar topics, and perform searches for each of them to retrieve additional relevant contexts. Mistral's ministral-8b is used for quick summarization of the previous output. We utilize structured outputs for both APIs to ensure information is presented in a consistent fashion. ChromaDB embeds and stores search results and browser data for quick and effective retrieval. We get semantic vector embeddings here obtained from using the SentenceTransformers all-MiniLM-L6-v2 model.\n\nActive Workflow Steps:\n\nThe user enters a query for the AI Code Editor. For context for Perplexity, RAG retrieves the top-k most relevant results from ChromaDB with semantic KNN with respect to the user query. Mistral's ministral-8b is used for quick summarization of the top-k results. We then use Perplexity’s Sonar API to retrieve additional similar topics. These results are parsed using Mistral and embedded in the ChromaDB. With these new results in the table, we use RAG to again retrieve the top-k most relevant results from ChromaDB with semantic KNN with respect to the user query. Mistral's ministral-8b is used for quick summarization of the top-k results resulting in a nice summarized context We created a custom MCP server/tool that is called by Cascade to inject a summary into its context. Cascade uses the summarized context to generate more relevant tips and code completions.\n\nFrontend\n\nIn addition to the main tool, we built a frontend where hackers can view and delete database entries, for added flexibility and control."},{"heading":"How we built it","content":"We built a Chrome extension to extract browser data, an agentic backend to process our information and store it in ChromaDB, a callable tool (MCP) that is callable by Cascade, and a visualization tool that lets the user view and edit the context generation process in real-time."},{"heading":"Challenges we ran into","content":"Creating MCP tool: Learning about MCP tools, building one, and integrating it into Cascade's tool kit was something completely new to us. We had to research different potential methods of injecting dynamic context into coding assistants' contexts e.g., we were unable to find a method for Cursor. Debugging the MCP workflow for Codeium and making it fit smoothly into our pipeline took a significant amount of effort. Obtaining real-time browser context : We also struggled with getting real-time browser context in a usable format. We overcame this by taking advantage of Mistral’s new multimodal model Pixtral to form a pipeline to process the information and Mistral’s smaller ministral-8b model to summarize it into a more efficient form."},{"heading":"Accomplishments that we're proud of","content":"When we first decided on our project, we acknowledged that there were going to be several moving parts, and it's great to see that we were able to integrate them perfectly. We're also quite proud of our idea, as it expands a coding agent's ability to learn about its project outside of its IDE and at the same time minimizes context switches for the user. Furthermore, our product works almost exactly as intended and is a great proof of concept; with more work, we're confident it could be a move towards the next level of AI coding assistance."},{"heading":"What we learned","content":"We learned about...\n\nHow to make a browser extension that interacts with external servers. The MCP protocol, which was an introduction to how LLM tools are defined and used. Codeium's Windsurf/Cascade workspace and how to introduce personalized functionality. Perplexity's and Mistral's APIs and how to take advantage of structured outputs. RAG and how to use ChromaDB to retrieve semantically relevant information."},{"heading":"What's next for PulseAI","content":"We aim to improve overall latency and implement a more automated browser + AI code editor experience that is even more hands-off while still respecting users' privacy."},{"heading":"Built With","content":"cascade chroma codeium css3 fastapi html5 javascript mistral mpc perplexity python rag react sonar structuredoutputs windsurf"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Motherly.ai","project_url":"https://devpost.com/software/ai-mama","tagline":"Your mom knows you better than you do.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/398/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Delve: Best Agentic Workflow (AirPods + Delve Founder Numbers + Onsite)"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Scrapybara: Capy Prize (Japan offsite + $1k credits + Scrapybara Pro 6 months [1st] & $1k credits + Scrapybara Pro 3 months [2nd] & $100 credits + Scrapybara Pro 1 month [3rd])"}],"team_members":[],"built_with":[{"name":"next.js","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/MingkuanY/ai-mommy"}],"description_sections":[{"heading":"Inspiration","content":"In the past year, 74% of people have felt overwhelmed by stress, and 30% of young adults in the U.S. experience loneliness multiple times a week. We wanted to create a large-scale solution and turned to what helps us most with both stress and loneliness: our moms. While AI companions like Character AI provide emotional support, they can’t truly take care of you. Our goal was to build an AI agent that deeply understands your emotions and takes action to help you."},{"heading":"What it does","content":"Stress management is deeply personal - what works for one person might not work for another. That’s why we built Motherly.ai, an AI-powered agent that understands how you experience stress and takes real-world actions to support you. Using biometric data like EMG, heart rate, and blood pressure, it detects your mental state in real-time and responds with personalized interventions, whether it’s scheduling a break, ordering comfort food, or adjusting your environment to help you relax. Unlike traditional AI companions, Motherly.ai doesn’t just listen – it takes care of you.\n\nSome things Motherly.ai can do for you\n\nIf reading the news makes you stressed out She can play calming music and surface good news in the world If you mention you’re stressed out and constantly doom-scrolling Motherly.ai can interrupt your scrolling, order food for you, give you a massage, and contact your friends so they can reach out to you To help your child stay motivated while studying Motherly.ai can automatically give them a YouTube break between problems on Khan Academy when they’re stressed, or alternatively nudge them back to studying when they spend too much time distracted. If you want her to help you stay productive and avoid distractions She can recognize when you’re on Instagram and bring you back to your work"},{"heading":"Accomplishments that we're proud of","content":"Hardware Hacking : We hacked a TENS unit to give an AI agent the capability of controlling the human body. Cutting-edge research : We used EMGs to create a stress monitoring system based on several papers that correlate EMG activity in the trapezius to stress with p<0.0001 Experimentally Induced Stress Validated by EMG Activity, A Comprehensive Analysis of Trapezius Muscle EMG Activity in Relation to Stress and Meditation The impact of adverse childhood experiences on EMG reactivity: A proof of concept study Trained an LSTM classifier to predict your emotions based on your stress levels and biometric data Live Biometric Processing : Manage 4 separate real time data streams. Distinct Agent Actions : Created several distinct actions for our agent to take: Close the tab you’re on Switch the music to match your mood Order a surprise Domino’s order Text a friend Order matcha Prescribe a massage (using TENS) Book a yoga class Find fun events in your area Interactive Avatar : AI mother makes eye contact with your cursor as she helps you"},{"heading":"Challenges we ran into","content":"To get the EMGs to have consistent readings, it took SOO many different sensor placements on the trapezius muscle in order to get any sort of reading. After learning a lot about anatomy of shoulder muscles and bones, we finally got the electrodes in the right place to measure stress from these muscles, and got really amazing readings from our sensor! We initially tried to use the same signal processing methods mentioned in the papers (bandpass and butter filters, extracting only low-frequency data, etc.) but these were too slow for real time feedback, so we had to create more optimized methods: We accidentally ordered $20 worth of Dominos while testing our Scrapybara automation Making the AI agent do the right thing based on its prompts was very difficult."},{"heading":"What we learned","content":"How to take readings using EMG sensors and how to give massages using TENS stimulation How to stream data between different services in real time How to create agents with LangChain"},{"heading":"What's next for Motherly.ai","content":"Shrink the EMG sensor to be a 24/7 wearable to enable long-term stress / mental health tracking Allow the user to give Motherly.ai custom personalities, making help more familiar and comforting"},{"heading":"Built With","content":"next.js"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ProtestLink","project_url":"https://devpost.com/software/protestlink","tagline":"Secure, real-time protest coordination via encrypted mesh and AI insights","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/269/910/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Delve: Most Secure App (AirPods + Delve Founder Numbers + Onsite)"}],"team_members":[],"built_with":[{"name":"axios","url":null},{"name":"bluetooth","url":"https://devpost.com/software/built-with/bluetooth"},{"name":"esp32","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"leaflet.js","url":"https://devpost.com/software/built-with/leaflet-js"},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"openai","url":null},{"name":"openstreetmap","url":"https://devpost.com/software/built-with/openstreetmap"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sentence-transformers","url":null},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"stadia","url":null},{"name":"tailwindcss","url":null},{"name":"transformers","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"web","url":"https://devpost.com/software/built-with/web"},{"name":"webrtc","url":"https://devpost.com/software/built-with/webrtc"},{"name":"whisper","url":null}],"external_links":[{"label":"protest.morelos.dev","url":"http://protest.morelos.dev/"},{"label":"github.com","url":"https://github.com/wyang563/protest_app"}],"description_sections":[{"heading":"Inspiration","content":"In times of social unrest, the ability to communicate is as vital as the cause itself. Protesters across the world face increasing challenges when it comes to coordination, safety, and information sharing. Governments have been known to shut down cellular networks, limit internet access, or monitor online communication, making it difficult for protesters to organize effectively. Even when networks remain online, they often become overloaded, rendering them useless at critical moments. Protesters struggle with locating medical aid, distributing supplies, and avoiding high-risk areas where clashes may occur.\n\nBeyond digital suppression, protesters also struggle with navigating volatile environments, distributing supplies efficiently, and requesting medical assistance when situations escalate. How can protesters stay connected and informed when traditional communication systems fail? That was the problem we set out to solve.\n\nWe asked ourselves: How can we create a communication system that is secure, decentralized, and resilient enough to function even under extreme restrictions? With this goal in mind, we built ProtestLink, a Wi-Fi mesh-based protest coordination tool that provides secure messaging, real-time resource allocation, and risk assessment without relying on internet access or centralized servers. ProtestLink ensures that protesters stay connected, protected, and prepared, even when traditional communication methods fail."},{"heading":"What it does","content":"ProtestLink is a decentralized communication and coordination platform designed to keep protesters connected, informed, and protected even in the face of network disruptions, internet blackouts, and active surveillance. By leveraging Wi-Fi networking, live location sharing, and AI-driven speech analysis, ProtestLink provides an autonomous protest network that helps protesters navigate safely, request critical resources, and broadcast urgent alerts without relying on traditional service providers.\n\nWith ProtestLink, protesters can:\n\nShare real-time location data with organizers and trusted participants to create an interactive protester map for coordination and safety tracking. Identify and avoid high-risk areas through real-time protest heatmaps that visualize areas of tension and clashes. Indicate an emergency alert status to notify others if they are in danger, being arrested, in need of medical aid, or require supplies. Broadcast emergency alerts to organizers and trusted participants. Automatically transcribe and analyze protest audio using Whisper AI, detecting keywords, sentiment, and potential escalation zones in real-time. Ensure trust-based access with a zero-trust authentication model, where protesters join securely via QR code, eliminating the need for personal data like phone numbers."},{"heading":"How we built it","content":"At the core of ProtestLink is a self-hosted web server that facilitates location sharing, encrypted communication, and live protest heatmaps. We built a custom Wi-Fi network to allow devices to connect and interact without relying on traditional cellular infrastructure. Protesters can open the web app, share their live location, and send alerts—all without requiring a constant internet connection.\n\nTo enhance real-time protest intelligence, we integrated Whisper AI, an edge-based transcription and NLP system capable of converting live protest audio into actionable data. This means that if a protester yells for help, reports police movement, or requests medical assistance, the system can analyze and categorize the request, ensuring it reaches organizers or other protest members in real time. We also used sentiment analysis to detect potential conflict zones, allowing ProtestLink to generate a live map of risk areas.\n\nThe protester map is powered by WebSockets and real-time geolocation tracking, ensuring fluid updates without relying on centralized data sources. Protesters can pin their locations on the map, mark areas as safe or dangerous, and request water, medical aid, or evacuation support—creating a distributed protest intelligence network that operates independently of government-controlled systems.\n\nSecurity was a critical concern throughout development. We implemented end-to-end encryption to ensure that messages, locations, and alerts remain private and tamper-proof. The zero-trust authentication model allows protesters to join the network anonymously via QR codes, eliminating the need for phone numbers, email, or personal data. This ensures that no centralized entity can track or compromise protesters through ProtestLink.\n\nBy combining Wi-Fi-based communication, AI-powered speech processing, and encrypted location sharing, we created a robust and decentralized coordination system that empowers protesters to act swiftly, securely, and strategically."},{"heading":"Challenges we ran into","content":"One of the biggest technical challenges we faced was working with Bluetooth on iOS. Initially, we planned to leverage Bluetooth mesh networking for decentralized communication, but Apple's restrictive CoreBluetooth API made this extremely difficult to implement. Unlike Android, which offers flexible background BLE (Bluetooth Low Energy) support, iOS heavily limits background Bluetooth communication, making it nearly impossible to maintain persistent message relays between devices. After multiple failed optimizations, we ultimately pivoted to Wi-Fi-based networking, which provided greater reliability, better message throughput, and cross-platform compatibility.\n\nNavigating Apple’s strict development ecosystem also posed a challenge. Integrating Swift-based networking protocols with React Native required bridging complex native APIs while ensuring that location tracking and WebSockets functioned reliably across both iOS and Android. Debugging Bluetooth issues in Apple’s sandboxed environment further complicated development, as maintaining persistent connections in the background was heavily restricted.\n\nDespite these challenges, we successfully transitioned to a more scalable Wi-Fi-based approach, built a real-time protest tracking map, and integrated speech recognition AI into an autonomous protest coordination system. These obstacles forced us to rethink our architecture, leading to a more powerful and adaptable solution."},{"heading":"Accomplishments that we're proud of","content":"Building ProtestLink required pushing beyond conventional networking and security models, and we are incredibly proud of what we accomplished. One of our biggest milestones was successfully implementing a live protester map that provides real-time protest intelligence without relying on external services. The ability to share live locations, report threats, and visualize protest dynamics makes ProtestLink a game-changer for secure protest coordination.\n\nWe are also proud of integrating Whisper AI’s real-time transcription engine, which allows ProtestLink to analyze protest audio, detect emergency phrases, and generate dynamic risk assessments. Protesters can now transmit critical alerts without needing to type, ensuring faster responses in high-risk situations.\n\nAdditionally, we successfully implemented a zero-trust authentication system, ensuring that protesters can join securely and anonymously without exposing personal information or metadata. By making ProtestLink fully decentralized and resistant to surveillance, we created a communication tool that protesters can truly rely on."},{"heading":"What we learned","content":"This project reinforced just how fragile centralized communication systems can be in times of crisis. Governments actively shut down networks, throttle internet speeds, and exploit surveillance laws to suppress protests. ProtestLink has shown us that truly decentralized, peer-to-peer communication is not just a convenience—it’s a necessity.\n\nWe also gained extensive experience in Wi-Fi networking, encrypted communication protocols, and real-time AI processing on edge devices. Working with Whisper AI and NLP-driven event detection expanded our knowledge of how machine learning can be used to enhance real-world safety.\n\nMost importantly, we learned that technology has the power to protect human rights. ProtestLink is more than just an app—it’s a lifeline for those who need a secure way to communicate, mobilize, and protect themselves in uncertain times."},{"heading":"What's next for ProtestLink","content":"ProtestLink has the potential to redefine protest communication, and we’re excited to take it even further:\n\nEnhancing AI-driven risk detection to provide smarter, more responsive alerts. Expanding to more languages for global accessibility. Exploring LoRa-based long-range communication to expand mesh connectivity even further.\n\nProtestLink isn’t just a project—it’s a movement. We’re committed to building censorship-resistant, decentralized technology that empowers people to speak, organize, and protect their rights—no matter what."},{"heading":"Built With","content":"axios bluetooth esp32 flask leaflet.js numpy openai openstreetmap python react sentence-transformers sqlite stadia tailwindcss transformers typescript web webrtc whisper"},{"heading":"Try it out","content":"protest.morelos.dev github.com"}]},{"project_title":"Agent Ricky","project_url":"https://devpost.com/software/jarvis-mhfy78","tagline":"Monitoring the Health and Safety of First Responders and Military Personnel","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/705/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"DAIN Labs: AI Agent Excellence & Innovation Awards ($5k Cash [1st] & $2.5K Cash [2nd] & $500 Cash [3rd] + Agent Launchpad Invitation)"}],"team_members":[],"built_with":[{"name":"brev.dev","url":null},{"name":"dain","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"tensorflow","url":null},{"name":"terraapi","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/nipunbatra8/healthbot"},{"label":"github.com","url":"https://github.com/nipunbatra8/mapagent"},{"label":"docs.google.com","url":"https://docs.google.com/document/d/1iCNAtqf1H5ELC9MBTEaelI2FQOK2W6TXy2yYBOtoppM/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Our team wanted to use this opportunity at TreeHacks to create something bigger than ourselves, something that serves others. Who better can we serve than the first responders and the military personnel who have devoted their lives to protect and serve us every day? These first responders and military personnel must respond quickly because they are placed in very delicate situations. Thus, we realized that, we needed an AI that can be proactive and create real-time decision-making. Unlike rule-based automation or passive AI that waits for user input, agentic AI goes beyond this traditional AI by enabling autonomous decision-making in critical situations where speed and efficiency are very important."},{"heading":"What it does","content":"Agent Ricky is a custom AI agent tailor-made for first responders and military personnel to monitor the status, safety, and health of soldiers, squadrons, and first responders in training and in action. It is essentially another member of the team.\n\nLive biometric and geolocation data collection enables Agent Ricky to detect abnormalities and dangers in real time by providing data for a series of health metrics. These health metrics are then utilized by a custom DAIN-driven AI agent to provide timely alerts, statistics, and information that could prove to be the key to save many lives."},{"heading":"How we built it","content":"Languages: Python · TypeScript\n\nFrameworks and Tools: DAIN · Terra API · Nvidia Brev.dev · TensorFlow"},{"heading":"1. Biometric and Geolocation Data Collection","content":"Originally, we began with the goal of using Terra's PPG waveform data to aid an AI agent in recognizing arrhythmia in order to monitor heart health. However, once we implemented a prototype system, we thought of other data that is pertinent to first responders and military personnel that could also be analyzed, monitored, and reported through the same system. As such, the data collection step now involves collection of a variety of biometric and geolocation data."},{"heading":"2. Heart Rhythms Neural Network Classification","content":"PPG waveform data provided by Terra API allowed us to train a keras sequential neural network from scratch using Nvidia's Brev.dev. The neural network is designed to differentiate between regular, irregular, and atrial fibrillation (afib) heart rhythms and provides crucial data for the health of first responders and military personnel."},{"heading":"3. Data Processing","content":"We took a different approach for processing other important biometric and geolocation data. For example, we calculated important metrics such as respiratory rate, SpO~2~, HRV, stress estimation, sleep quality, pulse wave analysis, and peak detection. These calculations could then be used for detecting alarming conditions. In addition, the preprocessed data can also be passed to the AI agent for visualization purposes."},{"heading":"4. Custom DAIN-powered AI Alerts Agent","content":"DAIN is a custom SDK that allows us to build custom AI agents on top of LLMs to communicate with them in an efficient manner. We designed a DAIN-powered AI service the provides timely alerts to keep safe. It takes in the variety of detected regularities or irregularities in biometric and geolocation data and determines if an alert is necessary to notify leaders and supervisors of."},{"heading":"5. Custom DAIN-powered AI Visualization Agent","content":"We also designed a DAIN-powered AI service that provides important visualization information to help leaders stay better informed about training and combat situations. For example, respiratory rates or location over time can prove to be crucial information."},{"heading":"Challenges we ran into","content":"One area that we spent a significant time in was learning how to use DAIN and all its possibilities. Although DAIN draws on the power of LLMs to create AI agents, DAIN's functionality and SDK is quite different from any LLMs we had previously worked with. However, after poking and prodding our way through documentation and experimentation, we discovered the interesting potential of DAIN. Learning through the demo services, the UI manipulation, and background processes took a lot of experimentation, but also offered a novel learning experience in creating and launching AI services."},{"heading":"Accomplishments that we're proud of","content":"Creating our own Agentic AI that has the potential to make a real impact. Coming into the Hackathon, agentic AI systems were novel to all our team members. Yet, hackathons are the best places to explore new topics, new ideas outside of comfort zones. As such, we are really proud that we, 4 strangers, not only met each other, but synergized into a cohesive unit that collectively explored agentic AI through DAIN's products. In addition, together we were able to build Agent Ricky, an agentic AI system that has the ability to provide first responders and military personnel and their leaders with the crucial information to keep them safe in training, service, and combat."},{"heading":"What we learned","content":"The next powerful wave of AI may come from Agentic AI that leverages the abilities of LLMs to more effectively and efficiently perform specific, precise, and detailed tasks."},{"heading":"What's next for Agent Ricky","content":"The potential of Agent Ricky only grows the more data there is available. With an expanded user base and many more data points, we see Agent Ricky as having real potential in greatly improving the safety of first responders and military personnel."},{"heading":"Built With","content":"brev.dev dain python tensorflow terraapi typescript"},{"heading":"Try it out","content":"github.com github.com docs.google.com"}]},{"project_title":"CodeForge Pages","project_url":"https://devpost.com/software/codeforge-pages","tagline":"Aesthetic React Landing Pages in Minutes – AI, Code Preview / Export, OAuth, Databases","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/787/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Delve: Most Intuitive UX (AirPods + Delve Founder Numbers + Onsite)"}],"team_members":[],"built_with":[{"name":"beautiful-soup","url":"https://devpost.com/software/built-with/beautiful-soup"},{"name":"elasticsearch","url":"https://devpost.com/software/built-with/elasticsearch"},{"name":"fastapi","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"supabase","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/KshitijTeotia06/frontendTreeHacks"},{"label":"github.com","url":"https://github.com/KshitijTeotia06/backendTreeHacks"}],"description_sections":[{"heading":"Inspiration","content":"We love to build, but design is hard. To make something visually appealing, it requires a lot of experimentation and research into what needs to be added. Landing pages are often the first thing a customer sees about a business. We thought current AI tools fall short of our needs. They were either too rigid, didn't integrate well into our workflow of Next.JS, or didn't have enough complexity. So, we thought, why not automate it?"},{"heading":"What it does","content":"CodeForge Pages asks users to enter information about their company along with the name of the company. Based on this information, CodeForge generates a multi–component landing page in React that contains beautiful layout, text, and style. Users can view the rendered code in either the web-view or a simulated mobile view. When pleased with the website, they can export it–which will download a zip of all the react components. All users are also authenticated – meaning when you create a project, we associate it with your profile and it can be accessed at any time through the dashboard."},{"heading":"How we built it","content":"CodeForge Pages is built in Next.JS. The AI pipeline is quite intricate. We first recorded the data of existing landing pages and then extracted images of every single component of that page (header, hero, feature, etc). Every landing page was then added to an Elasticsearch serverless vector database with a corresponding description generated automatically. Now, when the user prompts the app, it finds the most similar company and generates components through a section-by-section workflow. This backend operation is all coded in Python and utilizes FastAPI. To render the generated component, we use Sandpack from CodeSandbox. In order to authenticate each user, we use Supabase – allowing GitHub, Google, and verified email log-in. To save the data about each project for every user, we use Firebase. Specifically, every component is stored as a String and is rendered every time the user opens the project."},{"heading":"Challenges we ran into","content":"It was quite difficult to render the generated React components on the screen. At first, we created an implementation using Babel. This nearly took 5 hours and was quite complicated as there were many issues with UI libraries we were using such as Tailwind. Later, we experimented with some other features and asked those at the booths for help. After some more research, we found CodeSandbox. Additionally, the model was glitching – generating white text on white background or dark text on dark backgrounds. To fix this, we utilized Tailwind to ensure the model was accurately placing classes on elements. Additionally, we improved the prompting through iteration."},{"heading":"Accomplishments that we're proud of","content":"The app generates beautiful and aesthetic components as shown in the images. These designs can be implemented at a real business with a great level of acceptance. Additionally, the code also displays code and an accurate preview. Preview at first seemed difficult to generate accurately, but now it is quite accurate and rarely errors."},{"heading":"What we learned","content":"We gained many insights about UI / UX automation through AI and what it takes to generate such components. Additionally, we learned more about how code rendering really works in React projects. In the future, we should check if there are any libraries to simplify our tasks."},{"heading":"What's next for CodeForge Pages","content":"We plan to add more features to the app and ship it to the public. Some of the features we plan to add are chat to add features, an interactive no-code editor, on-demand hosting, and image generation / implementation."},{"heading":"Built With","content":"beautiful-soup elasticsearch fastapi firebase next.js python react supabase"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"theo","project_url":"https://devpost.com/software/theo-k1enau","tagline":"a cognition copilot that distills your thoughts from pen to perspective","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/881/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Elastic: Best use of Elasticsearch Serverless ($2,000 Cash [1st] & $1,000 Cash [2nd])"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"ai","url":null},{"name":"elasticsearch","url":"https://devpost.com/software/built-with/elasticsearch"},{"name":"embeddings","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"mistral-large","url":null},{"name":"rag","url":null},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"}],"external_links":[{"label":"github.com","url":"https://github.com/RALS-TreeHacks-25/backend"},{"label":"github.com","url":"https://github.com/RALS-TreeHacks-25/frontend"}],"description_sections":[{"heading":"Overview","content":"theo is a cognition copilot that distills your thoughts from pen to perspective.\n\nYou can think of theo in two ways.\n\nIn buddhist thinking, papañcha is the word for conceptual proliferation — when you’re overthinking about the world and have a swirl of thoughts attacking you from all directions until you’re stressed but don’t know what to do. theo helps you take this swirl of thoughts and distill them into understanding and action.\n\nFrom the perspective of an “epidemic of busyness”, we are constantly surrounded by stimuli and noise that prevent us from taking 5 minutes to take a step back. theo is a cognition OS that analyzes your written notes, prompts you for reflection, sparks connections with your earlier thoughts, and suggests actionable steps in your calendar.\n\nA lot of people are building a second brain for productivity — we are building a second brain for cognition."},{"heading":"Problem","content":"Everyone is in a constant swirl of work, studies, and chores, with little time in a day to step back and process emotions and feelings, reflect on actions and priorities, and identify values and goals. On top of that, with social media, informational noise around us, and pressing global challenges, making this conscious step to recap your day became only more difficult. In the midst of these modern factors, people end up feeling overwhelmed, lost, or anxious, without the means to truly understand why or how to work through it. Helping people understand themselves better is a key to a more lucid, productive, purposeful future for humanity."},{"heading":"Solution","content":"theo is an AI-powered cognitive workflow that helps users clarify their thinking through writing. When a user inputs journeys – whether scattered ideas, reflections, or unstructured thoughts – theo extracts valuable insights, identifies patterns across past entries, and asks intelligent follow-up questions to spark self-reflection.\n\nFor every thought journey you write down, theo can leave three types of annotations:\n\nconnections: relations between the current journey and your previous web of thoughts; questions: focused questions based on the recent notes to spark reflection; actions: suggestions for calendar events to take actionable steps to work on your goals -- one click integration with calendar apps.\n\nAdditionally, it can prompt you with personalized questions to start your journeys."},{"heading":"Why theo","content":"Unlike a standard note-taking app, theo transforms raw cognition into structured insight. Unlike a typical agent, it aims to gently guide rather than overcorrect and oversuggest. For example, in annotations, theo first and foremost shows the user their previous thoughts, but deliberately does not give its own analysis on top of them. Additionally, the number of question annotations is kept to be low to not cause a feeling of overwhelm.\n\nWe assist the user in distilling their thinking instead of thinking for them."},{"heading":"Accomplishments that we're proud of","content":"We’re proud of building an AI tool that doesn’t just optimize for efficiency but also for depth of thinking. Seeing theo make successful connections between separate thoughts and building valuable insights was a major win. We also prioritized user experience, making sure theo feels like a natural extension of the user’s mind rather than an overwhelming AI assistant."},{"heading":"How we built it","content":"For this hackathon, we tried incorporating a myriad of new and exciting frameworks into our tech stack. We relied on a React-Native frontend as well as Firebase’s Cloud Functions for our backend, as we wanted our users to be able to journal on the fly. We used Firestore as a general-purpose no-SQL datastore and ElasticSearch to run vector search on our embeddings. Finally, we used Mistral’s fast models for our agentic behavior. This combination of frameworks enabled us to achieve flexible functionality when it came to meeting our app’s functional requirements."},{"heading":"Challenges we ran into","content":"The architecture of the foundation – the full stack app itself was smooth sailing. However, as many developers who work with AI agents know, we quickly learned that AI agents are unreliable, with LLM output outright hallucinating inconsistent behavior. In particular, we struggled with querying elastic search for semantically similar journals using a vector search, as Elastic search’s semantic search function returned to us different output every time we called one of our HTTP endpoints. Getting the annotations working proved an enormous difficulty as well, requiring us to design an entire mental module associated with indexing the characters on screen."},{"heading":"What we learned","content":"We gained deeper insight into how AI can facilitate cognition rather than just automate tasks. Through testing, we saw firsthand how prompting users with the right reflection questions can unlock new insights, giving us a glimpse of how AGI might go about functioning. On the technical side, we learned a lot about embeddings, vector similarity, and how to design an effective note-retrieval system."},{"heading":"What's next for theo","content":"We see theo evolving into a more personalized and adaptive cognition copilot. Future iterations will include improved contextual understanding, allowing theo to better detect a user’s current mindset and tailor its prompts accordingly, and orienting user within their complex web of thoughts by visualizing connections between different journeys.\n\ntheo is more than a productivity workflow — it’s a thinking partner for a clearer mind."},{"heading":"Built With","content":"agents ai elasticsearch embeddings firebase google-cloud mistral-large rag react-native"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"<PriorityQueue>","project_url":"https://devpost.com/software/priorityqueue","tagline":"Democratizing information across the electric grid.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/271/502/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Elastic: Best use of Elasticsearch Serverless ($2,000 Cash [1st] & $1,000 Cash [2nd])"}],"team_members":[],"built_with":[{"name":"elastic","url":null},{"name":"mapbox","url":"https://devpost.com/software/built-with/mapbox"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/shrenikpatel17/treeHacks2025"}],"description_sections":[{"heading":"Inspiration 🌱","content":"The U.S. interconnection queue faces a massive backlog, with two terawatts of mostly clean energy projects waiting for grid access, more than the 1.25 terawatts currently online, causing delays of three to five years or more. High interconnection costs and uncertainty lead to frequent project dropouts, forcing restudies that further slow the process and subsequently the clean energy transition. The current transmission planning system is inefficient, requiring the next project in line to bear upgrade costs, discouraging development and creating cascading delays. While FERC Order 2023 introduces reforms like batch processing and penalties for delays, the lack of proactive regional transmission planning remains a major bottleneck.\n\nDriven by genuine climate interest and frustration about the lack of progress in the energy transition, our team was drawn by the unique challenges that the grid interconnection queue offered, an often overlooked reason for lack of progress in clean energy transition. Like many other climate-adjacent issues, it all comes down to regulation. In a brief published by the Federal Energy Regulatory Commission, the commission detailed the great need for an efficient and transparent queue management system. Highlighted customer insights can be found here . Without one, American energy goals cannot be met. The US is not the first to think about this issue. France, for example, has taken strides to streamline its grid connection processes, leveraging a centralized system to integrate renewable projects faster and ensure grid stability. Similarly, parts of Texas have piloted innovative approaches, such as distributed energy resource integration and priority queues for renewables, to accelerate clean energy deployment. These efforts reflect growing public and institutional support for systems that not only simplify bureaucracy but also unlock the potential for cleaner, more reliable energy. It’s clear that people want this change and believe it will help bridge the gap toward a sustainable energy future."},{"heading":"What it does 💡","content":"Our solution is a user-centric platform that visualizes geospatial data of generation stations, manages each developer’s projects, and organizes said projects into clusters using proprietary algorithms. Each developer has access to current energy generator information, energy generators in the grid interconnection queue, and their own projects. Developers can take part in a “bidding system”, which will be pooled towards fees associated with connecting to the grid as a cluster. Developers also have access to a RAG agent, Link the Energy Expert, who is contextualized by all of PQ’s data and understanding of the grid interconnection system, and can consult with Link to plan which clusters to join and more.\n\nAnother user of our platform is the ISO manager, or government-affiliated project manager. This user has access to all project proposals, and has the ability to grant or reject said projects."},{"heading":"How we built it 🛠️","content":"Our infrastructure is centered around a web app model through NextJS that is both scalable and can easily work with large amounts of data as we use MongoDB as our database. We made extensive use of Elastic for our RAG model and geospatial analysis, along with OpenAI for open-ended location interpretation based on user prompts."},{"heading":"Challenges we ran into 🌃","content":"As beginner hackers, we were inexperienced in working with the tools we had access to at Treehacks, but we were eager to try out new software. Unfortunately, after spending a great deal of time working through a particular sponsor’s product, we realized that the product was not compatible with our web app, and the information could not be embedded into our final product. A drastic pivot had to be made and we tackled steep learning curves during the latter half of the hackathon."},{"heading":"What we learned 📝","content":"Many of our teammates have never built out a project of this scale at the speed we worked at during Treehacks. Moreover, we had varying degrees of experience with working with complex project files, so many of us learned an incredible amount about web app files and planning for larger project structures.\n\nDue to the research intensity of our project, we now know a significant amount about the nuances of the electric grid problem that we didn’t before. When working with niche pain points such as the interconnection queue, intuition for what our clients need will be built on a lot of painstaking research and prototyping through methods such as speaking with users."},{"heading":"What's next? 🚀","content":"PriorityQueue is far from finished. We envision a future where PQ works alongside government and regulation to promote transparency inter-developer and between the ISO and generators. Moreover, PQ currently sets up an environment in which developers can accurately assess the benefits to joining a cluster. However, by incorporating AI-facilitated decision making and analytics, PQ’s consulting service abilities could be bettered by tenfold. Imagine a platform that vertically integrates the energy project planning process, scrapes all the data that exists on which projects are currently being implemented, and can help developers make informed decisions about their business!\n\nWe are inspired by the potential of PQ to:\n\nFoster collaboration and information trade between generators. Accelerate permitting reform & power the country. Develop a dynamic market for generators.\n\nPitch deck can be found here ."},{"heading":"Built With","content":"elastic mapbox mongodb node.js openai react tailwind"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CodeCrack","project_url":"https://devpost.com/software/codecrack","tagline":"Master coding interviews with CodeCrack: a voice-driven AI agent simulating dynamic, real-time mock interviews, delivering interactive feedback to sharpen your skills.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"ElevenLabs: Best Use of ElevenLabs (11M Credits + AirPods Max per team member [1st] & 8.11M Credits + Merch [2nd] & 5.11M Credits + Merch [3rd])"}],"team_members":[],"built_with":[{"name":"elevenlabs","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"vercel","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/kkarenvoo/TreeHacks2025"}],"description_sections":[{"heading":"Inspiration","content":"Want to ace that upcoming technical interview for your dream job? LeetCode is a great place to start for developing good problem solving skills, but it lacks the interactive communication demands of a live coding interview where interviewers assess your problem-solving, debugging, and communication skills in addition to your ability to code. It’s fun to recruit your friends to help conduct a mock interview but these resources are not available to everyone.\n\nFrom under-resourced students to experienced builders, interview preparation materials and access to experienced coders can be an obstacle. As engineering students who have undergone Software Engineering recruiting, we wanted to build a tool that bridges this gap - allowing coders to improve their skill set while simulating the pressure of a live interview with real-time feedback. We built CodeCrack to empower anyone to crack their interview with confidence."},{"heading":"How we built it","content":"We built CodeClack on a Vercel web platform with functional modifications made for integrating the AI agents to execute their client tasks, keep track of state, and upstream information to the voice. On top of this, we built on the user interface to reflect the various components of our project. For instance, we’ve integrated an IDE with Monaco, the code editor behind VScode, and Pyodide, a Python interpreter so users can run and execute their code. Leveraging ElevenLabs voice agents, CodeCrack offers both behavioral and technical interviews. To simulate a Software Engineering coding interview as closely as possible, In the technical coding challenge, the AI agent will introduce themselves, give an overview of the interview, give the user a moment to introduce themselves, and then will dive into the coding challenge, populating the user’s screen with the challenge and reading it outloud. The agent is designed to take on the persona of a more senior level Software Engineering interviewer, processing how the user breaks down the problem and explores various paths before diving into an approach, offering an open-ended hint if the user indicates they are stuck or asking a follow-up question if they are on the right track. Within our agentic system, there are two main flows of information sharing: agent to client and client to agent. For the coding challenge question, we retrieved leetcode questions and solutions from a HuggingFace dataset, and in the first agent to client stream, we developed a client tool, sendNewQuestion, that randomly retrieves a question within the user’s difficulty preference, upstreaming it to the voice agent to read aloud. The other mode, client to agent, is activated when the user clicks “Run Code”, so the user’s code submission can be tested for functionality and the agent can generate feedback on code style. The agentic system is keeping track of the state of interactions, making decisions to pivot the conversation to get the user closer to a functional solution without revealing too much information. After the user ends the interview, an API call is sent to ElevenLabs’ criteria evaluation feature to get the agent’s feedback on communication, problem-solving, and debugging based on the overall state of interactions."},{"heading":"Challenges we ran into","content":"With no prior experience building with AI agents, the primary challenge we initially faced was understanding how agentic frameworks are built, and the tradeoffs between multi-agent frameworks and single agent frameworks. The next challenge we faced was learning how to influence single agent frameworks, and specifically, how to send information to it and retrieve information from it for various components of our project’s functionality. For instance, after retrieving a leetcode question we wanted to upstream this information to populate the user’s screen and have the AI agent read the problem out loud, but we faced difficulty understanding and building on the information streams that come with client tools. Expanding on this, all the builders on the team are most comfortable with Python and have no experience building extensive frontends. After realizing our Python client tools were not easily compatible with the AI agent codebase, we converted that entire part of our codebase to lay within Next.js and React, with some help from the ElevenLabs sponsor with only hours left before the deadline."},{"heading":"What we learned","content":"Over the last 36 hours we learned how to build our first tools with AI agents. In particular, we designed an AI agent for our interviewer persona and built various client tools to handle retrieving leetcode questions, evaluating the user’s code on run, and populating the UI with the coding challenge and comprehensive feedback. Specifically in terms of new technologies, we had no prior experience working with Node.js and React, so it was an exciting challenge to focus on user interface design."},{"heading":"What's next for CodeCrack","content":"We look forward to building a more comprehensive system including System Design, Python trivia, and resume-based questions!"},{"heading":"Built With","content":"elevenlabs javascript node.js python vercel"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"EcoBite","project_url":"https://devpost.com/software/ecobite-qcujae","tagline":"Track your food waste in a gamified way— EcoBite aims at combating global food waste by allowing its users to upload images of their food, eaten or uneaten, to determine the weight of what is wasted.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/143/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Flutterflow: Best Use of FlutterFlow ($2,000 Cash + meet w/ founders [1st] & $1,500 Cash [2nd] & iPad 10.9\"s [3rd])"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"flutterflow","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"perplexity","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/SirKentut/EcoBite"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/share/eco-bite-9o3s7v"},{"label":"eco-bite-9o3s7v.flutterflow.app","url":"https://eco-bite-9o3s7v.flutterflow.app/"}],"description_sections":[{"heading":"Inspiration","content":"In the USA alone, food waste makes up 22% of landfilled solid waste, and “globally, food loss and waste represent 8 percent of anthropogenic greenhouse gas emissions” (EPA 2021). Our inspiration for EcoBite was an interest in this incredibly relevant issue within our world, as well as the Stanford Sustainability Challenges , specifically Challenge #3: the development of a program for determining the weight of food in an image. From a technical standpoint, this challenge aligned greatly with our interests:\n\nfull-stack application development the use of AI / large language models generalized machine learning Our team felt we could each bring our strengths into this project, with the significant added benefit of creating a tool that could make an impact in combating food waste and therefore helping our planet’s environment. We also resonated greatly with the message of S tanford’s Ecopreneurship program , of accelerating environmental and sustainability efforts fueled by passions within the field of climate change or tech."},{"heading":"What it does","content":"EcoBite is a mobile app meant for users to take pictures of their leftover food for the main purpose of estimating the food’s weight. After this action on the user’s end, the app will analyze the weight of the food in the image along with classifying its type. On top of being able to record and display this data to users, EcoBite keeps track of individual progress when it comes to food waste, allowing for milestones to be reached when individuals to a good job not creating as much food waste. This information is recorded and held in a backend to be analyzed for generalized food waste trends, all of it location-based. This process of recording images is also great for businesses that produce much food waste in their day to day in order to better keep track of food costs as well as intercept food waste."},{"heading":"How we built it","content":"EcoBite utilizes a variety of sponsor technologies to complete this project. For a professional responsive frontend, we leveraged FlutterFlow to rapidly develop a polished mobile application that enables users to track their food weight, view detailed analytics of their consumption patterns, and participate in an engaging points-based reward system. FlutterFlow's intuitive low-code platform streamlined our development process, enabling us to rapidly create a polished mobile app with professional UI components and functionality that would have otherwise required significantly more development time. Various API endpoints were created to retrieve any necessary information the user may need.\n\nFor the backend, we used Flask as the AI APIs perform best in a Python environment. Google Gemini was used to perform essential functionalities such as food classification and segmentation, volume estimation, and weight calculations. Our team learned prompt engineering principles to optimize these AI interactions, ensuring accurate and consistent responses while minimizing token usage and processing time. For obtaining the density of different food items, we used the Food and Agriculture of the United Nations' Official Density Database as our primary source. If the database did not contain a food item, we relied on Perplexity's Sonar Pro model to determine density values."},{"heading":"Challenges we ran into","content":"On top of using Google Gemini for estimating the density of food via prompt engineering, our team wanted to use a food volume estimator deep neural net trained on many food images to assist with the classification of food items and estimation of their weights. There were many great examples of older work being done in this field, however a lot of the code was incredibly difficult into modern python / usability standards to this day, so our team ran out of time when it came to implementing this model alongside the AI model. This neural net would have been trained on 100100 images of food , using ResNet50 for image classification."},{"heading":"Accomplishments that we're proud of","content":"Our team is most proud of our development of EcoBite using a tool none of us had worked with previously: FlutterFlow. Thanks to TreeHacks for hosting the company as a sponsor, we were given a perfect opportunity to pick up the tool and became highly motivated to try our best to master it over the weekend. Overall it was an amazing to learn how to use, all employees present helping us out greatly throughout countless steps of our app development process; end-to-end! Thanks to all of the assistance, as well as FlutterFlow’s user friendliness, we have a great looking mobile application that was incredibly straightforward to learn how to build."},{"heading":"What we learned","content":"Thanks to Keegan Cooke , the head of Stanford's Sustainability Challenge #3 and Director of Stanford Ecopreneurship, our team gained valuable insights about the challenge and received a food scale for our preparations. Through our research, we discovered previous attempts to estimate food volume (rather than weight) using image classification. Along the way, our team quickly picked up and mastered several technical tools, including the Perplexity API, Google Gemini API, FlutterFlow for app development, and Flask for backend implementation."},{"heading":"What's next for EcoBite","content":"We believe a great product is one that listens to its users. We look to gain as much feedback from users on their experience using EcoBite and what features have been useful and those in need of improvement. Through our beta testing phase, we will actively collect user feedback through in-app surveys, user interviews, and analytics tracking to understand usage patterns and pain points. This data will guide our development roadmap, ensuring we prioritize features that provide the most value to our users while continuously refining the app's functionality and user experience to better serve our community's needs.\n\nSome future goals include expanding to helping out businesses interested in keeping track of their food waste for the sake of improving sustainability and helping with professional costs.\n\nWe would also love to expand the backend model used from AI tools to an actual existing estimation model trained on real images, both ones found in existing databases and those taken by users of the application. A big goal of EcoBite would be to create a data expansion pipeline wherein which the food density estimator could continuously train on new datapoints.\n\nEcoBite aims to counter food waste and instill better practices in how we prepare and consume food. Through our gamified approach and real-time food tracking, we empower users to make more conscious decisions about their portions and develop sustainable habits that benefit both their personal well-being, community, and environment."},{"heading":"Built With","content":"flask flutterflow gemini perplexity python"},{"heading":"Try it out","content":"github.com app.flutterflow.io eco-bite-9o3s7v.flutterflow.app"}]},{"project_title":"The Duck You Mean?","project_url":"https://devpost.com/software/the-duck-you-mean","tagline":"- if you can explain it to a duck, you can explain it to anyone.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/498/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"ElevenLabs: Best Use of ElevenLabs (11M Credits + AirPods Max per team member [1st] & 8.11M Credits + Merch [2nd] & 5.11M Credits + Merch [3rd])"}],"team_members":[],"built_with":[{"name":"elevenlabs","url":null},{"name":"luma","url":null},{"name":"mistralai","url":null},{"name":"openai","url":null},{"name":"zoomapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/joycechen721/duck-you-mean/"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1wbRdoGqOnnMw5rVU0nRgBMmOErweNsQdrtsrA7q-214/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"We were motivated by the power of rubber duck debugging and the wisdom of Richard Feynman’s quote:\n\n\"If you can't explain it simply, you don't understand it well enough.\"\n\nToo often, AI is used to give answers, but true learning happens when we explain concepts ourselves . We wanted to flip AI on its head. Rather than asking the duck for answers, you teach it , and it pushes back with evaluations, scores, and follow-up questions to deepen your understanding."},{"heading":"What it does","content":"The Duck You Mean is an interactive web app that lets users pick a topic and engage in a back-and-forth conversation where AI plays the role of a curious student . Users must explain concepts, and the AI:\n\nScores their explanations based on clarity, correctness, and depth Asks intelligent follow-up questions to test deeper understanding Provides feedback to refine and strengthen their knowledge\n\nWe also created zone called \"The Pond\", where users can create Zoom Meetings to study with friends or chill out after completing a lesson."},{"heading":"Featured APIs","content":"- ElevenLabs - facilitates the full conversation process from user mic input to duck voice generation\n\n- Zoom API - creates meetings for peer-to-peer study sessions\n\n- OpenAI API - generates the starting question based on the prompt,\n\n- Mistrel AI - supports topic ideation and question formation\n\n- Luma AI - powers duck thoughts through created images"},{"heading":"Challenges we ran into","content":"Fine-tuning AI’s response evaluation —balancing depth of feedback while keeping interactions engaging Generating meaningful follow-up questions without redundancy Integrating voice interactions smoothly for an immersive experience"},{"heading":"Accomplishments that we're proud of","content":"Successfully flipped AI’s role from answering questions to being the student Developed an engaging voice-driven AI tutor experience Created an effective learning tool that enforces deeper understanding Seamlessly integrated multiple AI-powered services"},{"heading":"What we learned","content":"Teaching a concept cements learning better than passive studying Thoughtful follow-up questions are key to uncovering misunderstandings Conversational AI can enhance education beyond just answering queries"},{"heading":"What's next for The Duck You Mean?","content":"Learning History to intelligently track user progress over time Gamification: Achievements, streaks, and leaderboards Mobile app version to promote learning on the go"},{"heading":"Built With","content":"elevenlabs luma mistralai openai zoomapi"},{"heading":"Try it out","content":"github.com docs.google.com"}]},{"project_title":"Caren AI","project_url":"https://devpost.com/software/caren-ai","tagline":"Because everyone deserves someone who cares","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/903/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Flutterflow: Best Use of FlutterFlow ($2,000 Cash + meet w/ founders [1st] & $1,500 Cash [2nd] & iPad 10.9\"s [3rd])"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"ai-agents","url":null},{"name":"dart","url":null},{"name":"elevenlabs","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"flutterflow","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"perplexity","url":null},{"name":"twilio","url":"https://devpost.com/software/built-with/twilio"}],"external_links":[{"label":"github.com","url":"https://github.com/Tap2Call/flutterflow"},{"label":"tap2call.flutterflow.app","url":"http://tap2call.flutterflow.app/"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/project/translateusing-groq-streaming-a-p-is-046jc0"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/run/1c0qLm3zk9jT4WiNtcHJ"}],"description_sections":[{"heading":"Inspiration","content":"Every year, 850 million insurance claims are denied in the U.S., leaving patients with unexpected medical bills and financial stress. Shockingly, less than 1% of these denied claims are ever appealed not because they lack merit, but because the appeal process is frustrating, time-consuming, and complicated (Wall Street Journal).\n\nAs a result, nearly 1/3 of Americans contribute to $220 billion in medical debt, often due to denied or mishandled claims (Consumer Financial Protection Bureau).\n\nWe wanted to level the playing field by putting AI in the patient’s corner. What if an AI negotiator could read the fine print, fight back against denials, and handle everything seamlessly? That’s how Caren AI was born—to empower patients, recover their rightful coverage, and eliminate unfair medical debt."},{"heading":"What it does","content":"Caren AI automates the insurance claim appeal process and negotiates with insurance providers on behalf of patients. Our AI reads insurance denial letters, cross-references them with Evidence of Coverage (EOC) documents, and calls insurance companies to dispute denials using legally backed arguments.\n\nKey Features:\n\nAI-Powered Claim Negotiation – Caren AI analyzes insurance policies and denial letters, then calls the insurance provider via Twilio to dispute the claim. Automated Healthcare Scheduling – The AI finds available doctor appointments and books them for the patient in seconds. Intelligent Appeals Processing – Uses ElevenLabs Conversational AI, trained on insurance documents, to craft strong, evidence-based appeal arguments.\n\nCaren AI features two AI agents dedicated to simplifying healthcare management:\n\nSam (Scheduling AI) – Helps users book and manage doctor appointments based on availability and medical history. Noa (Negotiation AI) – Calls insurance providers, disputes denied claims, and negotiates fair settlements to reduce costs.\n\nResult: Patients save thousands on medical bills without the hassle of navigating complex insurance policies themselves."},{"heading":"How we built it","content":"We designed Caren AI as a full-stack AI-powered assistant, ensuring seamless automation of insurance claim disputes and healthcare scheduling.\n\nFrontend : FlutterFlow (Dart) – A clean, user-friendly interface for patients.\n\nAI Backend : ElevenLabs Conversational AI – Trained on insurance policy documents for intelligent claim analysis.\n\nFastest AI Model : Gemini 2.0 Flash – Parses complex medical and insurance documentation.\n\nLive Call Automation : Twilio API – AI negotiates claims directly with insurers via automated phone calls.\n\nAuthentication : Firebase Authentication – Secure user logins and data protection.\n\nBy combining AI, automation, and real-time negotiation, we eliminated the need for manual claim appeals, making the process fast, accurate, and effortless for patients."},{"heading":"Challenges we ran into","content":"Training AI on Complex Insurance Policies – Extracting accurate insights from dense, jargon-heavy documents required custom model fine-tuning.\n\nReal-Time Insurance Call Handling – Ensuring AI could hold a live, logical conversation with insurers using Twilio without sounding robotic was a key challenge.\n\nUsing Flutter for the First Time – Since our team was new to FlutterFlow, we had to learn its UI-building framework and state management quickly to build a seamless frontend experience.\n\nThrough iterative testing, API optimizations, and AI refinement, we successfully overcame these hurdles to create a seamless patient experience."},{"heading":"Accomplishments that we're proud of","content":"Built a Fully Functional AI-Powered Claim Negotiator – Our AI successfully disputes insurance claims in real-time.\n\nImplemented End-to-End Automation – From claim upload to resolution, everything happens without manual intervention.\n\nIntegrated Live Twilio Call Handling – Caren AI makes phone calls to insurers, delivering legally sound counterarguments.\n\nDesigned a Scalable, Patient-Friendly Platform – Built using FlutterFlow, making it accessible to all users, regardless of tech expertise.\n\nOur solution doesn’t just solve a problem it revolutionizes patient advocacy in healthcare."},{"heading":"What we learned","content":"The Healthcare System is Stacked Against Patients – Insurance denials are often not justified, but most patients don’t have the knowledge or time to fight back.\n\nAI Can Level the Playing Field – With intelligent claim analysis and negotiation, AI can recover thousands of dollars for patients.\n\nUser Experience Matters – Many users aren't tech-savvy, so creating a seamless, intuitive interface was crucial.\n\nReal-Time Communication with Insurers is Game-Changing – Automating insurance negotiations was far more effective than just document-based appeals.\n\nCaren AI isn’t just an app—it’s a movement. By leveraging AI negotiation, we’re fighting for patient rights and bringing fairness back to healthcare.\n\nJoin the revolution, and let AI handle the hassle, so you can focus on your health."},{"heading":"What's next for Caren AI","content":"Callback Feature for Dropped Calls – If an insurance call disconnects, Caren AI will automatically redial and resume the negotiation.\n\nMultilingual Support – Expanding beyond English to support Spanish, French, and more to help non-native speakers navigate insurance issues.\n\nFamily Insurance Management – A family plan feature that allows users to track and manage multiple insurance claims in one place."},{"heading":"Built With","content":"ai ai-agents dart elevenlabs firebase flutterflow gemini perplexity twilio"},{"heading":"Try it out","content":"github.com tap2call.flutterflow.app app.flutterflow.io app.flutterflow.io"}]},{"project_title":"Sprout","project_url":"https://devpost.com/software/sprout-io","tagline":"Where imagination shapes learning","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/012/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Flutterflow: Best Use of FlutterFlow ($2,000 Cash + meet w/ founders [1st] & $1,500 Cash [2nd] & iPad 10.9\"s [3rd])"}],"team_members":[],"built_with":[{"name":"dart","url":null},{"name":"fastapi","url":null},{"name":"flutterflow","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"genai","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"imgur","url":"https://devpost.com/software/built-with/imgur"},{"name":"luma","url":null},{"name":"lumalabs","url":null},{"name":"multimodal","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"treehackssprout.flutterflow.app","url":"https://treehackssprout.flutterflow.app/"},{"label":"app.flutterflow.io","url":"https://app.flutterflow.io/project/story-8owtoa"},{"label":"github.com","url":"https://github.com/sdoshi4/Treehacks2025"}],"description_sections":[{"heading":"Inspiration","content":"It all started with a TikTok video: a teacher shared how she turned her kid’s doodles into adventure books, which sparked curiosity and made the excitement in the classroom undeniable. ✨ What if we could bring that magic to every child, everywhere?\n\nThe Challenge:\n\nKids are at their peak in creativity, but are often bored and unengaged by the traditional method of passive reading. Parents and teachers need ways to keep students engaged in learning and gauge their understanding.\n\nThe Solution: Sprout is the future of storytelling, interactive learning, and self expression. Through Sprout kids can:\n\nUpload their drawings from their imagination Read a Sprout generated interactive choose-your-own adventure story Be engaged through vivid imagery Learn vocabulary while having fun Get tested on their comprehension, adapted to their grade level\n\nWith Sprout, children aren’t just reading stories—they’re living them. They upload their own drawings, personalize their adventures, and build language skills through play. Teachers can tailor stories to lessons, integrate vocabulary, and expand beyond just reading into science, history, and more. Sprout.io is a win-win."},{"heading":"What it does","content":"Sprout starts with your student’s drawing. From there, we build an engaging, choose-your-own story, with generative visuals based on your input. The adventure is filled with grade-level vocabulary to hit comprehension goals, with a quick comprehension check at the end.\n\n✨ Unleash your creativity Draw something magical- characters, a wild adventure, animals, your wildest dreams. 📸 Upload Your Drawing Snap a picture and upload it to Sprout.io. 📚 Pick Reading Level Pick grade level so the story and included vocabulary matches your reading comprehension. 🔎 Choose Your Own Adventure Read as the story unfolds with interactive choices, engaging visuals, and exciting vocabulary blended in along the way. Choose your own adventure 🧠 Test Your Knowledge Take a quiz at the end to see how far you have come and check your comprehension of the vocab words."},{"heading":"How it works","content":"Simply go to: demo select your grade level, upload a drawing of your wildest creations, and click “Begin Your Story”! Enjoy this seamless learning experience and story as it unfolds, with vocab words integrated along the way."},{"heading":"How we built it","content":"Sprout.io was developed using Flutterflow, for a seamless user experience. We integrated several AI-driven APIs:\n\nGoogle Cloud - Gemini Flash 2.0 AI Studio generates dynamic, age-appropriate storylines with the student’s image and a prompt we integrated. We specifically chose Gemini for the Multimodal 2.0 Flash model, which let us combine both visual and text inputs. We also utilized the multi-category content filtering Gemini provides to prioritize safety for kids on this platform. Luma API powers our image generation, bringing stories to life in the most creative way possible. We used the Dream Lab API and tinkered with the reference and styling weights to ensure that we could maintain consistent visual storylines, while also injecting a bit of creativity and unexpected cliff-hangers for students. FlutterFlow for our front-end! This tool was really helpful, since it let us visually diagram our control flow while also integrating our custom FastAPI endpoints for reactive generative AI, all with a seamless integration and clean UI perfect for kids! Imgur API handles quick image uploads within the FlutterFlow platform, passing them along to Luma’s API. Custom FastAPI Endpoints for our text and image generation and modifications, since our control-flow was pretty complex. Google Cloud and Docker , for persistent hosting of the backend solution. We also used ngrok and uvicorn for local testing. Perplexity for help with expanding our prompt engineering with its strong natural language and research capabilities Lots of free food, swag, and entertainment from the Stanford TreeHacks organizers and sponsors! Thank you!\n\nBy combining these resources, Sprout creates a high value learning experience with personalized and interactive storytelling. Project link with code"},{"heading":"Challenges we ran into","content":"Front-End via Flutterflow : At the end of the day, we all know that a technical feat is not very impactful without consideration of the users. While our team had some strong back-end background, we are all beginners to FrontEnd. Thankfully, we found out about FlutterFlow. We spent time understanding how the platform works, figuring out how to add our API calls to our custom FastAPI endpoints, and getting our data formatted, encoded, and displayed onto each page. This took significant time, but led us to create a seamless product that runs on multiple devices, from phones to web screens. Hiding User-Visible Latency : We faced issues with latency of generative AI within our platform. Kids have short attention spans, and our goal is to keep them engaged as long as possible in an educational state of mind. This comes in conflict with the long time frames required for generative AI; for example, we noticed that each Luma Labs API query could take up to 10 seconds, and our Gemeni Flash 2.0 API queries via Studio also took roughly 8 seconds. To fix this issue, we generated possibilities of all future story trajectories 1 chapter ahead, allowing us to use the time the user spends reading to cover part of the future generation time. Even though we will not choose them all, doing some of this work concurrently while students are reading allows us to make our project faster for the end user. Protections & Safety via Gemini Filters : We’ve all seen Generative AI go off-the-rails with its creations, and that is particularly a situation we want to avoid when presenting to younger audiences. As a result, we specifically chose Gemini Flash 2.0 for the API’s adjustable safety filtering weights. We set our levels to explicitly filter out dangerous or harmful content for minors. Maintaining Consistent Context : It's important that our stories balance consistency with creativity as students go through the choose-your-own adventure process, but this can be hard to do with the reactiveness of generative AI, especially when considering both visual and text input AND output. We picked multi-model tools that let us manage context (Gemini Flash 2.0's context capabilities) and focused on maintaining strong references to past information by tuning LumaLabs Dream Lab API's adjustable styling and reference weightage components."},{"heading":"Accomplishments that we're proud of","content":"Working together well! Collaboration across our different backgrounds let us connect at the event and build out a really creative idea. Figuring out how to use FlutterFlow from square one! Being adaptive and persistent in debugging Livesharing code and pair programming when things got tough Building a project with technical depth, social impact, and having a fun time through all of it! Our use of multiple different technologies and optimizations for our use case, from our focus on safety, to integrating multi-modal creativity, and even a focus on user-experience with reducing visible latency."},{"heading":"What we learned","content":"We learned a TON about FlutterFlow. Not knowing much about frontend design, FlutterFlow was a great tool that we spent the majority of our hackathon working in. We managed to get our app to work seamlessly cross-platform and in sync with our dynamic backend endpoints, which took a ton of time, but was a great learning experience for creating a full-stack app from nothing but an idea. Special shoutout to the FlutterFlow team for staying late into the night with hackers to help debug ❤️. We learned a ton about front-end and had great conversations. We also developed a ton of API routes in FastAPI, which we then interfaced within FlutterFlow, adding actions to route data across the frontend, and sending and receiving information from our Generative AI models. Working with these generative AI models was very new to us as well, and we picked up a lot on how to use Gemini’s Multimodal 2.0 Flash API and Luma’s API to generate the content we need, doing a ton of prompt engineering along the way. We also learned the value of discussion and diagraming our workflows and API connections using visual tools like excalidraw for a better design process."},{"heading":"What's next for Sprout","content":"We are planning on expanding Sprout.io’s functionality to better benefit teachers when creating personalized learning plans for students!\n\nImplement a teacher side flow to better monitor individual student progress, see what kids are drawing, and quiz results for their class In addition, us giving out comprehension quizzes allows us to create tons of meaningful data that can be used for better data-driven, or even AI-driven strategies in the classroom We’d love to add ElevenLabs or similar API to read the text-to-speech to enhance the user experience, especially for kids who struggle with text-reading comprehension. Gemini Flash 2.0 is also releasing audio capabilities at the end of the month; we'd love to integrate them. Bold, highlight, and underline key words throughout the story for better immersion. We were thinking of including Markdown support (just like Devpost!) but didn't have enough time to fully flesh out this idea. Make Sprout.io a more kid focused application in terms of adding animation to their drawings (further Luma integration) Add a wider selection of quiz questions and topics that quizzes can cover! Adding a point system (something like Duolingo's Streaks) to keep kids engaged in learning!"},{"heading":"Built With","content":"dart fastapi flutterflow gemini genai google-cloud imgur luma lumalabs multimodal python"},{"heading":"Try it out","content":"treehackssprout.flutterflow.app app.flutterflow.io github.com"}]},{"project_title":"Spheroid Tumor Invasion Kinetics (STIK) GUI","project_url":"https://devpost.com/software/tumor-invasion-kinetics-gui","tagline":"An AI-powered tool for tracking and predicting spheroid tumor invasion kinetics that eliminates the need for manual tracing, featuring an intuitive graphical user interface.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/270/975/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Hudson River Trading: Best Use of Data Hack (HRT-branded JBL earbuds per team member)"}],"team_members":[],"built_with":[{"name":"conda","url":null},{"name":"cvat","url":null},{"name":"pyqt","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"yolo","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/sonnyloweus/STIK_GUI/"}],"description_sections":[{"heading":"Inspiration","content":"Last summer one of our teammates worked in a lab studying glioblastoma, an aggressive strain of brain cancer. The project involved knocking out genes of interest to better characterize tumor invasion patterns at a molecular level. Such a project necessitated a copious amount of data; the lab produced well-plates upon well-plates of treated cells and subsequent images, all of which required tiresome analysis involving manual hand tracing the images and lots of coffee. It was the kind of work for an unassuming undergrad. Thus, our teammate traced hundreds of photos on an unreliable, finicky platform. Somewhere in between image number 250 and 255, our teammate thought: no way this can't be automated."},{"heading":"What it does","content":"STIK_GUI (Spheroid Tumor Invasion Kinetics GUI) is an AI-powered tool designed to automate the tracking and prediction of tumor spheroid invasion. Instead of manually tracing hundreds of images, researchers can upload their data and let STIK_GUI handle segmentation, analysis, and visualization. The intuitive graphical interface makes it easy to process well plate images, extract invasion kinetics, and generate meaningful insights—saving time and reducing human error."},{"heading":"How we built it","content":"STIK is developed using PyQt, a set of Python bindings for the Qt application framework, which enables the creation of a robust and interactive graphical user interface (GUI) . This user-friendly interface allows researchers to seamlessly import datasets, visualize segmentation results, and analyze generated plots with ease.\n\nThe platform leverages instance segmentation through the CVAT (Computer Vision Annotation Tool) framework, providing precise image annotation and labeling. For model training, STIK utilizes YOLOv8 for both segmentation and bounding box detection, ensuring high accuracy and You-Only-Look-Once speed in identifying and delineating objects.\n\nWith a best precision of 99.739% and a mean Average Precision (mAP50-90) of 97.369%, STIK delivers state-of-the-art segmentation performance, making it an invaluable tool for researchers working with complex image datasets.\n\nMoreover, STIK utilizes multi-threading in PyQt , leveraging its signals and slots mechanism to efficiently manage parallel processing. By running the GUI and the model on separate threads, the platform ensures a smooth and responsive user experience without delays or freezing.\n\nMulti-threading allows the GUI to remain interactive while the model performs computationally intensive tasks, such as loading large datasets, running segmentation models, and generating plots. This separation enhances performance, prevents UI lag, and enables real-time updates using PyQtGraph."},{"heading":"What's next for Spheroid Tumor Invasion Kinetics (STIK) GUI","content":"The first version of STIK GUI is particularly well suited for spheroid invasion assays, a common wet lab experiment that mimics the 3D environment of human tissue to study how cancer cells invade. Naturally, we are curious to see if we can extend our model beyond spherical subjects to automate invasion patterns in, for example, patient scans, tissue slices/histology data, animal models, etc.\n\nFrom the user interface standpoint, we plan on developing a feature that visualizes the images continuously by displaying invasion patterns in a time-lapse format. Additionally, we aim to expand the data capacity to analyze multiple wells, lanes, or even entire well plates at once. This would enable us to perform sub-comparisons between past and current experimental treatments in real time. We believe that STIK_GUI has the potential and versatility to enhance the efficiency of wet lab analysis pipelines across a range of applications."},{"heading":"Built With","content":"conda cvat pyqt python yolo"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Navi","project_url":"https://devpost.com/software/navi-7sp5mz","tagline":"Navigate smarter: a co-pilot for your browser Navi streamlines your browsing experience with immediate assistance, voice navigation, and website recommendations based on predicted user intent.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/070/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Groq: Best on Groq Multimodal App Challenge ($1,000 of Groq Swag Store Credit)"}],"team_members":[],"built_with":[{"name":"context","url":null},{"name":"elevenlabs","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mistral","url":null},{"name":"next.js","url":null},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"scrapybara","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"windsurf","url":null}],"external_links":[{"label":"navi-browser-agent.vercel.app","url":"https://navi-browser-agent.vercel.app/"},{"label":"github.com","url":"https://github.com/danieltyx/navi-browser-agent"}],"description_sections":[{"heading":"Inspiration","content":"Given the substantial lack of accessibility resources and compliance in modern website UI—over 96% of websites fail to meet accessibility requirements—we wanted to develop an AI-driven copilot agent, Navi, for human browsing, in addition to creating an easier web infrastructure for AI models to interact with. 🌐🤖"},{"heading":"What it does","content":"In short, Navi makes web browsing easier for human users, with accessible voice navigation 🎤, intuitive browsing with user intent prediction 🔮, instant feedback to questions 📝, and personalized further reading suggestions 📚. In terms of serving AI models, Navi extracts web pages more efficiently compared to traditional VLMs. ⚡"},{"heading":"How we built it","content":"To integrate visual context, we captured screenshots of the user's navigation, which were then processed by visual language models (VLMs) to inform Navi's AI. This enabled Navi to understand and interpret website layouts in real time 🖼️. Simultaneously, we harnessed Context, a robust context fetching engine, to analyze user interactions, predicting browsing intent and tailoring recommendations. In the background, Scrapybara autonomously suggests related websites 🌐, while Groq converts audio commands into prompts for OpenAI's 4-o LLM, ensuring rapid and accurate responses ⚙️. Additionally, we leveraged Mistral for fast, simplified website renderings that highlight key information, making navigation even easier for the user 🏃‍♂️."},{"heading":"Challenges we ran into","content":"Balancing rapid AI responses with efficient web page extraction was just one layer of our challenge 🏔️. A deeper complexity arose from integrating multiple AI components into a unified, accessible system. Each component—whether it was the Context engine, VLMs, or voice processing with Groq—had its own processing speeds, data formats, and dependencies. Merging these disparate systems required careful orchestration to ensure they communicated seamlessly in real time ⏱️. We designed robust interfaces and error-handling protocols to bridge differences in performance and data structure ⚖️. This meant synchronizing outputs from slow VLMs with faster processing modules, ensuring that delays in one area wouldn’t disrupt the overall user experience."},{"heading":"Accomplishments that we're proud of","content":"We're proud to have built Navi as a truly integrated, AI-driven copilot that enhances web accessibility and usability 🌟. Navi not only improves browsing efficiency through context-aware predictions and voice navigation but also serves as a high-performance data extractor for AI models. This project sets a new benchmark for creating accessible digital experiences and bridging the gap between human and AI interaction 🌉."},{"heading":"What we learned","content":"Throughout this project, we gained invaluable insights into prompt orchestration, context fetching, and the nuances of designing for accessibility. We learned how critical it is to balance technical sophistication with user-centric design, ensuring that advanced features translate into real-world usability for diverse audiences 🎓💡."},{"heading":"What's next for Navi","content":"Looking ahead, we plan to expand Navi's compatibility with more websites and enhance its voice and intent recognition capabilities 🔧. Our next steps include optimizing the system for even faster and more accurate responses ⏩, refining personalized recommendations, and exploring additional accessibility features to further empower users with disabilities ♿."},{"heading":"Built With","content":"context elevenlabs gemini groq javascript mistral next.js openai perplexity scrapybara typescript windsurf"},{"heading":"Try it out","content":"navi-browser-agent.vercel.app github.com"}]},{"project_title":"Plot","project_url":"https://devpost.com/software/plot-3pi8c4","tagline":"Plot delivers comprehensive insights on any location. Whether you're scouting investments or shaping skylines, Plot helps you shape your plot’s plot—making smarter decisions before you break ground.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/273/971/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"LangChain: Best Use of LangChain (LangChain Swag Box, $250 LangSmith Credits, and access to LangGraph Cloud per team member)"}],"team_members":[],"built_with":[{"name":"langchain","url":null},{"name":"mapbox","url":"https://devpost.com/software/built-with/mapbox"},{"name":"mistralai","url":null},{"name":"next.js","url":null},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"recharts","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"zustand","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/toni-akintola/jats"}],"description_sections":[{"heading":"Inspiration","content":"This project was born out of our own personal pain points as students. Like many others, we’ve faced the challenges of searching for housing at school, for summer internships, full-time roles, and traveling. When planning my trip to Rome, I had no idea which area was safe to stay in for an Airbnb. I ended up spending hours on Reddit looking for reviews and suggestions, which felt inefficient and time-consuming. This made us realize that finding the right places to stay could be far more seamless with the right insights from the right sources.\n\nWe saw a gap in the market for a platform that could quickly and accurately provide sentiment data on locations, helping people make informed decisions about housing, whether for short-term stays or long-term investments. What if we could make this process faster and more efficient?"},{"heading":"Building the Project","content":"Our first step was building a sentiment analysis engine, since we knew there was an abundance of social media data available to us. We could easily aggregate what people were saying about different places—whether it’s about the safety of neighborhoods, or the overall vibe of a city. Using AI agents, we orchestrated a workflow to perform searches and analyze sentiment across multiple data sources.\n\nOur initial goal was to build a simple map for sentiment analysis, but as we refined our approach, we realized that there’s a much broader context to consider when evaluating a location. Beyond social media data, factors like economic trends, natural disasters, and local events all impact people's perceptions of a place. This insight led us to expand our project to build a comprehensive property intelligence platform.\n\nWe also worked closely with mentors and property developers who helped us understand the nuances of the industry. It became clear that our platform could be extremely valuable to property developers, who need to analyze many of the same factors as renters, but with a much longer-term view. This shift in perspective led us to add features specifically catered to property developers, such as real-time property data streams and a user profile page to inform the agentic search.\n\nCore Technology\n\nThe core of our project lies in a swarm of AI agents that execute parallel tasks to gather, analyze, and update data in real time. The following are some notable components of this project.\n\nCore Architecture : The project is built as a Next.js application with TypeScript, focusing on real estate market analysis using sentiment analysis. The main components are organized in a modular structure with clear separation of concerns between UI, services, and data processing. Data Collection : The agentic workflow kicks off by scouring various data sources, including social media, news outlets, economic reports, and environmental data. The agents target specific factors that influence a location's desirability, such as crime rates, job market trends, housing prices, and local events. Sentiment Analysis : The SentimentAnalyzer class serves as the core engine for processing sentiment data. It evaluates the tone and context of online discussions—whether people are generally positive or negative about a particular area. Multiple data sources are supported through a pluggable architecture, and the analyzer processes text data in parallel. Results include sentiment scores, keyword extraction, and temporal analysis. Sentiment Dashboard Interface and Real-Time Updates : The SentimentDashboard acts as the main visualization interface. Its main features include multi-location analysis support, real-time sentiment scoring, interactive charts showing sentiment trends, keyword highlighting, and mention tracking. Risk Assessment : Parallel to sentiment analysis, other agents use the Federal Emergency Management Agency API to assess the risk factors of a location, such as history of natural disasters (e.g., floods, earthquakes), economic volatility, or crime rates. These agents pull data from global news, scientific studies, and financial reports to generate a comprehensive risk profile. Data Synthesis and Updates : As new information comes in, agents continuously update the platform (map, search results) in real time. If a major news event occurs or if sentiment shifts dramatically, the agents immediately re-analyze the data and adjust the results. This enables our users to have access to the most up-to-date information, helping them make decisions on the fly.\n\nThe orchestration of these technologies allows us to gather and process a wide variety of data points quickly and efficiently, ensuring that the platform is always fresh and relevant."},{"heading":"Challenges Faced","content":"One of the major challenges we faced was aligning the project’s goals with our different interests and backgrounds. We were all beginners at some aspects of the technologies we wanted to use, so balancing the desire to learn new things with the need to create something useful was tricky. However, the sentiment analysis aspect of the project—along with the real-world applications for both travelers and property developers—kept us focused and motivated.\n\nAnother challenge was figuring out how to make the platform both effective and scalable. We needed to ensure the sentiment analysis could be done in real-time and that it could incorporate data from diverse sources. This is where our sponsors played a huge role in helping us streamline the process. Thanks to Langchain, Mistral AI, and Perplexity, we were able to perform high-efficiency searches and provide insights quickly."},{"heading":"Development Tools & Technologies","content":"Throughout this journey, we leaned heavily on tools from our sponsors to improve efficiency and enhance our workflow:\n\nPerplexity was instrumental in answering queries and performing real-time searches, which helped us gather valuable insights quickly. The search integration within Perplexity Sonar made it ideal for our needs, and we frequently queried it during the project. Windsurf by Codeium became our go-to tool for boosting productivity. It helped us 10x our efficiency, thanks to its memory and model context protocol features, which were vital for informing Cascade to make useful code edits and suggestions. Langchain served as our foundation for building complex AI workflows. Its composable chains and agent frameworks helped us orchestrate our swarm of AI agents effectively, enabling seamless integration of different data sources and analysis tools into our pipeline. Mistral AI powered our core sentiment analysis engine with its robust language understanding capabilities. We leveraged its models for processing and analyzing text data from various sources, providing accurate sentiment scoring and context-aware analysis that formed the backbone of our real-time insights."},{"heading":"What We Learned","content":"This project taught us a lot about the importance of balancing innovation with utility. While we were initially excited about the \"cool\" factor of sentiment analysis, it was only by catering to the needs of property developers that we were able to make it truly useful. Additionally, we learned how to work effectively as a team despite coming from different schools and backgrounds.\n\nWe also gained a deeper understanding of how AI can be applied to solve real-world problems at scale—particularly in fields like property development, where data-driven decisions can make or break a project."},{"heading":"Conclusion","content":"We’re incredibly proud of what we’ve built. We’ve created something unique that taps into the power of real-time, sentiment-driven property intelligence. By incorporating community sentiment, economic indicators, and real-time updates, our platform offers a truly comprehensive view of any location. There’s no other product on the market quite like ours, and we’re excited to see where it goes from here.\n\nWe hope you enjoy exploring our project as much as we enjoyed building it!"},{"heading":"Built With","content":"langchain mapbox mistralai next.js openai perplexity react recharts typescript zustand"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"The Anachronist","project_url":"https://devpost.com/software/the-anachronist","tagline":"History isn’t just studied, it’s lived. Enter a 3D time-travel adventure.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/624/datas/medium.jpeg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"LumaLabs: Reimagining Visual Creation with AI ($2,500 Cash + $5,000 Luma Credits)"}],"team_members":[],"built_with":[{"name":"elevenlabs","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"godot","url":null},{"name":"langchain","url":null},{"name":"lumalabs","url":null},{"name":"nitrode","url":null},{"name":"openai","url":null}],"external_links":[{"label":"www.nitrode.com","url":"https://www.nitrode.com/games/the-anachronist"},{"label":"github.com","url":"https://github.com/rgu0114/the-anachronist"}],"description_sections":[{"heading":"Inspiration","content":"When students hear the subject “history,” they are likely reminded of a textbook marred with numbers, dates, and timelines; when in fact history consists of real people and real events, which serve as a powerful tie that conjoins even the most dissimilar individuals through strong ties. Driven by a vision to bridge this rift, we devised a game that promises the development of a deeper, more immersive understanding of history by placing players directly into pivotal moments of the past. By warping through time and stepping into the shoes of people who lived through transformative events, players can experience history not as a collection of facts, but as a living, breathing narrative shaped by human resilience, emotion, and decision-making. Our game takes place at the heart of the Manhattan Project—Los Alamos, 1943—and allows players to bond with eminent historical figures and pedestrians, vicariously reliving the diverse ethical dilemmas, scientific breakthroughs, and political tensions that permeated the age. Through these interactions, players develop empathy and a nuanced appreciation for the complexities of historical events, turning history from a mere paragraph between pages to a dynamic and sentient being, that breathes through us today."},{"heading":"What it does","content":"The Anachronist is an educational mystery game that “time warps” players into a historical period that is unknown to them prior to completing the game. Their task? Identify the time and place by carefully observing surroundings, interacting with characters, and analyzing historical details, with a slight twist: Don’t get caught! While learning hints and gaining further context through every conversation, the player must also be conscious of the fact that they should not arouse suspicion, that they are potentially from the future.\n\nFigures within our game range from children, who were brought to Los Alamos as their parents were involved in the Manhattan Project, to figures who symbolize the Manhattan Project to us today. Navigating this world is not just knowing the “timeline” or “facts” of the events, which we grade through a quiz; but instead understanding the very fabric of the community that they are teleported to."},{"heading":"How we built it","content":"We built The Anachronist using the Nitrode game engine, custom-made Blender models, and advanced triggers, interactions, and animation loops. Our 20+ NPC characters were designed through reconstruction of historical figures (and others) based on VLMs, and tailored to our purpose through mesh texture rigging.\n\nLLMs, such as Open AI and Gemini’s models, allow players to engage in conversations not only with historical figures but also with everyday people from the past, bringing history to life. We implemented LLM-powered dialogue systems, leveraging retrieval augmented generation. As such, we were able to prompt 20+ NPC based on the historical context and agent-specific profiles, which allowed for conversations that were both accurate and consistently “in-character” that performed robustly across diverse conversational inputs of the player, which could range from questions and comments about everyday life, technical questions tailored to the historical protegé in their presence, to conversations which engage with deep ethical questions of the age. Furthermore, we leveraged LangChain to manage our agent workflow, which involved both synchronous generation of “evaluation” of the player’s ability to blend in with their environment, as part of our game objective and generating fruitful interactions, fulfilling our education purpose.\n\nAdditionally, we integrated the LumaLabs AI API to generate realistic videos, which both open and close our game with an immersive storyline and stunning visuals, which immerse the player in the gaming environment. We also used the ElevenLabs API for high-quality voice synthesis, which enhanced the storytelling experience with authentic-sounding dialogue while providing oral guidance to the player regarding the game-playing rules."},{"heading":"Challenges we ran into","content":"One of our biggest challenges was balancing historical accuracy with engaging gameplay—ensuring realism without sacrificing playability. On the technical side, building an entire 3D world from scratch was a major hurdle, especially since it was our first time developing a 3D game. Creating lifelike characters with unique traits was complex, but we leveraged VLM and an AI-powered pipeline to generate, rig, and animate 3D models from text-to-image inputs. This pushed us to refine our design and animation processes, making the world feel more immersive and authentic."},{"heading":"What we learned","content":"Achieving the immersive gameplay of The Anachronist taught us extreme attention to 3D modeling, developing complex programmatic interactions, and learning all the technologies of Godot game development from scratch. The challenge taught us how to learn quickly, experiment iteratively, and come up with novel solutions to problems like building 3D interactive AI NPCs with unique personalities, movement patterns, and levels of comfort with sharing information. Moreover, we gained a deeper appreciation for historical nuance. We discovered how small details, such as speech patterns, architecture, and cultural practices, can reveal a great deal about a historical era. Most importantly, we realized the value of player agency in learning. When users are empowered to make their own independent discoveries in the world and inquire openly about tricky issues, they develop a stronger connection to the material. This is the power of immersive worlds in technology – it heightens their ability to think critically about not just the past, but also the present and the future."},{"heading":"What's next for The Anachronist","content":"Moving forward, we aim to expand our historical settings to include underrepresented periods and cultures, ensuring a more diverse and comprehensive learning experience. We are also exploring the possibility of adding multiplayer or collaborative modes, where players can work together to deduce their location and time period. Another exciting development is the introduction of VR / AR support, which would create an even more immersive time-travel experience. All in all, The Anachronist has the potential to get into classrooms, museums, and cultural institutions as an interactive history-learning tool. By blending history with interactive problem-solving, we hope our game redefines how we engage with the past—making learning not just informative, but truly thrilling."},{"heading":"Built With","content":"elevenlabs gemini godot langchain lumalabs nitrode openai"},{"heading":"Try it out","content":"www.nitrode.com github.com"}]},{"project_title":"Sherlock","project_url":"https://devpost.com/software/sherlock-n4fikj","tagline":"Connecting the dots for investigative minds.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/164/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Neo: Most Likely to Become a Business (Airfare + Accomodations at Neo Summer Retreat)"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"vite","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Honyant/TreeHacks2025"}],"description_sections":[{"heading":"Built With","content":"python react vite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Mirror","project_url":"https://devpost.com/software/mirror-lmd54a","tagline":"Reflecting on your past work, relationships, and learning has never been easier. Mirror turns your minute-to-minute into a comprehensive and indexable knowledge base in the form of a journal.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/172/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Mistral: Best Use of Mistral AI API (AirPods x4 + $1,000 Cash + $1,000 API Credits)"}],"team_members":[],"built_with":[{"name":"augmentos","url":null},{"name":"elasticsearch","url":"https://devpost.com/software/built-with/elasticsearch"},{"name":"java","url":"https://devpost.com/software/built-with/java"},{"name":"mistral","url":null},{"name":"nextjs","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"windsurf","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/kevinzhu12/treehacks-glasses"},{"label":"github.com","url":"https://github.com/sarahjsu/tree_augmentos"}],"description_sections":[{"heading":"Inspiration","content":"Remembering things is hard – and staying on top of your self improvement and reflection while juggling a busy schedule is even harder. As a team of busy students, one of whom recently started working a full time job, we decided to build Mirror as a tool to leverage unique smart glasses technology to revolutionize the way people handle self-improvement and reflection."},{"heading":"What it does","content":"Mirror is an AI-powered app compatible on all AugmentOS -enabled smart glasses which transcribes your conversations and generates actionable insights and summaries tuned for reflection in real time. Our processing agents will automatically update on the Mirror web interface as you speak, intelligently picking out the most appropriate points to give a well-rounded overview of your day, extracting todos, a general snapshot of your day, and important reflection questions.\n\nJust as important as understanding the specific details of your day is being able to look back in time at past memories and see how they fit into the context of the rest of your memories. Mirror offers full-text semantic search through all memory content to allow for quick and easy retrieval and recall of specific details even if you only have a small idea of what you’re looking for.\n\nFurthermore, visualizing the relationships between memories is made easy and beautiful via Mirror’s 3-Dimensional memory map which displays latent connections between memories. Each edge and connected component tells a story and provides insights into the relationships between your experiences.\n\nThe best part – all of this is generated via our processing pipeline in real time as you talk! Throughout the entire day, Mirror will always be up to date with your latest memory insights."},{"heading":"How we built it","content":"Mirror is built in 3 parts:\n\nThe AugmentOS Smart Glasses Client is an Android application written in Java which handles the interface with the smart glasses – handling both the voice transcription and relaying text back to the user via the display. Shoutout to the Mentra team for providing mentorship and lending us the Even Realities G1 Glasses that we built Mirror to work with.\n\nThe backend is written in TypeScript via NextJS and handles interactions with both the smart glasses client and the web interface. Using ElasticSearch, Mistral AI, Vercel’s NextJS, and Codeium’s Windsurf, we created an intuitive interface to interact with your thoughts throughout the day."},{"heading":"What we learned","content":"We learned that working with experimental hardware is hard – and persistence is key. We are the R&D for the software for these glasses, testing the limits of what’s possible with them and trying to push the field forward."},{"heading":"What's next for Mirror","content":"We would love to continue expanding Mirror’s interaction with our users, especially in terms of active reflection throughout the day. We envision a world where our agents can prompt you during conversations about related past conversations, remind you of your past reflections, and help guide you towards a more productive, social future."},{"heading":"Built With","content":"augmentos elasticsearch java mistral nextjs react typescript windsurf"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"TreeTrash","project_url":"https://devpost.com/software/treetrash","tagline":"Quantify, analyze, and improve sustainability with intelligent trash monitoring!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/268/751/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"OpenAI: Most Creative Use of OpenAI API (OpenAI SF HQ lunch + Swag)"}],"team_members":[],"built_with":[{"name":"adafruit","url":"https://devpost.com/software/built-with/adafruit"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"vercel","url":null},{"name":"vespa","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/sonnetx/treehacks-2025"}],"description_sections":[{"heading":"Inspiration","content":"During TreeHacks, we noticed a significant number of items being placed in the wrong waste bins, which reminded us of a broader problem with tracking and understanding waste patterns, so that they could be effectively mitigated in the future. This sparked our idea to expand upon challenge #3 on the sustainability track and leverage AI to not only identify and address this issue but also take it a step further. We wanted to go beyond simply classifying waste and analyzing the weight — we aimed to develop a program that could analyze images of trash, recycling and compost and generate a report that could provide organizations like Stanford with valuable insights into food waste patterns, enabling them to implement more effective sustainability initiatives and reduce unnecessary waste."},{"heading":"What It Does","content":"TreeTrash leverages computer vision to analyze images of a trash bin taken at regular intervals. We determine which items were misplaced and what the environmental impact of misplacing those items using a custom search pipeline. When we first notice a misplaced item, we use a display and speaker to notify the person. Then, we generate a custom report about the environmental impact of misplacing certain items, as well as specific recommendations about how to improve."},{"heading":"How We Built It","content":"We use computer vision to list out the items in the image, compare those items to the items in the previous image, use OpenAI's GPT-4o to determine the proper placement of those items, use Perplexity search to identify the environmental impact in terms of kg of CO2e emissions, use gpt-4o-mini to parse this output into a structured output, use Vespa AI RAG to identify how this relates to the sustainability goals of a specific organization, and use Gemini to generate a report."},{"heading":"Challenges We Ran Into","content":"Implementing RAG with Vespa was a very buggy process and it took a lot of learning about the platform. Also, Perplexity was not able to consistently output a structured output, so we had to add in a layer in the pipeline to transform it into a structured output."},{"heading":"Accomplishments That We're Proud Of","content":"We're proud of how we improved the accuracy of predictions using grounded information retrieved by perplexity. We are also proud of how we implemented RAG to augment the report by making it specific to an organization's sustainability goals. Being able to leverage our teammates' EE skills to develop a custom monitor to display everything was also important!"},{"heading":"What We Learned","content":"We learned how to construct pipelines that incorporate multiple AI models and implement RAG with new tools (namely Vespa.ai), which we had never had exposure to before."},{"heading":"What's Next For TreeTrash","content":"We plan to take TreeTrash beyond the hackathon by testing it with real trash bins at Stanford. Our next steps focus on key technical improvements to enhance the system’s accuracy and functionality:\n\nEnhancing Retrieval-Augmented Generation (RAG): We have already implemented a RAG system to provide real-time, context-aware explanations about waste sorting. Next, we aim to improve its retrieval accuracy and response relevance by exploring different embedding models and expanding our dataset. Advancing Computer Vision for Waste Classification: We will refine our AI model using a more diverse dataset of waste images, improving its ability to classify items and estimate food waste weight with greater precision. Integrating a Robotic Arm for Automated Sorting: As a long-term goal, we envision incorporating a robotic arm that can physically sort waste based on AI predictions, reducing human error and streamlining waste management."},{"heading":"Built With","content":"adafruit gemini openai perplexity vercel vespa"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Fable","project_url":"https://devpost.com/software/fable-br8e1d","tagline":"Fable tells stories that adapt to your state of mind. As your brainwaves shift, we generate a story and create an immersive visual journey that mirrors your inner state in real time.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/273/081/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"OpenAI: Most Creative Use of OpenAI API (OpenAI SF HQ lunch + Swag)"}],"team_members":[],"built_with":[{"name":"3.js","url":null},{"name":"eeg","url":null},{"name":"elevenlabs","url":null},{"name":"glsl","url":"https://devpost.com/software/built-with/glsl"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"javascript/typescript-(next.js)","url":null},{"name":"lumalabs","url":null},{"name":"mind-monitor","url":null},{"name":"muse","url":"https://devpost.com/software/built-with/muse"},{"name":"ocs","url":null},{"name":"openai-api","url":null},{"name":"perplexity","url":null},{"name":"python-(fastapi)","url":null},{"name":"tailwind-css","url":null},{"name":"websocket","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/StiopaPopa/fable"}],"description_sections":[{"heading":"Inspiration💡","content":"Fable began with a simple question: What if a story could adapt to your state of mind? As our team explored EEG technology, we became excited about the idea of measuring brain waves and uncovering hidden patterns of thought. What if a narrative could respond to your level of focus and relaxation—your shifting mental state? With EEG headsets reading brain activity, Fable dynamically shapes its plot, characters, sound effects, and visuals, crafting a living, breathing story that unfolds uniquely for you. This isn’t just storytelling—it’s an experience powered by your mind."},{"heading":"What it does 🔮","content":"Fable reads your brainwaves to craft a living story that responds to your every thought. As your mental state shifts, the plot dynamically evolves—changing tone, pacing, or twists to reflect how you feel. Simultaneously, the visuals adapt to match the unfolding narrative, immersing you in a real-time, brain-driven adventure where each scene is shaped by your mind. Even the moving gradient color background gradually fades and shifts, mirroring your inner state and heightening the immersive feel of your journey."},{"heading":"How we built it 🧱","content":"We built Fable by combining a Muse 2 EEG headset with a dynamic storytelling pipeline, powered by a Python FastAPI backend and a Next.js frontend styled with Tailwind CSS. The headset reads beta, alpha, theta, and gamma wave patterns—interpreted as relaxed, neutral, or focused states—using Mind Monitor and streams this data in real time through OCS. Our main story loop runs every 30 seconds, checking the user’s current state and using the OpenAI API to generate story scripts (and corresponding sound effect prompts) based on those EEG readings. We map each state to a specific narrative direction—for neutral, we steer the story toward fresh discoveries; for focused, we introduce more details or challenges; and for relaxed, we invite ease and wonder. To enhance immersion, we integrated 3.js and GLSL shaders to create an EEG-based dynamic gradient background that shifts in real time with the user’s mind state. We then feed both the text-to-speech lines and OpenAI-generated SFX prompts into ElevenLabs (streamed via WebSocket), allowing us to seamlessly produce both TTS dialogue and ambient sound effects. Finally, we bring the story to life visually with the Luma Labs API. Throughout this process, we employ a multithreading approach to process text, voice, and video asynchronously, ensuring smooth, parallel generation of each element."},{"heading":"Challenges we ran into 🚧","content":"We faced several hurdles bringing Fable to life. Hooking up OpenAI’s story generation so that each paragraph was generated sentence by sentence required careful orchestration within our webapp. Integrating ElevenLabs’ text-to-speech to ensure that audio and subtitles streamed seamlessly in real time was another challenge. Working around lengthy inference times on video generation models forced us to asynchronously and concurrently queue and segment videos to keep up with the script and audio. Our hacking led us to benchmark at least 10 different video generation APIs against each other for speed, including text-to-image and image-to-video generation. We also developed our own text-to-image and image-to-text pipeline to improve efficiency and control over the entire process."},{"heading":"Accomplishments that we're proud of 🏆","content":"We’re proud of creating a seamless, real-time storytelling platform that translates brainwave data into dynamically shifting narratives, audio, and visuals. By integrating multiple APIs—Muse for EEG, OpenAI for generative text, ElevenLabs for audio, and Luma Labs for visuals—we managed to build an immersive, multi-sensory experience that feels both personalized and technically robust. And most of all, we love the experience of listening to the stories that come out of our product!"},{"heading":"What we learned 📚","content":"Through developing Fable, we gained a deeper understanding of real-time data processing, from parsing EEG signals to synchronizing audio and video outputs. Integrating diverse tools like OpenAI, ElevenLabs, and Luma Labs taught us the value of modular design and clear communication between APIs. We also discovered how critical it is to balance technical complexity with user experience, ensuring that the shifting storyline remains both immersive and coherent."},{"heading":"What's next for Fable 🚀","content":"We’re excited to broaden Fable’s capabilities by refining our EEG interpretation for an even wider range of emotions and deeper engagement tracking, exploring additional wearable sensors beyond the Muse headset, and advancing our storytelling techniques—potentially introducing multiple branching storylines, co-op experiences, and VR integration."},{"heading":"Built With","content":"3.js eeg elevenlabs glsl javascript javascript/typescript-(next.js) lumalabs mind-monitor muse ocs openai-api perplexity python-(fastapi) tailwind-css websocket"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Lattice: Orchestrating end-to-end web workflows at scale","project_url":"https://devpost.com/software/lattice-data-at-your-fingertips","tagline":"Massively parallelized agentic research workflows using spreadsheets as a primitive","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/270/406/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Pear VC: Best Customer Insights ($25k Unncapped SAFE + Office Hours w/ Partners [1st] & Office Hours w/ Partners [2nd])"},{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Paradigm: Best Spreadsheet-Adjacent Hack (Air Pods Max per team member)"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"apis","url":null},{"name":"llms","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/WeltonWang02/spreadsheet_hackathon"}],"description_sections":[{"heading":"Summary","content":"Over the past few months, there've been a lot of really cool agentic research products that've been released (Exa, OpenAI/Perplexity Deep Research, etc). In general, these are point solutions and relatively sequential in manner (ie, the next action is conditioned on the previous actions).\n\nBut AI much more exciting if we can:\n\nBuild products around end-to-end workflows Do this at scale that humans never could (ie, browse hundreds of websites at once)\n\nWe want to build something that takes advantage of the parallelized nature of research agents to solve end-to-end workflows en masse (we can run hundreds of agents at once!)."},{"heading":"What is this?","content":"Lattice allows you to orchestrate complex end-to-end research workflows at scale, with complex/structured dependencies.\n\nConsider a sales representative sending cold outbound:\n\nThis involves:\n\nFinding potential companies to sell to and qualifying them Finding potential points of contact at these companies Finding the right form of outreach for the best point of contact Draft + sending the email\n\nToday, Lattice solves the vast majority of this workflow – and we do this at scale, across hundreds of companies in parallel.\n\nWe first find potential companies, and then for each company, find a set of defined attributes.\n\nhttps://imgur.com/a/bsq1S3i\n\nThen, for each company, we can find the key people we want to reach and augment this information.\n\nhttps://imgur.com/a/7b0FhLY\n\nThen, for each companies' list of people, we can qualify them based on attributes we defined + have researched.\n\nhttps://imgur.com/a/dl6gfzL\n\nFinally, we can pipe all the research information to a language model to produce a final work product.\n\nhttps://imgur.com/a/fAdaElA\n\nWe do this at scale and save a considerable amount of time over manual research – but also existing tools that only do this for a single part of the workflow (ie, Clay)! As a page one end-to-end workflow.\n\nWe let you build any workflow like this and chain spreadsheets together."},{"heading":"More Use Cases","content":"Anything that involves scaled web research! But concretely:\n\nInsurance companies\n\nStart with a provided list of companies and find company-level information. Then for each company, find all their products, and for each product, create a grid of if they flag certain policies. Then aggregate all the violations across the products, run some risk/modeling calculation, and compile into a per-company report.\n\nFinance\n\nFind a list of recently funded startups (Series A), then for each startup catalogue all of their rounds, how much they raised, and who they raised from at each round. Across the rounds of each company, count up the number of T1 VCs. Draft emails to send outbound to companies with more than 3 T1 VCs on their cap table.\n\nFun Example\n\nFind a list of film festivals in the past year. Then for each festival, find which films won the top prizes, the names of the movies, and the authors. Then, choose the film across with the funniest title from each of the festivals. For each of those films, then find other films created by those artists. For each of those films, then collect the Rotten Tomato and IMDB scores. Then, choose the film for each artist with the highest aggregate scores and write a congratulations emails!\n\nWe know that all three (maybe two) and millions of other similar use cases are very important and significant workflows conducted by humans today."},{"heading":"Tech","content":"We treat spreadsheets (or really tables) as a core primitive. This is how data is displayed, edited, and passed between different steps in a workflow.\n\nMost of the heavy lifting is on the frontend (Next/Typescript/Tailwind) – we build a primitive spreadsheet interface from scratch, added multiple dimension to it, then created a chain of dependencies between the different spreadsheets, and then added a bunch of agent orchestration on top of it!\n\nThe cool thing is this all actually works, and works quite well!"},{"heading":"Why Spreadsheets","content":"Frankly, we thought Paradigm's challenge was really cool, and were inspired in part by what they do. We think spreadsheets are a fun primitive to build around:\n\nIt's a surprisingly challenging task. Spreadsheets are one of the best ways to visualize bulk agentic outputs and enable a lot of cool things.\n\nWe started with what we thought were an interesting limitation of 2D spreadsheets – we can't do a nested series of \"Find a list of XYZ, then for each find a list of ABC\"-style inputs. There's a lot of powerful workflows though, that depend on being able to serve multiple layers of these kinds of queries – so we designed a \"3D spreadsheet\" where each sheet is mapped from a row in a 2D spreadsheet.\n\nOur spreadsheet workflows, in some ways, are essentially a directed graph with dependencies. This is roughly how we treated the orchestration / execution of individual agents within our system.\n\nAt any point, we can have a one-to-many relationship between incoming and outgoing edges, or many-to-one relationship. This allows us to manage state/context passing between different steps within a workflow. For example, when going from a single row into a spreadsheet -> a sheet in a 3D spreadsheet -> a single row in a spreadsheet again, we pass parts of the initial row through the entire chain so our agents have the right context.\n\nAnother cool thing that comes out of this is that we provided verifiable + auditable traces into how exactly the end to end workflow was executed. As any step, you can look up the full table of data that the end considers when making decisions, all the raw attributes collected, and how they flowed in the workflow."},{"heading":"What Next?","content":"A lot! Here's a few:\n\nMaking systems and workflows more robust (prototypes are easy; building production grade agentic systems are extremely hard + painful) Make orchestration simpler (we wanted to do this but ran out of time) – describe what you want to complete, and we'll handle the orchestration itself Improve latency - our end to end workflows are slow. We think this is okay, because most of our use cases aren't latency bound. But there is definitely a lot of value in optimizing latency further"},{"heading":"Built With","content":"agents apis llms typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Zue Research","project_url":"https://devpost.com/software/zue-research","tagline":"interactive multimodal AI experiences for neurodivergent learners run entirely on the edge","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/271/965/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Orbstack: Best Use of Rust ($1k Cash)"}],"team_members":[],"built_with":[{"name":"elevenlabs","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"langchain","url":null},{"name":"lumalabs","url":null},{"name":"mistral","url":null},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"rust","url":"https://devpost.com/software/built-with/rust"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/zcsabbagh/zu-lm/"}],"description_sections":[{"heading":"Inspiration","content":"56% of students in the world do not have internet access at school [1]. Students learn best through multi-sensory, hands-on, structured experiences that are tailored to their interests. Yet, the primary method of learning for them continues to be textbooks.\n\nThis is even more pertinent to the 129 million students globally who have ADHD [2]. We built in our product with neurodivergent and offline learners in mind. Drawing on bleeding edge research, we proved that multimodal applications can be deployed entirely at the edge using distributed inference using heterogeneous compute [3] [4]."},{"heading":"What It Does","content":"After students describe what they’d like to learn more about, we create multiple adversarial agents which perform “deep research” (i.e. search the web, synthesize, reflect, and repeat). Think anything from “debate the entirety of modern Tunisian history” to “tell me how the organelles of a plant cell differ from those of an animal”.\n\nWhen these agents finish, they summarize their findings and provide related “branches” the students can look into. If the student is done researching, we generate a podcast with visual aids that shows multiple agents debating each other. At any point in the podcast, a student can double click on a segment to have a new research agent answer questions or clarify topics."},{"heading":"How We Built It","content":"We needed to create a dynamic and engaging frontend that would be easy-to-use for teachers and students alike. We chose Typescript and spun up an application that could provide live insights into the actions that the agents were taking, and playback the podcast in a lively manner.\n\nFor our backend, we faced the challenge of orchestrating researchers engaging in Multi-Agent Debates . Despite being first-time users, we chose Rust for its superior concurrent performance and memory safety features, which were crucial for managing shared state across multiple research agents and handling asynchronous web operations safely.\n\nTo support our cloud implementations, we chose ElevenLabs to generate the voices of our podcast and to create an agent to converse with the student after the podcast to test their understanding using the Feynman technique . We used LumaLabs for the podcast image generation. We also made creative use of the Perplexity Sonar web search API and Mistral via groq ."},{"heading":"Accomplishments We're Proud Of","content":"We were able to get our backend to run locally by sharding full size large language models across multiple hardware devices (i.e. we ran Llama on between a MacBook Pro and 2x MacMinis).\n\nDespite never having written code in Rust, we wrote our entire research agent server in a Rust implementation of LangChain.\n\nSome cool things we did with Rust:\n\nTokio for async runtime and concurrent processing for multiple agents let (track_one_result, track_two_result) = tokio::join!( self.process_track(state.clone(), \"one\"), self.process_track(state.clone(), \"two\") ); Arc> for thread-safe shared state let state = Arc::new(Mutex::new( SummaryState::with_research_topic(input.research_topic.clone()) ));"},{"heading":"What We Learned","content":"Building for education is both technologically challenging and highly rewarding. The members of our team were able to learn Rust from the ground up, taking advantage of its supreme efficiency, and learn how to build for the modern Ed-Tech consumer."},{"heading":"Sources","content":"[1] https://ourworldindata.org/grapher/primary-schools-with-access-to-internet?tab=table\n\n[2] https://chadd.org/about-adhd/general-prevalence/\n\n[3] https://arxiv.org/html/2405.14371v1\n\n[4] https://github.com/exo-explore/exo"},{"heading":"Built With","content":"elevenlabs groq javascript langchain lumalabs mistral openai perplexity rust typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Spielberg AI","project_url":"https://devpost.com/software/spielburg-ai","tagline":"AI-powered video editing with CUDA optimization—our advanced AI agent executes complex edits with natural language, slashing 10-minute processing tasks to seconds using the GPU!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/273/992/datas/medium.JPG","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Rox: Best Agents Hack ($7k Cash)"}],"team_members":[],"built_with":[{"name":"cuda","url":"https://devpost.com/software/built-with/cuda"},{"name":"elevenlabs","url":null},{"name":"fastapi","url":null},{"name":"ffmpeg","url":"https://devpost.com/software/built-with/ffmpeg"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"googlegenai","url":null},{"name":"huggingfacetransformers","url":null},{"name":"langchain","url":null},{"name":"lumalabs","url":null},{"name":"next.js","url":null},{"name":"openai-gpt","url":null},{"name":"openai-whispr","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"tailwind","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/adeng27/treehacks-25"}],"description_sections":[{"heading":"Inspiration & Social Impact","content":"Our inspiration came from a close friend—a YouTuber who constantly struggled with the repetitive and time-consuming nature of short-form content editing. Despite having great content ideas, they found the process of cutting clips, applying effects, and formatting videos for multiple platforms incredibly tedious.\n\nWe wanted to eliminate the frustration and automate as much of the workflow as possible. Our goal was to build an AI-powered video editor that understands natural language commands and applies professional-grade edits in seconds, letting creators focus on their content, not the editing grind. We achieved this goal through Spielberg AI, a novel video editor that works solely with natural language (and one small button that does all the work for you—if you want).\n\nCurrently, there are so many stories that people want to share and virality will spread them across the entire world. This is all limited by editing which is extremely difficult and unintuitive. People often do not know the formula for virality and Spielberg AI will eliminate the barrier of people telling their stories by making editing trivial."},{"heading":"What it does","content":"Spielberg AI is an AI-powered video editor that allows creators to edit videos using only natural language. Instead of manually cutting, trimming, and applying effects, users can describe their desired edits in plain English, and Spielberg AI—powered by the agent, our AI agent—handles the rest. There is a strong emphasis on developing and editing viral videos for the users."},{"heading":"How we built it","content":"FFMPEG - Video Encoding\n\nThe core of our project is built around FFMPEG, the industry-standard video encoding and decoding backend. We started with a simple real-time preview system using FFMPEG to encode frames and allow for precise, frame-focused cuts. From there, we integrated AI-driven enhancements to streamline and automate complex video editing tasks.\n\nTo push performance to the extreme, we custom-compiled FFMPEG with NVENC encoding and CUDA hardware acceleration. This optimization massively reduced processing times, enabling near-instantaneous rendering of complex edits. What once took minutes now executes in mere seconds (on our gaming computer).\n\nAI Agent\n\nSpielberg is driven by the agent, a highly advanced AI agent designed to interpret natural language commands and execute sophisticated editing tasks seamlessly. The agent's intelligence is built upon a multi-layered NLP and processing pipeline, leveraging:\n\nGoogle Gemini for high-level natural language parsing and all multimodal support. All image and video understanding is completed through Gemini to enable the agent to understand what is occurring in the video that is provided by the user. We tested the video and frame processing capabilities of many models and Gemini's multimodal capabilities far surpassed all other models. The agent would fail to understand what is happening in the video, and thereby be unable to process and edit the videos properly without Gemini. We would say using Gemini to make a video editor possible is a pretty cool use case! OpenAI models & Whisper for speech-to-text, intelligent audio processing, and chat integrations. The agent utilizes OpenAI in a fairly creative way leveraging structured output and repeated sampling to understand what the user wants accomplished and how we can go about accomplishing it. The Whispr API is also used for video understanding. We found that audio is a great way to understand the video and this is all accomplished through Whispr, as other APIs were incredibly faulty. Through the agent's understanding developed with Whispr combined with GPT calls, we could identify where to put sound effects and trim the video based on dead audio. Multimodal Retrieval-Augmented Generation (RAG) Pipeline for dynamically sourcing editing patterns and context-aware adjustments. We consulted various YouTubers with millions of subscribers and views on how to make viral short-form content. They pointed us to resources and gave us some of their own personal guides. We also used OpenAI DeepResearch & Perplexity to create informational documents on how to make a viral video. The agent then uses these documents in a Multimodal RAG pipeline to understand how to make a viral video so with the press of a button, the agent knows what's best and can execute it by interacting with our software infrastructure. Verification Layers for the agent to ensure correct outputs at each step of the process and limit hallucinations. Given our research experience in inference time compute at Hazy Research in Stanford AI Laboratory (SAIL), we have a strong understanding of SOTA Verification. We ran quick small-scale experiments regarding sampling, statistical methods, Unit Test Generation/Evaluation, LM Judges, and Reward Models, and used the best judge for each layer. More often than not, it was LM Judges with some small combination of statistical methods.\n\nThe backend is structured into multiple Python layers, each optimized for efficiency, with distinct modules handling inference, video processing, and audio synthesis. The agent seamlessly works across these layers.\n\nAdvanced Hardware Acceleration with CUDA\n\nOur project takes full advantage of NVIDIA GPUs and CUDA acceleration to handle the most computationally intensive workloads.\n\nEncoding & Decoding: By using a custom-compiled FFMPEG with CUDA-powered NVENC, we parallelized the encoding of 4K video, dramatically improving performance for long-form content like podcasts. Real-time Color Grading: The most computationally demanding process was applying AI-driven color filters to videos. Instead of relying on FFMPEG’s built-in filters (which took 40+ seconds for a 10-minute 720p video), we wrote a fully custom CUDA kernel from scratch.\n\nUsing OpenAI APIs, we generated LUT-based color filters from natural language prompts.\n\nOur CUDA kernel applied these filters nearly instantly, outperforming traditional GPU-accelerated FFMPEG operations.\n\nThread-Level Parallelism: CUDA threads were optimized not only for video processing but also for speeding up AI operations, such as frame analysis via Gemini, transcription, and audio synthesis tasks.\n\nAPI Integrations\n\nBeyond video processing, we integrated several key APIs to enhance the intelligence and automation of the editor:\n\nLuma API for advanced visual effects. Luma AI enables us to generate videos using their SOTA VLM. We aim to keep image characteristics the same and generate videos. This is incredibly important as these days some of the leading viral short-form content is AI-generated videos, so we needed a way for creators to develop AI videos using natural language, our integration allows the user to do this. ElevenLabs API is used to generate voiceovers. Most viral short-form content uses AI voiceovers and does not use a real person's voice. Our integration allows the user to generate an AI voiceover which is trendy and goes viral quicker, using their natural language chats."},{"heading":"Challenges we ran into","content":"The biggest challenge we faced was the sheer computational demand of our backend operations. Web GPUs are significantly weak for video processing, which is incredibly demanding, and we quickly realized we needed a proper GPU to achieve this.\n\nTo solve this Reilly drove down to UC Santa Cruz, convincing a friend to lend their high-end gaming PC for the project. This borrowed GPU became the foundation of our CUDA optimizations, allowing us to crank out the custom CUDA kernel that enabled real-time AI-driven edits. We wanted to use a GPU that someone's actual computer has so real editors could leverage features in Spielberg and not need to use brev.dev to rent an H100. Additionally, this only optimizes the best features of Spielberg, but the average person can still use Spielberg, but they will just wait for longer.\n\nWith the newfound power, we tuned our CUDA kernels, implemented multi-threaded AI inference, and reduced processing times by orders of magnitude. What started as an impossible challenge turned into our biggest breakthrough—a GPU-powered AI editor that processes in seconds what used to take minutes.\n\nAlso, we found version control when we had a sloppy codebase was very difficult. In the end, \"adeng\" is our working branch as the main branch has been slightly corrupted and is no longer functional."},{"heading":"Accomplishments that we're proud of","content":"Building a video editor in 36 hours? That's pretty cool and something we are proud of doing. Working with videos is extremely difficult and processing them with LLMs and having agents work with them and interact with them is very difficult. Successfully built an AI-powered video editor that edits using only natural language. Achieved real-time AI-driven video editing using custom CUDA-accelerated processing. Developed a Multimodal RAG system that learns from YouTube creators to optimize viral content. Integrated Google Gemini, OpenAI Whisper, Luma AI, and ElevenLabs to create a seamless editing experience."},{"heading":"What we learned","content":"As corny as it sounds, we learned that we can literally do anything if we put our minds to it. We learned so many new APIs, tools, and tricks and accomplished so much stuff we thought would be impossible without an army of engineers like Netflix or Adobe."},{"heading":"What's next for Spielberg AI","content":"We want to expand the agent's capabilities and finally take the time to deploy this project for the world to use. We know this is a serious problem for creators and we want to democratize content creation so everyone can tell stories about their lives. We would love to talk to Neo about turning this into a company."},{"heading":"Built With","content":"cuda elevenlabs fastapi ffmpeg gemini googlegenai huggingfacetransformers langchain lumalabs next.js openai-gpt openai-whispr opencv tailwind"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"NeuroPilot: Brainwave Computer Control","project_url":"https://devpost.com/software/mindcontrolpc-control-computer-with-brainwaves","tagline":"Control your computer with just your mind.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/272/421/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Scrapybara: Capy Prize (Japan offsite + $1k credits + Scrapybara Pro 6 months [1st] & $1k credits + Scrapybara Pro 3 months [2nd] & $100 credits + Scrapybara Pro 1 month [3rd])"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"brainflow","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"huggingface","url":null},{"name":"openai","url":null},{"name":"openbci","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"scrapybara","url":null}],"external_links":[{"label":"github.com","url":"http://github.com/nicolesplaining/treehacks/tree/main"}],"description_sections":[{"heading":"What it does","content":"NeuroPilot translates neural EEG signals (brainwaves) into natural language commands and allows users to control computer functions simply by imagining speech; it is a real-time, end-to-end EEG-to-action pipeline. Put simply, it allows users to control computers with their minds."},{"heading":"Inspiration","content":"We explore the world through motion and words; yet 5.4 million people in the United States alone have some form of paralysis, and over 15 million people worldwide have spinal cord injuries. Imagine a world where all individuals with limited mobility can navigate a desktop or generate files – simply by thinking. Whether it’s a student attending online classes, a software engineer working their job, or an artist bringing ideas to life, the ability to seamlessly control a computer without physical limitations opens countless opportunities for independence, productivity, and creativity. Our team was driven by the challenge faced by individuals with limited motor/speech function , with personal experience of stroke-induced paralysis in the relative of one of our team members. NeuroPilot, the accessible autonomous digital assistant , was born from the idea that thought alone should be enough to control a computer."},{"heading":"How we built it","content":"NeuroPilot integrates neural and language data modalities with agentic workflows powered by Scrapybara .\n\nThe foundation of NeuroPilot is its brain-to-text component, which uses a Brain-Computer-Interface (BCI) hardware (OpenBCI electrode headset) to translate neural EEG signals into natural language commands. We used BrainFlow to extract real-time data from the headset’s Cyton chip.\n\nOur system interprets activity detected from OpenBCI EEG electrodes through a multi-step system. First, it uses an RNN model to identify phonemes (basic linguistic units) in imagined speech signals. To enhance performance in limited-resource settings, we applied transfer learning by first pretraining on high-quality invasive data to leverage relevant neural features. We then fine-tuned the model using our own dataset, which we collected and labeled with our non-invasive EEG equipment. Our dataset focuses primarily on phonemes found in words commonly used for computer control, such as “search” and “Google,” ensuring that the system is optimized for real-world task execution.\n\nNext, a language model -based decoder is used to determine the most likely sequence of phonemes according to the probabilities assigned by the RNN and its knowledge of n-gram patterns in language. Finally, we use prompt engineering through the OpenAI API to clarify and isolate a command from the thought stream. We iteratively tested and refined our prompts to align well with Scrapybara’s agentic framework. For example, when a user intends to open an application, the system recognizes the intent and modifies the command accordingly (e.g., instead of just interpreting the phrase “manage file,” the model refines it into a structured directive like “Open file manager in applications”).\n\nWe use Firebase as our database to communicate in real-time between the BCI, the model, the resulting natural language command, and the client end of the program.\n\nOnce the natural language command is determined, it is fed into an automated system interaction framework powered by Scrapybara . The system translates the natural language commands into system actions on a virtual desktop and enables agents to execute user intent."},{"heading":"Challenges we ran into","content":"Brain-to-text and other works exploring using EEG signals have become a field of interest recently and have achieved very high performance with the introduction of more advanced deep learning techniques. However, they often rely on invasive methods to obtain the signals. In order to make these systems more accessible, we faced the challenge of using relatively weaker and lower-quality signals from much less channels. We innovatively applied a transfer learning approach by leveraging high-quality and extensive invasive datasets to pretrain our model and then fine-tuning to data collected from the electrode cap. This allows us to make use of relevant features and patterns the models can learn from these other cases and apply them to perform better in low-data and limited resource settings . This increases the inclusivity of these technologies while minimizing the sacrificed effectiveness."},{"heading":"What we learned","content":"A huge learning from the process of working on NeuroPilot was the experience of integrating a network of many different components into a single, streamlined pipeline. From streaming inputs from our hardware and processing them live through our multi-step algorithm to communicating their outputs with our database and executing agentic workflows with Scrapybara, we learned how to build a complete and multifaceted product."},{"heading":"Accomplishments that we're proud of","content":"In bringing together our brain-to-text and agentic workflow components, we implemented an LLM-driven prompt engineering system that would optimize the very simple extracted natural language commands into effective commands that could make the most of the full powerful potential of Scrapybara’s platform’s powerful workflow automation."},{"heading":"What's next for NeuroPilot: Brainwave Computer Control","content":"NeuroPilot is the future of digital accessibility and effective human-AI workflows with the power of your mind. A next step is exploring the potential of further modalities, such as tapping into a webcam to include limited facial movement visuals to improve the performance of the model. There are always constant developments in the field of BCI and neuroprosthesis, such as the recent Meta paper released this past week on novel non-invasive brain-to-text methods, and we hope to improve NeuroPilot to become more robust and accurate, bringing tomorrow into the now."},{"heading":"Built With","content":"api brainflow firebase huggingface openai openbci python pytorch scrapybara"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CodeFusion","project_url":"https://devpost.com/software/codefusion-v9eq2x","tagline":"Talk, gesture, and bring your code to life.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/270/252/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Warp: Best Developer Tool (Keychron Mechanical Keyboard per team member)"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"intersystems","url":null},{"name":"openai","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"visual-studio-code","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/vsahni3/CodeFusion/tree/main"}],"description_sections":[{"heading":"Inspiration 💡","content":"We were inspired by the thought of, \"what if Cursor could do this?\" Cursor composer, a powerful tool in it of itself, how much crazier would it be for Cursor to have access to video and audio, along with our terminals so not only could it write code, it would verify it's capabilities and more."},{"heading":"What it does ⁉️","content":"You start with screen recording and talking to describe the issue or enhancement needed. We then process this using gemini flash 2.0 and turn it into a structured schema organized by timestamps. We extract relevant files and urls and then extract html from the web pages and turn them into embeddings. We extract relevant text using a RAG pipeline with Intersystems-IRIS. Once we have all this info we use codegen to edit relevant code and verify correctness by running the application."},{"heading":"How we built it 📝","content":"TypeScript/React - VSCode Extension Python-Flask - Server Backend Codegen + Langchain - Agentic Code Editing Gemini - Video recording parsing IRIS - RAG -OpenAI - Embeddings"},{"heading":"Challenges we ran into 😡","content":"Streaming using the gemini live API was very difficult. We were able to stream video and audio and get real time responses asynchronously, but the text inputs stopped working. We instead processed the video before sending it to gemini flash 2.0 with a specific schema enforced. Setting up the VS Code extension to have the UI in the side panel instead of the tab was challenging. In addition, the extension was a web view and restricted display capture so we had to find a workaround to send video to the backend."},{"heading":"Accomplishments that we're proud of 😸","content":"We are proud of getting a fully functional backend with a multifaceted backend. We were able to get 3 distinct components (along with the frontend VS code extension) working: the video processing into a structured schema by gemini, the RAG with intersystems, and the agentic flow and tool calling with codegen."},{"heading":"Built With","content":"flask gemini intersystems openai opencv visual-studio-code"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Supernova","project_url":"https://devpost.com/software/supernova-p1tum5","tagline":"End-to-end AI-generated ads – the world's influencers at your fingertips.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/273/617/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"TreeHacks Grand Prize | 2nd Place ($7.5k Cash)"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"cartesia","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"heygen","url":null},{"name":"nextjs","url":null},{"name":"openai","url":null},{"name":"perplexity","url":null},{"name":"runway","url":null},{"name":"supabase","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pr28416/ai-ugc"},{"label":"supernova0.vercel.app","url":"https://supernova0.vercel.app/"},{"label":"supernova0.vercel.app","url":"https://supernova0.vercel.app/gallery"}],"description_sections":[{"heading":"Inspiration","content":"The creator economy is evolving at lightning speed. With platforms like TikTok and Instagram fueling the rise of short-form user-generated content (UGC) , influencer marketing has become a powerful way for brands to connect with their audiences. Yet, despite breakthroughs in generative AI—like dynamic video creation and realistic talking avatars—few platforms (including notable startups like Suno) have truly tapped into these tools to automate UGC ads from start to finish in a truly methodical (and not AI-sloppy) manner. We saw an exciting opportunity to leverage generative workflows and advanced AI research to radically streamline influencer-driven content and make it more accessible as a channel. That vision led us to build Supernova: a fully end-to-end, AI-powered ad generator with true access to the world’s knowledge."},{"heading":"What it does","content":"Supernova takes a single product link as its input and handles everything else:\n\nAutonomous Market Research Agents : Our autonomous AI agents powered by Perplexity continuously traverse the web, assembling a dynamic, self-evolving market research schema that updates over time. These agents aggregate and synthesize real-time data streams—uncovering solved pain points, mapping target audience behaviors, analyzing sentiment from customer reviews, tracking competitor positioning, and extracting domain-specific insights. The result is a continuously enriched, multi-dimensional market profile that sharpens product positioning and informs every creative decision with precision. Deep Profiling for Influencer Selection: We deploy AI agents (Audience, Controversy, and Orchestrator Agents) to regularly monitor major news feeds, perform nested Perplexity searches, web scraping, and deep dives across platforms like social media and FamousBirthdays.com to synthesize comprehensive psychographic profiles of influencers and top celebrities. Our agents analyze their content themes, audience demographics, engagement patterns, controversies, and core belief systems. We cross-reference these insights with the product’s market profile to ensure precise alignment between the influencer’s persona and the target audience’s values and interests. Precise B-Roll Generation: We craft a precise 30-second script tailored to the influencer’s persona, then deploy workflows searching for reference images and contextual details to design B-roll prompts. Our system determines whether the product should be featured in each segment and assembles detailed, tightly guided prompts to drive highly specific, on-brand B-roll generation using Runway ML's Gen3 Alpha Turbo image-to-video model. Hyper-Realistic AI Avatar Creation: We create a HeyGen or SyncLabs avatar (only for celebrity avatars), steering its appearance to closely match the influencer’s likeness through guided visual prompts and reference modeling. This ensures consistency with the influencer’s style and persona. AI-Enhanced Voice Selection and Matching: Then, we start a voice optimization pipeline that focuses on finding a voice with tonality and expression that is optimal for the target demographic. We surf the web for psychographic data on individuals in the target audience and cross-reference it with the product’s positioning to understand emotional triggers, communication styles, and preferred tonal qualities. Then, using Cartesia’s voice-to-voice API, our AI agents look through a set of indexed voice profiles and select the most suitable one, optimizing for vocal delivery, emotional expression, and speech patterns to maximize audience conversions. Seamless Final Assembly: Lastly, we stitch everything together with FFmpeg and add stylized captions using ZapCap, delivering a ready-to-post short-form video ad."},{"heading":"How we built it","content":"We built Supernova using a multi-agent orchestration framework guided by OpenAI’s agent-first principles , enabling asynchronous operations and tool-based model calls for efficient task execution. Each major workflow was designed with specialized tools , running concurrently to maximize speed and accuracy.\n\nFor market research, we integrated Perplexity’s API and Firecrawl Extract pipelines with a custom search schema builder, enabling recursive queries and nested searches for audience trends, pain points, and competitor insights. Real-time data aggregation from Brave Search and eCommerce reviews feeds into a vectorized knowledge base, which is continuously updated during the monitoring process (as news updates occur) to power downstream workflows.\n\nDuring influencer profiling, we used OpenAI models with tool-calling capabilities to execute searches, scrape data from social platforms and FamousBirthdays with Firecrawl, and extract insights from articles and interviews. Psychographic profiles are generated by scoring influencers on audience fit, controversies, and value alignment using weighted embeddings . A Llama 3.2 11B via Groq vision model matches these profiles against static HeyGen avatars to produce the closest visual likeness or, for celebrities, directly using SyncLabs on real video clips of the personality.\n\nFor B-roll generation, we prompt gpt-4o to generate structured shot lists based on the influencer profile and market insights. Simultaneously, a reference search pipeline collects contextual images to steer Runway ML’s Gen3Alpha Turbo image-to-video generation model in generating accurate scenes with and without product placements (depending on b-roll requirements).\n\nIn the voice pipeline, we first expand psychographic audience insights using agents with web scraping and search tools. Using a voice embedding search over indexed profiles , we select a voice that best matches audience preferences and fine-tune it for tonal accuracy and emotional resonance, properly create the new voice clips with Cartesia’s voice-to-voice API , and then match the new voice clips to the corresponding A-roll and B-roll segments for timestamps using OpenAI Whisper .\n\nFinally, video assembly is fully automated using FFmpeg for clip merging and synchronization. A ZapCap API layer adds captions and carefully selected music generated from the script, ensuring style consistency and platform readiness.\n\nThis orchestrated, tool-based architecture allows Supernova to generate high-impact, data-informed video ads with minimal human intervention and maximum creative precision."},{"heading":"Challenges we ran into","content":"Getting the HeyGen avatars to match both the influencer’s tone and visual style without infringing on personal likeness rights was tricky. Generating consistent product shots via Runway ML required careful prompts and image references—especially when the product had unique packaging or shapes. Balancing multiple AI calls (research, generation, search) and aligning them with a coherent UI pipeline was a real juggling act. Ensuring each step awaited the correct data and kicked off the next step seamlessly took considerable design effort."},{"heading":"Accomplishments that we're proud of","content":"We managed to create a truly one-click solution—from product link to final video output—by orchestrating multiple cutting-edge generative technologies. Our approach to matching influencers’ content style with product demographics, then mapping them to avatar equivalents, is both innovative and legally sound. The combination of Runway ML B-roll, refined voice-to-voice audio, and precise scripting elevates the generated content far beyond typical AI video demos."},{"heading":"What we learned","content":"Coordinating specialized AI models for research, generation, and video production can unlock powerful end products. Our pivot to avatar-likeness matching reminded us to prioritize IP rights and influencers’ personal brand. Generating realistic imagery and scripts with minimal repetition or artifacting is heavily dependent on fine-tuned prompts. This was our first time using Codeium Windsurf! We really enjoyed using this AI code editor and found it very useful in iterating quickly."},{"heading":"What's next for Supernova","content":"We plan to integrate more robust metrics (like engagement rates, niche expertise) and widen our influencer catalog for more precise matching. We aim to extend Supernova to automatically generate videos in multiple languages and adapt them for platforms beyond TikTok and Instagram. A next step is allowing brands to tweak scripts, b-roll prompts, or avatar attributes in real time, providing a more interactive creative experience. We’re exploring post-release analytics to measure ad performance, feedback loops for improvement, and deeper integration of generative AI in the overall marketing pipeline.\n\nWe envision a future where marketing becomes seamless and effortless—yet every frame of an advertisement is purposeful, delivering meaningful value to sellers, creators, and viewers alike.\n\nWe'd love for you to check out our gallery of public-facing ads that we generated at the link below: https://supernova0.vercel.app/gallery"},{"heading":"Built With","content":"agents cartesia groq heygen nextjs openai perplexity runway supabase"},{"heading":"Try it out","content":"github.com supernova0.vercel.app supernova0.vercel.app"}]},{"project_title":"Miru","project_url":"https://devpost.com/software/mapdash","tagline":"Indoor navigation for the visually impaired, made easy.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/274/971/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2025","hackathon_url":"https://treehacks-2025.devpost.com/","prize_name":"Vespa.ai: Best Hack Using a Vision Language Model ($500 Amazon gift card per team member)"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"clip","url":null},{"name":"edge","url":null},{"name":"embedding","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"healthcare","url":null},{"name":"llms","url":null},{"name":"meta-rayban","url":null},{"name":"metaraybans","url":null},{"name":"navigation","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vespa","url":null},{"name":"vlms","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/flatypus/miru"}],"description_sections":[{"heading":"The Inspiration","content":"More than 20 million Americans experience visual impairment, with 1 million classified as legally blind. For many, navigating indoor spaces independently is a significant challenge, limiting daily activities and self-sufficiency. While outdoor navigation solutions—powered by GPS and mapping services like Google Maps—are widely available, indoor navigation remains a relatively unsolved problem due to the lack of reliable spatial data and GPS limitations.\n\nGiven that people spend most of their time indoors, the status quo is the reverse of what we need. We set out to create an intuitive, lightweight, and accessible indoor navigation aid for the visually impaired. With the rise of advanced video capture technology—such as the Apple Vision Pro and Meta Ray-Ban glasses—we saw an opportunity to harness computer vision and wearable technology to make indoor spaces more navigable for those with visual impairments."},{"heading":"What Miru Does","content":"Miru is an indoor navigation tool for the blind, effectively serving as an indoor Google Maps. Miru provides real-time, step-by-step guidance to help visually impaired users navigate complex indoor environments with ease.\n\nA picture of the floor plan of the building—readily available on fire escape plans near elevators or fire extinguishers—is uploaded to Miru, which automatically generates waypoints and pathfinding routes. The user wears a pair of Meta Ray-Ban smart glasses for real-time visual input and localization, and a servo-fitted belt provides gentle haptic feedback to guide movement, ensuring a non-intrusive and intuitive experience. By integrating wearable technology with intelligent navigation, Miru empowers visually impaired individuals to move confidently and independently in indoor spaces."},{"heading":"How We Built It","content":"To convert fire escape plans into functional map data, we implemented Canny edge detection, extracting key architectural features such as walls and doors. Analyzing the map, we then generated waypoints, followed by A* pathfinding to compute optimal navigation routes.\n\nLive video footage was acquired through Meta’s Ray-Ban glasses. We tried to use streaming through Instagram to acquire the footage, but realized there was around 30 seconds of latency which wouldn't work. We found that streaming footage to WhatsApp provided data much faster, with less than 1 second of latency.\n\nWe also had the challenge of extending sensory capabilities to the visually impaired; in particular, a system that could reliably provide direction. We ended up deciding on creating a wearable belt, created using servos as a tactile feedback mechanism, and it has five servos capable of generating directional instructions (left, right, forward, left-forward, right-forward). As the user turns around, we use our orientation mechanisms to provide feedback in the correct direction.\n\nLocalization—identifying the user's real-time position indoors—was one of our most challenging tasks. Specifically, because it is impossible to access GPS, no indoor navigation system that relies on GPS will work. Our innovative approach involved two key steps:\n\nFirst, we generated a reference dataset of approximately 1,500 geotagged images in the building and embedded them using CLIP. We stored the vectors in an database using Vespa AI. We then embedded the live footage from the Ray-Ban and queried for similar images in the Vespa vector database. Then through the most similar images we averaged the coordinate metadata that we got from those images. Most importantly was filtering, after gathering these coordinates we compared it to the data we already had and would throw out any values that were greater than 5.0 meters away from the current pose. As much as we tried to reduce noise, there will always be noise in a system and filtering is something we can always work to improve.\n\nBesides localization, determining orientation was also crucial for our project’s success. We built our own iOS compass app that could interact with our websocket system using the iOS compass data to determine orientation and integrated it with a wearable utility built to guide user interaction. We found a bug in the iOS compass app, where the compass would drift randomly for no apparent reason. This resulted in even more pain as it meant that we had one less source of truth to trust."},{"heading":"Challenges We Ran Into","content":"We immediately ran into latency issues with Meta’s Ray-Ban glasses. Previous projects we’d looked at involving Meta Ray-Ban glasses streamed the Ray-Ban footage to Instagram Live, because Meta Ray-Ban glasses are incapable of natively streaming to a laptop. Instagram Live, however, had a stream delay of ~30 seconds. We resolved this issue by streaming to WhatsApp and mirroring the phone screen onto a laptop, providing latency of <1 second.\n\nParsing through vector embeddings via OpenAI’s vision model meant high latency, which was undesirable for real-time use. As a result, we opted to use Vespa AI’s API for efficient updates and queries.\n\nThe most difficult challenge we ran into was localization. Determining where a user was proved extremely challenging, and we opted to use computer vision and similarity detection to determine the location of the user.\n\nOrientation was also an issue. Initially, we tried using tri-sensor IMU fusion, but sensor drift rendered IMUs inaccurate. We managed to pull iOS compass data as an effective and simple method of determining user orientation."},{"heading":"Accomplishments We’re Proud Of","content":"We’re extremely proud of the progress we were able to make in just 36 hours. Prior to this hackathon, none of us had extensive experience with remote sensing, and we’re proud that we were able to develop a real-time, GPS-free indoor navigation system that required only a floor plan and live video feed. This serves as a simple alternative to beacon-based positioning. We’re also happy that we were able to create a functional and intuitive haptic feedback system that allows users to “see” through feel.\n\nWe think that localization is one of the most difficult challenges within robotics and we think that the new age of AI has so much potential and ability to help robots better understand the world around us."},{"heading":"What We Learned","content":"We learned how to work extensively with computer vision, vector embeddings, basic hardware, and software integration, all of which were extremely challenging but rewarding to work with. We also learned how to integrate software with hardware for an intuitive and powerful experience."},{"heading":"What’s Next for Miru","content":"A few ideas we hope to implement:\n\nMulti-Floor Path Finding : It’d be cool to direct users to stairwells and elevators and enable travel across more than just one floor. SLAM-Powered Real-Time Mapping : If done effectively, using Simultaneous Localization and Mapping (SLAM) would allow us to dynamically build and update indoor maps without the need for reference images—saving a lot of time. Scaling to Large-Scale Public Spaces : Expanding to airports, malls, hospitals, and transit hubs would enable us to provide invaluable services to the visually impaired in more than just small buildings. We hope that while we had to create our own dataset to query from for the Huang building, often-visited locations like train stations and museums have publically available floor plans and Google maps data that our system can easily apply into."},{"heading":"Built With","content":"arduino clip edge embedding groq healthcare llms meta-rayban metaraybans navigation python typescript vespa vlms"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-18T16:39:41.589429Z"}}