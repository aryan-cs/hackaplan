{"version":"v1","hackathon_url":"https://cal-hacks-10.devpost.com","generated_at":"2026-02-17T18:22:57.699781Z","result":{"hackathon":{"name":"Cal Hacks 10.0","url":"https://cal-hacks-10.devpost.com","gallery_url":"https://cal-hacks-10.devpost.com/project-gallery","scanned_pages":10,"scanned_projects":240,"winner_count":40},"winners":[{"project_title":"Nexus","project_url":"https://devpost.com/software/nexus-27zakp","tagline":"Our project is a real-time voice chat app that pairs users intelligently based on their interests.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/293/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Ddoski's Favorite: 1st Overall"}],"team_members":[],"built_with":[{"name":"100ms","url":null},{"name":"clerk","url":"https://devpost.com/software/built-with/clerk"},{"name":"convex","url":null},{"name":"daisyui","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/Sankalpsp21/Nexus"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for our project, Nexus, comes from our experience as individuals with unique interests and challenges. Often, it isn't easy to meet others with these interests or who can relate to our challenges through traditional social media platforms.\n\nWith Nexus, people can effortlessly meet and converse with others who share these common interests and challenges, creating a vibrant community of like-minded individuals.\n\nOur aim is to foster meaningful connections and empower our users to explore, engage, and grow together in a space that truly understands and values their uniqueness."},{"heading":"What it Does","content":"In Nexus, we empower our users to tailor their conversational experience. You have the flexibility to choose how you want to connect with others. Whether you prefer one-on-one interactions for more intimate conversations or want to participate in group discussions, our application Nexus has got you covered.\n\nWe allow users to either get matched with a single person, fostering deeper connections, or join one of the many voice chats to speak in a group setting, promoting diverse discussions and the opportunity to engage with a broader community. With Nexus, the power to connect is in your hands, and the choice is yours to make."},{"heading":"How we built it","content":"We built our application using a multitude of services/frameworks/tool:\n\nReact.js for the core client frontend TypeScript for robust typing and abstraction support Tailwind for a utility-first CSS framework DaisyUI for animations and UI components 100ms live for real-time audio communication Clerk for a seamless and drop-in OAuth provider React-icons for drop-in pixel perfect icons Vite for simplified building and fast dev server Convex for vector search over our database React-router for client-side navigation Convex for real-time server and end-to-end type safety 100ms for real-time audio infrastructure and client SDK MLH for our free .tech domain"},{"heading":"Challenges We Ran Into","content":"Navigating new services and needing to read a lot of documentation -- since this was the first time any of us had used Convex and 100ms, it took a lot of research and heads-down coding to get Nexus working. Being awake to work as a team -- since this hackathon is both in-person and through the weekend , we had many sleepless nights to ensure we can successfully produce Nexus. Working with very poor internet throughout the duration of the hackathon, we estimate it cost us multiple hours of development time."},{"heading":"Accomplishments that we're proud of","content":"Finishing our project and getting it working! We were honestly surprised at our progress this weekend and are super proud of our end product Nexus. Learning a ton of new technologies we would have never come across without Cal Hacks. Being able to code for at times 12-16 hours straight and still be having fun! Integrating 100ms well enough to experience bullet-proof audio communication."},{"heading":"What we learned","content":"Tools are tools for a reason! Embrace them, learn from them, and utilize them to make your applications better. Sometimes, more sleep is better -- as humans, sleep can sometimes be the basis for our mental ability! How to work together on a team project with many commits and iterate fast on our moving parts."},{"heading":"What's next for Nexus","content":"Make Nexus rooms only open at a cadence, ideally twice each day, formalizing the \"meeting\" aspect for users. Allow users to favorite or persist their favorite matches to possibly re-connect in the future. Create more options for users within rooms to interact with not just their own audio and voice but other users as well. Establishing a more sophisticated and bullet-proof matchmaking service and algorithm."},{"heading":"üöÄ Contributors üöÄ","content":"Jeff Huang Derek Williams Tom Nyuma Sankalp Patil"},{"heading":"Built With","content":"100ms clerk convex daisyui node.js react tailwind typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Speech Master","project_url":"https://devpost.com/software/speech-master","tagline":"Leveraging ML to improve public speaking","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/645/514/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Loved by Ddoski: 2nd Overall"}],"team_members":[],"built_with":[{"name":"cloudinary","url":"https://devpost.com/software/built-with/cloudinary"},{"name":"cockroachdb","url":null},{"name":"eslint","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"humeai","url":null},{"name":"husky","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"next.js","url":null},{"name":"nextauth","url":null},{"name":"prettier","url":null},{"name":"prisma","url":null},{"name":"socket.io","url":"https://devpost.com/software/built-with/socket-io"},{"name":"tailwindcss","url":null},{"name":"tensorflow","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/shahdivyank/speechmaster"},{"label":"www.figma.com","url":"https://www.figma.com/file/4K7YLO6OBMkJxFm3DElFMo/speechmaster?type=design&node-id=0-1&mode=design&t=kWm58njNRzqRPG9d-0"},{"label":"speechmaster.vercel.app","url":"https://speechmaster.vercel.app/"}],"description_sections":[{"heading":"Inspiration","content":"The post-COVID era has increased the number of in-person events and need for public speaking. However, more individuals are anxious to publicly articulate their ideas, whether this be through a presentation for a class, a technical workshop, or preparing for their next interview. It is often difficult for audience members to catch the true intent of the presenter, hence key factors including tone of voice, verbal excitement and engagement, and physical body language can make or break the presentation.\n\nA few weeks ago during our first project meeting, we were responsible for leading the meeting and were overwhelmed with anxiety. Despite knowing the content of the presentation and having done projects for a while, we understood the impact that a single below-par presentation could have. To the audience, you may look unprepared and unprofessional, despite knowing the material and simply being nervous. Regardless of their intentions, this can create a bad taste in the audience's mouths.\n\nAs a result, we wanted to create a judgment-free platform to help presenters understand how an audience might perceive their presentation. By creating Speech Master, we provide an opportunity for presenters to practice without facing a real audience while receiving real-time feedback."},{"heading":"Purpose","content":"Speech Master aims to provide a practice platform for practice presentations with real-time feedback that captures details in regard to your body language and verbal expressions. In addition, presenters can invite real audience members to practice where the audience member will be able to provide real-time feedback that the presenter can use to improve.\n\nWhile presenting, presentations will be recorded and saved for later reference for them to go back and see various feedback from the ML models as well as live audiences. They are presented with a user-friendly dashboard to cleanly organize their presentations and review for upcoming events.\n\nAfter each practice presentation, the data is aggregated during the recording and process to generate a final report. The final report includes the most common emotions expressed verbally as well as times when the presenter's physical body language could be improved. The timestamps are also saved to show the presenter when the alerts rose and what might have caused such alerts in the first place with the video playback."},{"heading":"Tech Stack","content":"We built the web application using Next.js v14 , a React-based framework that seamlessly integrates backend and frontend development. We deployed the application on Vercel , the parent company behind Next.js. We designed the website using Figma and later styled it with TailwindCSS to streamline the styling allowing developers to put styling directly into the markup without the need for extra files. To maintain code formatting and linting via Prettier and EsLint . These tools were run on every commit by pre-commit hooks configured by Husky .\n\nHume AI provides the Speech Prosody model with a streaming API enabled through native WebSockets allowing us to provide emotional analysis in near real-time to a presenter. The analysis would aid the presenter in depicting the various emotions with regard to tune, rhythm, and timbre.\n\nGoogle and Tensorflow provide the MoveNet model is a large improvement over the prior PoseNet model which allows for real-time pose detection. MoveNet is an ultra-fast and accurate model capable of depicting 17 body points and getting 30+ FPS on modern devices.\n\nTo handle authentication, we used Next Auth to sign in with Google hooked up to a Prisma Adapter to interface with CockroachDB , allowing us to maintain user sessions across the web app. Cloudinary , an image and video management system, was used to store and retrieve videos. Socket.io was used to interface with Websockets to enable the messaging feature to allow audience members to provide feedback to the presenter while simultaneously streaming video and audio. We utilized various services within Git and Github to host our source code, run continuous integration via Github Actions , make pull requests , and keep track of issues and projects ."},{"heading":"Challenges","content":"It was our first time working with Hume AI and a streaming API. We had experience with traditional REST APIs which are used for the Hume AI batch API calls, but the streaming API was more advantageous to provide real-time analysis. Instead of an HTTP client such as Axios, it required creating our own WebSockets client and calling the API endpoint from there. It was also a hurdle to capture and save the correct audio format to be able to call the API while also syncing audio with the webcam input.\n\nWe also worked with Tensorflow for the first time, an end-to-end machine learning platform. As a result, we faced many hurdles when trying to set up Tensorflow and get it running in a React environment. Most of the documentation uses Python SDKs or vanilla HTML/CSS/JS which were not possible for us. Attempting to convert the vanilla JS to React proved to be more difficult due to the complexities of execution orders and React's useEffect and useState hooks. Eventually, a working solution was found, however, it can still be improved to better its performance and bring fewer bugs.\n\nWe originally wanted to use the Youtube API for video management where users would be able to post and retrieve videos from their personal accounts. Next Auth and YouTube did not originally agree in terms of available scopes and permissions, but once resolved, more issues arose. We were unable to find documentation regarding a Node.js SDK and eventually even reached our quota. As a result, we decided to drop YouTube as it did not provide a feasible solution and found Cloudinary."},{"heading":"Accomplishments","content":"We are proud of being able to incorporate Machine Learning into our applications for a meaningful purpose. We did not want to reinvent the wheel by creating our own models but rather use the existing and incredibly powerful models to create new solutions. Although we did not hit all the milestones that were hoping to achieve, we are still proud of the application that we were able to make in such a short amount of time and be able to deploy the project as well.\n\nMost notably, we are proud of our Hume AI and Tensorflow integrations that took our application to the next level. Those 2 features took the most time, but they were also the most rewarding as in the end, we got to see real-time updates of our emotional and physical states. We are proud of being able to run the application and get feedback in real-time, which gives small cues to the presenter on what to improve without risking distracting the presenter completely."},{"heading":"What we learned","content":"Each of the developers learned something valuable as each of us worked with a new technology that we did not know previously. Notably, Prisma and its integration with CockroachDB and its ability to make sessions and general usage simple and user-friendly. Interfacing with CockroachDB barely had problems and was a powerful tool to work with.\n\nWe also expanded our knowledge with WebSockets, both native and Socket.io. Our prior experience was more rudimentary, but building upon that knowledge showed us new powers that WebSockets have both when used internally with the application and with external APIs and how they can introduce real-time analysis."},{"heading":"Future of Speech Master","content":"The first step for Speech Master will be to shrink the codebase. Currently, there is tons of potential for components to be created and reused. Structuring the code to be more strict and robust will ensure that when adding new features the codebase will be readable, deployable, and functional. The next priority will be responsiveness, due to the lack of time many components appear strangely on different devices throwing off the UI and potentially making the application unusable.\n\nOnce the current codebase is restructured, then we would be able to focus on optimization primarily on the machine learning models and audio/visual. Currently, there are multiple instances of audio and visual that are being used to show webcam footage, stream footage to other viewers, and sent to HumeAI for analysis. By reducing the number of streams, we should expect to see significant performance improvements with which we can upgrade our audio/visual streaming to use something more appropriate and robust.\n\nIn terms of new features, Speech Master would benefit greatly from additional forms of audio analysis such as speed and volume. Different presentations and environments require different talking speeds and volumes of speech required. Given some initial parameters, Speech Master should hopefully be able to reflect on those measures. In addition, having transcriptions that can be analyzed for vocabulary and speech, ensuring that appropriate language is used for a given target audience would drastically improve the way a presenter could prepare for a presentation."},{"heading":"Built With","content":"cloudinary cockroachdb eslint git github humeai husky javascript next.js nextauth prettier prisma socket.io tailwindcss tensorflow"},{"heading":"Try it out","content":"github.com www.figma.com speechmaster.vercel.app"}]},{"project_title":"Auto-Teach","project_url":"https://devpost.com/software/auto-teach","tagline":"Teaching everywhere at anytime. One click between a simple submission and a personalized detailed feedback.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/646/226/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Beginner Hack"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"The inspiration for our Auto-Teach project stemmed from the growing need to empower both educators and learners with a self-directed and adaptive learning environment. We were inspired by the potential to merge technology with education to create a platform that fosters personalized learning experiences , allowing students to actively engage with the material while offering educators tools to efficiently evaluate and guide individual progress ."},{"heading":"What it does","content":"Auto-Teach is an innovative platform that facilitates self-directed learning . It allows instructors to create problem sets and grading criteria while enabling students to articulate their problem-solving methods and responses through text input or file uploads (future feature). The software leverages AI models to assesses student responses, offering constructive feedback , pinpointing inaccuracies , and identifying areas for improvement . It features automated grading capabilities that can evaluate a wide range of responses, from simple numerical answers to comprehensive essays, with precision."},{"heading":"How we built it","content":"Our deliverable for Auto-Teach is a full-stack web app. Our front-end uses ReactJS as our framework and manages data using convex . Moreover, it leverages editor components from TinyMCE to provide student with better experience to edit their inputs. We also created back-end APIs using \"FastAPI\" and \"Together.ai APIs\" in our way building the AI evaluation feature."},{"heading":"Challenges we ran into","content":"We were having troubles with incorporating Vectara's REST API and MindsDB into our project because we were not very familiar with the structure and implementation. We were able to figure out how to use it eventually but struggled with the time constraint. We also faced the challenge of generating the most effective prompt for chatbox so that it generates the best response for student submissions."},{"heading":"Accomplishments that we're proud of","content":"Despite the challenges, we're proud to have successfully developed a functional prototype of Auto-Teach. Achieving an effective system for automated assessment, providing personalized feedback, and ensuring a user-friendly interface were significant accomplishments. Another thing we are proud of is that we effectively incorporates many technologies like convex, tinyMCE etc into our project at the end."},{"heading":"What we learned","content":"We learned about how to work with backend APIs and also how to generate effective prompts for chatbox. We also got introduced to AI-incorporated databases such as MindsDB and was fascinated about what it can accomplish (such as generating predictions based on data present on a streaming basis and getting regular updates on information passed into the database)."},{"heading":"What's next for Auto-Teach","content":"Divide the program into two mode : instructor mode and student mode Convert Handwritten Answers into Text (OCR API) Incorporate OpenAI tools along with Together.ai when generating feedback Build a database storing all relevant information about each student (ex. grade, weakness, strength) and enabling automated AI workflow powered by MindsDB Complete analysis of student's performance on different type of questions, allows teachers to learn about student's weakness. Fine-tuned grading model using tools from Together.ai to calibrate the model to better provide feedback. Notify students instantly about their performance (could set up notifications using MindsDB and get notified every day about any poor performance) Upgrade security to protect against any illegal accesses"},{"heading":"Built With","content":"fastapi javascript react"}]},{"project_title":"Virality Pro","project_url":"https://devpost.com/software/virality-pro","tagline":"Lower content production costs by 95% using our AI-assisted marketing system that will take care of your viral social media growth","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/645/617/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Shoutout from Ddoski: 3rd Overall"}],"team_members":[],"built_with":[{"name":"capcut","url":null},{"name":"dalle","url":null},{"name":"ffmpeg","url":"https://devpost.com/software/built-with/ffmpeg"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"shopify","url":"https://devpost.com/software/built-with/shopify"}],"external_links":[{"label":"github.com","url":"https://github.com/WesleyBLDC/dalle"},{"label":"virality.pro","url":"https://virality.pro/"}],"description_sections":[{"heading":"Virality Pro: 95% reduced content production costs, 2.5x rate of going viral, 4 high ticket clients","content":"We‚Äôre already helping companies go viral on instagram & TikTok, slash the need for large ad spend, and propel unparalleled growth at a 20x lower price."},{"heading":"The problem: growing a company is HARD and EXPENSIVE","content":"Here are the current ways companies grow reliably:\n\nFacebook ads / Google Ads : Expensive Paid Ads Producing ads often cost $2K - $10K+ Customer acquisition cost on Facebook can be as much as $100+, with clicks being as high as $10 on google ads Simply untenable for lower-ticket products Organic Social Media : Slow growth Takes a long time and can be unreliable; some brands just cannot grow Content production, posting, and effective social media management is expensive Low engagement rates even at 100K+ followers, and hard to stay consistent"},{"heading":"Solution: Going viral with Virality Pro, Complete Done-For-You Viral Marketing","content":"Brands and startups need the potential for explosive growth without needing to spend $5K+ on marketing agencies, $20K+ on ad spend, and getting a headache hiring and managing middle management.\n\nWe take care of everything so that you just give us your company name and product, and we manage everything from there.\n\nThe solution: viral social media content at scale .\n\nUsing our AI-assisted system, we can produce content following the form of proven viral videos at scale for brands to enable consistent posting with rapid growth."},{"heading":"Other brands: Spends $5K to produce an ad, $20K on ad spend.","content":"They have extremely thin margins with unprofitable growth."},{"heading":"With Virality Pro: $30-50 per video, 0 ad spend, produced reliably for fast viral growth","content":"Professional marketers and marketing agencies cost hundreds of thousands of dollars per year.\n\nWith Virality Pro, we can churn out 400% more content for 5 times less.\n\nThis content can easily get 100,000+ views on tik tok and instagram for under $1000, while the same level of engagement would cost 20x more traditionally."},{"heading":"Startups, Profitable Companies, and Brands use Virality Pro to grow","content":"Our viral videos drive growth for early to medium-sized startups and companies, providing them a lifeline to expand rapidly."},{"heading":"4 clients use Virality Pro and are working with us for growth","content":"Minute Land is looking to use Virality Pro to consistently produce ads, scaling to $400K+ through viral videos off $0 in ad spend Ivy Roots Consulting is looking to use Virality Pro to scale their college consulting business in a way that is profitable without the need for VC money . Instead of $100 CAC through paid ads, the costs with Virality Pro are close to 0 at scale. Manifold is looking to use Virality Pro to go viral on social media over and over again to promote their new products without needing to hire a marketing department Yoodli is looking to use Virality Pro to manage rapid social media growth on TikTok/Instagram without the need to expend limited funding for hiring middle managers and content producers to take on headache-inducing media projects"},{"heading":"Our team: Founders with multiple exits, Stanford CS+Math, University of Cambridge engineers","content":"Our team consists of the best of the best, including Stanford CS/Math experts with Jane Street experience, founders with multiple large-scale exits multiple times, Singaporean top engineers making hundreds of thousands of dollars through past ventures, and a Cambridge student selected as the top dozen computer scientists in the entire UK."},{"heading":"Business Model","content":"Our pricing system charges $1900 per month for our base plan (5 videos per week), with our highest value plan being $9500 per month (8 videos per day).\n\nWith our projected goal of 100 customers within the next 6 months, we can make $400K in MRR with the average client paying $4K per month."},{"heading":"How our system works","content":"Our technology is split into two sectors: semi-automated production and fully-automated production.\n\nCurrently, our main offer is semi-automated production, with the fully-automated content creation sequence still in production."},{"heading":"Semi-Automated AI-Powered Production Technology","content":"We utilize a series of templates built around prompt engineering and fine-tuning models to create a large variety of content for companies around a single format.\n\nWe then scale the number of templates currently available to be able to produce hundreds and thousands of videos for a single brand off of many dozens of formats, each with the potential to go viral (having gone viral in the past)."},{"heading":"Creating the scripts and audios","content":"Our template system uses AI to produce the scripts and the on-screen text, which is then fed into a database system. Here, a marketing expert verifies these scripts and adjusts them to improve its viral nature. For each template, a series of seperate audios are given as options and scripts are built around it."},{"heading":"Sourcing Footage","content":"For each client, we source a large database of footage found through filmed clips, AI-generated video, motion-graphic images, and taking large videos on youtube and using software to break them down into small clips, each representing a shot."},{"heading":"Text to Speech","content":"We use realistic-sounding AI voices and default AI voices to power the audio. This has proven to work in the past and can be produced consistently at scale."},{"heading":"Stitching it all together","content":"Using our system, we then compile the footage, text script, and audio into one streamlined sequence, after which it can be reviewed and posted onto social media."},{"heading":"All done within 5 to 15 minutes per video","content":"Instead of taking hours, we can get it done in 5 to 15 minutes , which we are continuing to shave down."},{"heading":"Fully Automated System","content":"Our fully automated system is a work in progress that removes the need for human interaction and fully automates the video production, text creation, and other components, stitched together without the need for anyone to be involved in the process."},{"heading":"Building the Fully Automated AI System","content":"Our project was built employing Reflex for web development, OpenAI for language model integration, and DALL-E for image generation. Utilizing Prompt Engineering alongside FFmpeg, we synthesized relevant images to enhance our business narrative."},{"heading":"Challenges Faced","content":"Challenges encountered included slow Wi-Fi, the steep learning curve with Prompt Engineering and adapting to Reflex, diverging from conventional frameworks like React or Next.js for web application development."},{"heading":"Future of Virality Pro","content":"We are continuing to innovate our fully-automated production system and create further templates for our semi-automated systems. We hope that we can reduce the costs of production on our backend and increase the growth."},{"heading":"Projections","content":"We project to scale to 100 clients in 6 months to produce $400K in Monthly Recurring Revenue, and within a year, scale to 500 clients for $1.5M in MRR."},{"heading":"Built With","content":"capcut dalle ffmpeg openai python reflex shopify"},{"heading":"Try it out","content":"github.com virality.pro"}]},{"project_title":"Jarvis","project_url":"https://devpost.com/software/jarvis-wbfx7n","tagline":"Revolutionizing the visually impaired peoples life‚Äìour tech enables full environmental understanding, interactive queries via smartwatch, camera advancing with cutting-edge computer vision.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/641/967/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - GitHub] Most Creative Use of GitHub"},{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Hack for Health"},{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Milvus"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"firebase-storage","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"hume","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"langchain","url":null},{"name":"llava","url":null},{"name":"milvus","url":null},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"nextjs","url":null},{"name":"ocr","url":null},{"name":"pusher","url":"https://devpost.com/software/built-with/pusher"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"zepp","url":null},{"name":"zilliz","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/CalHacksJarvis"},{"label":"cal-hacks-cyqnrgyf6-ravs-gt.vercel.app","url":"https://cal-hacks-cyqnrgyf6-ravs-gt.vercel.app/"}],"description_sections":[{"heading":"Inspiration","content":"Our project, \" Jarvis ,\" was born out of a deep-seated desire to empower individuals with visual impairments by providing them with a groundbreaking tool for comprehending and navigating their surroundings. Our aspiration was to bridge the accessibility gap and ensure that blind individuals can fully grasp their environment. By providing the visually impaired community access to auditory descriptions of their surroundings, a personal assistant , and an understanding of non-verbal cues , we have built the world's most advanced tool for the visually impaired community."},{"heading":"What it does","content":"\" Jarvis \" is a revolutionary technology that boasts a multifaceted array of functionalities. It not only perceives and identifies elements in the blind person's surroundings but also offers auditory descriptions , effectively narrating the environmental aspects they encounter. We utilize a speech-to-text and text-to-speech model similar to Siri / Alexa , enabling ease of access. Moreover, our model possesses the remarkable capability to recognize and interpret the facial expressions of individuals who stand in close proximity to the blind person, providing them with invaluable social cues. Furthermore, users can ask questions that may require critical reasoning, such as what to order from a menu or navigating complex public-transport-maps. Our system is extended to the Amazfit , enabling users to get a description of their surroundings or identify the people around them with a single press."},{"heading":"How we built it","content":"The development of \" Jarvis \" was a meticulous and collaborative endeavor that involved a comprehensive array of cutting-edge technologies and methodologies. Our team harnessed state-of-the-art machine learning frameworks and sophisticated computer vision techniques to get analysis about the environment, like , Hume , LlaVa , OpenCV , a sophisticated computer vision techniques to get analysis about the environment, and used next.js to create our frontend which was established with the ZeppOS using Amazfit smartwatch ."},{"heading":"Challenges we ran into","content":"Throughout the development process, we encountered a host of formidable challenges. These obstacles included the intricacies of training a model to recognize and interpret a diverse range of environmental elements and human expressions. We also had to grapple with the intricacies of optimizing the model for real-time usage on the Zepp smartwatch and get through the vibrations get enabled according to the Hume emotional analysis model, we faced issues while integrating OCR (Optical Character Recognition) capabilities with the text-to speech model. However, our team's relentless commitment and problem-solving skills enabled us to surmount these challenges."},{"heading":"Accomplishments that we're proud of","content":"Our proudest achievements in the course of this project encompass several remarkable milestones. These include the successful development of \" Jarvis \" a model that can audibly describe complex environments to blind individuals, thus enhancing their situational awareness . Furthermore, our model's ability to discern and interpret human facial expressions stands as a noteworthy accomplishment."},{"heading":"What we learned","content":"Hume\n\nHume is instrumental for our project's emotion-analysis . This information is then translated into audio descriptions and the vibrations onto Amazfit smartwatch , providing users with valuable insights about their surroundings. By capturing facial expressions and analyzing them, our system can provide feedback on the emotions displayed by individuals in the user's vicinity. This feature is particularly beneficial in social interactions, as it aids users in understanding non-verbal cues .\n\nZepp\n\nOur project involved a deep dive into the capabilities of ZeppOS , and we successfully integrated the Amazfit smartwatch into our web application. This integration is not just a technical achievement; it has far-reaching implications for the visually impaired. With this technology, we've created a user-friendly application that provides an in-depth understanding of the user's surroundings, significantly enhancing their daily experiences. By using the vibrations , the visually impaired are notified of their actions. Furthermore, the intensity of the vibration is proportional to the intensity of the emotion measured through Hume .\n\nZiiliz\n\nWe used Zilliz to host Milvus online, and stored a dataset of images and their vector embeddings. Each image was classified as a person; hence, we were able to build an identity-classification tool using Zilliz's reverse-image-search tool. We further set a minimum threshold below which people's identities were not recognized, i.e. their data was not in Zilliz . We estimate the accuracy of this model to be around 95% .\n\nGithub\n\nWe acquired a comprehensive understanding of the capabilities of version control using Git and established an organization. Within this organization, we allocated specific tasks labeled as \" TODO \" to each team member. Git was effectively employed to facilitate team discussions, workflows, and identify issues within each other's contributions.\n\nThe overall development of \" Jarvis \" has been a rich learning experience for our team. We have acquired a deep understanding of cutting-edge machine learning , computer vision , and speech synthesis techniques. Moreover, we have gained invaluable insights into the complexities of real-world application, particularly when adapting technology for wearable devices. This project has not only broadened our technical knowledge but has also instilled in us a profound sense of empathy and a commitment to enhancing the lives of visually impaired individuals."},{"heading":"What's next for Jarvis","content":"The future holds exciting prospects for \" Jarvis. \" We envision continuous development and refinement of our model, with a focus on expanding its capabilities to provide even more comprehensive environmental descriptions . In the pipeline are plans to extend its compatibility to a wider range of wearable devices , ensuring its accessibility to a broader audience. Additionally, we are exploring opportunities for collaboration with organizations dedicated to the betterment of accessibility technology . The journey ahead involves further advancements in assistive technology and greater empowerment for individuals with visual impairments."},{"heading":"Built With","content":"azure firebase-storage flask html5 hume javascript langchain llava milvus mongodb nextjs ocr pusher python zepp zilliz"},{"heading":"Try it out","content":"github.com cal-hacks-cyqnrgyf6-ravs-gt.vercel.app"}]},{"project_title":"explr.ai","project_url":"https://devpost.com/software/trvl-ai","tagline":"Your next vacation planned in just a few swipes.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/645/277/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Hack for Fun"}],"team_members":[],"built_with":[{"name":"convex","url":null},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"together.ai","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/kieronong/travel-aoo"}],"description_sections":[{"heading":"Inspiration","content":"Planning vacations can be hard. Traveling is a very fun experience but often comes with a lot of stress of curating the perfect itinerary with all the best sights to see, foods to eat, and shows to watch. You don't want to miss anything special, but you also want to make sure the trip is still up your alley in terms of your own interests - a balance that can be hard to find."},{"heading":"What it does","content":"explr.ai simplifies itinerary planning with just a few swipes. After selecting your destination, the duration of your visit, and a rough budget, explr.ai presents you with a curated list of up to 30 restaurants, attractions, and activities that could become part of your trip. With an easy-to-use swiping interface, you choose what sounds interesting or not to you, and after a minimum of 8 swipes, let explr.ai's power convert your opinions into a full itinerary of activities for your entire visit."},{"heading":"How we built it","content":"We built this app using React Typescript for the frontend and Convex for the backend. The app takes in user input from the homepage regarding the location, price point, and time frame. We pass the location and price range into the Google API to retrieve the highest-rated attractions and restaurants in the area. Those options are presented to the user on the frontend with React and CSS animations that allow you to swipe each card in a Tinder-style manner. Taking consideration of the user's swipes and initial preferences, we query the Google API once again to get additional similar locations that the user may like and pass this data into an LLM (using Together.ai's Llama2 model) to generate a custom itinerary for the user. For each location outputted, we string together images from the Google API to create a slideshow of what your trip would look like and an animated timeline with descriptions of the location."},{"heading":"Challenges we ran into","content":"Front-end and design require a LOT of skill. It took us quite a while to come up with our project, and we originally were planning on a mobile app, but it's also quite difficult to learn completely new languages such as swift along with new technologies all in a couple of days. Once we started on explr.ai's backend, we were also having trouble passing in the appropriate information to the LLM to get back proper data that we could inject back into our web app."},{"heading":"Accomplishments that we're proud of","content":"We're proud at the overall functionality and our ability to get something working by the end of the hacking period :') More specifically, we're proud of some of our frontend, including the card swiping and timeline animations as well as the ability to parse data from various APIs and put it together with lots of user input."},{"heading":"What we learned","content":"We learned a ton about full-stack development overall, whether that be the importance of Figma and UX design work, or how to best split up a project when every part is moving at the same time. We also learned how to use Convex and Together.ai productively!"},{"heading":"What's next for explr.ai","content":"We would love to see explr.ai become smarter and support more features. explr.ai, in the future, could get information from hotels, attractions, and restaurants to be able to check availability and book reservations straight from the web app. Once you're on your trip, you should also be able to check in to various locations and provide feedback on each component. explr.ai could have a social media component of sharing your itineraries, plans, and feedback with friends and help each other better plan trips."},{"heading":"Built With","content":"convex google react together.ai typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Cancer360¬∞","project_url":"https://devpost.com/software/cal-hacks-project","tagline":"Helping Cancer Patients with AI Nursing and Patient Care","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/641/489/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Hack on AI"},{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Most Creative Use of Reflex"}],"team_members":[],"built_with":[{"name":"intel-developer-cloud","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"together","url":null},{"name":"zeppos","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/MarcusUniversee/Cancer360"},{"label":"helpingcancerpatientswith.tech","url":"http://helpingcancerpatientswith.tech"}],"description_sections":[{"heading":"üí• - How it all started","content":"Cancer is a disease that has affected our team quite directly. All of our team members know a relative or loved one that has endured or lost their life due to cancer. This makes us incredibly passionate about wanting to improve cancer patient care. We identi fied a common thread of roadblocks that our loved ones went through during their journey through their diagnosis/treatment/etc:\n\nDiagnosis and Staging: Properly diagnosing the type and stage of cancer is essential for determining the most appropriate treatment plan. Treatment Options: There are many different types of cancer, and treatment options can vary widely. Selecting the most effective and appropriate treatment for an individual patient can be challenging. Multidisciplinary Care: Coordinating care among various healthcare professionals, including oncologists, surgeons, radiologists, nurses, and others, can be complex but is necessary for comprehensive cancer care. Communication: Effective communication among patients, their families, and healthcare providers is crucial for making informed decisions and ensuring the patient's needs and preferences are met."},{"heading":"üìñ - What it does","content":"We built Cancer360¬∞ to create a novel, multimodal approach towards detecting and predicting lung cancer. We synthesized four modes of data qualitative (think demographics, patient history), image (of lung CT scans), text (collected by an interactive chatbot), and physicical (via the ZeppOS Smartwatch) with deep learning frameworks and large language models to copmute a holistic metric for patient likelihood of lung cancer. Through this data-driven approach, we aim to address what we view as \"The BFSR\": The 'Big Four' of Surmountable Roadblocks:\n\nDiagnosis: Our diagnosis system is truly multimodal through our 4 modes: quantitative (uses risk factors, family history, demographics), qualitative (analysis of medical records like CT Scans), physical measurements (through our Zepp OS App), and our AI Nurse. Treatment Options: Our nurse can suggest multiple roadmaps of treatment options that patients could consider. For accessibility and ease of understanding, we created an equivalent to Google's featured snippets when our nurse mentions treatment options or types of treatment. Multidisciplinary Care: The way Cancer360¬∞ has been built is to be a digital aid that bridges the gaps with the automated and manual aspects of cancer treatment. Our system prompts patients to enter relevant information for our nurse to analyze and distribute what's important to healthcare professionals. Communication: This is a major need for patients and families in the road to recovery. Cancer360's AI nurse accomplishes this through our emotionally-sensitive responses and clear/instant communication with patients that input their information, vitals, and symptoms."},{"heading":"üîß - How we built it","content":"To build our Quantitative Mode, we used the following:\n\nnumpy : for general math and numpy.array Pandas : for data processing, storage SKLearn : for machine learning (train_test_split, classification_report) XGBoost : Extreme Boosting Trees Decision Trees\n\nTo build our Qualitative Mode, we used the following:\n\nOpenCV and PIL (Python Imaging Library): For Working With Image Data MatPlotLib and Seaborn : For Scientific Plotting Keras : Image Data Augmentation (think rotating and zooming in), Model Optimizations (Reduce Learning Rate On Plateau) Tensorflow : For the Convolutional Neural Network (CNN)\n\nTo build our AI Nurse, we used the following:\n\nTogether.ai: We built our chatbot with the Llama2 LLM API and used tree of thought prompt engineering to optimize our query responses\n\nTo build the portal, we used the following:\n\nReflex: We utilized the Reflex platform to build our entire frontend and backend, along with all interactive elements. We utilized front end components such as forms, buttons, progress bars, and more. More importantly, Reflex enabled us to directly integrate python-native applications like machine learning models from our quantitative and qualitative modes or our AI Nurse directly into the backend."},{"heading":"üìí - The Efficacy of our Models","content":"With Quantitative/Tabular Data:\n\nWe collected quantitative data for patient demographic, risk factors, and history (in the form of text, numbers, and binary (boolean values)). We used a simple keyword search algorithm to identify risk keywords like ‚ÄúSmoking‚Äù and ‚ÄúWheezing‚Äù to transform the text into quantitative data. Then we aggregated all data into a single Pandas dataframe and applied one-hot-encoding on categorical variables like gender. We then used SKLearn to create a 80-20 test split, and tested various models via the SKLearn library, including Logistic Regression, Random Forest, SVM, and K Nearest Neighbors. We found that ultimately, XGBoost performed best with the highest 98.39% accuracy within a reasonable 16-hour timeframe. Our training dataset was used in a research paper and can be accessed here. This high accuracy speaks to the reliability of our model. However, it's essential to remain vigilant against overfitting and conduct thorough validation to ensure its generalizability, a testament to our commitment to both performance and robustness.\n\nView our classification report here\n\nWith Image Data: Our solution is well-equipped to handle complex medical imaging tasks. Using data from the Iraq-Oncology Teaching Hospital/National Center for Cancer Diseases (IQ-OTH/NCCD) lung cancer dataset, and deep learning frameworks from tensorflow and keras, we were able to build a convolution neural network to classify patient CT scans as malignant or benign. Our convolutional neural network was fine-tuned for binary image classification of 512x512 RGB images, with multiple convolutional, max-pooling, normalization, and dense layers, compiled using the Adam optimizer and binary crossentropy loss. We also used OpenCV, PIL, Matplotlib, and Numpy to deliver a commendable 93% accuracy over a 20-hour timeframe. The utilization of dedicated hardware resources, such as Intel developer cloud with TensorFlow GPU, accelerates processing by 24 times compared to standard hardware. While this signifies our technological edge, it's important to acknowledge that image classification accuracy can vary based on data quality and diversity, making the 93% accuracy an achievement that underscores our commitment to delivering high-quality results.\n\nMalignant CT Scan\n\nBenign CT Scan\n\nAI Nurse: The AI Nurse powered by Together.ai and LLMs (such as Llama2) introduces an innovative approach to patient interaction and risk factor determination. Generating \"trees of thoughts\" showcases our ability to harness large language models for effective communication. Combining multiple AI models to determine risk factor percentages for lung cancer demonstrates our holistic approach to healthcare support. However, it's essential to acknowledge that the efficacy of this solution is contingent on the quality of language understanding, data processing, and the integration of AI models, reflecting our dedication to continuous improvement and fine-tuning."},{"heading":"üö© - Challenges we ran into","content":"Challenges we fixed: Loading our neural network model into the Reflex backend. After using keras to save the model as a ‚Äú.h5‚Äù extension, we were able to load and run the model locally on my jupyter notebook, however when we tried to load it in the Reflex backend, we kept getting a strange Adam optimizer build error. we tried everything: saving the model weights separately, using different file extensions like .keras, and even saving the model on as a .json file. Eventually, we realized that this was a known issue with m1/m2 macs and tensorflow Fixed the Get Started Button in Reflex header (Issue: button wouldn‚Äôt scale to match the text length) - Moved the button outside the inner hstack, but still the outer hstack Integrating together ai chatbot model into Reflex: A lot of our time was spent trying to get the integration working. Challenges we didn‚Äôt fix: Left aligning the AI response and right aligning the user input in the chatbot Fine tuning a second model to predict lung cancer rate from the chatbot responses from the first model - Could not get enough training data, too computationally taxing and few shot learning did not produce results Fixing bugs related to running a virtual javascript environment within Python via PyV8"},{"heading":"üèÜ - Accomplishments that we're proud of","content":"Going from idea generation to working prototype, with integration of 4 data modalities - Qualitative Mode, Quantitative Mode, and our AI Nurse, and Smartwatch Data, within the span of less than two days Integrating machine learning models and large language models within our application in a way that is directly accessible to users Learning a completely new web development framework (Reflex) from scratch without extensive documentation and ChatGPT knowledge Working seamlessly as a team and take advantage of the component-centered nature of Reflex to work independently and together"},{"heading":"üìù - What we learned","content":"Ameya: \"I was fortunate enough to learn a lot about frameworks like Reflex and Together.ai.\" Marcus: \"Using Reflex and learning its components to integrate backend and frontend seamlessly.\" Timothy: \"I realized how I could leverage Reflex, Intel Developer Cloud, Together.ai, and Zepp Health to empower me in developing with cutting edge technologies like LLMs and deep learning models.\" Alex: \"I learned a lot of front end development skills with Reflex that I otherwise wouldn‚Äôt have learned as a primarily back-end person.\""},{"heading":"‚úàÔ∏è - What's next for Cancer360¬∞","content":"Just like how a great trip has a great itinenary, we envision Cancer360¬∞ future plans in phases.\n\nPhase 1: Solidifying our Roots\n\nPhase 1 involves the following goals:\n\nRevamping our user interface to be more in-line with our mockups Increasing connectivity with healthcare professionals\n\nPhase 2: Branching Out\n\nView the gallery to see this. Phase 2 involves the following goals:\n\nCreating a mobile app for iOS and Android of this service Furthering development of our models to detect and analyze other types of cancers and create branches of approaches depending on the cancer Completing our integration of the physical tracker on Zepp OS\n\nPhase 3: Big Leagues\n\nPhase 3 involves the following goals:\n\nExpanding accessibility of the app through having our services be available in numerous different languages Working with healthcare institutions to further improve the usability of the suite"},{"heading":"üìã - Evaluator's Guide to Cancer360¬∞","content":"Intended for judges, however the viewing public is welcome to take a look.\n\nHey! We wanted to make this guide in order to help provide you further information on our implementations of certain programs and provide a more in-depth look to cater to both the viewing audience and evaluators like yourself.\n\nSponsor Services We Have Used This Hackathon\n\nReflex\n\nThe founders (Nikhil and Alex) were not only eager to assist but also highly receptive to our feedback, contributing significantly to our project's success.\n\nIn our project, we made extensive use of Reflex for various aspects:\n\nProject Organization and Hosting: We hosted our website on Reflex, utilizing their component-state filesystem for seamless project organization. Frontend: We relied on Reflex components to render everything visible on our website, encompassing graphics, buttons, forms, and more. Backend: Reflex states played a crucial role in our project by facilitating data storage and manipulation across our components. In this backend implementation, we seamlessly integrated our website features, including the chatbot, machine learning model, Zepp integration, and X-ray scan model.\n\nTogether AI\n\nIn our project, Together AI played a pivotal role in enhancing various aspects:\n\nCloud Service: We harnessed the robust capabilities of Together AI's cloud services to host, run, and fine-tune llama 2, a Large Language Model developed by META, featuring an impressive 70 billion parameters. To ensure seamless testing, we evaluated more than ten different chat and language models from various companies. This was made possible thanks to Together AI's commitment to hosting over 30 models on a single platform. Integration: We seamlessly integrated Together AI's feature set into our web app, combined with Reflex, to deliver a cohesive user experience. Tuning: Leveraging Together AI's user-friendly hyperparameter control and prompt engineering, we optimized our AI nurse model for peak performance. As a result, our AI nurse consistently generated the desired outputs at an accelerated rate, surpassing default performance levels, all without the need for extensive tuning or prompt engineering.\n\nIntel Developer Cloud\n\nOur project would not have been possible without the massive computing power of Intel cloud computers. For reference, here is the CNN training time on my local computer.\n\nAnd here is the CNN training time on my Intel¬Æ Xeon 4th Gen ¬Æ Scalable processor virtual compute environment and tensorflow GPU.\n\nA remarkable 20x Speedup! This huge leap in compute speed empowered by Intel¬Æ cloud computing enabled us to re-train our models with lightning speed as we worked to debugg and worked to integrate it into our backend. It also made fine-tuning our model much easier as we can tweak the hyperparameters and see their effects on model performance within the span of minutes.\n\nZepp Health\n\nWe utilized the ZeppOS API to query real-time user data for calories burned, fat burned, blood oxygen, and PAI (Personal Activity Index). We worked set up a PyV8 virtual javascript environment to run javascript code within Python to integrate the ZeppOS API into our application. Using collected data from the API, we used an ensemble algorithm to compute a health metric evaluating patient health, which ultimately feeds into our algorithm to find patient risk for lung cancer.\n\nGitHub\n\nWe used GitHub for our project by creating a GitHub repository to host our hackathon project's code. We also ensured that our use of GitHub stood out with a detailed ReadMe page, meaningful pull requests, and a collaboration history, showcasing our dedication to improving cancer patient care through Cancer360¬∞. We leveraged GitHub not only for code hosting but also as a platform to collaborate, push code, and receive feedback.\n\n.Tech Domains\n\nWe harnessed the potential of a .tech domain to visually embody our vision for Cancer360¬∞, taking a step beyond traditional domains. By registering the website helpingcancerpatientswith.tech, we not only conveyed our commitment to innovative technology but also made a memorable online statement that reflects our dedication to improving the lives of cancer patients."},{"heading":"Built With","content":"intel-developer-cloud python reflex together zeppos"},{"heading":"Try it out","content":"github.com helpingcancerpatientswith.tech"}]},{"project_title":"WorldDex","project_url":"https://devpost.com/software/worlddex-6diekg","tagline":"WorldDex is a real-life Pok√©dex: on our mobile app, you can scan and capture any object, talk to your collection, and share your experiences (using Computer Vision, LLMs, Web3, and more)!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/641/556/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - Hedera] Best Blockchain Project Using Hedera"},{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Web3 Hack"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"cockroachdb","url":null},{"name":"elevenlabs","url":null},{"name":"ethereum","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"gpt3.5","url":null},{"name":"gpt4","url":null},{"name":"hedera","url":null},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"ipfs","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"nft","url":null},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"reflex","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"web3.storage","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/antqin/WorldDex"}],"description_sections":[{"heading":"Inspiration","content":"As we go about our lives, all of us experience so much, witness so much, learn so much. When we were kids, exploration and excitement were a crucial part of every day, but as adults, responsibility can cause us to lose some of that magic. We wanted to remind each other that although we may not live in the Kanto region, the Earth still offers an enormous amount to see‚Äîand infinite amount to collect.\n\nSo, we built WorldDex. WorldDex is a real-life PokeDex (and so much more) that lets you scan and save anything into an ever-growing collection, offering a portal for you to learn, share, enjoy, and remember."},{"heading":"What it does","content":"WorldDex is primarily available through our mobile iOS application. Via this application, you can take a picture of any object and speak the name of what is being scanned into the interface. Be careful though! You will have a limited number of catch tries per day.\n\nWhether it's a simple lamp on your desk or a rare species of salamander, our app analyzes your image and spoken label, computing a catch probability which informs a roll of the dice. If you managed to catch your item, it is automatically added to your collection both in-app and on the blockchain. You also have the opportunity to record some information about where you were, who you were with, and any other details you're excited to include about your capture.\n\nWorldDex allows you to have a spoken or written conversation with your collection about the items you have caught. All you need is a microphone, a speaker, and an object; from there, our intelligent models can tell you all about your item. This puts the knowledge of the world's best LLMs at your fingertips, just one scan away from a detailed educational discussion about some of your favorite memories.\n\nOnce you have completed a scan, WorldDex creates an NFT of your image. No need to panic‚Äîwe are not crypto devs gearing for a rug-pull. Instead, the NFT means your ownership of that scan is provable and trade-able. Yes, someone can screenshot your NFT, but they will not be able to get it into their WorldDex!\n\nOn the community page, you can check out other people's scans and take a look at what your family, friends, and acquaintances have been collecting‚Äîin real time!\n\nYour WorldDex account also comes with access to our web application. There, you can access a more dynamic view of your collection. Coming soon to the web version: detailed analytics, rarities, trading, and more!"},{"heading":"How we built it","content":"The workhorse of our mobile application is written in Swift, with iOS native speech-to-text translation and tons of custom styling and design. The app interacts with a variety of powerful back-ends:\n\nOBJECT DETECTION, CAPTURE, AND PROBABILITY are handled by an AWS EC2 instance with a Deep Learning AMI running PyTorch 2.0 with an NVIDIA T4 GPU. The instance runs inference on a state-of-the-art pre-trained zero-shot object detection model called GroundingDINO (Liu et al., 2023).\n\nAssociated technology: GroundingDINO, AWS EC2, PyTorch, iOS speech-to-text, GPT-3.5 Turbo Instruct, GPT 4\n\nA flask server runs on the EC2 instance. When the user takes a scan and speaks into their phone, iOS speech-to-text transcribes the user's message and sends both the image and the user's provided information to the back-end. We use GPT-3.5 Turbo Instruct (instruct-tuned version of GPT exceptionally good at executing novel, precise instructions) to extract the object label within the user's spoken text. With some careful prompt engineering and in-context learning, the label extraction works like a charm even when given complex and long audio transcriptions. GroundingDINO is a pre-trained transformer-based object detection model that can predict on any natural language label (rather than only on labels it was trained on), which enables WorldDex users to collect anything they can dream of! GroundingDINO uses the label spoken by the user as a prompt to search for all instances of their desired object in the photo taken on their phone. It assigns these instances with a confidence score, and the instance with the max confidence score (logits) is bounded and scanned. In WorldDex, the success of a catch is a function of this confidence score. This bounding box is later used in the mobile app to crop a perfect thumbnail image for the object.\n\nDATA STORAGE AND BLOCKCHAIN INTERACTIONS are handled by an intricate JavaScript microservice architecture. This back-end uses Node.js and Express to route across a variety of on and off-chain services.\n\nAssociated Technology: Hedera, CockroachDB, Express, web3.storage, Ethereum, IPFS\n\nDatabase information (user account info, friends, image metadata, images, etc) are stored in a server-less CockroachDB instance. Through our endpoints, the back-end writes SQL queries to properly interact with the DB. When a catch is confirmed, the back-end uses the web3.storage framework to deploy the image onto IPFS for decentralized storage. Although a copy is maintained in our internal database for quick access, this grants the user the ability to share and interact with their image external to our centralized application. After a catch has been hosted by IPFS, our custom WLDD Token programmatically deployed on the open-source PoS Hedera network's testnet is used to mint an original NFT based on the image. This NFT contains in on-chain metadata a pointer to the IPFS metadataCID. The Hedera network offers straightforward and highly cost-effective functionality. Once the image has been stored on IPFS and an associated NFT has been minted, the unique ID we generate per image is granted to the user's collection on our custom Ethereum smart contract deployed on the Sepolia Testnet. Why the need for interactions with Ethereum AND Hedera? Our Ethereum smart contract allows for our own two-party trading functionality. However, we plan to deploy this EVM byte code to the Hedera network to simplify in the future.\n\nCHATTING WITH COLLECTION is handled by another API hosted on the same GPU-accelerated EC2 instance used by the models. When the user records an audio message in the mobile app, we use iOS speech to text to transcribe the message and send it to this API. The API uses GPT-4 to determine the response, with additional context retrieved from the database's image labels and metadata. Then, it uses the state-of-the-art text-to-speech model from ElevenLabs (with streaming functionality for minimized latency) to read out the model's response and hold a conversation with the user.\n\nAssociated Technology: GPT-4, ElevenLabs TTS, AWS EC2\n\nCOMMUNITY & PROFILE is built on the frontend using Swift and portrays your community's captures in real time. You can see people's captured items, their capture location, time of capture, probability of capture, and live audio transcription notes. Each user's captures are linked to their unique accounts, for which they can sign up for an account or log in on the profile page of the app. The frontend is linked to the CockroachDB database, making GET and POST requests to display community posts and manage creation of profiles.\n\nAssociated Technology: Swift, XCode, CockroachDB\n\nTHE WEB APPLICATION is built with Reflex and backed by the same JavaScript API and databases as the other front-end. Here, we display a more dynamic interface meant to be more nostalgic of opening the PokeDex upon logging in to your account. However, we also wanted to include a web app to give us flexibility for future extensions, with the ability to display detailed visualizations, analytics, trade, and more.\n\nAssociated Technology: Reflex\n\nWe know that's a lot of text and a lot of technologies. So:"},{"heading":"The TLDR is:","content":"Your image and label are sent to a Python back-end, which use a combination of models to determine if you actually correctly identified what you scanned. If you did (and get a bit lucky), your catch completes. At this point, we store a copy in a centralized CockroachDB server less database. We also store your image in a decentralized manner on IPFS, mint you an NFT on Hedera associated with the IPFS metadata, and register your ownership on Ethereum in case you want to start trading. The user can chat with an item from their collection. Their audio message is transcribed and sent to a back-end, which asks GPT-4 for a prompted response and plays this back with ElevenLabs Text-to-Speech for the user. Users can share their captures and see other's captures in the community page. Each user has a unique password-protected account with their captures that they can create/login to on the account page. There's also a Reflex web application, from which you can check out your collection upon logging in. In the future, it offers a great home for more complex analytics and displays.\n\nDOUBLE TAP!\n\nCheck out our Sepolia Ethereum Smart Contract (code in the GitHub Repo) governing trading. Check out what's being minted on our custom Hedera WLDD Token . Check out one of our favorite IPFS hosted images , which was recently collected and minted!"},{"heading":"Challenges we ran into","content":"HIGH LATENCY Our APIs are highly complex and interact with a variety of frameworks. This introduces latency and robustness issues. Specifically, on a successful catch, we have to:\n\nPerform speech-to-text with iOS built-in functionality Analyze text using fine-tuned GPT 3.5 Identify instances of an object type using GroundingDINO and rank them by probability, selecting the highest probability one and determining if a catch will happen Writing caught image and associated metadata to CockroachDB Deploying caught image to IPFS Minting NFT including IPFS metadata CID on Hedera Transferring NFT from treasury to user Logging catch onto Ethereum smart contract\n\nTo solve this problem, we divided this functionality into two portions: low-latency required and high-latency allowed. The user needs to know if they caught their object, which requires steps 1-3 to happen rapidly. To adjust, we used built-in iOS speech-to-text instead of a more general-purpose transcription model like the one offered by AssemblyAI (we tested it, but latency was too high). We used a fine-tuned version of GPT 3.5 Turbo Instruct, which is geared towards novel instruction types to reduce the performance tradeoff you get from avoiding the cost and time required by GPT-4. We also hosted on an AWS EC2 instance using a T4 GPU to speed up GroundingDINO computation.\n\nOnce the user has caught their item, we display a cached version of the image, allowing our back-end time to handle the higher-latency blockchain and database interactions.\n\nEXPLODING COMPLEXITY Our three back-ends threatened to create an exploding complexity problem which would have made debugging impossible. However, by siloing endpoints and routing related calls to more abstracted sections handling functionality, we were able to develop iteratively and minimize failure. We have:\n\nA set of independent endpoints handling Ethereum smart contract interactions A set of independent functions handling IPFS deployment A set of independent functions handling Hedera minting and transfer A set of endpoints wrapping IPFS deployment, Hedera interactions, and Ethereum calls into a single abstraction. A set of endpoints wrapping serverless CockroachDB functionality and caching.\n\nWe used a similar approach to carefully create an effective back-end for our AWS EC2 instance.\n\nINTUITIVE UI Building simple, intuitive UI for a social app with so much functionality and unique technologies was a big challenge. From figuring out the best flow of user experience to adding the smallest things that provided users with more information about what's happening behind the scenes (e.g. loading v.s. empty screen, flashing dot for speaking AI), we invested a lot of time in optimizing the human-computer interaction (HCI) and designing a novel and forward-looking interface that leverages large language models (LLMs)."},{"heading":"Accomplishments that we are proud of","content":"User Interface and Design We had a vision for our application‚Äîto interweave the joy of exploration with the extraordinary nature of everyday life. Although our mobile app, back-ends, and models are far from perfect, we are incredibly excited at just how fun it is for the four of us to scan random objects, talk to our collection, and share what we have discovered.\n\nIntegration of New Technologies We rapidly prototyped and iterated over a wide variety of new technologies. We knew just how extensible a real-life PokeDex could be; there were infinite directions to take! Using the sponsor list and our own field knowledge as inspiration, we tested out different APIs, frameworks, and models until we found the combination that matched our vision. Although this means we took an uncommon and often complex approach to our project, it has also resulted in some very cool functionality.\n\nPurposeful Incorporation of AI and Crypto We find using the blockchain or artificial intelligence simply because they exist to be a silly approach. Instead, as we read about different new technologies, we asked ourselves how their design aligns with our vision for WorldDex. We used a variety of state-of-the-art models for a plethora of functions, but nothing is casual; difference instances of GPT help to contextualize or explain, object-detection models... detect objects, text-to-speech models bring alive the PokeDex we remember from watching Cartoon Network as kids.\n\nMeanwhile, decentralization lets us put the collection in the hands of the users. If we could, the model would be running on the blockchain too (trust us - we looked into some ideas related to that). While NFTs and smart contracts allow for trading, monetization, and a market economy, they more fundamentally mean that when you collect a picture, it's verifiably yours . Hedera lets us do these mints at a remarkably low cost."},{"heading":"What we learned","content":"It is hard to describe how much we learned in such a short period of time! We are frankly shocked at how many different technologies and functionalities we were able to weave into the same project in less than two days.\n\nIntuitive UI, UX, and HCI We experimented a lot on our UI through user testing and feedback, especially in the more novel aspects of the app (e.g. LLM and voice integration). We learned about the value of simplicity but also the value of information, finding a balance between keeping the experience enjoyable and intuitive while providing the user with all the information they needed to fully leverage the app.\n\nModel Comparison Shopping We learned a ton about different state-of-the-art models and their relative tradeoffs. We had to select the best models to our latency, accuracy, cost, and storage requirements. While we all know a lot about machine learning and artificial intelligence in the academic environment, watching the impact of different real-world models on these crucial in-production outcomes was fascinating.\n\nCrypto and Blockchain Integrations It is becoming increasingly clear that while adoption of crypto nosedives, technology and infrastructure for the field is exploding. We are ridiculously excited at how efficiently new crypto protocols and chains are offering high levels of programmability at low costs without sacrificing core blockchain principles. While we might still be most comfortable in Solidity, the once-painful smart contract programming process has gotten so much easier.\n\nDatabases and Back-end Integrations It is remarkable how easy it is to get up and running on a database like CockroachDB, with direct SQL queries after minimal setup. High-performance back-ends interacting with a variety of different APIs and data storage solutions are increasingly viable every day. It was so much fun to construct one (or a couple) and watch them work in real-time.\n\nAnd finally, a lot of what we learned and are excited about happens in what we did not use. In the future, we want to spend more time exploring vector databases for RAG, increased privacy chains and crypto-native languages, and more when extending this project and others."},{"heading":"What's next for WorldDex","content":"We want to expand on the community. We imagine planned events in which scavenger hunts and collection events ask users to capture a certain subset of items in a set period or within a set geographical location. Imagine: collect the SF 100 Set of Items in 24 hours to win a prize! We want to build out the social side of the application. Being able to interact with other users, trade more robustly, and create a more open marketplace will allow for tons of fun and has cool implications for rarity and more. We want to improve the educational features. While being able to talk with an object is a feature we are really happy about, augmenting GPT's ability to explain facts about an item with a vector database would supercharge our app's ability to teach."},{"heading":"Built With","content":"amazon-web-services cockroachdb elevenlabs ethereum express.js gpt3.5 gpt4 hedera ios ipfs javascript llm nft pytorch reflex swift web3.storage"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MindScape: Neural Odyssey","project_url":"https://devpost.com/software/eeg-placeholder","tagline":"Because peace of mind is not a destination, it's a journey.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/463/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Intel Developer Cloud"}],"team_members":[],"built_with":[{"name":"cockroachdb","url":null},{"name":"eeg","url":null},{"name":"hume","url":null},{"name":"intel-developer-cloud","url":null},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"scipy","url":"https://devpost.com/software/built-with/scipy"},{"name":"ssh","url":null},{"name":"tensorflow","url":null},{"name":"together-ai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/assasin2gamer/CalHacks"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/17ahTTy1EFVmGAcDsX3hUma_ammdUM_Q84VL8W7gPkeA/edit#slide=id.p"},{"label":"github.com","url":"https://github.com/mishcoder/calhacks"}],"description_sections":[{"heading":"Inspiration","content":"Our project draws inspiration from the challenge of classifying emotions, a complex task. We aim to provide a reliable and cost-effective solution for training EEGs (Electroencephalograms), which can contribute to better understanding and analysis of emotions. We used a research grade EEG headset to get reliable data and do accurate sentiment analysis."},{"heading":"What it does","content":"Our project leverages machine learning to train on vast datasets of EEG data and Hume's audio analysis. We use this training to develop a sentiment analysis model specifically tailored to EEGs. EEGs capture brainwaves resulting from the neuro-physiological interactions in the brain. By employing techniques like Fast Fourier Transform (FFT) and random tree (RT) modeling of time series data, we can identify EEG characteristics associated with different emotions. Our model allows us to generate visually appealing images using TogetherAI's image generation service based on the emotion classification and even further!"},{"heading":"How we built it","content":"We built our product using three main components: 1) Hume for sentiment data labelling 2) CockroachDB serves as our database for storing EEG data. 3) Intel Cloud Compute to compute our model 4) TogetherAI to generate images based on the emotion classification. 5) Reflex to host our website which combines the multiple data streams into one websocket."},{"heading":"Challenges we ran into","content":"We ran into a few challenges: 1) We could not figure out why, but when we pinged spesifically the CockroachDB from the Intel Compute instance, the instance would freeze. 2) Our concept of using the model to be input sources for a VR game fell through when we realized the computer we brought could not handle the processing required."},{"heading":"Accomplishments that we're proud of","content":"We take pride in successfully integrating multiple services, including Intel's cloud computing resources, CockroachDB for data storage, and TogetherAI for image generation, to create a cohesive solution, leverage platform advantages to create a complete and scalable tech stack."},{"heading":"What's next for MindScape","content":"In the future, we plan to refine our model further and explore additional applications, such as incorporating it into a VR game or expanding our analysis capabilities for emotion classification."},{"heading":"We can read your mind","content":"The details: Using reverse referencing through bloom filters, we figured out a way to introduce entropy with specific feature matches and using advanced software and hardware noise reduction we can do something most teams cannot. Through EEG data timeseries we can estimate brain activity and through that, classify emotions among other brain activities."},{"heading":"Built With","content":"cockroachdb eeg hume intel-developer-cloud pandas python reflex scipy ssh tensorflow together-ai"},{"heading":"Try it out","content":"github.com docs.google.com github.com"}]},{"project_title":"Ivee","project_url":"https://devpost.com/software/ivee","tagline":"Forget typing. Stories are best spoken, speak it and we'll write it.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/646/494/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Intel Developer Cloud"}],"team_members":[],"built_with":[{"name":"adobe-illustrator","url":"https://devpost.com/software/built-with/adobe-illustrator"},{"name":"api","url":null},{"name":"apple","url":null},{"name":"dall-e","url":null},{"name":"deepgram","url":null},{"name":"elevenlabs","url":null},{"name":"embedding","url":null},{"name":"expo.io","url":"https://devpost.com/software/built-with/expo-io"},{"name":"figma","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"generation","url":null},{"name":"huggingface","url":null},{"name":"hume","url":null},{"name":"hume.ai","url":null},{"name":"intel","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"milvus","url":null},{"name":"multi-ligual","url":null},{"name":"no-framework)","url":null},{"name":"openai","url":null},{"name":"photoshop","url":"https://devpost.com/software/built-with/photoshop"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"reactnative","url":null},{"name":"styled-components-(fully-custom-styles","url":null},{"name":"styledcomponents","url":null},{"name":"v2","url":null},{"name":"zilliz","url":null}],"external_links":[{"label":"github.com","url":"http://github.com/KaluJo/Memoir"}],"description_sections":[{"heading":"Inspiration","content":"While video-calling his grandmother, Tianyun was captivated by her nostalgic tales from her youth in China. It struck him how many of these cherished stories, rich in culture and emotion, remain untold or fade away as time progresses due to barriers like time constraints, lack of documentation, and the predominance of oral traditions.\n\nFor many people, however, it can be challenging to find time to hear stories from their elders. Along with limited documentation, and accessibility issues, many of these stories are getting lost as time passes.\n\nWe believe these stories are valuable and deserve to be heard. That‚Äôs why we created a tool that provides people with a dedicated team to help preserve these stories and legacies."},{"heading":"What it does","content":"Forget typing. Embrace voice. Our platform boasts a state-of-the-art Speech-To-Text interface. Leveraging cutting-edge LLM models combined with robust cloud infrastructures, we ensure swift and precise transcription. Whether your narration follows a structured storyline or meanders like a river, our bot Ivee skillfully crafts it into a beautiful, funny, or dramatic memoir."},{"heading":"How we built it","content":"Our initial step was an in-depth two-hour user experience research session. After crafting user personas, we identified our target audience: those who yearn to be acknowledged and remembered.\n\nThe next phase involved rapid setup and library installations. The team then split: the backend engineer dived into fine-tuning custom language models and optimizing database frameworks, the frontend designer focused on user authentication, navigation, and overall app structure, and the design team commenced the meticulous work of wireframing and conceptualization.\n\nAfter an intense 35-hour development sprint, Ivee came to life. The designers brought to life a theme of nature into the application, symbolizing each story as a leaf, a life's collective memories as trees, and cultural groves of forests. The frontend squad meticulously sculpted an immersive onboarding journey, integrating seamless interactions with the backend, and spotlighting the TTS and STT features. Meanwhile, the backend experts integrated technologies from our esteemed sponsors: Hume.ai, Intel Developer Cloud, and Zilliz Vector Database.\n\nOur initial segregation into Frontend, Design, Marketing, and Backend teams soon blurred as we realized the essence of collaboration. Every decision, every tweak was a collective effort, echoing the voice of the entire team."},{"heading":"Challenges we ran into","content":"Our foremost challenge was crafting an interface that exuded warmth, empathy, and familiarity, yet was technologically advanced. Through interactions with our relatives, we discovered overall negative sentiment toward AI, often stemming from dystopian portrayals in movies."},{"heading":"Accomplishments that we're proud of","content":"Our eureka moment was when we successfully demystified AI for our primary users. By employing intuitive metaphors and a user-centric design, we transformed AI from a daunting entity to an amiable ally. The intricate detailing in our design, the custom assets & themes, solving the challenges of optimizing 8 different APIs, and designing an intuitive & accessible onboarding experience are all highlights of our creativity."},{"heading":"What we learned","content":"Our journey underscored the true value of user-centric design. Conventional design principles had to be recalibrated to resonate with our unique user base. We created an AI tool to empower humanity, to help inspire, share, and preserve stories, not just write them. It was a profound lesson in accessibility and the art of placing users at the heart of every design choice."},{"heading":"What's next for Ivee","content":"The goal for Ivee was always to preserve important memories and moments in people's lives. Below are some really exciting features that our team would love to implement:\n\nReinforcement Learning on responses to fit your narration style Rust to make everything faster Multimodal storytelling. We want to include clips of the most emotion-fueled clips, on top of the stylized and colour-coded text, we want to revolutionize the way we interact with stories. Custom handwriting for memoirs Use your voice and read your story in your voice using custom voices\n\nIn the future, we hope to implement additional features like photos and videos, as well as sharing features to help families and communities grow forests together."},{"heading":"Built With","content":"adobe-illustrator api apple dall-e deepgram elevenlabs embedding expo.io figma firebase flask generation huggingface hume hume.ai intel javascript milvus multi-ligual no-framework) openai photoshop python react react-native reactnative styled-components-(fully-custom-styles styledcomponents v2 zilliz"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Blik.","project_url":"https://devpost.com/software/blik","tagline":"The ‚ú®Perfect‚ú® AI companion for your Cryptocurrency trading needs. Forecast token values, ask chatbot for a personalized investment plan.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/642/063/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of MindsDB"}],"team_members":[],"built_with":[{"name":"cohere","url":null},{"name":"fastapi","url":null},{"name":"flutter","url":"https://devpost.com/software/built-with/flutter"},{"name":"mindsdb","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/dhanush17-tech/blik"},{"label":"github.com","url":"https://github.com/dhravya/blik"}],"description_sections":[{"heading":"Inspiration","content":"Over the last five years, we've seen the rise and the slow decline of the crypto market. It has made some people richer, and many have suffered because of it. We realized that this problem can be solved with data and machine learning - What if we can, accurately, predict forecast for crypto tokens so that the decisions are always calculated? What if we also include a chatbot to it - so that crypto is a lot less overwhelming for the users?"},{"heading":"What it does","content":"Blik is an app and a machine learning model, made using MindsDB, that forecasts cryptocurrency data. Not only that, but it also comes with a chatbot that you can talk to, to make calculated decisions for your. Next trades. The questions can be as simple as \"How's bitcoin been this year?\" to something as personal as \"I want to buy a tesla worth $50,000 by the end of next year. My salary is 4000$ per month. Which currency should I invest in?\" We believe that this functionality can help the users make proper, calculated decisions into what they want to invest in. And in return, get high returns for their hard-earned money!"},{"heading":"How we built it","content":"Our tech stack includes:\n\nFlutter for the mobile app MindsDB for the ML model + real time finetuning Cohere for AI model and NLP from user input Python backend to interact with MindsDB and CohereAI FastAPI to connect frontend and backend. Kaggle to source the datasets of historic crypto prices"},{"heading":"Challenges we ran into","content":"We started off using the default model training using MindsDB, however, we realized that we would need many specific things like forecasting at specific dates, with a higher horizon etc. The mentors at the MindsDB counter helped us a real lot. With their help, we were able to set up a working prototype and were getting confident about our plan.\n\nOne more challenge we ran into was that the forecasts for a particular crypto would always end up spitting the same numbers, making it difficult for users to predict Then, we ended up using the NeuralTS as our engine, which was perfect. Getting the forecasts to be as accurate as possible was definitely a challenge for us, while keeping it performant enough. Solving every small issue would give rise to another one; but thanks to the mentors and the amazing documentations, we were able to figure out the MindsDB part.\n\nThen, we were trying to implement the AI chat feature, using CohereAI. We had a great experience with the API as it was easy to use, and the chat completions were also really good. We wanted the generated data from Cohere to generate an SQL query to use on MindsDB. Getting this right was challenging, as I'd always need the same datatype in a structured format in order to be able to stitch an SQL command. We figured this also out using advanced prompting techniques and changing the way we pass the data into the SQL. We also used some code to clean up the generated text and make sure that its always compatible."},{"heading":"Accomplishments that we're proud of","content":"Honestly, going from an early ideation phase to an entire product in just two days, for an indie team of two college freshmen is really a moment of pride. We created a fully working product with an AI chatbot, etc. Even though we were both new to all of this - integrating crypto with AI techologies is a challenging problem, and thankfully MindsDB was very fun to work with. We are extremely happy about the mindsDB learnings as we can now implement it in our other projects to enhance them with machine learning."},{"heading":"What we learned","content":"We learnt AI and machine learning, using MindsDB, interacting with AI and advanced prompting, understanding user's needs, designing beautiful apps and presenting data in a useful yet beautiful way in the app."},{"heading":"What's next for Blik.","content":"At Blik, long term, we plan on expanding this to a full fledged crypto trading solution, where users can sign up and create automations that they can run, to \"get rich quick\". Short term, we plan to increase the model's accuracy by aggregating news into it, along with the cryptocurrency information like the founder information and the market ownership of the currency. All this data can help us further develop the model to be more accurate and helpful."},{"heading":"Built With","content":"cohere fastapi flutter mindsdb python"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"EasyPC","project_url":"https://devpost.com/software/easypc","tagline":"Your first stop when PC building, featuring a state-of-the-art PC-Parts AI refined and trained over valuable data.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/699/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of MindsDB"}],"team_members":[],"built_with":[{"name":"cockroachdb","url":null},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mindsdb","url":null},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"react-three-fiber","url":null},{"name":"selenium","url":"https://devpost.com/software/built-with/selenium"},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"},{"name":"together.ai","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/TimTimich6/easypc"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration all stems from one of our users terrible experiences building a PC. Building a PC can be an incredibly rewarding but also frustrating experience, especially for first-time builders. Our team member had a particularly disastrous PC build - incorrect parts that weren't compatible, unstable overclocking leading to crashes, and way over budget. It was a valuable learning experience but very costly in both time and money. We realized there had to be a better way than trial and error for PC novices to configure and buy the right components for their needs and budget. Our project aims to leverage AI and data to match users with PC parts tailored to their individual use case, whether that's high-end gaming, media editing, programming, etc."},{"heading":"What it does","content":"The website takes in a user's custom PC build needs and budget, then gets to work finding the optimal parts combination. Users simply describe what they plan to use the computer for, such as high-end 3D editing under $2,000. Within seconds, the website outputs a complete part list with prices, specs, and descriptions tailored to the user's intended workload."},{"heading":"How we built it","content":"Utilizing a Convex fullstack, we looked for cutting edge, pioneering softwares to help propel our project. We landed on utilizing together.ai to base our main AI system. Our fined-tuned llama-7b was trained on countless data-points, and works to create accurate and timely recommendations. Going further down, we used a secondary AI in MindsDB for bulk jobs to generate accurate descriptions of Pc Parts. We pulled from a scraped database in"},{"heading":"Challenges we ran into","content":"Along the way, there were many challenges. One included running on only caffeine, but that was deemed worth the trouble knowing the amazing project we built. On a more technical level, as the technologies we planned on using were newer, there wasnt that large of a network of integrations. To combat this, we produced our own implementations. Specifically for a MindsDB integration for together.ai. To further the usefulness of together.ai, we also created an integration for Discord. Furthermore, obtaining data was a monumental obstacle. As a group of 4 without much capital, we had to create our own legal web scrapping tools. We ran into countless issues but eventually created a capable web scrapping tool to gather publicly available data to train our model on. Eventually, we intend to invest into purchasing data from large PC Parts databases to finalize and secure data."},{"heading":"Accomplishments that we're proud of","content":"We are definitely proud of the project we built, keeping in mind we are not all trained and seasoned hackathon veterans. More specifically, the revolutionary integrations are definitely a high-point for our project. Coming from knowing nothing about integrations, LLM creation and ethical data-scraping, we now know how to implement these systems in the future. And even when we would get frustrated, we always encouraged and pushed eachother forward in new and creative ways."},{"heading":"What we learned","content":"We learned that even if we start from a basic understanding of how LLMs AIs and databases work, through passion and hard work we can become experts in this field."},{"heading":"What's next for EasyPC","content":"Scaling this project will be easy. With an already fully-functioning AI system, the possibilites would be endless. We can keep feeding it more data, and we plan on implementing a text-feature where you could ask a fined-tuned LLM on any pc related questions."},{"heading":"Built With","content":"cockroachdb css html javascript mindsdb postgresql python react react-three-fiber selenium three.js together.ai typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TuneAI","project_url":"https://devpost.com/software/tuneai","tagline":"Elevate Your Visuals with TuneAI: Seamless Soundscapes for Content Creators, Filmmakers, and Videographers. Simply upload a video, and TuneAI will give you the perfect musical accompaniment.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/644/528/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Intel Developer Cloud"}],"team_members":[],"built_with":[{"name":"blip","url":null},{"name":"figma","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"intel-cloud","url":null},{"name":"musicgen","url":null},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"socket.io","url":"https://devpost.com/software/built-with/socket-io"},{"name":"vercel","url":null}],"external_links":[{"label":"tune-ai.vercel.app","url":"https://tune-ai.vercel.app/"},{"label":"github.com","url":"https://github.com/ryanmckim/CalHacks2023/"},{"label":"www.figma.com","url":"https://www.figma.com/file/ZwCafop3tp0uWvukzrUuoO/VidTune?type=design&node-id=0%3A1&mode=design&t=0cyQMgJbSAyZrpUF-1"}],"description_sections":[{"heading":"Inspiration","content":"TuneAI was born out of a deep appreciation for the art of visual storytelling and the recognition that audio plays in enhancing the impact of videos. Our motivation to create TuneAI lies in the desire to provide content creators, filmmakers, and videographers with a powerful tool that simplifies the process of generating background audio and sound effects, making it more accessible and efficient than ever before."},{"heading":"What it does","content":"TuneAI is an intelligent visual content to audio generator tool. We provide users with access to generative AI technology to create unique musical samples based on the context of their images or videos. Our model analyzes the video for its mood, tone, objects, scenery, and a variety of other contextual elements, and generates music based on a suitable genre, instrumentation, style, etc. Our music is sure to inspire you!"},{"heading":"How we built it","content":"After the video file is uploaded, we splice the video into multiple frames which are then analyzed by Salesforce BLIP image-to-text model, which gives us a list of image descriptors. These are then passed on to OpenAI GPT, which takes the image descriptors and comes up with the musical prompt, which is fed into Meta's MusicGen. The audio generated by MusicGen can be edited to fit the video. This whole pipeline is squashed into a function which is served on the backend (Flask). We used React to build our frontend and deployed on Vercel. Intel Cloud Max Series GPU allows us to perform inferencing at speed, allowing us to generate the audio in reasonable time."},{"heading":"Challenges we ran into","content":"Complex tasks: how do we ... do video understanding? ... translate that understanding into a type of music? ... how do we define or generate music? We answered all of these questions after a thorough investigation of current methods, models, and applications. Model selection: Making decisions to select the best models for our choice of architecture and design constraints (compute speed and memory and accuracy tradeoffs). Attempts to fine tune our models: We are venturing into rather new territory in the generative AI space for videos and audio. There are limited datasets that would allow us to tune (Example: In targeting our model for short form content, we wanted to use Tiktok videos to fine-tune the BLIP model. However, we cannot easily analyze sentiment or understand moods with a \"describe what you see\" type of model, especially since Tiktok present a lot of social trends. Video descriptors and metadata also do not provide much context. This proved to be unreliable upon testing and set us back initially.)\n\nA second challenge was to figure out a way to increase the speed of inference, by setting up our environment in Intel Cloud as none of us had deep infrastructure experience . A huge shout-out to Rahul from Intel who helped us through some parts of the cloud setup, allowing us to use Max Series GPUs and 4th gen Xeon Processors for model training and inference."},{"heading":"Accomplishments that we're proud of","content":"Producing the first video-to-audio GenAI model pipeline in under 3 days. Overcoming various technical and non-technical obstacles along the way (overloaded network, laptop memory limit, little sleep) to produce a fully functioning web application which content creators, videographers and filmmakers can now use for audio inspiration."},{"heading":"What we learned","content":"State of the art generative AI Picked up new tech stack and skills: first time using Flask and websockets, experimented OpenAI prompt generation, learned about Transformer models. Quick problem solving, iterating, and pivoting if blocked."},{"heading":"What's next for TuneAI","content":"Accept longer form content, which would require access to more powerful compute (i.e. Intel Developer Cloud). Allow users to control the music generation process, providing additional context or specifications to their use cases. Fine tune the captioning model to understand context and moods for more agreeable generations. Speed up inference process by optimizing prompt generation."},{"heading":"Built With","content":"blip figma flask intel-cloud musicgen openai react socket.io vercel"},{"heading":"Try it out","content":"tune-ai.vercel.app github.com www.figma.com"}]},{"project_title":"Health Insurance 4 Dummies","project_url":"https://devpost.com/software/health-insurance-for-dummys","tagline":"Predicts and Explain health insurance costs on personal info.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/646/081/datas/medium.gif","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of InterSystems IntegratedML"}],"team_members":[],"built_with":[{"name":"convex","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"intersystem","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"},{"name":"together.ai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/jjin43/CalHack2023"}],"description_sections":[{"heading":"Inspiration üí™üèº","content":"Health insurance, everyone needs it, no one wants to pay for it. As soon-will-be adults, health insurance has been a growing concern. Since a simple ambulance ride easily costs up to thousands of dollars, not having health insurance is a terrible decision in the US. But how much are you supposed to pay for it? Insurance companies publish their rates, but just having formulas doesn't tell me anything about if they are ripping me off, especially for young adults having never paid for health insurance."},{"heading":"What it does? üîç","content":"Thus, to prevent being ripped off on health insurance after leaving our parents' household. We have developed Health Insurance 4 Dummies. A website utilizing a machine learning model that determines a fair estimate for the annual costs of health insurance, based on user inputs of their personal information. It also uses a LMM to provide detailed information on the composition of the cost."},{"heading":"How we built it üë∑üèº‚Äç‚ôÄÔ∏è","content":"The front-end is built using convex-react, creating an UI that takes inputs from the user. The backend is built using python-flask, which communicates with remote services, InterSystems and Together.AI. The ML model is a feed-forward neural network (MLP) for predicting the cost is built on InterSystems using the H2O ML workflow library, trained on a dataset consist of individual's information and their annual rate for health insurance. The explanation of costs is created using Together.AI's Llama-2 model."},{"heading":"Challenges we ran into üî®","content":"Full-stack development is tedious, especially when the functions require remote resources. Finding good datasets to train the model. Authentication in connecting and accessing the trained model on InterSystem using their IRIS connection driver. Choosing the right model to use from Together.AI."},{"heading":"Accomplishments that we're proud of ‚≠ê","content":"Trained and accessed ML model on a remote database open possibility for massive datasets, integrating LMMs to provide automated information."},{"heading":"What we learned üìñ","content":"Full-Stack Development skills, ML model training and utilizing. Accessing remote services using APIs, TLS authentication."},{"heading":"What's next for Health Insurance 4 Dummys üîÆ","content":"Gather larger datasets to make more parameters available and give more accurate predictions."},{"heading":"Built With","content":"convex flask intersystem javascript python react sql together.ai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Groundhog","project_url":"https://devpost.com/software/groundhog-jrnux5","tagline":"Using Together AI and Reflex we developed a auto generated text based adventure game for users to play. If you ever don't like an outcome of a choice you can go back in time to replay your own actions","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/141/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Together AI"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"togetherai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pnavab/calhacks2023"}],"description_sections":[{"heading":"Inspiration","content":"We love spending time playing role based games as well as chatting with AI, so we figured a great app idea would be to combine the two."},{"heading":"What it does","content":"Creates a fun and interactive AI powered story game where you control the story and the AI continues it for as long as you want to play. If you ever don't like where the story is going, simply double click the last point you want to travel back to and restart from there! (Just like in Groundhog Day)"},{"heading":"How we built it","content":"We used Reflex as the full-stack Python framework to develop an aesthetic frontend as well as a robust backend. We implemented 2 of TogetherAI's models to add the main functionality of our web application."},{"heading":"Challenges we ran into","content":"From the beginning, we were unsure of the best tech stack to use since it was most members' first hackathon. After settling on using Reflex, there were various bugs that we were able to resolve by collaborating with the Reflex co-founder and employee on site."},{"heading":"Accomplishments that we're proud of","content":"All our members are inexperienced in UI/UX and frontend design, especially when using an unfamiliar framework. However, we were able to figure it out by reading the documentation and peer programming. We were also proud of optimizing all our background processes by using Reflex's asynchronous background tasks, which sped up our website API calls and overall created a much better user experience."},{"heading":"What we learned","content":"We learned an entirely new but very interesting tech stack, since we had never even heard of using Python as a frontend language. We also learned about the value and struggles that go into creating a user friendly web app we were happy with in such a short amount of time."},{"heading":"What's next for Groundhog","content":"More features are in planning, such as allowing multiple users to connect across the internet and roleplay on a single story as different characters. We hope to continue optimizing the speeds of our background processes in order to make the user experience seamless."},{"heading":"Built With","content":"python reflex togetherai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CareerChain","project_url":"https://devpost.com/software/careerchain","tagline":"My platform is the future of hiring, leveraging blockchain and cryptography to solve major inefficiencies in the recruitment process.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/644/684/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of XRP Ledger"}],"team_members":[],"built_with":[{"name":"blockchain","url":"https://devpost.com/software/built-with/blockchain"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"next","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"ripple","url":"https://devpost.com/software/built-with/ripple"},{"name":"web3","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ShaneYokota72/CareerChain"}],"description_sections":[{"heading":"Inspiration","content":"My recent job application experience with a small company opened my eyes to the hiring challenges faced by recruiters. After taking time to thoughtfully evaluate each candidate, they explained how even a single bad hire wastes significant resources for small teams. This made me realize the need for a better system that saves time and reduces stress for both applicants and hiring teams. That sparked the idea for CareerChain."},{"heading":"What it does","content":"CareerChain allows job seekers and recruiters to create verified profiles on our blockchain-based platform.\n\nFor applicants, we use a microtransaction system similar to rental deposits or airport carts. A small fee is required to submit each application, refunded when checking status later. This adds friction against mass spam applications, ensuring only serious, passionate candidates apply.\n\nFor recruiters, our AI prescreens applicants, filtering out unqualified candidates. This reduces time wasted on low-quality applications, allowing teams to focus on best fits. Verified profiles also prevent fraud.\n\nBy addressing inefficiencies for both sides, CareerChain streamlines hiring through emerging technologies."},{"heading":"How I built it","content":"I built CareerChain using:\n\nXRP Ledger for blockchain transactions and smart contracts Node.js and Express for the backend REST API Next.js framework for the frontend"},{"heading":"Challenges we ran into","content":"Implementing blockchain was challenging as it was my first time building on the technology. Learning the XRP Ledger and wiring up the components took significant learning and troubleshooting."},{"heading":"Accomplishments that I'm proud of","content":"I'm proud to have gained hands-on blockchain experience and built a working prototype leveraging these cutting-edge technologies."},{"heading":"What I learned","content":"I learned so much about blockchain capabilities and got exposure to innovative tools from sponsors. The hacking experience really expanded my skills."},{"heading":"What's next for CareerChain","content":"Enhancing fraud detection, improving the microtransaction UX, and exploring integrations like background checks to further optimize hiring efficiency."},{"heading":"Built With","content":"blockchain express.js github next node.js react ripple web3"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Skill Chain","project_url":"https://devpost.com/software/skill-chain","tagline":"Skill Chain aims to redefine the job application process, making it more secure, transparent, and efficient for both job seekers and employers.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/643/382/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of XRP Ledger"}],"team_members":[],"built_with":[{"name":"angular.js","url":"https://devpost.com/software/built-with/angular-js"},{"name":"java","url":"https://devpost.com/software/built-with/java"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"redis","url":"https://devpost.com/software/built-with/redis"},{"name":"ripple","url":"https://devpost.com/software/built-with/ripple"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/Yash-kumar7/CalHacks2023"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Skill Chain came from the desire to create a more efficient and serious job application process. We wanted to reduce spam and ensure that applicants are genuinely interested in the positions they apply for. The concept is similar to Tinder, but with a twist - it‚Äôs for job applications!"},{"heading":"What it does","content":"Skill Chain is a blockchain-based platform where users pay to apply for jobs. The application fee is automatically deducted when a user decides to apply for a job. If the applicant is not selected (left-swiped by the recruiter), the money is refunded. This innovative approach reduces spam and ensures that only serious candidates apply."},{"heading":"How we built it","content":"We built Skill Chain using the XRP Ledger for blockchain transactions and smart contracts, ensuring a secure and transparent process. The backend REST API was developed using Java Spring Boot, providing robust and scalable server-side software. For the frontend, we used the Angular framework to create a user-friendly interface."},{"heading":"Challenges we ran into","content":"Implementing the blockchain transactions and smart contracts on the XRP Ledger was a significant challenge due to its complexity. Additionally, integrating the frontend with the backend while ensuring data consistency and security was also a hurdle we had to overcome."},{"heading":"Accomplishments that we're proud of","content":"We‚Äôre proud of developing a unique solution that addresses a real-world problem. The successful integration of different technologies (blockchain, backend, and frontend) to create a cohesive platform is a significant achievement for us."},{"heading":"What we learned","content":"Through this project, we learned about the practical implementation of blockchain technology and smart contracts. We also gained experience in backend and frontend development, and how to integrate them effectively."},{"heading":"What's next for Skill Chain","content":"The next step for Skill Chain is to incorporate more advanced features, such as AI-based matching of applicants to jobs. We also plan to expand our user base and collaborate with more companies to offer a wider range of job opportunities."},{"heading":"Built With","content":"angular.js java mongodb redis ripple typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"AtlasAI","project_url":"https://devpost.com/software/atlasai","tagline":"Take back control of your lives with our data-supported, individualized solutions!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/643/803/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Convex"}],"team_members":[],"built_with":[{"name":"convex","url":null},{"name":"materialui","url":null},{"name":"nextjs","url":null},{"name":"spotifyapi","url":null},{"name":"tailwind","url":null},{"name":"terraapi","url":null},{"name":"togetherai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/TJun-Jie/AtlasAI-"}],"description_sections":[{"heading":"Inspiration","content":"In today's fast-paced world, highly driven individuals often overwork themselves without regard for how it impacts their health, only experiencing the consequences when it is too late . AtlasAI aims to bring attention to these health issues at an early stage, such that our users are empowered to live their best lives in a way that does not negatively impact their health."},{"heading":"What it does","content":"We realized that there exists a gap between today's abundance of wearable health data and meaningful, individualized solutions which users can implement. For example, many smart watches today are saturated with metrics such as sleep scores and heart rate variability , many of which actually mean nothing to their users in practice. Therefore, AtlasAI aims to bridge this gap to finally empower our users to use this health data to enhance the quality of their lives.\n\nUsing our users' individual health data, AtlasAI is able to:\n\nsuggest event rescheduling provide targeted , actionable feedback recommend Spotify playlists depending on user mood"},{"heading":"How we built it","content":"Our frontend was built with NextJS , with styling from Tailwind and MaterialUI .\n\nOur backend was built with Convex , which integrates technologies from TerraAPI , TogetherAI and SpotifyAPI .\n\nWe used a two-phase approach to fine-tune our model. First, we utilized TogetherAI's base models to generate test data (a list of rescheduled JSON event objects for the day). Then, we picked logically sound examples to fine-tune our model."},{"heading":"Challenges we ran into","content":"In the beginning, our progress was extremely slow as AtlasAI integrates so many new technologies. We only had prior experience with NextJS , Tailwind and MaterialUI , which essentially meant that we had to learn how to create our entire backend from scratch.\n\nAtlasAI also went through many integrations throughout this weekend as we strove to provide the best recommendations for our users. This involved long hours spent in fine-tuning our TogetherAI models and testing out features until we were satisfied with our product."},{"heading":"Accomplishments that we're proud of","content":"We are extremely proud that we managed to integrate so many new technologies into AtlasAI over the course of three short days."},{"heading":"What we learned","content":"In the development realm, we successfully mastered the integration of several valuable third-party applications such as Convex and TogetherAI. This expertise significantly accelerated our ability to construct lightweight prototypes that accurately embody our vision. Furthermore, we honed our collaborative skills through engaging in sprint cycles and employing agile methodologies, which collectively enhanced our efficiency and expedited our workflow."},{"heading":"What's next for AtlasAI","content":"Research indicates that health data can reveal critical insights into health symptoms like depression and anxiety. Our goal is to delve deeper into leveraging this data to furnish enhanced health insights as proactive measures against potential health ailments. Additionally, we aim to refine lifestyle recommendations for the user's calendar to foster better recuperation."},{"heading":"Built With","content":"convex materialui nextjs spotifyapi tailwind terraapi togetherai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MultiMed Vision+","project_url":"https://devpost.com/software/nutrisense","tagline":"A cutting-edge healthcare app designed for generational wellness, harnessing the power of multi-modal LLMs to offer personalized AI-driven health management.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/647/878/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of InterSystems IntegratedML"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"azure-ai","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"intersystems","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"next.js","url":null},{"name":"ocr","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react.js","url":null},{"name":"sql","url":"https://devpost.com/software/built-with/sql"},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/SaitejaKa/JS-CalHacks-Fa23"}],"description_sections":[{"heading":"Inspiration","content":"This project came to us when one of our teammates mentioned his grandma struggling to keep track of her cholesterol and the medications she was taking to lower it. We realized that to help alleviate this, we would need to approach the problem from both sides.\n\nHigh blood cholesterol causes 4.4 million deaths each year (World Heart Federation, 2019), and other nutrient deficiencies and surplusses take many more. By leveraging new multimodal LLMs, we set out to solve this complex and multi-faceted problem."},{"heading":"What it does","content":"MultiMed Vision+ allows users to track both their medications and nutrients with the snap of a picture. It then uses the information scanned by the user to generate advice in the context of the user's current health situation. MultiMed Vision+ integrates with our Raspberry Pi \"watch\", desktop app, and mobile app, easing access and user-friendliness for this demographic.\n\nThe project comprises several key components, including:\n\nIntegration of Prescription and Nutrition Data: Incorporating scanned prescriptions and food items to provide personalized recommendations based on individual health contexts. Analyzing prescription data to offer tailored health advice and reminders related to medication adherence.\n\nSmartwatch Integration: Facilitating easy access to health data without the need for a smartphone. Streamlining the monitoring of vital health indicators for elderly individuals.\n\nUser-Friendly Interface: Designing an intuitive and straightforward interface specifically tailored to the needs of older users. Offering clear and concise advice on dietary choices and providing real-time health monitoring.\n\nReal-Time Sensor Data Analysis: Utilizing machine learning models integrated with real-time sensor data to predict the risk of heart attacks. Providing timely alerts and notifications to both the user and their family members, enabling proactive health management and intervention."},{"heading":"How we built it","content":"Our project is split into a frontend and a backend stack. Our front end includes all of our UI/UX designs, and it utilizes Next.js + Typescript to build a UI design. It has authentication from Firebase + Clerk.js, and uses Tailwind CSS for styling. In the backend, we have our machine learning pipeline in Python as well as our API routes through FastAPI. We utilized OpenAI, Azure AI, Hugging Face, and Intersystems IntegratedML."},{"heading":"Challenges we ran into","content":"One of the main issues we ran into was understanding and integrating InterSystems into our product. Since it was the first time our entire team was working with the IntegratedML tool, we had to spend quite a bit of time debugging and reading tool documentation to understand how we could implement this pipeline."},{"heading":"Accomplishments that we're proud of","content":"We were able to simulate a real-life scenario where a user could scan their prescription and dishes from either their smartwatch (simulated through a raspberry pi) or mobile phone seamlessly. The integration of our tool into a wearable device allows a user to go about their entire day while also keeping track of their health in just a few seconds. We were able to hyper-personalize our context window and integrate an ML model so that our tool could give reliable insights to users based on their pre-existing conditions and eating habits."},{"heading":"What we learned","content":"We learned a lot about integrations with different systems and models. Specifically, we learn how to use InterSystems as well as integrations with Raspberry Pi."},{"heading":"What's next for MultiMed Vision+","content":"The next step for MultiMed Vision is to launch this idea fully, expanding our vast data sources and improving on our hardware + software systems. Moreover, we are looking to consider expanding our platform that can tell us more about where our food came form, such as where it's being produced/processed. We could potentially integrate with blockchain technology by briding the real world with web&web3."},{"heading":"Built With","content":"amazon-web-services azure-ai firebase intersystems javascript llm next.js ocr python react.js sql tailwind typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"LifeLine Aid","project_url":"https://devpost.com/software/lifeline-aid","tagline":"LifeLine Aid: A beacon for refugees in conflict zones. Instantly see nearby food, shelter, and aid. Get alerts on regional hazards. Direct links to humanitarian agencies. Hope in chaos.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/717/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of CockroachDB Serverless"}],"team_members":[],"built_with":[{"name":"android-studio","url":"https://devpost.com/software/built-with/android-studio"},{"name":"asyncpg","url":null},{"name":"axios","url":null},{"name":"bash","url":"https://devpost.com/software/built-with/bash"},{"name":"cockroachdb","url":null},{"name":"discord","url":null},{"name":"expo-go","url":null},{"name":"expo.io","url":"https://devpost.com/software/built-with/expo-io"},{"name":"fastapi","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"google-maps","url":"https://devpost.com/software/built-with/google-maps"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"neovim","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"npm","url":"https://devpost.com/software/built-with/npm"},{"name":"orjson","url":null},{"name":"pip","url":null},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"powershell","url":null},{"name":"pydantic","url":null},{"name":"pyright","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"react-native-map","url":null},{"name":"ruff","url":null},{"name":"shapely","url":null},{"name":"sql","url":"https://devpost.com/software/built-with/sql"},{"name":"tyscript","url":null},{"name":"uizard","url":null},{"name":"uvicorn","url":null},{"name":"vscode","url":null},{"name":"xcode","url":"https://devpost.com/software/built-with/xcode"},{"name":"zsh","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/No767/lifelineaid"}],"description_sections":[{"heading":"Inspiration","content":"In light of the ongoing global conflict between war-torn countries, many civilians face hardships. Recognizing these challenges, LifeLine Aid was inspired to direct vulnerable groups to essential medical care, health services, shelter, food and water assistance, and other deprivation relief."},{"heading":"What it does","content":"LifeLine Aid provides multifunctional tools that enable users in developing countries to locate resources and identify dangers nearby. Utilizing the user's location, the app alerts them about the proximity of a situation and centers for help. It also facilitates communication, allowing users to share live videos and chat updates regarding ongoing issues. An upcoming feature will highlight available resources, like nearby medical centers, and notify users if these centers are running low on supplies."},{"heading":"How we built it","content":"Originally, the web backend was to be built using Django, a trusted framework in the industry. As we progressed, we realized that the amount of effort and feasibility of exploiting Django were not sustainable; as we made no progress within the first day. Quickly turning to the in-depth knowledge of one of our team member‚Äôs extensive research into asyncio, we decided to switch to FastAPI, a trusted framework used by Microsoft. Using this framework had both its benefits and costs. Realizing after our first day, Django proved to be a roadblock, thus we ultimately decided to switch to FastAPI.\n\nOur backend proudly uses CockroachDB, an unstoppable force to be reckoned with. CockroachDB allowed our code to scale and continue to serve those who suffer from the effects of war."},{"heading":"Challenges we ran into","content":"In order to pinpoint hazards and help, we would need to obtain, store, and reverse-engineer Geospatial coordinate points which we would then present to users in a map-centric manner. We initially struggled with converting the Geospatial data from a degree, minutes, seconds format to decimal degrees and storing the converted values as points on the map which were then stored as unique 50 character long SRID values. Luckily, one of our teammates had some experience with processing GeoSpatial data so drafting coordinates on a map wasn‚Äôt our biggest hurdle to overcome. Another challenge we faced were certain edge cases in our initial Django backend that resulted in invalid data. Since some outputs would be relevant to our project, we had to make an executive decision to change backend midway through. We decided to go with FastApi. Although FastApi brought its own challenge with processing SQL to usable data, it was our way of overcoming our Jango situation. One last challenge we ran into was our overall source control. A mixture of slow and unbearable WiFi, combined with tedious local git repositories not correctly syncing create some frustrating deadlocks and holdbacks. To combat this downtime, we resort to physically drafting and planning out how each component of our code would work."},{"heading":"Accomplishments that we're proud of","content":"Three out of the four in our team are attending their first hackathon. The experience of crafting an app and seeing the fruits of our labor is truly rewarding. The opportunity to acquire and apply new tools in our project has been exhilarating. Through this hackathon, our team members were all able to learn different aspects of creating an idea into a scalable application. From designing and learning UI/UX, implementing the React-Native framework, emulating iOS and Android devices to test and program compatibility, and creating communication between the frontend and backend/database."},{"heading":"What we learned","content":"This challenge aimed to dive into technologies that are used widely in our daily lives. Spearheading the competition with a framework trusted by huge companies such as Meta, Discord and others, we chose to explore the capabilities of React Native. Joining our team are three students who have attended their first hackathon, and the grateful opportunity of being able to explore these technologies have led us to take away a skillset of a lifetime.\n\nWith the concept of the application, we researched and discovered that the only best way to represent our data is through the usage of Geospatial Data. CockroachDB‚Äôs extensive tooling and support allowed us to investigate the usage of geospatial data extensively; as our backend team traversed the complexity and the sheer awe of the scale of said technology. We are extremely grateful to have this opportunity to network and to use these tools that would be useful in the future."},{"heading":"What's next for LifeLine Aid","content":"There are a plethora of avenues to further develop the app, which include enhanced verification, rate limiting, and many others. Other options include improved hosting using Azure Kubernetes Services (AKS), and many others. This hackathon project is planned to be maintained further into the future as a project for others who may be new or old in this field to collaborate on."},{"heading":"Built With","content":"android-studio asyncpg axios bash cockroachdb discord expo-go expo.io fastapi git github google-maps javascript neovim node.js npm orjson pip postgresql powershell pydantic pyright python react-native react-native-map ruff shapely sql tyscript uizard uvicorn vscode xcode zsh"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Crypto Forecaster","project_url":"https://devpost.com/software/crypto-forecaster","tagline":"Cheat Sheet to smart quantitative investments in Crypto!","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best AI x Crypto with Infinity"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"github.com","url":"http://github.com/aryanshetty03/cryptoforecast.git"}],"description_sections":[{"heading":"Inspiration","content":"To help solve a real world problem and have a large scale impact on the public."},{"heading":"What it does","content":"It gives users a guide to understanding quantitative investing through forecast data decisions. Users can understand the likely behavior of cryptocurrencies based on past pattern analysis to decide on whether to invest or short the assets. The interface also allows users to build their own index, allowing them to forecast how their portfolio behaves."},{"heading":"How we built it","content":"We built the backend using Python and SQL. We utilized Mindsdb to access the Binance API. The web interface was created using JS React, CSS, and Flask."},{"heading":"Challenges we ran into","content":"Spent a lot of time trying to understand how mindsdb works, while trying to navigate through integration issues. Another major challenge we ran into was when two members of our team who were responsible for the front end decided to drop out of the hackathon yesterday."},{"heading":"Accomplishments that we're proud of","content":"We are proud that we were able to get good forecasts of coin prices, meaning that it has real-world use for the general public."},{"heading":"What we learned","content":"Technically, we learned AI automation, while also learning a lot about front-end development."},{"heading":"What's next for Crypto Forecaster","content":"We look forward to getting data on stocks and index funds. This would amplify the use case given the magnitude of publicly traded stocks."},{"heading":"Built With","content":"css flask javascript python pytorch react sql"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Midas","project_url":"https://devpost.com/software/midas-t5ncua","tagline":"Midas analyzes traders' facial expressions with Hume API to log data about their emotions as they trade, then using a LLM to provide feedback based on the collected data and guide their trades.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best AI x Crypto with Infinity"}],"team_members":[],"built_with":[{"name":"hume","url":null},{"name":"mindsdb","url":null},{"name":"psycopg2","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"github.com","url":"https://github.com/adithg/Whatdadogdoin"}],"description_sections":[{"heading":"Inspiration","content":"The intersection between human emotion and technology has been rapidly expanding, providing Data Science and Computer Science majors such as us to explore and push the boundaries of our collective knowledge through new technologies that grow and improve everyday."},{"heading":"What it does","content":"Midas uses a webcam to process data about traders' facial expressions in real-time, creating and contributing to a user database of processed statistical information about their emotions during their different trades. By pairing information about the trades, their outcomes, and the emotions exhibited throughout, a LLM then generates custom feedback for each user to improve their performance based on their personal emotional data."},{"heading":"How we built it","content":"Midas was built using a combination of Hume.AI and LLM training. We used Python for the backend as well as front end using REFLEX. Hume was used to get emotional data and face tracking. MindsDB was use for SQL database integration into our LLM."},{"heading":"Challenges we ran into","content":"Developing an accurate reading of someone's emotions over a period of time and how exactly to train the LLm based on these emotions. Additionally, ensuring user privacy and data security was a concern."},{"heading":"Accomplishments that we're proud of","content":"We successfully created a functional system to analyze traders' emotions during their trades. Integrating emotional data with trade-related information is a unique feature of our project. We're proud of our LLM's ability to generate personalized feedback based on a user's emotional data."},{"heading":"What we learned","content":"We gained a deeper understanding of emotion tracking, Natural Languge Processing, and data analysis techniques. Additionally, we learned about the challenges and ethical considerations involved in working with user data and privacy."},{"heading":"What's next for Midas","content":"There are several potential future directions for Midas. This might include expanding the types of emotional data collected, conducting user studies to assess the impact of emotional feedback on trading performance, and enhancing the overall user experience. We may also explore opportunities to integrate with trading platforms or provide more detailed analytics for traders. Further development could involve ensuring regulatory compliance in the finance industry and addressing potential privacy concerns. Additionally, we could expand the use of the technology beyond trading, such as in education or mental health applications."},{"heading":"Built With","content":"hume mindsdb psycopg2 python reflex sql"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MBTIFY","project_url":"https://devpost.com/software/mbtify","tagline":"Leverages advanced Natural Language Processing and Machine Learning technologies to administer streamlined and adaptive MBTI personality tests through fewer than 10 short-answer questions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/089/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of InterSystems IntegratedML"}],"team_members":[],"built_with":[{"name":"cohere","url":null},{"name":"intersystem","url":null},{"name":"kaggle","url":null},{"name":"openai","url":null},{"name":"openai-api","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/YXKelvinHuang/InsightCare.git"},{"label":"www.kaggle.com","url":"https://www.kaggle.com/datasets/anshulmehtakaggl/60k-responses-of-16-personalities-test-mbt/data"}],"description_sections":[{"heading":"Inspiration","content":"Frustrated by the traditional MBTI's barrage of questions? So were we. Introducing MBTIFY: our web application streamlines your MBTI assessment using advanced Natural Language Processing and Machine Learning. Forget the indecision of Likert scales; MBTIFY's conversational AI prompts you with tailored, open-ended questions. Answer naturally‚Äîthe AI adapts, selecting queries to pinpoint your personality type with ease. Welcome to a smarter, streamlined path to self-discovery."},{"heading":"What it does","content":"MBTIFY is a web application designed to administer MBTI personality tests in a more efficient and streamlined manner. Unlike traditional MBTI tests that require answering hundreds of questions, MBTIFY aims to achieve accurate results with fewer than 10 short-answer questions. Users can respond to these questions via text or audio input. The application leverages Natural Language Processing (NLP) and Machine Learning (ML) technologies to analyze the answers and determine the user's MBTI type."},{"heading":"How we built it","content":"Frontend: Reflex for a user-friendly interface.\n\nVoice Recognition: Converts spoken answers to text.\n\nNLP: Cohere and OpenAI analyze responses for emotional and syntactic insights.\n\nML: Intersystems IntegratedML utilizes 60,000+ Kaggle MBTI questionnaire responses to train a predictive AutoML model. This model interprets Likert scale responses, determining MBTI type with a confidence level. Immediate results are provided if a confidence threshold is met, or the system dynamically selects further clarifying questions."},{"heading":"Challenges we ran into","content":"Reflex on M1 Macs: Faced compatibility issues with the Reflex UI framework on M1 chipset Macs, requiring optimization for cross-platform functionality.\n\nSQLAlchemy with sqlalchemy-iris: Experienced limitations in integrating SQLAlchemy with the sqlalchemy-iris dialect, leading to custom code solutions for effective database operations.\n\nIRIS Cloud Connectivity: Encountered difficulties in connecting to the IRIS cloud server, necessitating adjustments in network and security settings for reliable deployment.\n\nModel Training Time: The machine learning model training took over 6 hours due to the large dataset and complex algorithms, prompting a need for pipeline optimization to enhance efficiency."},{"heading":"Accomplishments that we're proud of","content":"Responsive Design: Developed with Reflex for an adaptive user experience.\n\nReal-Time ML Analysis: Intersystems algorithms provide instant MBTI prediction with a confidence indicator using SQL. We infer Likert scores from user responses to our conversational prompts using LLM, then input these as features into our predictive model, thereby combining discriminative with generative AI models.\n\nSmart Questioning: A dynamic question bank evolves based on user inputs, distinguishing similar MBTI profiles through adaptive questioning and iterative model refinements with updated scores."},{"heading":"What we learned","content":"Stay on Track: Consistently ensure that you are on the right path by periodically reviewing your goals and progress.\n\nPurposeful Implementation: Before committing to a new feature or task, evaluate its significance to avoid exerting effort on non-impactful activities."},{"heading":"What's next for MBTIFY","content":"Audio Transcribing: Our roadmap includes the implementation of an advanced audio transcribing feature. This will allow us to extend our voice recognition capabilities to capture even more nuanced responses from users, further refining our MBTI analysis.\n\nEmotion Detection with HumeAI: We plan to integrate HumeAI technology for real-time emotion detection based on the user's voice. This will add an additional layer of depth to the analysis, enabling us to distinguish between closely matched MBTI types with a greater degree of accuracy.\n\nOptimized Machine Learning Algorithms: We aim to continually fine-tune our existing machine learning models within Intersystem to accommodate these new features, ensuring that our confidence levels and MBTI type predictions are as accurate as possible.\n\nDynamic Questioning 2.0: Building on our adaptive questioning framework, we will incorporate feedback loops that consider not only the content of the user‚Äôs responses but also the detected emotional tone. This will make our question selection even more responsive and targeted."},{"heading":"Built With","content":"cohere intersystem kaggle openai openai-api python reflex"},{"heading":"Try it out","content":"github.com www.kaggle.com"}]},{"project_title":"EduScribe","project_url":"https://devpost.com/software/eduscribe","tagline":"Using AI to turn minutes into moments","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/644/601/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Vectara"}],"team_members":[],"built_with":[{"name":"convex","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"linux","url":"https://devpost.com/software/built-with/linux"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"together.ai","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vectara","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/VinnyXP/EduScribe"},{"label":"vectara-eduscribe.onrender.com","url":"https://vectara-eduscribe.onrender.com/"}],"description_sections":[{"heading":"Inspiration","content":"In our journey as students, we have come to recognize the profound predicament that ensues when attempting to glean meaningful insights from lengthy video content. The struggle to synthesize information effectively from such resources is palpable. In light of this predicament, we embarked on a mission to craft a solution that not only ameliorates this issue but also bestows a boon to our fellow students by saving their precious time. We sought to empower them with a tool that not only facilitates the curation of their educational content but also ensures the utmost convenience by allowing them to revisit videos pertinent to specific topics."},{"heading":"What it does","content":"Our innovative product, EduScribe, is endowed with multifaceted capabilities aimed at enhancing the educational experience of its users. It excels in the art of distilling high-quality summaries from YouTube videos, thereby rendering the extraction of essential insights an effortless endeavor. Furthermore, EduScribe takes the onus of maintaining a comprehensive record of the user's viewing history, facilitating easy reference to previously watched videos. This feature is invaluable for those seeking to revisit content related to a specific subject matter. In addition, EduScribe offers a succinct overview of search results based on the user's content history, thereby streamlining the process of finding pertinent information."},{"heading":"How we built it","content":"The development of EduScribe was an endeavor that harnessed the capabilities of three cutting-edge technologies, namely Vectara AI, Together.ai, and Convex. The fusion of these technological marvels resulted in the creation of our robust product. Vectara AI contributed its prowess in frontend development, ensuring a seamless user interface. Together.ai played a pivotal role in facilitating data migration, a task often fraught with challenges. Convex, while a potent tool, presented its own set of challenges, particularly in comprehending the syntax for database querying. Additionally, the process of fine-tuning an AI model for summary generation and implementing zero-shot prompting for the LLaMa 2 models posed intricate challenges that we had to surmount."},{"heading":"Challenges we ran into","content":"Our journey in developing EduScribe was not without its share of obstacles. Foremost among these was the formidable challenge of querying from the Convex database. The intricacies of database querying, coupled with the necessity of understanding and implementing the correct syntax, proved to be a daunting task. On the frontend, integrating Vectara AI presented its own set of difficulties, albeit surmountable. Data migration, while a crucial step in the development process, was not without its complications. Fine-tuning an AI model for summary generation, a task requiring precision and expertise, demanded a significant investment of time and effort. Moreover, implementing zero-shot prompting for the LLaMa 2 models, though a promising avenue, presented its own unique challenges that required creative problem-solving."},{"heading":"Accomplishments that we're proud of","content":"Our journey in developing EduScribe culminated in a series of accomplishments that we hold in high esteem. Most notably, we successfully crafted a full-stack project, reflecting our proficiency in both frontend and backend development. The crowning achievement of our endeavor was the creation of a sophisticated machine learning model for summary generation, a feat that showcases our commitment to innovation and excellence in the field of artificial intelligence."},{"heading":"What we learned","content":"The development of EduScribe was a crucible of learning, where we immersed ourselves in a plethora of new and diverse technologies. Throughout this journey, we gained invaluable insights and knowledge, transcending our previous boundaries and expanding our technological horizons. Our acquisition of expertise in a variety of domains and the cultivation of our problem-solving skills have left an indelible mark on our development team."},{"heading":"What's next for EduScribe","content":"The horizon of possibilities for EduScribe is broad and promising. Our future endeavors include the transformation of EduScribe into a browser extension, thereby increasing its accessibility and usability. This expansion will further enhance the educational experience of our users, making EduScribe an indispensable tool for the academic journey."},{"heading":"Built With","content":"convex javascript linux node.js python react together.ai typescript vectara"},{"heading":"Try it out","content":"github.com vectara-eduscribe.onrender.com"}]},{"project_title":"The Mask","project_url":"https://devpost.com/software/the-mask","tagline":"An open standard to check if an image was generated with latent diffusion","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/646/449/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best AI x Crypto with Infinity"}],"team_members":[],"built_with":[{"name":"cuda","url":"https://devpost.com/software/built-with/cuda"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rust","url":"https://devpost.com/software/built-with/rust"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"As the lines between AI-generated and real-world images blur, the integrity and trustworthiness of visual content have become critical concerns. Traditional metadata isn't as reliable as it once was, prompting us to seek out groundbreaking solutions to ensure authenticity."},{"heading":"What it does","content":"\"The Mask\" introduces a revolutionary approach to differentiate between AI-generated images and real-world photos. By integrating a masking layer during the propagation step of stable diffusion, it embeds a unique hash. This hash is directly obtained from the Solana blockchain, acting as a verifiable seal of authenticity. Whenever someone encounters an image, they can instantly verify its origin: whether it's an AI creation or an authentic capture from the real world."},{"heading":"How we built it","content":"Our team began with an in-depth study of the stable diffusion mechanism, pinpointing the most effective point to integrate the masking layer. We then collaborated with blockchain experts to harness Solana's robust infrastructure, ensuring seamless and secure hash integration. Through iterative testing and refining, we combined these components into a cohesive, reliable system."},{"heading":"Challenges we ran into","content":"Melding the complex world of blockchain with the intricacies of stable diffusion was no small feat. We faced hurdles in ensuring the hash's non-intrusiveness, so it didn't distort the image. Achieving real-time hash retrieval and embedding while maintaining system efficiency was another significant challenge. As the lines between AI-generated and real-world images blur, the integrity and trustworthiness of visual content have become critical concerns. Traditional metadata isn't as reliable as it once was, prompting us to seek out groundbreaking solutions to ensure authenticity."},{"heading":"Accomplishments that we're proud of","content":"Successfully integrating a seamless masking layer that does not compromise image quality. Achieving instantaneous hash retrieval from Solana, ensuring real-time verification. Pioneering a solution that addresses a pressing concern in the AI and digital era. Garnering interest from major digital platforms for potential integration."},{"heading":"What we learned","content":"The journey taught us the importance of interdisciplinary collaboration. Bringing together experts in AI, image processing, and blockchain was crucial. We also discovered the potential of blockchain beyond cryptocurrency, especially in preserving digital integrity.\\"},{"heading":"What's next for The Mask","content":"We envision \"The Mask\" as the future gold standard for digital content verification. We're in talks with online platforms and content creators to integrate our solution. Furthermore, we're exploring the potential to expand beyond images, offering verification solutions for videos, audio, and other digital content forms."},{"heading":"Built With","content":"cuda python rust"}]},{"project_title":"Spotter - Revolutionizing Disaster Relief","project_url":"https://devpost.com/software/spotter-revolutionizing-disaster-relief","tagline":"Providing actionable information to first responders; we save lives with the power of robotics.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/646/112/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Hume"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"hume","url":null},{"name":"next","url":null},{"name":"openai","url":null},{"name":"spot","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/raviriley/code-spot"}],"description_sections":[{"heading":"Inspiration","content":"Currently, in war-torn and disaster-struck areas, first responders are risking their lives unnecessarily, as they lack the resources needed to accurately and safely assess a disaster zone. By using robotics, we can prevent the risk of human life."},{"heading":"What it does","content":"The SPOT robot has been enabled to be an independent rescue machine that understands human emotion and natural language using AI with a noted ability to detect a language and adjust output as such."},{"heading":"How we built it","content":"We built it using a variety of tools including Hume for AI/transcription, OpenCV for ML Models, Flask for the backend, and Next.js for the frontend."},{"heading":"Challenges we ran into","content":"Connecting to and controlling SPOT was extremely difficult. We got around this by building a custom control server that connects directly to SPOT and controls its motors. The Hume API was relatively friendly to use and we connected this to a live stream of data via the Continuity Camera."},{"heading":"Accomplishments that we're proud of","content":"Fixing SPOT's internal linux dependencies. This is something that blocked all teams from using SPOT and took up most of the first day. But by solving this, we enabled SPOT to be used by all teams."},{"heading":"What we learned","content":"We learned it is quite complex to combine various tech stacks across a variety of products both hardware and software. We learned to approach these problems by introducing levels of abstraction that would allow parts of the team to work parallely."},{"heading":"What's next for Spotter - Revolutionizing Disaster Relief","content":"We hope to fully autonomize SPOTTER so that SPOT can traverse and navigate disaster environments completely independently. In this way, SPOT can locate survivors and assess the situation globally."},{"heading":"Built With","content":"bun flask hume next openai spot"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Caffeine Intake Recommender ","project_url":"https://devpost.com/software/caffeine-intake-recommender","tagline":"With Caffeine Recommender, you can drink your favorite caffeinated beverage safely and wisely to improve your work performance without worrying about the side effects of caffeine.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/645/232/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Zepp Health"}],"team_members":[],"built_with":[{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mindsdb","url":null},{"name":"zepp","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/M-Abdelhakem/Caffeine-Intake-Recommender"}],"description_sections":[{"heading":"Inspiration","content":"The members are part of a working/studying community that tends to rely on caffeine to improve their performance and focus. Over the years, this community has been expanding, as the idea of drinking a coffee or energetic drink becomes a trending activity, fostering new types of beverages in the market and coffee shops. While caffeine benefits one's performance, people tend to overestimate the advantages and neglect caffeine's side effects. If you consumes in the wrong amount or at the wrong time, caffeine can take the entire productivity thing against you: you will feel more fatigue, headaches, excessive anxiety, or harm sleep quality. The team's members and many others have experienced the side effects, without exactly knowing the underlying cause and feeling even more frustrated because they assumed that caffeine would enhance their performance. Therefore, the team created the Caffeine Intake Advisor to prevent people from consuming caffeine in an ineffective and potentially unhealthy way. Zepp's Smart watch also contributes to the ideation process. The advisor relies on a person's biological data to recommend accurate caffeine amounts, thus facilitating the user's life. Given that Zepp's Smart Watch is capable of collecting relevant and various data, such as sleep quality and stress level, the team is confident that the product will impact people's well-being."},{"heading":"What it does","content":"In terms of direct interaction with the user: when the user wants to consume caffeine and have a productive session soon, they will enter the Caffeine Intake Advisor app in the smartwatch. The app will ask about their caffeine beverage or food preference, their goal ( duration of the work session), and based on the datasets that reveal the user's past biological responses to specific amounts of caffeine, the user's on-time health metrics, and the time of the day, recommend a final amount of caffeine to the user.\n\nIn terms of what happens in the back end:\n\nTo recommend the amount of caffeine a user can drink per day, the app needs to know: the user‚Äôs regular caffeine intake and the user‚Äôs caffeine sensitivity. These variables will be measured through Step 2 below."},{"heading":"How the user‚Äôs data will be collected and used:","content":"1. (First, the app will give an initial questionnaire when the person creates the account) The questionnaire mainly aims to gather this information:\n\nServing size: Amount of caffeinated drinks that the person drinks per day ( as well as the amount of caffeinated food) Time : The usual time period that the person is used to intake Variability : Does the person consume the same serving size every day? Or they drink more at specific times of the day Duration of habit: for how long does the person have this habit.\n\n2.After, the app will begin to measure, for 5 days, the coffee sensitivity based on these real-time data:\n\nSleeping pattern: sleep time and wake-up time Here is how the machine will track the sleeping pattern in these 5 days I* dentify the sleeping pattern baseline: * the usual sleep time and wake up time before the 5-days observation period. This can be extracted from the questionnaire above Identify the effects on sleeping pattern based on intake variation: During the monitoring week, the person can consume different amounts of caffeine and vary the timing of caffeine intake on different days. For example, they might have a standard amount of caffeine (their usual) on some days and a reduced or increased amount on other days. They can also adjust the time of caffeine consumption, such as having caffeine earlier or later in the day. Record the sleep-related data at regular intervals throughout the monitoring week.\n\nAfter getting these data, the machine should :\n\nCompare sleep patterns on days with standard caffeine intake to days with reduced or increased caffeine intake. Regular Timing and sleep pattern: Compare sleep patterns on days with standard caffeine intake to days with hours before the sleeping time ( this can be extracted from extracting the time of the day the person consume caffeine)\n\nStress level and Heart rate Here is how the machine will track the stress and heart rate in these 5 days Identify the person‚Äôs stress level and heart‚Äôs rate baseline***: starts recording stress levels and heart rate from the moment the individual wakes up in the morning, before consuming any caffeine. Measure the caffeine direct impact on the person :*** measure the stress and hear rate immediately after the person consumes their first dose of caffeine for the day.\n\n3. After knowing the user‚Äôs Regular Caffeine Intake after 5 days. The watch can now recommend the caffeine intake every time the user wants to consume. To do that, the machine needs to calculate the x amount of caffeine per y hours of focus without yielding negative effects. To do that the machine will use this formula\n\nCaffeine Amount (mg) = Regular Caffeine Intake x Caffeine Sensitivity Factor x Study Duration x Time Gap Factor\n\nEXAMPLE:\n\n**Hypothetical Values:** - Regular Caffeine Intake: 200 mg (the individual's typical daily caffeine consumption) - Caffeine Sensitivity Factor: 0.5 (a multiplier representing the individual's moderate caffeine sensitivity) - Study Goal: Stay awake and enhance focus - Study Duration: 4 hours (the intended duration of the study session) - Time of Study: 7:00 PM to 11:00 PM (4 hours before the individual's typical bedtime at 11:00 PM) - Desired Sleep Quality: The individual prefers to have high-quality sleep without disruptions. **Simplified Calculation:** Now, we'll consider the timing of caffeine consumption and its impact on sleep quality to estimate the amount of caffeine needed: 1. **Assessing Timing and Sleep Quality:** - Calculate the time gap between the end of the study session (11:00 PM) and bedtime (11:00 PM). In this case, it's zero hours, indicating the study session ends at bedtime. - Since the individual desires high-quality sleep, we aim to minimize caffeine's potential effects on sleep disruption. 2. **Caffeine Amount Calculation:** - To achieve the study goal (staying awake and enhancing focus) without impacting sleep quality, we aim to use the caffeine primarily during the study session. - We'll calculate the amount of caffeine needed during the study session to maintain focus, which is the 4-hour duration.\n\nT*ime gap factor* = hours before sleep time ( time when intaking caffeine - sleep time)"},{"heading":"How we built it","content":"We use intel data to train an algorithm. Use minds dp to connect the machine learning algorithm with our software. The process can be categorized into several components:\n\nResearching Key Factors and Features to Include in the Smartwatch We went through several scientific research about what factors affect people‚Äôs caffeine intake, and how different amounts of caffeine impact performance and trigger side effects based on people's caffeine sensitivity level and regular caffeine intake. We also organized a table that specifies types of caffeinated beverages and food according to the amount of caffeine based on their quantity in different units ( for example, in grams or ounces).\n\nUI/UX Design/ Product Management We first designed two types of wireframes of the smartwatch and tried which provided an easier and smoother process, so the user can get their caffeine intake recommendation as easily, convenient, and accurately as possible. After deciding on the user flow, we looked into specific features and their placements on the interfaces, brainstorming the questions:\n\nWhich feature is needed to accomplish task x Which feature is relevant ( but not necessary) to improve the user's experience during task x What is the hierarchy of the visual and textual elements that prevent cognitive load ( consider the smartwatch interface) and appeal to the user's intuitive navigation in the app\n\nTraining Algorithms We make use of MindsDB pre-train models to predict the amount of caffeine each person can have according to the goal and body's condition. This algorithm is based on two types of datasets:\n\nexisting datasets online backed up by scientific research: it includes the biological factors that contribute to user's caffeine sensitivity real-time data collected from the smartwatch of each user about their on-time body reactions ( through heartbeats, stress level, sleep quality) in the period of caffeine consumption. After research and training , the algorithm derives in the formula: Caffeine Amount (mg) = Regular Caffeine Intake x Caffeine Sensitivity Factor x Study Duration x Time Gap Factor.\n\nCode To implement back-end code to the hardware and display the codes in the smart watch's interface, the team used JavaScript in VS code. Additionally, we integrated our work with Zepp OS API and their AutoGUI, as well as Figma in into visualize the UI/UX aspect"},{"heading":"Challenges we ran into","content":"One of the most significant challenges we met was discerning which features of the app to focus on, so we can maximize the social impact considering the time and resources constraints of the hackathon. There is not enough real user datasets available to the public because of the confidentiality of human biological data and the lack of existing solutions that use these datasets ( the topic of caffeine intake has been limited to scholarly research, but not applied largely in today's enterprise solutions). This was time-consuming and frustrating at first since we didn't know which problem to work on as we didn't have previous user experiences to refer to. Therefore, we had to put extra effort and time into the research outlined in the section above, in which we had to calculate with MindDB the mathematical formulas, and from them, hypothesize the values and elaborate our own data sets."},{"heading":"Accomplishments that we're proud of","content":"One of our most notable achievements is integrating software with hardware given that no one in the team has any prior experience in this kind of development.\n\nAnother accomplishment is to work around our constraints and come up with a realistic and effective solution. Since there were no datasets regarding people‚Äôs reactions to varying amounts of caffeine, we had to draw on other types of data to estimate approximate statistics about a user‚Äôs caffeine sensitivity and regular intake. For example, we researched how factors like heart rate and sleep quality affect the caffeine effect on the user, and applied the insights on a mathematical formula to generate the data we needed"},{"heading":"What we learned","content":"The team improved its ability to connect the application's front end with the back end. Additionally, we enhanced our skills in critical thinking, helping us decide which datasets to gather and how to use them effectively to benefit the user.\n\nMoreover, we honed our problem-solving skills to explore methods that can have a substantial impact on the user.\n\nLastly, we enhanced our communication skills by presenting the key aspects of our solution concisely and providing clear responses to the judges' questions."},{"heading":"What's next for Caffeine Intake Recommender","content":"As we continue to develop our app, our aim is to make it more tailored to our users' needs. To provide even more personalized recommendations, we will also add questionnaire features for individual factors such as age, medications, pregnancy, menstrual cycles, and caffeine preferences. Our algorithms will monitor real-time data on users' responses to caffeine consumption and refine the predictions accordingly.\n\nMoreover, we are working on integrating our app with other health and fitness apps and devices to create a more comprehensive view of users' health and fitness data. With this approach, users can get a more holistic understanding of their health and fitness. Specifically, we plan to add caffeine intake tracking to AI assistants such as Apple‚Äôs Siri and Amazon‚Äôs Alexa, with simple commands like \"Alexa, log a cup of espresso.\"\n\nThese advancements will enable users to keep track of their caffeine intake more effectively and help them make better decisions for their overall health and wellness."},{"heading":"Built With","content":"javascript mindsdb zepp"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SecondSearch","project_url":"https://devpost.com/software/secondsearch","tagline":"Instantly search large lecture series to answer your questions!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/646/084/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Milvus"},{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Designed Human-Computer Interaction"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"milvus","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/KanishkGar/calhacks"}],"description_sections":[{"heading":"Inspiration","content":"With the Berkeley enrollment time just around the corner, everyone is stressed about what classes to take. Recently, we had a conversation with one of our friends who was especially stressed about taking CS 162 next semester, with her main concern being that the course has so much content and it will be hard for her to process and digest all the information before midterms. We got the idea to create SecondSearch, where her and all other students in any class can quickly and efficiently review class material by searching through lectures directly."},{"heading":"What it does","content":"SecondSearch answers any question about a course with a direct link to the lecture which explains the question. It performs a vector similarity search to determine which portion of lecture is most likely to answer your question and then displays that video."},{"heading":"How we built it","content":"We built SecondSearch on the Milvus open-source vector database, using OpenAI to help with the search, then completed the product with a companion React frontend built with Chakra UI component library. We implemented the backend using FastAPI and populated the Milvus docker containers with Jupyter Notebook."},{"heading":"Challenges we ran into","content":"We had trouble setting up Milvus and Docker at first, but were quickly able to find thorough documentation for the setup process. Working with React and frontend in general for the first time, we took a couple hours ramping up. It was smooth sailing after the difficult ramp up process :)"},{"heading":"Accomplishments that we're proud of","content":"We're proud of getting a full stack product working in the short span of the hackathon: the client, server, and Milvus docker instance."},{"heading":"What we learned","content":"We learned how to use Docker, FastAPI, and React, as well as the basics (struggles) of full stack development."},{"heading":"What's next for SecondSearch","content":"After creating the minimum viable product, we wanted to make the UI more friendly by using OpenAI to summarize the caption display from the video segments. However, we quickly realized that adding this change would slow the search time down from its current ~1 second to ~20 seconds. As we ran out of time to speed up this feature, we decided to temporarily remove it. However, we will be reimplementing it more efficiently as soon as possible. As for the big picture and the more distant future, currently our product works with lecture series uploaded to Youtube - we want to expand to lecture videos uploaded to other platforms, as some Berkeley classes upload recordings to bCourses, and other institutions use different platforms. After we expand the project further, some reaching goals for the far future include advertising the completed product to all university students, as lectures are often recorded and uploaded in some form. We also want to add new features on future patches such as saving previous searches, and more."},{"heading":"Built With","content":"fastapi milvus python react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Emergency Safety Widget","project_url":"https://devpost.com/software/emergency-safety-widget","tagline":"Ever feel like someone is following you when you're walking alone at night? With our widget, a simple press will allow you to sound an alarm, share your live location, & help u get to safety quickly.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/644/764/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Zepp Health"}],"team_members":[],"built_with":[{"name":"autogui","url":null},{"name":"google-places","url":"https://devpost.com/software/built-with/google-places"},{"name":"gps","url":"https://devpost.com/software/built-with/gps"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"zeuscli","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/conniely04/safetywidget"}],"description_sections":[{"heading":"Inspiration","content":"As female engineering students in a male-dominated field, we wanted to implement features that could often be overlooked. This includes safety features specifically for when college students feel unsafe walking alone at night."},{"heading":"What it does","content":"Our app aims to increase safety, specifically for individuals walking alone at night. We designed and implemented an SOS button, automatic rerouting to open locations, real-time tracking, and vibration and sound alarms."},{"heading":"How we built it","content":"Zepp Health is the company behind the innovative Amazfit Balance watch. We built this widget with the help of Zeus CLI, Google Places API, and libraries written by talented Zepp engineers. We have developed cutting-edge technology that enhances safety and provides safety for late-night walkers."},{"heading":"Challenges we ran into","content":"There were many challenges when creating this widget. We had challenges when downloading and setting up the simulator used for testing. This was the first challenge when getting started with coding and debugging our widget. Another main challenge we encountered was connecting backend data and the use of APIs to the frontend application."},{"heading":"Accomplishments that we're proud of","content":"Through this project, we accomplished many components of this widget. These accomplishments included enhanced safety by utilizing the advanced features by receiving immediate help when needed with just a press of the SOS button. We also created automatic rerouting to the nearest open place where our app reroutes users to the closest open areas for added security."},{"heading":"What we learned","content":"We learned how to work with Zepp's Developer Interfaces and different APIs to create various UI elements and fetch sensor data. Using this information, we tied together our system to create a functional app on Amazfit's Balance Watch."},{"heading":"What's next for Emergency Safety Widget","content":"Future implementations include sending the live-fetched location to local authorities and directing the user to the calculated safe spot. A back feature to the SOS alert should also be implemented."},{"heading":"Built With","content":"autogui google-places gps javascript zeuscli"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ARISTA : DEPLOYMENT OF SPOT ROBOT FOR PUBLIC SAFETY","project_url":"https://devpost.com/software/arista","tagline":"Introducing our patrol SPOT robot : ARISTA! It's a high-tech guardian offering real-time monitoring, quick response to threats, and reducing risks to human officers. Boost your security today!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/645/994/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Designed Human-Computer Interaction"}],"team_members":[],"built_with":[{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[],"description_sections":[{"heading":"What it does","content":"SPOT would be equipped with various sensors, cameras, and LIDAR technology to perform inspections in hazardous environments. A network of SPOT bots will be deployed in a within a 2.5 ‚Äì 3-mile radius surrounding a particular infrastructure for security and surveillance tasks, patrolling areas and providing real-time video feeds to human operators. It can be used to monitor large facilities, industrial sites, or public events, enhancing security effort.\n\nThese network of SPOT robots will be used to inspect and collect data/mages for analysis, tracking suspects, and gathering crucial intelligence in high-risk environments thus maintaining situational awareness without putting officers in harm's way.\n\nThey will be providing real-time video feeds . If it detects any malicious activity, the SPOT will act as the first respondent and deploy non-lethal measures by sending a distress signal to the closest law enforcement officer/authority who‚Äôd be able mitigate the situation effectively. Consequently, the other SPOT bots in the network would also be alerted. Its ability to provide real-time situational awareness without putting officers at risk is a significant advantage."},{"heading":"How we built it","content":"Together.ai : Used llama to enable conversations and consensus among agents MindsDB : Database is stored in postgres (render). The database is imported to Mindsdb. The sentiment classifier is trained with the help of demo data and the sentiments which are retrieved from every agent allows us to understand the mental state of every bot Reflex : UI for visualization of statistical measures of the bots -Intel : To train mobilevnet for classifying threats Intersystems : To Carry on Battery Life forecasting for the agent to enable efficient decisions"},{"heading":"Built With","content":"postgresql python"}]},{"project_title":"Zepptchi","project_url":"https://devpost.com/software/zepptchi","tagline":"Track steps, earn points, and customize your digital Tamagotchi-inspired pet. Fitness meets nostalgic fun. Every step enlivens your pocket-sized pal. Walk more, unlock more. Turn health into play!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/641/693/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Zepp Health"}],"team_members":[],"built_with":[{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"visual-studio","url":"https://devpost.com/software/built-with/visual-studio"},{"name":"zepp","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/MCGitHub15/zepptchi"}],"description_sections":[{"heading":"Inspiration:","content":"The inspiration for this project was finding a way to incentivize healthy activity. While the watch shows people data like steps taken and calories burned, that alone doesn't encourage many people to exercise. By making the app, we hope to make exercise into a game that people look forward to doing rather than something they dread."},{"heading":"What it does","content":"Zepptchi is an app that allows the user to have their own virtual pet that they can take care of, similar to that of a Tamagotchi. The watch tracks the steps that the user takes and rewards them with points depending on how much they walk. With these points, the user can buy food to nourish their pet which incentivizes exercise. Beyond this, they can earn points to customize the appearance of their pet which further promotes healthy habits."},{"heading":"How we built it","content":"To build this project, we started by setting up the environment on the Huami OS simulator on a Macbook. This allowed us to test the code on a virtual watch before implementing it on a physical one. We used Visual Studio Code to write all of our code."},{"heading":"Challenges we ran into","content":"One of the main challenges we faced with this project was setting up the environment to test the watch's capabilities. Out of the 4 of us, only one could successfully install it. This was a huge setback for us since we could only write code on one device. This was worsened by the fact that the internet was unreliable so we couldn't collaborate through other means. One other challenge was"},{"heading":"Accomplishments that we're proud of","content":"Our group was most proud of solving the issue where we couldn't get an image to display on the watch. We had been trying for a couple of hours to no avail but we finally found out that it was due to the size of the image. We are proud of this because fixing it showed that our work hadn't been for naught and we got to see our creation working right in front of us on a mobile device. On top of this, this is the first hackathon any of us ever attended so we are extremely proud of coming together and creating something potentially life-changing in such a short time."},{"heading":"What we learned","content":"One thing we learned is how to collaborate on projects with other people, especially when we couldn't all code simultaneously. We learned how to communicate with the one who was coding by asking questions and making observations to get to the right solution. This was much different than we were used to since school assignments typically only have one person writing code for the entire project. We also became fairly well-acquainted with JavaScript as none of us knew how to use it(at least not that well) coming into the hackathon."},{"heading":"What's next for Zepptchi","content":"The next step for Zepptchi is to include a variety of animals/creatures for the user to have as pets along with any customization that might go with it. This is crucial for the longevity of the game since people may no longer feel incentivized to exercise once they obtain the complete collection. Additionally, we can include challenges(such as burning x calories in 3 days) that give specific rewards to the user which can stave off the repetitive nature of walking steps, buying items, walking steps, buying items, and so on. With this app, we aim to gamify a person's well-being so that their future can be one of happiness and health."},{"heading":"Built With","content":"javascript visual-studio zepp"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CrisisSync","project_url":"https://devpost.com/software/crisissync","tagline":"Communication Elevated. Threats Eliminated.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/339/085/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Zepp Health"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"zeppapi","url":null},{"name":"zeppos","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/tuedolm/calhack2023zeppos"}],"description_sections":[{"heading":"Built With","content":"amazon-web-services javascript mongodb zeppapi zeppos"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Clock In","project_url":"https://devpost.com/software/clock-in","tagline":"A Dash Cam for the body, keeping workers safe from hazardous work conditions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/645/591/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Zepp Health"}],"team_members":[],"built_with":[{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"zepp","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/y-dejong/clock-in"}],"description_sections":[{"heading":"Inspiration","content":"Smart watches have centered around the health of their users for over a decade. Fitness apps have provided the ability to track calorie burning and heart rate for athletes, and those pursuing the ideal physique. At the other end of the spectrum, medical apps provide the ability for diabetes patients and the elderly to track and detect concerning medical phenomena. However, both these use cases miss the major demographic between them: middle class and blue collar workers. For construction workers, contractors, farmers, and millions of other manual laborers, the extreme highs and lows of biological tracking are not necessary, yet these apps are incapable of keeping them safe and healthy in their physically demanding workplaces. Clock In provides an answer."},{"heading":"What it does","content":"The moment a worker \"Clocks In\" at the start of their day, they can open our app on their watch to start tracking them. Clock In tracks several health and safety risk factors throughout the work day, and detects any major hazards or accidents that could affect the user. It tracks personal factors, such as heart rate, blood oxygen level, physical trauma via an accelerometer, atrial fibrillation, and arrhythmia. It also tracks environmental factors, such as temperature and humidity.\n\nTypically, Clock In will invalidate and delete this data after a few hours, but the moment that a workplace hazard or accident strikes, Clock In takes a snapshot of the user's working conditions at the time of the incident and in the hours leading up to it. In this way, Clock In acts like a dash cam for the body , freezing the data surrounding an incident for later investigation.\n\nClock In provides workers with the ability to record unsafe or inhumane working conditions, providing numerical, indisputable evidence in the case of mistreatment by their employer. In the case of a severe accident, this data can provide valuable proof of mistreatment by an employer, assisting the worker's case."},{"heading":"How we built it","content":"Using Zepp's OS and the vast array of sensors on the Amazfit Smart-Watch, we monitor and protect against 3 kinds of trigger events.\n\nimmediate triggers\n\nHigh-speed impact Loud Noise Self-Recorded Emergency\n\nshort term triggers\n\nAbnormal Heart Rate Low VO2 max\n\nlong term triggers\n\nUnsafe temperatures Unsafe altitudes"},{"heading":"Challenges we ran into","content":"Use of an unfamiliar API with mixed documentation and a foreign framework. Luckily, the Zepp dev team gave us 24-hour support, constantly helping with documentation and hardware issues."},{"heading":"Accomplishments that we're proud of","content":"Our technology will improve safety accountability in the workplace by ensuring all sides are held accountable for workplace mishaps.\n\nWe are proud of the high-speed impact detection. A user can unnaturally jerk their arm and the watch immediately responds.\n\nWe are also proud of our ability complete a project despite arriving late to the hackathon and having other obligations during the event."},{"heading":"What's next for Clock In","content":"Workplace injuries no more\n\nSuppose an employee was injured. There is little certainty that their medical bills will be\n\nWe hope to implement preventative measures to stop workplace injuries before they happen. By notifying workers when they are susceptible to stroke, drowsiness, and immune system failure we can prevent a hospital visit that is both expensive to the employee and the employer.\n\nIn the future, we hope to add more trackable factors to our product, such as air quality and elevation."},{"heading":"Built With","content":"javascript zepp"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Scrapbook","project_url":"https://devpost.com/software/scrapbook-5h0f4v","tagline":"While conducting research at UCSF, a grandfather with Alzheimers had access to a Google Doc, filled with hundreds of memories his family wanted him to remember. I decided to make him a site instead.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/645/389/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Designed Human-Computer Interaction"}],"team_members":[],"built_with":[{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"minds-db","url":null},{"name":"next-auth","url":null},{"name":"next.js","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/garvcodes/nailfungus"}],"description_sections":[{"heading":"Inspiration","content":"While a member of my team was conducting research at UCSF, he noticed a family partaking in a beautiful, albeit archaic, practice. They gave their grandfather access to a google doc, where each family member would write down the memories that they have with him. Nearly every day, the grandfather would scroll through the doc and look at the memories that him and his family wanted him to remember."},{"heading":"What it does","content":"Much like the Google Doc does, our site stores memories inputted by either the main account holder themself, or other people who have access to the account, perhaps through a shared family email. From there, the memories show up on the users feed and are tagged with the emotion they indicate. Someone with Alzheimers can easily search through their memories to find what they are looking for. In addition, our Chatbot feature trained on their memories also allows users to easily talk to the app directly, asking what they are looking for."},{"heading":"How we built it","content":"Next.js, React, Node.js, Tailwind, etc."},{"heading":"Challenges we ran into","content":"It was difficult implementing our chatbot in a way where it is automatically update with data that our user inputs into the site. Moreover, we were working with React for the first time and faced many challenges trying to build out and integrate the different technologies into our website including setting up MongoDB, Flask, and different APIs."},{"heading":"Accomplishments that we're proud of","content":"Getting this done! Our site is polished and carries out our desired functions well!"},{"heading":"What we learned","content":"As beginners, we were introduced to full-stack development!"},{"heading":"What's next for Scrapbook","content":"We'd like to introduce Scrapbook to medical professionals at UCSF and see their thoughts on it."},{"heading":"Built With","content":"javascript minds-db next-auth next.js node.js python react tailwind"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Pinnect","project_url":"https://devpost.com/software/pinnect","tagline":"Pinnect: Interactive map-based storytelling platform, enabling users to craft, connect, and share narratives collaboratively.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/641/973/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Best Use of Eluvio"}],"team_members":[],"built_with":[{"name":"eluv.io","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"minddb","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rust","url":"https://devpost.com/software/built-with/rust"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"vectara","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/AnnieTianyuHuang/pinnect-calhacks-proj"}],"description_sections":[{"heading":"Pinnect","content":"Game-map collaboration, all for gammers. Our vision is to create a dynamic and interactive platform that functions like a game itself. Imagine this: you click on a spot on a map, and just like that, you're crafting your story. What's more, you can connect your narrative to others' stories, creating a collaborative storytelling experience like never before. Pinnect is not just a platform; it's an adventure waiting to happen."},{"heading":"Core Features Built with Eluvio","content":"Eluvio Content Fabric - Tokenization\n\nConverts content into digital assets, including both fungible and non-fungible tokens (NFTs). Transforms story cards created on the map into NFTs. Utilize the Eluvio Media Wallet Creators use the Media Wallet when logging into Pinnect. Story of the Day feature called \"Story of the Day\" where users on Pinnect can submit and view a featured story each day. This story gets tokenized using Eluvio and showcased in a special section. Each day, a new story is selected, tokenized, and featured. The integration with Eluvio ensures secure and verifiable content management, while the use of the Media Wallet can authenticate submissions and access."},{"heading":"Story of the day!","content":"Recommend daily story for user which fits their preference the most The AI framework support for the Pinnect platform is enhanced by MindsDB\n\nPersonalized Recommendation Engine: By integrating the new AI framework with MindsDB, a more accurate recommendation system is built to suggest stories or games that users may be interested in. Vectara Content Recommendation Engine Application scenario: Use Vectara to provide personalized content recommendations for Pinnect users. Implementation method: Analyze user behavior and preferences, and combine with Vectara to generate relevant article or post recommendations."},{"heading":"Game-map Collaboration","content":"This page allows you to control the display of the map and its tags through filtering, sorting, and other methods to find the information you need. Priority of feature requirements: Understandable but not obvious method\n\nTag List: Display an evenly arranged list of all primary tags and their associated secondary tags. Clicking on any will dim it and no longer display its corresponding tag on the map. Clicking a primary tag will hide all the markers corresponding to its secondary tags. Show/Hide All Switch: This switch controls the visibility of all tags. Search Box: This allows for searching for specific tags or markers. Progress Radio Button: By selecting different chapters (game progress), you can choose different degrees of war fog coverage to prevent spoilers. For example, when selecting the first chapter, markers in the tag data that belong to the second chapter and later will be hidden. Heatmap Display Mode Switch: You can select \"Heatmap Priority\" to display the markers most frequently visited/submitted by current players. This is useful for viewing the progress of the majority of players. Mini-map and Zoom Buttons: Includes a thumbnail of the map and zoom in/out buttons."},{"heading":"Tag Editing","content":"Editing interface for tags. The layout, from top to bottom, is as follows:\n\nImage and its submission button. Title and its text input box. Description and its text input box. Dropdown single selection box for primary tags. This can be tasks, NPCs, items, buildings, regions, or any other types. Dropdown single selection box for secondary tags. Only two levels of tags are set. Belonging chapter. Submission button.\n\nFunction\n\nIn the description input box, hyperlinks to other tags can be created using [[]] syntax. Transaction bundling: every 'n' transactions should be bundled together for on-chain attestation (this number 'n' needs to be set). AI duplication detection: when a user makes a submission, the content is analyzed to determine if it is a tag that already exists. If it is, a prompt should appear asking if this is a mistake, with two options: 'Yes, submit an updated version' or 'No, submit as a new version'. If 'No' is chosen, the submission should undergo backend review."},{"heading":"Relationship Flow","content":"Relationship diagram of the current tag is highlighted. When another tag is clicked, a line between the current tag and the clicked tag is displayed, along with an adjacent edit box.\n\nThe layout of the edit box, from top to bottom, is as follows:\n\nDirection and its option box: There are two types, preceding and following, similar to the arrowhead editing box at the end of the line in Keynote. Relationship and its option box: This can be connectors like \"owns\", \"manages\", \"belongs to\", \"requires\", etc."},{"heading":"Connect Wallet with cross-platform support","content":"Contribute on-chain for fair incentive and collaboration"},{"heading":"Developers","content":"Annie Huang CS + Business Sophomore @ UC Berkeley, ETH Denver NFT Track Award Winner, Canada National Book Award Winner, NCWIT Award Winner."},{"heading":"About the Project","content":"Inspiration : My passion for gaming and interactive storytelling sparked the idea of Pinnect. I envisioned a platform where users could not only share their stories but also collaborate to create a larger, intertwined narrative using a dynamic game map.\n\nLearning Journey : Embarking on this project solo, every step was a profound learning experience. I immersed myself in technologies like Eluvio Content Fabric, honed skills in AI with MindsDB, and navigated the world of content recommendation using Vectara.\n\nBuilding the Project : As a solo developer, multitasking was the name of the game. From conceptualizing the interactive game-map to integrating tokenization of stories into NFTs, and implementing a comprehensive tagging system, every phase posed unique challenges.\n\nChallenges : The balance between crafting a robust backend and ensuring a seamless user experience was a tightrope walk. Tokenizing stories into NFTs while retaining a user-friendly platform interface was particularly demanding. Further, fine-tuning the recommendation engine to curate stories tailored to user preferences was both challenging and rewarding, as it underscored the platform's commitment to delivering a personalized experience."},{"heading":"Built With","content":"eluv.io javascript minddb node.js python rust swift vectara"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"BiteBuddy","project_url":"https://devpost.com/software/bonapp","tagline":"A GraphML + LLM-powered all-in-one meal planner and personal CRM.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/645/930/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"Most Creative Use of Reflex"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"scikit-learn","url":"https://devpost.com/software/built-with/scikit-learn"},{"name":"together.ai","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"We set out to build a product that solves two core pain points in our daily lives: 1) figuring out what to do for every meal üòã and 2) maintaining personal relationships üë•.\n\nAs college students, we find ourselves on a daily basis asking the question, ‚ÄúWhat should I do for lunch today?‚Äù üçî ‚Äî many times with a little less than an hour left before it‚Äôs time to eat. The decision process usually involves determining if one has the willpower to cook at home, and if not, figuring out where to eat out and if there is anyone to eat out with. For us, this usually just ends up being our roommates, and we find ourselves quite challenged by maintaining depth of relationships with people we want to because the context windows are too large to juggle.\n\nEnter, BiteBuddy."},{"heading":"What it does","content":"We divide the problem we‚Äôre solving into two main scenarios.\n\nSpontaneous (Eat Now!) : It‚Äôs 12PM and Jason realizes that he doesn‚Äôt have lunch plans. BiteBuddy will help him make some! üç± Futuristic (Schedule Ahead!) : It‚Äôs Friday night and Parth decides that he wants to plan out his entire next week (Forkable, anyone?). üïí\n\nEat Now allows you to find friends that are near you and automatically suggests nearby restaurants that would be amenable to both of you based on dietary and financial considerations. Read more below to learn some of the cool API interactions and ML behind this :‚Äô). üó∫Ô∏è\n\nSchedule Ahead allows you to plan your week ahead and actually think about personal relationships. It analyzes closeness between friends, how long it‚Äôs been since you last hung out, looks at calendars, and similar to above automatically suggests time and restaurants. Read more below for how! üß†\n\nWe also offer a variety of other features to support the core experience:\n\nFeed . View a streaming feed of the places your friends have been going. Enhance the social aspect of the network. Friends (no, we don‚Äôt offer friends). Manage your relationships in a centralized way and view LLM-generated insights regarding relationships and when might be the right time/how to rekindle them."},{"heading":"How we built it","content":"The entire stack we used for this project was Python, with the full stack web development being enabled by the Reflex Python package, and database being Firebase.\n\nEat Now is a feature that bases itself around geolocation, dietary preferences, financial preferences, calendar availability, and LLM recommendation systems. We take your location, go through your friends list and find the friends who are near you and don‚Äôt have immediate conflicts on their calendar, compute an intersection of possible restaurants via the Yelp API that would be within a certain radius of both of you, filter this intersection with dietary + financial preferences (vegetarian? vegan? cheap?), then pass all our user context into a LLAMA-13B-Chat üí¨ to generate a final recommendation. This recommendation surfaces itself as a potential invite (in figures above) that the user can choose whether or not to send to another person. If they accept, a calendar invite is automatically generated.\n\nSchedule Ahead is a feature that bases itself around graph machine learning, calendar availability, personal relationship status (how close are y‚Äôall? When is the last time you saw each other?), dietary/financial preferences, and more. By looking ahead into the future, we take the time to look through our social network graph with associated metadata and infer relationships via Spectral Clustering üìä. Based on how long it‚Äôs been since you last hung out and the strength of your relationship, it will surface who to meet with as a priority queue and look at both calendars to determine mutually available times and locations with the same LLM.\n\nWe use retrieval augmented generation (RAG) üìù throughout our app to power personalized friend insights (to learn more about which friends you should catch up with, learn that Jason is a foodie, and what cuisines you and Parth like). This method is also a part of our recommendation algorithm."},{"heading":"Challenges we ran into","content":"Dealing with APIs. We utilized a number of APIs to provide a level of granularity and practicality to this project, rather than something that‚Äôs solely a mockup. Dealing with APIs though comes with its own issues. The Yelp API, for example, continuously rate limited us even though we cycled through keys from all of our developer accounts :‚Äô). The Google Calendar API required a lot of exploration with refresh tokens, necessary scopes, managing state with google auth, etc. New Technologies. We challenged ourselves by exploring some new technologies as a part of our stack to complete this project. Graph ML for example was a technology we hadn‚Äôt worked with much before, and we quickly ran into the cold start problem with meaningless graphs and unintuitive relationships. Reflex was another new technology that we used to complete our frontend and backend entirely in Python. None of us had ever even pip installed this package before, so learning how to work with it and then turn it into something complex and useful was a fun challenge. üí° Latency. Because our app queries several APIs, we had to make our code as performant as possible, utilize concurrency where possible, and add caching for frequently-queried endpoints. üñ•Ô∏è"},{"heading":"Accomplishments that we're proud of","content":"The amount of complexity that we were able to introduce into this project made it mimic real-life as close as possible, which is something we‚Äôre very proud of. We‚Äôre also proud of all the new technologies and Machine Learning methods we were able to use to develop a product that would be most beneficial to end users."},{"heading":"What we learned","content":"This project was an incredible learning experience for our team as we took on multiple technically complex challenges to reach our ending solution -- something we all thought that we had a potential to use ourselves."},{"heading":"What's next for BiteBuddy","content":"The cool thing about this project was that there were a hundred more features we wanted to include but didn‚Äôt remotely have the time to implement. Here are some of our favorites üôÇ:\n\nGroups. Social circles often revolve around groups. Enabling the formation of groups on the app would give us more metadata information regarding the relationships between people, lending itself to improved GNN algorithms and recommendations, and improve the stickiness of the product by introducing network effects. New Intros: Extending to the Mutuals. We‚Äôve built a wonderful graph of relationships that includes metadata not super common to a social network. Why not leverage this to generate introductions and form new relationships between people? More Integrations. Why use DonutBot when you can have BiteBuddy?"},{"heading":"Built with","content":"Python, Reflex, Firebase, Together AI, ‚ù§Ô∏è, and boba üßã"},{"heading":"Built With","content":"python reflex scikit-learn together.ai"}]},{"project_title":"Nurture","project_url":"https://devpost.com/software/nurture-m7p5wg","tagline":"An AI tool to better understand the needs of special education children in the classroom and at home.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/646/238/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - Redis] Most Creative Use of Redis Cloud"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"get","url":null},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"hume","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"redis","url":"https://devpost.com/software/built-with/redis"},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Digvijay-Bokey/nurture-ai"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration behind \"Nurture AI\" stemmed from understanding the unique challenges faced by parents, especially those of special needs children. Recognizing that constant supervision can be both exhausting and impractical, we sought to create a tool that would assist in monitoring their child‚Äôs emotional and mental state, especially during times when direct supervision isn't possible. This tool is not only meant to offer peace of mind but also aims to foster independence in children, giving them the confidence to engage in activities like homework or play, knowing they're still under a watchful, caring eye."},{"heading":"What it does","content":"Nurture AI uses AI-driven facial recognition and emotion detection technology through a simple webcam to analyze a child's emotional state in real-time. The app recognizes a range of emotions ‚Äì from distress and frustration to joy and contentment. When it detects negative emotional shifts or signs of distress, it promptly notifies the parent or caregiver via text. This feature ensures that the caregiver can intervene quickly when necessary, yet also allows them to manage other tasks without constant physical supervision."},{"heading":"How we built it","content":"We used both Hume, Reflex, Twillo, and Redis to make this project possible."},{"heading":"Challenges we ran into","content":"Making the user interface friendly for children who may struggle with autism or epilepsy.\n\nBalancing Sensitivity and Specificity: Adjusting the system to correctly interpret emotions without overloading the parent with notifications.\n\nUser Trust and Privacy: Ensuring the application maintains high standards of user privacy and security, crucial for building trust."},{"heading":"Accomplishments that we're proud of","content":"It works! It very accurately can send texts when someone is stressed or any other emotion."},{"heading":"What's Next for Nurture","content":"By continuously evolving and adapting, we aim to ensure that Nurture AI not only keeps pace with technological advancements and user expectations but also extends its reach and positive impact on more families and communities. We hope to add more solutions to help children calm down after detection, and make it adult-oriented too so we can check our own emotions."},{"heading":"Built With","content":"amazon-web-services get github hume python redis reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ChickyAI","project_url":"https://devpost.com/software/chickyai","tagline":"Translating medical jargon to everyday language, one report at a time.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/643/073/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - Fidelity] Best Accessibility Hack sponsored by Fidelity"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"convex","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"lucia","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vite","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Both of our families immigrated to the US roughly two decades ago.\n\nMany of the elders in our family are not comfortable navigating the medical system alone and since a young age, both Brandon and I have been translating important documents...\n\nWe believe in empowering this user base with the ability to have empathetic conversations with their medical report . We want users to navigate their health journey with clarity and confidence ."},{"heading":"What it does","content":"ChickyAI securely studies your medical report and extracts the relevant information about what the patient is presenting with, the physician's opinions, and plans for the next steps. Users are then able to ask our GenAI chatbot about the report's details and how to confidently move forward. Users can take notes on what they discovered and be fed relevant public health insights to aid them in their health journey."},{"heading":"How we built it","content":"We used Convex.dev and an assortment of tools in-between. We also use Lucia for authentication."},{"heading":"Challenges we ran into","content":"Unreliable Wi-Fi made it so we had to take \"trust falls\" and go without the help of ChatGPT at times in order to keep going."},{"heading":"Accomplishments that we're proud of","content":"Responsive site with a seamless user journey. Gaining a working intuition for full-stack development. Escaping \"tutorial hell\"."},{"heading":"What we learned","content":"A lot. This is our first time making a website of any kind! Also, our first hackathon! \"All rabbit holes are good rabbit holes\" when it comes to becoming more confident software engineers."},{"heading":"What's next for ChickyAI","content":"Blockchain encryption and ensure HIPPA compliance. Connecting users with live certified medical professionals to replace the GenAI feature. Connecting users with more public health resources based on location, demographics, and more. Allowing users to securely establish a profile and improve offline access."},{"heading":"Built With","content":"api convex git github javascript lucia react typescript vite"}]},{"project_title":"Let Them Cook","project_url":"https://devpost.com/software/let-them-cook-q5bupc","tagline":"Cooking for someone with dietary restrictions? Input your already existing recipe ingredients and allergens you would like to avoid, and we'll pick the tastiest replacements for your allergies!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/646/099/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - .Tech] Best .Tech Domain Name"},{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - TinyMCE] Best Use of TinyMCE"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"fastapi","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tinymce","url":null},{"name":"zilliz","url":null}],"external_links":[{"label":"letthemcook.tech","url":"http://letthemcook.tech"},{"label":"github.com","url":"https://github.com/calhacks2023"},{"label":"github.com","url":"https://github.com/calhacks2023/calhacks-2023-backend"},{"label":"github.com","url":"https://github.com/calhacks2023/calhacks-2023-frontend"}],"description_sections":[{"heading":"Inspiration","content":"We love to cook for and eat with our friends and family! Sometimes though, we need to accommodate certain dietary restrictions to make sure everyone can enjoy the food. What changes need to be made? Will it be a huge change? Will it still taste good? So much research and thought goes into this, and we all felt as though an easy resource was needed to help ease the planning of our cooking and make sure everyone can safely enjoy our tasty recipes!"},{"heading":"What is Let Them Cook?","content":"First, a user selects the specific dietary restrictions they want to accommodate. Next, a user will copy the list of ingredients that are normally used in their recipe into a text prompt. Then, with the push of a button, our app cooks up a modified, just as tasty, and perfectly accommodating recipe, along with expert commentary from our \"chef\"! Our \"chef\" also provides suggestions for other similar recipes which also accomidate the user's specific needs."},{"heading":"How was Let Them Cook built?","content":"We used React to build up our user interface. On the main page, we implemented a rich text editor from TinyMCE to serve as our text input, allong with various applicable plugins to make the user input experience as seamless as possible.\n\nOur backend is Python based. We set up API responses using FastAPI. Once the front end posts the given recipe, our backend passes the recipe ingredients and dietary restrictions into a fine-tuned large language model - specifically GPT4.\n\nOur LLM had to be fine-tuned using a combination of provided context, hyper-parameter adjustment, and prompt engineering. We modified its responses with a focus on both dietary restrictions knowledge and specific output formatting.\n\nThe prompt engineering concepts we employed to receive the most optimal outputs were n-shot prompting, chain-of-thought (CoT) prompting, and generated knowledge prompting."},{"heading":"UI/UX","content":"User Personas\n\nWe build some user personas to help us better understand what needs our application could fulfil\n\nUser Flow\n\nThe user flow was made to help us determine the necessary functionality we wanted to implement to make this application useful\n\nLo-fi Prototypes\n\nThese lo-fi mockups were made to determine what layout we would present to the user to use our primary functionality\n\nHi-fi Prototypes\n\nHere we finalized the styling choice of a blue and yellow gradient, and we started planning for incorporating our extra feature as well - the recipe recomendations"},{"heading":"Engineering","content":"Frontend: React, JS, HTML, CSS, TinyMCE, Vite\n\nBackend: FastAPI, Python\n\nLLM: GPT4\n\nDatabase: Zilliz\n\nHosting: Vercel (frontend), Render (backend)"},{"heading":"Challenges we ran into","content":"Frontend Challenges\n\nOur recipe modification service is particularly sensitive to the format of the user-provided ingredients and dietary restrictions. This put the responsibility of vetting user input onto the frontend. We had to design multiple mechanisms to sanitize inputs before sending them to our API for further pre-processing. However, we also wanted to make sure that recipes were still readable by the humans who inputted them. Using the TinyMCE editor solved this problem effortlessly as it allowed us to display text in the format it was pasted, while simultaneously allowing our application to access a \"raw\", unformatted version of the text.\n\nTo display our modified recipe, we had to brainstorm the best ways to highlight any substitutions we made. We tried multiple different approaches including some pre-built components online. In the end, we decided to build our own custom component to render substitutions from the formatting returned by the backend.\n\nWe also had to design a user flow that would provide feedback while users wait for a response from our backend. This materialized in the form of an interim loading screen with a moving GIF indicating that our application had not timed out. This loading screen is dynamic, and automatically re-routes users to the relevant pages upon hearing back from our API.\n\nBackend Challenges\n\nThe biggest challenge we ran into was selecting a LLM that could produce consistent results based off different input recipes and prompt engineering. We started out with Together.AI, but found that it was inconsistent in formatting and generating valid allergy substitutions. After trying out other open-source LLMs, we found that they also produced undesirable results. Eventually, we compromised with GPT-4, which could produce the results we wanted after some prompt engineering; however, it it is not a free service.\n\nAnother challenge was with the database. After implementing the schema and functionality, we realized that we partitioned our design off of the incorrect data field. To finish our project on time, we had to store more values into our database in order for similarity search to still be implemented."},{"heading":"Takeaways","content":"Accomplishments that we're proud of\n\nFrom coming in with no knowledge, we were able to build a full stack web applications making use of the latest offerings in the LLM space. We experimented with prompt engineering, vector databases, similarity search, UI/UX design, and more to create a polished product. Not only are we able to demonstrate our learnings through our own devices, but we are also able to share them out with the world by deploying our application.\n\nAll that said , our proudest accomplishment was creating a service which can provide significant help to many in a common everyday experience: cooking and enjoying food with friends and family.\n\nWhat we learned\n\nFor some of us on the team, this entire project was built in technologies that were unfamiliar. Some of us had little experience with React or FastAPI so that was something the more experienced members got to teach on the job.\n\nOne of the concepts we spent the most time learning was about prompt engineering.\n\nWe also learned about the importance of branching on our repository as we had to build 3 different components to our project all at the same time on the same application.\n\nLastly, we spent a good chunk of time learning how to implement and improve our similarity search.\n\nWhat's next for Let Them Cook\n\nWe're very satisfied with the MVP we built this weekend, but we know there is far more work to be done.\n\nFirst, we would like to deploy our recipe similarity service currently working on a local environment to our production environment. We would also like to incorporate a ranking system that will allow our LLM to take in crowdsourced user feedback in generating recipe substitutions.\n\nAdditionally, we would like to enhance our recipe substitution service to make use of recipe steps rather than solely ingredients . We believe that the added context of how ingredients come together will result in even higher quality substitutions.\n\nFinally, we hope to add an option for users to directly insert a recipe URL rather than copy-and-pasting the ingredients. We would write another service to scrape the site and extract the information a user would previously paste."},{"heading":"Built With","content":"ai fastapi javascript llm openai python react tinymce zilliz"},{"heading":"Try it out","content":"letthemcook.tech github.com github.com github.com"}]},{"project_title":"Article Semantic Comprehension ","project_url":"https://devpost.com/software/article-semantic-comprehension","tagline":"Articles are often misleading, this Accessibility technology determines bias in language from articles as-well as ensure summarisation","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Cal Hacks 10.0","hackathon_url":"https://cal-hacks-10.devpost.com/","prize_name":"[MLH - Taipy] Best Use of Taipy"}],"team_members":[],"built_with":[{"name":"hume","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"taipy","url":null},{"name":"together","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/NickCoding22/article_analysis"}],"description_sections":[{"heading":"Inspiration","content":"One of our project partners came up with this idea during the summer. We decided that the hackathon is a great opportunity for us to bring the project to life."},{"heading":"What it does","content":"What our article analysis does is that it analyzes and provides a summary of the article that the person entered. It summarizes the article by giving five key bullet points that are important to help with the user's understanding of the article. The program is also able to detect misinterpretation through semantic analysis."},{"heading":"How we built it","content":"We used Hume.ai to be able to analyze semantic in text. We also used Together.ai for our LLM. We coded the project in python and Taipy is used in the backend."},{"heading":"Challenges we ran into","content":"The challenges we ran into were being able to implement Hume.ai to be able to analyze the article that we entered and being able to create the LLM."},{"heading":"Accomplishments that we're proud of","content":"We were proud of the fact that we were able to fully implement the Hume.ai to work with the articles, as this took us the longest time to figure out."},{"heading":"What we learned","content":"We learned that API are very helpful in creating our programs."},{"heading":"What's next for Article Semantic Comprehension","content":"The next step is to be able to type in a concept and multiple articles popup with analysis and summaries."},{"heading":"Built With","content":"hume python taipy together"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-17T18:22:57.699781Z"}}