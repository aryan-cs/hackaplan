{"version":"v1","hackathon_url":"https://microsoftfabric.devpost.com","generated_at":"2026-02-18T16:41:10.520331Z","result":{"hackathon":{"name":"Microsoft Fabric and AI Learning Hackathon","url":"https://microsoftfabric.devpost.com","gallery_url":"https://microsoftfabric.devpost.com/project-gallery","scanned_pages":4,"scanned_projects":82,"winner_count":13},"winners":[{"project_title":"InfraGen","project_url":"https://devpost.com/software/project-s9okw0","tagline":"Transforming AI development with adaptive, high-quality automatic image datasets generation on Microsoft Fabric.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/132/695/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Best Microsoft Fabric + AI Innovation"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"datalake","url":null},{"name":"fabric","url":null},{"name":"particle","url":"https://devpost.com/software/built-with/particle"},{"name":"powerbi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"github.com","url":"https://github.com/58191554/InFreGen"}],"description_sections":[{"heading":"Inspiration","content":"The need for precise, high-quality datasets is critical in AI development. We saw an opportunity to leverage Microsoft Fabric’s powerful ecosystem to create a solution that automates and customizes real world multimodal dataset generation, helping developers save time and increase the quality of their AI models."},{"heading":"What it does","content":"InfraGen is a cloud-native data pipeline that automatically generates diverse, user-specific image datasets. It combines structured and unstructured data, AI-enhanced labeling, and multimodal integration to produce high-quality datasets ready for model training, testing, and validation."},{"heading":"How we built it","content":"Using Microsoft Fabric, we integrated a data lake to store tables and datasets, while leveraging Azure OpenAI GPT-4 for advanced data processing. By utilizing Microsoft Fabric’s data science features—such as notebooks, machine learning models, and multimodal AI tools like CLIP—we implemented precise classification and detection capabilities for accurate image categorization. With integrated Power BI and AI Copilot, we can seamlessly expand multimodal insights and perform real-time visual analysis on generated datasets. The pipeline is designed to be modular, scalable, and highly adaptable to diverse data needs."},{"heading":"Challenges we ran into","content":"We faced challenges in integrating different AI models smoothly into a single pipeline when each model requires different environment to run. It is also challenge to ensuring real-time data retrieval and processing for efficient performance. Making the pipeline modular and adaptable across diverse AI models was another complex task we had to tackle."},{"heading":"Accomplishments that we're proud of","content":"We’re proud of using machine learning models and Fabric platform to creating a robust, end-to-end pipeline that simplifies dataset generation for AI developers. Successfully incorporating multimodal AI, retrieval-augmented generation, and scalable architecture into Microsoft Fabric has made InfraGen a valuable tool for handling diverse and specialized data needs."},{"heading":"What we learned","content":"We gained a deeper understanding of Microsoft Fabric’s ecosystem and the flexibility it offers for integrating AI and data science tools. We also learned the importance of modular design and adaptability in building solutions that can meet a wide range of data requirements."},{"heading":"What's next for InfraGen","content":"We plan to expand InfraGen’s capabilities by incorporating more data types and adding features like automated data labeling. Future development includes optimizing the pipeline for real-time data updates and improving interoperability with other AI tools like Azure Machine Learning Platform, making InfraGen even more versatile for various AI applications."},{"heading":"Built With","content":"azure datalake fabric particle powerbi python pytorch sql"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Data-Driven Intelligence with Microsoft Fabric and OpenAI","project_url":"https://devpost.com/software/data-driven-intelligence-with-microsoft-fabric-and-openai","tagline":"Our project integrates Azure OpenAI and Microsoft Fabric to streamline multi-source data ingestion, transformation, and AI-driven insights, empowering fast, data-driven sales and marketing decisions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/133/472/datas/medium.jpeg","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Grand Prize"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azure-fabric","url":null},{"name":"crm","url":null},{"name":"erp","url":null},{"name":"lakehouse","url":null},{"name":"medallion","url":null},{"name":"onelake","url":null},{"name":"openai","url":null},{"name":"powerbi","url":null},{"name":"pyspark","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"github.com","url":"https://github.com/Srujan1993/datadabblers"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration came from the ongoing challenge that companies face in making sense of data scattered across various SaaS platforms. Sales directors often struggle to extract clear insights from an overload of charts across sales, marketing, and customer relationship data—a process that can take days or even weeks to interpret. We saw an opportunity to streamline data management and empower businesses with faster, more informed decision-making through the capabilities of Microsoft Fabric and Azure OpenAI."},{"heading":"What It Does","content":"Our project centralises data from multiple sources (HubSpot CRM, Azure SQL Database, AWS S3) into a unified platform using Microsoft Fabric. Leveraging Azure OpenAI's large language models (LLMs), it generates actionable insights, such as sales trends, performance drivers, and strategic recommendations, all presented through an intuitive PowerBI report."},{"heading":"How We Built It","content":"We started by ingesting data from HubSpot CRM, ERP data in Azure SQL, and social media reviews from AWS S3, leveraging core features such as ETL pipelines, mirroring and shortcuts. We implemented Medallion Architecture in Microsoft Fabric’s Lakehouse, organising data from raw to refined stages. We started with Bronze for raw ingestion, moved to Silver for cleaned, structured data, and finished with Gold for analytics-ready insights. Azure OpenAI was then integrated to analyse sales and review data, generating AI-driven insights that is visualised in PowerBI."},{"heading":"Challenges We Ran Into","content":"One of our main challenges was optimising Azure OpenAI's responses to ensure clarity and relevance. We also worked to find the right balance between refining data through specific queries and relying on the language model to sift through large amounts of raw data to produce meaningful insights."},{"heading":"Accomplishments That We're Proud Of","content":"We’re proud of delivering a robust, scalable solution that simplifies data analysis for business users. Automating the flow of data and insights, our project produces valuable business insights in minutes—tasks that traditionally would have taken days, saving time and improving strategic agility. We maximised the potential of Fabric's built-in features to reduce time to market."},{"heading":"What We Learned","content":"Through this project, we gained valuable insights into Microsoft Fabric's platform, particularly its low-code/no-code tools for data ingestion, processing, and modelling. We also explored the benefits of the Medallion Architecture and PySpark Notebooks for efficient data processing. Additionally, we experienced the capability of Azure OpenAI to analyse raw data and generate meaningful insights, highlighting its potential for comparative analysis."},{"heading":"What's Next for Data-Driven Intelligence with Microsoft Fabric and OpenAI","content":"Looking ahead, we plan to expand the platform to support a wider range of data sources and industries, such as manufacturing and operations, and to explore analysing streaming data providing near real-time insights. Our goal is to make the solution increasingly configurable, allowing users greater flexibility in tailoring queries to meet specific requirements. Testing with larger datasets will be crucial for assessing scalability and performance. Furthermore, we plan to explore newer language models, such as GPT o1, to extract deeper insights from data while potentially reducing the need for extensive upfront data manipulation."},{"heading":"Built With","content":"api azure azure-fabric crm erp lakehouse medallion onelake openai powerbi pyspark python sql"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MeteoWatch","project_url":"https://devpost.com/software/dajoma","tagline":"MeteoWatch is a real-time intelligence solution designed to monitor flights for potential weather hazards.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/149/137/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Best use of Real-Time Intelligence in Microsoft Fabric"}],"team_members":[],"built_with":[{"name":"eventhouse","url":null},{"name":"eventstream","url":null},{"name":"fabric","url":null},{"name":"kql","url":null},{"name":"notebook","url":null},{"name":"openai","url":null},{"name":"pipeline","url":null},{"name":"powerbi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/MeteoWatch/MeteoWatch/tree/main"}],"description_sections":[{"heading":"Inspiration","content":"We brainstormed around use cases solvable with open data that we could tackle using real-time intelligence technologies, as none of us had experience with it and were eager to try it using Microsoft Fabric. We all work at Deutsche Bahn, but unfortunately, there isn’t much open railway data available. However, there is actually a lot of open aviation data! So, we began with the question, \"What can you do with OpenSky Network data?\" Then we discovered open data on weather hazards through OpenAviation, as well as route and vehicle reference data on ADSDB. We reached out to ADSDB and were granted permission to use their data for the hackathon. Then we put it all together: Why not build a monitoring and alerting system based on Fabric Eventhouse and use Azure OpenAI to customize alert messages?"},{"heading":"What it does","content":"The system consumes data from OpenSky Network, ADSDB, and AviationWeather. Using this data, MeteoWatch knows the current position of aircraft and their flight routes. The system is also aware of areas where weather hazards, known as SIGMETs (Significant Meteorological Information), are reported. MeteoWatch calculates whether a flight will be affected by a SIGMET and creates a natural language warning message, which could be tailored and sent to a specific aircraft. Additionally, MeteoWatch includes a dashboard where stakeholders such as air control or pilots can view the flight route, relevant weather hazards, and other useful information."},{"heading":"How we built it","content":"The ingestion is handled with notebooks, and all data is accessed via web APIs. We send the data to EventStream to ingest it into an Eventhouse. Essentially, we built a real-time intelligence medallion architecture. We use update policies to perform basic transformations and cleanup from the bronze to the silver layer. On top of the silver layer, we use materialized views to create geometries in different formats, calculate intersections, track alerts, aggregate the latest information, and so on. We then use a notebook to send alerts to the Azure OpenAI API to generate more meaningful warning messages. These are sent to EventStream, and from there to Reflex, where alerting via email, Teams messages, etc., can be configured. Additionally, we built a dashboard on top of the gold layer, displaying all geometries (positions, trajectories, hazards) and other useful information."},{"heading":"Challenges we ran into","content":"We encountered issues calling web APIs from Eventhouse using KQL. Although it provides some functionality in this regard, it wasn’t sufficient for our ingestion needs, so we opted to use notebooks instead, which we probably wouldn’t do in a real-life scenario. We also faced limitations with access to OpenAI models. We would have loved to experiment with text-to-speech to broadcast messages to aircraft, but this model wasn’t available in the hackathon environment. The rate limit also posed challenges in real-time scenarios, as the system couldn’t process all alerts in time under these constraints. We also used onmicrosoft accounts based on a personal Azure environment, so we didn’t have access to Teams and Outlook, which prevented us from testing alerting through those channels. Unfortunately, Copilots weren’t available in our Fabric trial environment, although we would have liked to use them. Finally, since we are not domain experts in aviation, there was an additional challenge in understanding and applying the domain knowledge required."},{"heading":"Accomplishments that we're proud of","content":"We are proud that we managed to build such a complex system in our limited free time!"},{"heading":"What we learned","content":"Although we all have experience in big data, this was our first time building a complex system focused on real-time intelligence. We learned a lot about KQL and real-time processing. Furthermore, we learned about geospatial analysis, formats like GeoJSON and WKT, and how to use generative AI in a data solution. It was also exciting to explore the domain of aviation."},{"heading":"What's next for MeteoWatch","content":"We plan to use this project for upcoming hackathons. For example, we have a private hackathon at Deutsche Bahn where we’ll use MeteoWatch to demonstrate the potential capabilities and benefits of Microsoft Fabric to our colleagues. In the future, we may experiment with text-to-speech for alert messages and develop a RAG architecture to enable an agent to provide tailored answers to questions about flight routes and weather hazards. If we find more time, we’ll also fix bugs and improve the dashboard!"},{"heading":"Built With","content":"eventhouse eventstream fabric kql notebook openai pipeline powerbi python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Book Recommendations AI Assistant","project_url":"https://devpost.com/software/ai-assistant-in-web-app","tagline":"This project is about building a new Recommendations AI Engine leveraging the power of Azure SQL Database, AI Search Vector Index, Microsoft Fabric and Azure Web Apps.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/065/599/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: Microsoft Fabric + AI Innovation"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/passadis/ai-assistant"},{"label":"webaiapp003.azurewebsites.net","url":"https://webaiapp003.azurewebsites.net/"}],"description_sections":[{"heading":"Inspiration","content":"The Microsoft Fabric Platform made a huge impact as a unified SaaS offering with a well established set of integration capabilities. The inspiration came from the idea to create an AI powered recommendations engine Web App, where developers can work from a single platform for all the aspects of the build lifecycle."},{"heading":"What it does","content":"This a recommendations AI Assistant hosted in Azure Web Apps. The Data is one hand user SignUp details with Book Genre preferences stored in Azure SQL, and a Books Dataset stored in Fabric OneLake. Two Separate pipelines create and update on schedule Vector Indexes within Azure AI Search, where embeddings are generated from Azure OpenAI, using Jupyter Notebooks. The Notebooks perform also data cleaning and transformation, as well as incorporate API rate Limiting backoff procedures for successful embeddings generation. Users are able to login to the Web APP with username \\ password where a React Frontend allows them to interact with the Assistant, a NodeJS backend, that recognizes keywords like recommendation or rating and provides answers to the users. When new users sign up a Reflex is activated by CDC on Azure SQL and it fires up a new indexing process for only the new users. The User is updated in near real time with a polling mechanism and a new field created in Azure SQL upon embeddings and index update."},{"heading":"How we built it","content":"We utilized Terraform for the Azure Infrastructure like KeyVaut , Azure SQL, Azure Container Registry and Microsoft Fabric Environment, Fabric Pipelines, Fabric Onelake and Notebooks. The Frontend is a React WebApp containerized and the backend an expressJS , containerized as well. We utilize an Azure Function to update the SQL Users Table field \"RecommendationsReady' and a message updates the user in the UI that their personalized recommendations are ready."},{"heading":"Challenges we ran into","content":"The challenge was to create a backoff strategy for API Rate Limits, and to manage correctly the Environment and the Cleaning Code for the Books Dataset. Also the decision to use CDC proved to save a lot of coding since we just re index the AI Search Index with a trigger from new user registration on Azure SQL"},{"heading":"Accomplishments that we're proud of","content":"Integrating a Custom Identity Solution on Azure SQL with AI Search and the final result where users Data upon registration are stored in AI Search and a separate Index is created from the books dataset. The Index is updated in Real Time with a CDC Event, with specific rule only to watch the Id Column so we can still update the Table without falling in a loop. All users can chat with the engine about their favorite books, ask about specific Genres or Ratings and even words in the description. We are also very happy with the last minute functionality to update the UI when the embeddings are completed and the index is updated."},{"heading":"What we learned","content":"We learned a lot about AI Search Indexes, moreover we understood the reach and the power of Fabric and how Jupyter Notebooks can be really automated within an environment. Also connecting Azure DevOps repositories provided the ability to have another option for DevOps developers to deploy their code. Fabric Pipelines added value to the overall solution making it a self updating engine when it comes to new data."},{"heading":"What's next for Book Recommendations AI Assistant","content":"The plan is to make the UI more appealing, provide more flexibility into the recommendations engine and most important create a Fabric Dashboard that can get logging and monitoring data from all sources, and provide Insights of the Application with precision."},{"heading":"Built With","content":"azure docker express.js node.js python"},{"heading":"Try it out","content":"github.com webaiapp003.azurewebsites.net"}]},{"project_title":"Hiking Alerts","project_url":"https://devpost.com/software/hiking-alerts","tagline":"Hiking alerts aims to standardize events issued by institutions for hiking trails in a global context. By linking the events with geospatial data users have a better overview on trail conditions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/133/825/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Best SQL And AI Integration"}],"team_members":[],"built_with":[{"name":"angular.js","url":"https://devpost.com/software/built-with/angular-js"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"spring-boot","url":null},{"name":"tsql","url":null}],"external_links":[{"label":"hiking-alerts.org","url":"https://hiking-alerts.org"},{"label":"github.com","url":"https://github.com/lukas8920/hikealerts"}],"description_sections":[{"heading":"Inspiration","content":"When viewing and planning tracks, popular hiking apps do not display current information by official institutions for hiking tracks in a standard format. Instead, those apps heavily rely on their respective communities updating the information manually.\n\nThis relevant information for hikers from institutions might include dangers, weather impacts, closures etc (in the following called events). When planning hiking tours solely via apps, there is a risk of overseeing important events."},{"heading":"What it does","content":"Microsoft's AI and Fabric services enable to standardize the events issued by different institutions in a global context and connect the events with geospatial data. The purpose of the website and the API is to provide transparency on the accessibility of hiking tracks, making hiking safer and a more enjoyable experience ( https://www.hiking-alerts.org )."},{"heading":"How I built it","content":"Essentially, there are three types of fabric notebooks extracting raw data from country specific data sources:\n\nRaw Event Parsers, which query the events every four hours Trail Parsers, which query trail geodata every month Region Parsers, which query boundaries for regions, e.g. parks, every month All those parsers insert the data into an Azure SQL database. For the raw events, there is a separate table (dbo.raw_events_CT) which tracks updates, insertions and deletes.\n\nAn additional notebook job queries data from the raw_events_CT table every 4 hours and asks Open AI to standardize the raw events. The responses from Open AI are pushed to an Azure Blob Storage Queue.\n\nA Spring Boot application is responsible for querying the queue every 5 minutes and then mapping the input provided by Open AI to trail geodata. Any matches are then saved into an Azure SQL database. The Spring Boot application also serves the REST endpoints for the Angular frontend and a public facing API which allows users to insert, query and delete events from the database."},{"heading":"Challenges I ran into","content":"One challenge were varying Open AI responses and correctness of the responses. The service handles those cases by making matching configurations in the Spring Boot application very strict. The intention is to make the service as accurate as possible. This means that not all events / alerts are mapped."},{"heading":"Accomplishments that I am proud of","content":"Design of a scalable setup which allows to extend the service to further countries, thereby providing a unique service."},{"heading":"What I learned","content":"Very valuable lessons were how to make Azure services available to Fabric notebooks and how to integrate Azure services with on-premise servers (here: the Spring Boot server)."},{"heading":"What's next for Hiking Alerts","content":"Onboarding further institutions. Switzerland / Swiss Alpine Club and Ireland / Sportireland are in progress. Reaching out to institutions which don't have a public facing endpoint, but collect data centrally, e.g. Finland / Metsähallitus & Canada / Parks Canada. Integrate a translation service to connect non-english data providers"},{"heading":"Built With","content":"angular.js openai python spring-boot tsql"},{"heading":"Try it out","content":"hiking-alerts.org github.com"}]},{"project_title":"Research Of Research","project_url":"https://devpost.com/software/research-of-research","tagline":"R.O.R: Researching Our Research","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/134/229/datas/medium.jpeg","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Best Azure Cosmos DB + Microsoft Fabric Integration"}],"team_members":[],"built_with":[{"name":"azure-ai-search","url":null},{"name":"azure-openai","url":null},{"name":"cosmosdb","url":null},{"name":"fabric","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"power-automate","url":null},{"name":"powerbi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/choisiulun1/Research_Of_Research"},{"label":"app.fabric.microsoft.com","url":"https://app.fabric.microsoft.com/groups/2007662d-6654-4a40-be93-23217fe4b693/reports/2a61e997-77b5-4418-bde7-afb3946cc2c6?ctid=b5dc206c-17fd-4b06-8bc8-24f0bb650229&pbi_source=linkShare&bookmarkGuid=981410d3-24fa-42c8-8327-c1163d8e9fb2"}],"description_sections":[{"heading":"Inspiration","content":"The vast amount of academic research available today is both a treasure trove of knowledge and a challenge to navigate. We wanted to create a tool that empowers researchers by streamlining the discovery process, automating data handling, and providing relevant insights at the right time. The goal was to make academic exploration as efficient and insightful as possible, allowing researchers to focus on their work without getting bogged down in tedious data searches."},{"heading":"What it does","content":"Research of Research is an automated platform that manages research queries, gathers and organizes relevant research data, and delivers actionable insights directly to users. Leveraging multiple APIs and AI-driven tagging, the system fetches, processes, and enriches metadata from scholarly databases. It updates reports dynamically in Power BI and sends them to users, ensuring they have access to the latest academic insights. Additionally, the platform continuously updates its AI-powered search index, making it a powerful tool for discovering new information with ease."},{"heading":"How we built it","content":"The platform is built with a modular architecture, combining technologies like Flask, CosmosDB, and Azure functions for efficient data handling. The main components include: • Data Retrieval: Using Arxiv and Semantic Scholar APIs to fetch research paper metadata. • Concurrent Processing: Managed by arxiv_search_master, which runs arxiv_search and arxiv_tag tasks in parallel for faster data processing. • Lakehouse Storage: Storing raw metadata in the Bronze Lakehouse, processed and tagged data in the Silver Lakehouse, and formatted data for Power BI in the Gold Lakehouse. • Power BI and Automation: Power Automate handles dataset refreshes, report generation, and automated email notifications to users. • AI-Driven Search: An AI-powered search index, updated through CosmosDB indexers, ensures users can access the latest research results."},{"heading":"Challenges we ran into","content":"One of the main challenges was ensuring seamless integration between multiple APIs and handling large volumes of data concurrently. Managing data flow from Bronze to Silver to Gold Lakehouses while maintaining data integrity and performance was another hurdle. Ensuring that the AI-driven tagging and search index updates were accurate and efficient required substantial testing and tuning."},{"heading":"Accomplishments that we're proud of","content":"We’re proud of building a system that automates the end-to-end research discovery process, making academic insights accessible with minimal effort from users. Successfully integrating concurrent processing with multi-layered data storage and automated reporting has allowed us to create a solution that feels both robust and responsive. Our AI-powered tagging and search indexing functions have also demonstrated significant improvement in the relevance of search results, which was a major milestone for us."},{"heading":"What we learned","content":"This project taught us a lot about managing complex data workflows and the importance of modular design for scalability. We deepened our understanding of API integration, concurrency management, and data lakehouse architectures. Working with Power Automate for seamless user notifications and report automation was also a valuable experience, helping us appreciate the power of automated workflows in reducing manual effort."},{"heading":"What's next for Research Of Research","content":"Moving forward, we aim to enhance the platform’s AI capabilities by incorporating more advanced natural language processing for better topic suggestion and tagging accuracy. We also plan to expand the data sources beyond Arxiv and Semantic Scholar to provide a more comprehensive research experience. Additionally, implementing user personalization features and an interactive dashboard could make the system even more intuitive and tailored to individual research needs."},{"heading":"Built With","content":"azure-ai-search azure-openai cosmosdb fabric flask javascript power-automate powerbi python"},{"heading":"Try it out","content":"github.com app.fabric.microsoft.com"}]},{"project_title":"Real Time Equipment Monitoring and Predictive Maintenance","project_url":"https://devpost.com/software/azure-database-for-postgresql-integration","tagline":"An intelligent monitoring system for equipment sensors that offers real-time insights and predictive analysis for optimal performance.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/124/056/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Best Azure Database for PostgreSQL Integration"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"eventhouse","url":null},{"name":"eventstream","url":null},{"name":"kql","url":null},{"name":"microsoftfabric","url":null},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"pyspark","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"},{"name":"visualstudiocode","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/revathi2592/Real-Time-Equipment-Monitoring-and-Predictive-Maintenance/tree/main/Real%20Time%20Equipment%20Monitoring%20and%20Predictive%20Maintenance"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for this project stemmed from the need to continuously monitor the health of industrial equipment using real-time sensor data. Sensors on devices capture metrics like temperature and vibration levels every minute, and by analyzing this data, we can predict failures before they occur. This can drastically reduce downtime and improve operational efficiency. Leveraging Azure’s cloud ecosystem and modern data architecture, we aimed to build a system that enables proactive maintenance and equips decision-makers with the data they need to ensure smooth operations."},{"heading":"What it does","content":"This project focuses on real-time monitoring of equipment health through sensor data. The core functionality includes:\n\nCapturing real-time temperature and vibration data from devices via Azure Database for PostgreSQL with Change Data Capture (CDC) enabled. Streaming this data to a KQL database for real-time analysis and storage. Using a snapshot table to maintain the latest device status using Medallion Architecture for efficient reporting. Providing an interactive Power BI report that visualizes the device health, status trends, and sensor readings, enabling stakeholders to track equipment performance in real-time and take action proactively."},{"heading":"How we built it","content":"We built the project using several Fabric components and best practices for real-time data streaming and analytics:\n\nAzure Database for PostgreSQL with CDC : We started by setting up a PostgreSQL database with CDC enabled to track changes in equipment sensor readings. This allowed us to stream data updates as soon as they occurred. Event Stream Creation : We created an event stream that linked the PostgreSQL database to the downstream system. The stream was configured to select specific fields (temperature, vibration, transaction type, and timestamp) from the CDC payload. Downstream KQL Database : Data was then streamed into a KQL (Kusto Query Language) database, where a new table was created to store the raw sensor data for analysis. Snapshot Table (SCD Type 1) : A snapshot table was set up using the Medallion Architecture to store the latest status of each device (normal, warning, or faulty). This table was essential for providing an up-to-date view of the equipment for reporting purposes. Power BI Reporting : Finally, a Power BI report was built using data from the KQL table and the snapshot table. The report displayed key metrics such as: Device health status (normal, warning, faulty) Trends in device status changes Line charts showing temperature and vibration levels A table visual with complete device details for further analysis"},{"heading":"Challenges we ran into","content":"Real-Time Data Stream Integration : Integrating the real-time sensor data from PostgreSQL into a downstream system posed a few challenges. Ensuring data consistency and minimizing delays in the streaming process required fine-tuning of the CDC configuration. Handling High-Frequency Data : Since sensor data is captured every minute, managing a high volume of data in near real-time was a challenge. Optimizing the event stream for performance was crucial to ensure smooth operation. Data Consistency in the Snapshot Table : Maintaining accurate and consistent device status in the snapshot table was tricky. We had to ensure that the latest data was correctly reflected, without introducing duplicates or missing entries. Power BI Performance : Since real-time data is constantly being streamed, ensuring that Power BI reports didn’t lag when loading large datasets was another hurdle. We had to optimize queries and Power BI’s data models to handle this in a scalable way."},{"heading":"Accomplishments that we're proud of","content":"Real-Time Monitoring : Successfully set up a real-time data streaming pipeline that allows for continuous monitoring of equipment health, providing timely insights into the status of devices. Medallion Architecture Implementation : Implemented a Medallion Architecture to maintain a snapshot of the latest equipment statuses, which is essential for providing accurate and up-to-date reports. Interactive Power BI Report : Built a dynamic Power BI dashboard that visualizes trends, device statuses, and sensor readings. This gives stakeholders a clear picture of equipment health and allows them to track changes over time. Predictive Insights : By combining historical and real-time data, the system lays the foundation for predictive maintenance, helping identify patterns in sensor data that could indicate impending equipment failures."},{"heading":"What we learned","content":"Microsoft Fabric as a One-Stop Solution : A major learning from this project was the power of Microsoft Fabric as an integrated, end-to-end solution for data processing and reporting. We were able to leverage Microsoft Fabric to: Seamlessly fetch and stream data via Event Stream , which connects directly to our source (Azure Database for PostgreSQL). Use the Event House to land the streaming data in OneLake , a unified data lake that acts as the central repository. Efficiently integrate this data into reporting and analytics by connecting it to downstream services, such as KQL databases and Power BI for dynamic, real-time insights. Data Architecture : Implementing the Medallion Architecture helped us understand how to efficiently store raw data and derive insights from it, ensuring scalability and performance. Power BI Optimization : We learned how to optimize Power BI reports to handle large amounts of real-time data without sacrificing performance, enabling users to interact with the data efficiently."},{"heading":"What's next for Real Time Equipment Monitoring and Predictive Maintenance","content":"Predictive Analytics : With historical and real-time data in place, the next phase will focus on building machine learning models to predict device failures based on sensor data patterns. This will help shift the system from reactive to truly predictive maintenance. Enhanced Reporting : We plan to enhance the Power BI dashboards further by integrating more advanced visualizations, such as anomaly detection, to highlight potential issues proactively."},{"heading":"Built With","content":"azure eventhouse eventstream kql microsoftfabric postgresql pyspark python sql visualstudiocode"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"LinkedIn Job Market Analysis in Sweden","project_url":"https://devpost.com/software/ms-hackathon","tagline":"Struggling to find a job in Sweden? Want to accelerate your career path? Gain insights into the job market by analyzing LinkedIn posts to uncover key trends with the help of AI..","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/133/557/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: Microsoft Fabric + AI Innovation"}],"team_members":[],"built_with":[{"name":"elt","url":null},{"name":"etl","url":null},{"name":"https://api.scb.se/ov0104/v1/doris/sv/ssd/start/be/be0101/be0101a/befolkmanad","url":null},{"name":"https://www.linkedin.com/jobs-guest/jobs/api/jobposting/{job-id}","url":null},{"name":"https://www.linkedin.com/jobs-guest/jobs/api/seemorejobpostings/","url":null},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"openai","url":null},{"name":"pyspark","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"requests","url":"https://devpost.com/software/built-with/requests"},{"name":"sparksql","url":null},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"github.com","url":"https://github.com/AnasMofleh/Linkedin_jobs_datalake"}],"description_sections":[{"heading":"Inspiration","content":"This project is designed to assist individuals who are struggling to find jobs in Sweden by providing insights into the job market. By analyzing job postings, it reveals which tools, skills, and requirements are most in demand by employers, which is particularly beneficial for those who may not have a strong understanding of the market, such as new graduates or career changers. Additionally, the project offers city-level insights, which can be especially useful for newcomers or those considering relocating to Sweden. It provides a clearer picture of which cities have the most job opportunities, making it easier for job seekers to align their job search with current market trends."},{"heading":"What it does","content":"This project is an in-depth analysis designed to derive insights from LinkedIn job posts within Sweden on an hourly basis. It utilizes a robust architecture integrating various technologies and services to capture, store, process, and analyze jobs posted on LinkedIn on an hourly basis. Additionally, it incorporates population data at the city level to provide a comprehensive view of the Swedish job market dynamics."},{"heading":"How we built it","content":"The project leverages the medallion architecture using Microsoft Fabric service. It utilizes A fabric workspaces with two lakehouses—bronze and silver—and two main data pipelines.\n\nThe Job Data Hourly pipeline extracts recent job posts from Sweden on an hourly basis, along with each company's follower count. This raw data is stored in the bronze data lakehouse, then cleaned, processed, and transferred to the silver data lakehouse. Using Azure OpenAI, we analyze each job post to identify tools, requirements, offers, and work types, which Power BI then uses for reporting.\n\nThe Population Data Monthly pipeline retrieves monthly population data from SCB, including demographic details by city. This data is stored, cleaned, and refined in the data lakehouses and integrated into Power BI reports to provide city-level insights."},{"heading":"Challenges we ran into","content":"Extracting information from LinkedIn proved more complex than we initially anticipated. Gathering accurate data on job posts and company follower counts required overcoming technical and logistical challenges, as LinkedIn data is not always easily accessible or structured for extraction. Additionally, ensuring the data was consistently reliable and up-to-date added another layer of complexity to the process."},{"heading":"Accomplishments That We're Proud Of","content":"We successfully built a robust data pipeline capable of handling large volumes of data. We’re also proud of the detailed, interactive Power BI reports, which offer valuable insights to job seekers and stakeholders alike. We've familiarized ourselves with Microsoft Fabric with most of its different components. We've also got to work with data engineering principles hand on which is beneficial for our carriers."},{"heading":"What we learned","content":"Throughout this project, we learned the importance of data accuracy and the challenges associated with using APIs and data integration. We gained experience in using Azure OpenAI for natural language processing and improved our skills in building and managing data pipelines. Used different dialect of spark and got them to play in the same notebooks. Additionally, we learned how to create effective visualizations in Power BI to communicate complex data insights clearly."},{"heading":"What's next for Linkedin jobs analysis","content":"The next step is to provide job seekers with an advanced tool that finds job posts aligning with their experience and qualifications. We want to be able to integrate with Linkedin using it is newer API version so we can scale the solution into bigger markets and countries. This tool could include an advanced search feature allowing users to upload their CV, which would then be analyzed to match them with relevant job postings. The tool would provide a list of job opportunities that align closely with the user’s skills and experience, along with a probability score indicating the likelihood of securing each position."},{"heading":"Built With","content":"elt etl https://api.scb.se/ov0104/v1/doris/sv/ssd/start/be/be0101/be0101a/befolkmanad https://www.linkedin.com/jobs-guest/jobs/api/jobposting/{job-id} https://www.linkedin.com/jobs-guest/jobs/api/seemorejobpostings/ json openai pyspark python requests sparksql sql"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CDC LLM Data Architecture with OpenSource and Fabric","project_url":"https://devpost.com/software/cdc-llm-data-architecture-with-opensource-and-fabric","tagline":"As billions of data get generated daily having a robust system that can manage such an influx of data is of high importance. Building a LLM model on Microsoft Fabric that can easily scale.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/073/060/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: Real-Time Intelligence in Microsoft Fabric"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azureopenai","url":null},{"name":"azurepostgresql","url":null},{"name":"chatgptapi","url":null},{"name":"datalake","url":null},{"name":"debezium","url":null},{"name":"elasticsearch","url":"https://devpost.com/software/built-with/elasticsearch"},{"name":"eventhouse","url":null},{"name":"eventstream","url":null},{"name":"fabric","url":null},{"name":"kafka","url":null},{"name":"kql","url":null},{"name":"powerbi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/kiddojazz/CDC_Stream_Kafka_Fabric"}],"description_sections":[{"heading":"Inspiration","content":"As billions of data get generated daily having a robust system that can manage such an influx of data is of high importance. With OpenSource technology achieving such a process is possible but the question arises, how can this be done also on an enterprise level for business users?\n\nHaving a solution that can stream data in real-time to OpenSource technology like Kafka and also doing the same in Microsoft Fabric to create an LLM solution that can help businesses make quick and fast decisions."},{"heading":"What it does","content":"A real-time data stream collects data from a producer and stores it in an Azure PostgreSQL Database. An LLM is built out of the stored data to respond to user queries (RAG Solution)."},{"heading":"How we built it","content":"Part 1\n\nThe solution performs a CDC approach for a database where data is generated by a producer (mobile application) and sent to the Azure PostgreSQL database via a connection string. A CDC is done with Apache Kafka and Debezium to capture real-time changes in the Azure PostgreSQL and send to Kafka topics.\n\nA consumer script is created to capture the data from Kafka topics and inserted into ElasticSearch index for storage and Kibana for Visualization. The same consumer script also captures data from Kafka topics and sends a JSON file format to the Azure Storage Account (adls).\n\nPart 2:\n\nUsing the CDC feature available in Microsoft Fabric EventStream we connected to Azure PostgreSQL database and consumed data in real-time from it. A KQL database was created for querying and standardizing the entire streaming data. We then created a near-to-real-time dashboard solution with Power BI from the KQL database queries.\n\nA Fabric pipeline was created to load data incrementally from the Azure Storage Account to the Fabric Lakehouse file folder. This file will be used in our LLM creation.\n\nPart 3\n\nData transformation was done on the JSON data written to the Lakehouse File folder. It was flattened and converted to a Spark dataframe. The Spark dataframe was further cleansed and standardized before being written as a delta table.\n\nWe created an LLM (RAG) and read the delta table created and promoted the LLM to create SQL queries based on the data in the dalta table to answer any user question about the table."},{"heading":"Challenges we ran into","content":"There was a conflict between the CDC of debezium and Microsoft Fabric. We found out that the PostgreSQL Database does not support multiple slots.\n\nThe previous slot used for the Debezium connector affected the connection and setting for Fabric CDC for PostgreSQL. I had to provision a new server to solve this issue."},{"heading":"Accomplishments that we're proud of","content":"We were able to create a real-time CDC for streaming data between a Mobile Application and OpenSource Technology and Fabric Enterprise solution. A near to real-time Analytic dashboard and LLM were created based on the streaming data for semantic mining purposes."},{"heading":"What we learned","content":"How to perform CDC with both OpenSource Technology and Enterprise Solutions. During the whole solution we learnt how to integrate the LLM solution with an existing table(delta) to derive a solution."},{"heading":"What's next for CDC LLM Data Architecture with OpenSource and Fabric","content":"The next step is to scale the solution for the production use case using Kubernetes on Docker and moving from Local Kafka infrastructure to Confluent Kafka on the cloud to better streamline the process."},{"heading":"Built With","content":"azure azureopenai azurepostgresql chatgptapi datalake debezium elasticsearch eventhouse eventstream fabric kafka kql powerbi python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TrendTrackr","project_url":"https://devpost.com/software/trendtrackr","tagline":"\"Leveraging Microsoft Fabric to analyze GitHub activity trends in real-time, enabling deep insights into developer contributions, repository growth, and language popularity.\"","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/120/896/datas/medium.PNG","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: Azure Database for PostgreSQL Integration"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"fabric","url":null},{"name":"githubaction","url":null},{"name":"githubapis","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"powerbi","url":null},{"name":"pyspark","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/cxy012/TrendTrackr"}],"description_sections":[{"heading":"🏆 Accomplishments We’re Proud Of","content":"End-to-End Automation : Successfully integrated an automated workflow using GitHub Actions , Azure Data Factory , and Microsoft Fabric to keep data up-to-date without manual intervention. Real-Time Data Insights : We achieved near-real-time data collection and analysis, providing up-to-date insights into GitHub activity on an hourly basis. Interactive Power BI Dashboards : Created dynamic Power BI dashboards that provided stakeholders with a comprehensive view of GitHub trends—enabling effective decision-making based on clear visual data. Data Consolidation Across Sources : Managed to consolidate multiple data sources into a single Lakehouse structure, enabling consistent and reliable analysis across diverse data points, such as push events and trending repositories.\n\n🚀 Conclusion\n\nTrendTrackr showcases the powerful possibilities of analyzing GitHub event data, providing insights into community engagement, repository growth, and technology trends. By integrating Microsoft Fabric , Azure services , and Power BI , we created an end-to-end solution that turns GitHub activity into valuable insights.\n\nThrough this project, we learned not just about technology, but about the people behind open source—those who contribute, collaborate, and innovate. We hope TrendTrackr will be a stepping stone for more advanced analyses and inspire others to explore data-driven insights in the open-source community.\n\n📈 Future Directions\n\nExpand Data Sources : Integrate additional data sources such as GitHub Issues and Discussions to gain a more holistic understanding of community activity. Machine Learning Integration : Use Azure's machine learning capabilities to build predictive models that can forecast trends in open-source projects. Community Dashboards : Build public dashboards that provide insights to developers about the projects they care about the most.\n\nWe invite you to explore TrendTrackr , see the insights for yourself, and even contribute to its future development. Together, we can make the world of open-source more transparent and insightful for all."},{"heading":"Built With","content":"azure fabric githubaction githubapis javascript postgresql powerbi pyspark python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Finesse.ai","project_url":"https://devpost.com/software/first-li31bt","tagline":"Your AI-powered personal finance assistant: Smart, secure, and effortlessly in sync","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/134/204/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: SQL and AI Integration"}],"team_members":[],"built_with":[{"name":"azureopenaiservices","url":null},{"name":"azuresqldatabase","url":null},{"name":"expo.io","url":"https://devpost.com/software/built-with/expo-io"},{"name":"figma","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"java","url":"https://devpost.com/software/built-with/java"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"microsoftazure","url":null},{"name":"microsoftfabric","url":null},{"name":"openaiapi","url":null},{"name":"reactnative","url":null},{"name":"springboot","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/Ada6-6/billServer"},{"label":"github.com","url":"https://github.com/Ada6-6/billsBot"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration stems from the growing complexity of personal finance management. Many individuals struggle to track their expenses, income, and transfers, especially when it comes to synchronizing data across different devices and platforms. We aim to harness AI technology to create an intelligent, personalized solution that empowers users to effortlessly manage all aspects of their finances and make informed financial decisions."},{"heading":"What it does","content":"The AI Financial Assistant provides users with a smart and convenient way to manage their finances. Users can log expenses, income, and transfers, adding details such as payment locations and timestamps, while also supporting various subscription plans. The AI analyzes users' financial patterns to offer personalized insights and recommendations. Additionally, the app features identity verification, notification reminders, and secure data synchronization across multiple devices, ensuring that users can safely and easily access and manage their financial information."},{"heading":"How we built it","content":"We developed the back end using Java Spring Boot to manage data, while React Native and Expo were utilized to create a seamless mobile experience. For design, we employed Figma to craft an intuitive user interface. Azure SQL handles the app’s data needs, and OpenAI provides personalized financial analysis. By leveraging Microsoft Fabric and Azure AI services, we achieved AI-driven data insights, with Azure SQL ensuring secure storage and real-time data synchronization."},{"heading":"Challenges we ran into","content":"One of the significant challenges was implementing secure real-time data synchronization across multiple devices, requiring robust encryption and error handling to ensure data integrity. Additionally, achieving smooth AI conversational capabilities proved difficult, leading us to continually optimize performance and privacy protections during the OpenAI integration. Integrating multiple transaction types—expenses, income, and transfers—along with securely managing data flows also necessitated strong collaboration within the team."},{"heading":"Accomplishments that we're proud of","content":"We successfully developed a comprehensive and user-friendly AI-driven financial management system that goes beyond basic budgeting to encompass income, expenses, and transfers, offering intelligent analysis and real-time data access. The team overcame the challenge of limited React Native experience to create an optimized cross-platform solution. Additionally, the successful integration of OpenAI to enhance the financial experience for users is a highlight of our accomplishments."},{"heading":"What we learned","content":"This project deepened our understanding of data management and AI integration. We recognized the importance of designing efficient data flows for multi-device synchronization while also understanding the value of AI in enhancing user experience. Our collaborative development in a multi-platform environment improved our technical skills and strengthened the team’s problem-solving capabilities."},{"heading":"What's next for AI Financial Assistant","content":"Looking forward, we plan to add more advanced financial analysis features, such as predictive analytics and personalized budgeting recommendations. We also aim to expand integrations with external financial tools to provide users with a comprehensive financial management experience while enhancing data synchronization capabilities to ensure secure, real-time data access across devices. Furthermore, we plan to incorporate currency support to better serve a broader range of global users."},{"heading":"Built With","content":"azureopenaiservices azuresqldatabase expo.io figma git java javascript microsoftazure microsoftfabric openaiapi reactnative springboot typescript"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"Prompt ReAct: Real-Time Voice Intelligence ","project_url":"https://devpost.com/software/prompt-react-real-time-voice-intelligence","tagline":"Transform prompt engineering with real-time AI analytics powered by Microsoft Fabric and Azure OpenAI's GPT-4o. Seamlessly analyze voice and text interactions Power BI Embedded dashboard","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/133/823/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: Azure Cosmos DB + Microsoft Fabric Integration"}],"team_members":[],"built_with":[{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"hackathon.metroclouds.com","url":"https://hackathon.metroclouds.com/"}],"description_sections":[{"heading":"About the Project","content":"Inspiration\n\nAs conversational AI becomes increasingly central to business operations, the need to optimize AI prompts and responses in real time has grown exponentially. However, existing tools for prompt engineers often fall short, especially when it comes to integrating voice interactions with real-time analytics. This gap inspired us to develop Prompt ReAct —a platform designed to empower prompt engineers with actionable insights directly within Power BI Embedded .\n\nOur vision was simple: leverage Microsoft Fabric and Azure AI Services to create an environment where prompt engineers can refine AI interactions as they happen. We wanted to make Power BI Embedded dashboards not just a reporting tool, but a dynamic space for optimizing AI conversations in real-time.\n\nWhat We Learned\n\nBuilding Prompt ReAct deepened our understanding of integrating Microsoft’s ecosystem to unlock the potential of conversational AI:\n\nPower BI Embedded within Microsoft Fabric : Embedding analytics directly into Power BI allowed us to transform dashboards into interactive hubs for analyzing and optimizing AI prompts in real-time. Azure OpenAI (GPT-4o) : By integrating GPT-4o’s capabilities, we enhanced our ability to process both voice and text data, providing immediate insights for prompt adjustments. Azure AI Search Services : Implementing voice analytics with speech-to-text and text-to-speech capabilities enabled real-time interactions that are analyzed within Power BI. Feedback Loop with Cosmos DB & Azure Cognitive Search : Leveraging data stored in Azure Cosmos DB and indexed by Azure Cognitive Search , we created a continuous cycle of AI prompt optimization based on real-time user feedback.\n\nHow We Built It\n\nWe approached this project with a focus on creating a seamless integration within the Microsoft ecosystem:\n\nDesigning the Conversational Analytics Engine : Power BI Embedded dashboards serve as the central interface, allowing prompt engineers to visualize and optimize conversations as they happen. Voice commands are enabled through Azure Cognitive Services, letting users interact naturally with the dashboard. Leveraging Real-Time Data : Data Ingestion & Storage : Conversations are streamed into Azure Cosmos DB, with real-time access enabled via DirectQuery. This ensures that insights appear instantly in the Power BI dashboard. Embedding & Search Optimization : By generating embeddings using OpenAI models and storing them in Azure Cognitive Search, we enable rapid, context-aware retrieval of conversation data. Establishing a Continuous Feedback Loop : Every AI interaction is analyzed in real time, with insights fed back into Azure Cosmos DB and processed through Azure AI models. This feedback loop helps prompt engineers continuously refine their prompts based on live user interactions, making the system smarter with each conversation.\n\nChallenges We Faced\n\nReal-Time Data Processing : Achieving low-latency responses within Power BI Embedded required careful optimization of our DirectQuery connections to Cosmos DB and fine-tuning of Azure Functions for efficient data retrieval. Voice and Text Analytics Integration : Seamlessly combining voice inputs with GPT-4o text analysis was technically demanding, especially in ensuring both were reflected accurately within Power BI dashboards. Ensuring Scalability and Security : Balancing robust security measures with a smooth user experience was critical. We leveraged Azure Active Directory for secure access and designed our architecture to scale with growing data volumes. User Experience : Crafting an intuitive interface within Power BI meant iterating on design to make the analytics actionable, even for users with limited technical expertise.\n\nAccomplishments We’re Proud Of\n\nTransforming Power BI Dashboards into Interactive Conversational Analytics Hubs : By embedding real-time analytics, we turned Power BI into a platform where prompt engineers can optimize AI interactions on the spot. Closing the Feedback Loop for Continuous Optimization : Integrating data from Cosmos DB, GPT-4o, and Azure Cognitive Search, we built a self-improving system that refines AI prompts based on real-time user interactions. Delivering Actionable Insights : Whether refining a prompt mid-conversation or analyzing past interactions, users can make data-driven decisions instantly, optimizing their strategies on the go.\n\nWhat’s Next?\n\nWe’re excited about the potential to expand Prompt ReAct further:\n\nPredictive Analytics : We plan to introduce trend analysis and anomaly detection, helping users proactively optimize AI interactions. Advanced Conversational Analytics : Expanding capabilities to include sentiment analysis and conversation intent detection to make interactions more responsive. Mobile Compatibility : We’re developing mobile-friendly dashboards to ensure prompt engineers can optimize conversations from anywhere. Multi-Language Support : We aim to broaden the platform’s reach by integrating support for multiple languages, enabling prompt optimization for a global audience.\n\nCall to Action\n\nImagine having the power to refine your AI interactions dynamically, directly within your Power BI dashboards. That’s what Prompt ReAct offers—a new way to enhance prompt engineering, reduce costs, and improve AI engagement.\n\nTry Prompt ReAct today and transform your AI-driven conversations into meaningful, data-backed interactions. Discover how our platform can unlock new possibilities for your organization’s AI strategy."},{"heading":"Built With","content":"javascript typescript"},{"heading":"Try it out","content":"hackathon.metroclouds.com"}]},{"project_title":"SmartAIventory","project_url":"https://devpost.com/software/smartaiventory","tagline":"SmartAIventory: Revolutionizing retail with AI-driven sales forecasts and strategic batch inventory management. Smarter insights, better profits—powered by Microsoft Fabric.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/131/094/datas/medium.png","prizes":[{"hackathon_name":"Microsoft Fabric and AI Learning Hackathon","hackathon_url":"https://microsoftfabric.devpost.com/","prize_name":"Honorable Mention: SQL and AI Integration"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"dotenv","url":null},{"name":"fabric","url":null},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"gradio","url":null},{"name":"openai","url":null},{"name":"powerbi","url":null},{"name":"pyodbc","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"synapse","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/mneang/SmartAInventory"}],"description_sections":[{"heading":"About the Project","content":"What Inspired Us The world of retail is a delicate balancing act—maintaining optimal inventory without overspending or running out of stock. Inspired by the potential of data-driven decision-making, we set out to bridge this gap using cutting-edge AI and Microsoft technologies. Our vision was to empower businesses to transform raw data into strategic assets, ensuring efficiency and maximizing profitability.\n\nWhat We Learned Throughout this journey, we deepened our understanding of the capabilities of Microsoft Fabric, Azure SQL, and AI integration. We learned that sometimes simplicity, like using batch processing for strategic insights, can deliver more value than overcomplicated real-time analytics. We discovered the power of aligning technology with business needs and how to transform complex data into intuitive, actionable insights.\n\nHow We Built It Our solution, SmartAIventory , seamlessly integrates Microsoft Fabric for data ingestion and batch processing, Azure SQL for robust data storage, and Power BI for powerful visual storytelling. We extended the capabilities of our data pipeline by incorporating an AI-driven Gradio chatbot that taps into Azure OpenAI, providing tailored insights to optimize sales and inventory management. By leveraging batch processing, we ensured efficient, scalable data management without unnecessary complexity—keeping it smart, focused, and impactful.\n\nChallenges We Faced Integrating various Microsoft services and ensuring seamless data flow required meticulous planning. We faced firewall restrictions with Azure SQL, prompting us to develop a mock data solution for testing. Fine-tuning the AI to provide relevant and actionable insights while keeping performance efficient also posed a challenge, but we met it head-on with determination and creativity.\n\nOur Passion and Drive What fuels SmartAIventory is a relentless drive to innovate and a desire to make AI and data analytics accessible and practical for real-world impact. We wanted to show that AI isn't just about complexity—it's about providing clarity, driving smarter business decisions, and creating tangible value. With this project, we aim not just to meet expectations but to exceed them, embodying the true spirit of innovation and excellence."},{"heading":"Built With","content":"azure dotenv fabric github gradio openai powerbi pyodbc python sql sqlite synapse"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-18T16:41:10.520331Z"}}