{"version":"v1","hackathon_url":"https://openai.devpost.com","generated_at":"2026-02-18T16:41:05.563313Z","result":{"hackathon":{"name":"OpenAI Open Model Hackathon","url":"https://openai.devpost.com","gallery_url":"https://openai.devpost.com/project-gallery","scanned_pages":21,"scanned_projects":487,"winner_count":6},"winners":[{"project_title":"Steam Print: Optimized 3D Printing from Steam Deck","project_url":"https://devpost.com/software/steam-deck-3d-printing","tagline":"Steam Print: Turn your Steam Deck into a portable AI-powered 3D printing workstation—voice-controlled model generation, with model viewing, and direct printing.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/723/432/datas/medium.png","prizes":[{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"Weirdest Hardware | Sponsored by Ollama & NVIDIA"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gpt-oss:20b","url":null},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"linux-(steam-deck/arch-linux)","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"octopi-api","url":null},{"name":"opengl","url":"https://devpost.com/software/built-with/opengl"},{"name":"pyside6","url":null},{"name":"python-3.8+","url":null},{"name":"qfiledialog","url":null},{"name":"qthread-(pyside6)","url":null},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"shutil","url":null},{"name":"sounddevice","url":null},{"name":"threading","url":null},{"name":"trimesh","url":null},{"name":"vosk","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/JonathanSolvesProblems/SteamPrint-OpenAI-Open-Model-Hackathon"}],"description_sections":[{"heading":"Inspiration","content":"3D printing is powerful but often requires high-performance desktops and heavy slicing applications like Cura. I wanted to make 3D printing more practical and portable , especially from a Steam Deck. By combining lightweight STL viewing, offline AI voice commands, and remote slicing, users can control 3D printing without waiting on slow desktop apps. The idea was to bring dynamic 3D model generation and printing to a handheld device with minimal friction."},{"heading":"What it does","content":"Steam Print allows users to:\n\nLoad, and view 3D models on the Steam Deck Use offline voice commands to interact with models and trigger actions Send voice prompts to a backend server running gpt-oss:20b or gpt-oss:120b , which dynamically generates 3D models based on AI instructions Slice and upload models to OctoPi connected to a Raspberry Pi with camera monitoring Control 3D printing entirely from a handheld, touch-friendly interface\n\nThis makes 3D printing faster, more interactive, and portable."},{"heading":"How it was built","content":"Frontend : PySide6 for GUI, OpenGL and trimesh for 3D visualization, sounddevice and vosk for offline speech recognition Backend : Flask server that receives voice prompts from the Steam Deck, runs gpt-oss:20b or gpt-oss:120b to generate STL models dynamically, and returns them to the client Printing integration : OctoPi API to send sliced models to the 3D printer, with progress monitored via a camera module Threaded workers : All long-running tasks (server requests, slicing, printing) run in background threads to keep UI responsive"},{"heading":"Challenges","content":"Concurrency issues : Multiple threads for voice recognition, server requests, and slicing initially caused crashes Steam Deck performance : Even lightweight STL files could lag, so optimized 3D rendering and file handling Offline voice recognition : Ensuring Vosk worked reliably on Linux/Steam Deck without constant internet Real-time printing feedback : Integrating OctoPi and camera monitoring while maintaining smooth UI updates"},{"heading":"Accomplishments","content":"Dynamic AI-driven 3D model generation directly from the Steam Deck Seamless voice-to-print workflow without heavy desktop software Touch-optimized 3D viewer with smooth interaction on Steam Deck Successfully integrated OctoPi slicing and monitoring from a handheld device Robust threading architecture allowing multiple background tasks without UI crashes"},{"heading":"What I learned","content":"How to efficiently manage threaded operations in PySide6 for a responsive GUI Integrating a large language model (gpt-oss:120b) for real-time 3D content creation Optimizing STL loading and 3D rendering for limited hardware Building a modular frontend-backend system for AI-assisted 3D printing Practical challenges of combining voice recognition, model generation, and 3D printing into one streamlined workflow"},{"heading":"What's next for Steam Print","content":"Enhanced slicing controls directly from the Steam Deck Multi-format support (OBJ, 3MF, PLY) for broader 3D model compatibility AI-assisted print optimization : adjusting orientation, supports, and print parameters automatically Cloud integration : fetch and generate models from online repositories Expanded voice commands and model editing tools for fully hands-free 3D printing Multi-device monitoring : allow multiple Steam Decks or devices to queue prints and monitor remotely"},{"heading":"Built With","content":"flask gpt-oss:20b json linux-(steam-deck/arch-linux) numpy octopi-api opengl pyside6 python-3.8+ qfiledialog qthread-(pyside6) raspberry-pi shutil sounddevice threading trimesh vosk"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"A Printer… for Smell — (AI “Scent Songs”)","project_url":"https://devpost.com/software/a-printer-for-smell-ai-scent-songs","tagline":"An AI “scent compiler” turns prompts into a JSON plan (recipe + timeline). A syringe gantry doses ±2 µL-accurate drops, then two diffusers crossfade fresh and deep blends into time-synced scent songs.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/734/651/datas/medium.jpg","prizes":[{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"Best in Robotics | Sponsored by NVIDIA"}],"team_members":[],"built_with":[{"name":"chatgpt-oss","url":null},{"name":"fast-api","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"nextjs","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/enbotics/scent-printer"},{"label":"github.com","url":"https://github.com/enbotics/scent-printer-hw"}],"description_sections":[{"heading":"Inspiration","content":"Most diffusers are “one-note”: add drops by hand, turn on, hope for the best. We wanted an instrument—a printer for smell—that performs a scent like music: build, crossfade, end cleanly, repeat on command. At the same time, LLMs now turn words into code, images, even audio. But what about smell? If a model can compile text into structured plans, could it compile a scent? That question set our north star: treat air like a medium you can program. The printer metaphor gave us a concrete playbook (test page, recipe, duty cycle), and the “duet” idea—two coordinated atomizers—unlocked layered, evolving scents instead of a flat room smell."},{"heading":"What it does","content":"Prompt-to-Scent Printer turns natural language into a single plan file and then performs it with hardware. Prompt → plan.json (gpt-oss-120b): The model compiles your intent (preferences, duration, strength, sensitivities) into one JSON:\n\nRecipe: which essential oils, volumes (µL), and which atomizer (A = fresh/bright, B = warm/grounded). Timeline: minute-by-minute power for A/B (duty cycles) and stage labels (intro → crossfade → finish).\n\nPrecision dispensing: A syringe array doses essential oils with microliter precision into each atomizer—no manual mixing. Duet playback: Two water-based atomizers crossfade and layer according to the timeline, creating “scent songs.” Clean cutoffs: Automatic purge cycle between blends to prevent carry-over. Safety by default: Allergy/avoid list in the compiler, intensity caps, ventilation reminder, and a physical EMERGENCY STOP. Use cases: focus, sleep, yoga, gaming ambience, wake-up alarm; plus business installs (wellness, galleries, retail demos, R&D labs)."},{"heading":"How we built it","content":"Software Model: gpt-oss-120b (open weights), served on our hosted GPU. We designed a strict JSON schema so the model emits a single plan.json that drives both dosing and playback. Compiler UX: Web app with presets and a two-track timeline (A/B). “Generate” shows the JSON + the graph. Runtime: A controller daemon parses plan.json and orchestrates pumps, gantry moves, and A/B duty cycles in real time (1 s tick), with a state machine for Start / Pause / Stop / Purge.\n\nHardware Syringe array (~100 slots): pre-loaded 2.5 ml syringes on micro linear actuators / syringe pumps; gravity-safe 5 mm input ports; ~5 cm drop height. 3-axis motion: shared linear rail/gantry positions each atomizer under assigned syringes, then returns to center for diffusion. Dual atomizers: large water tanks, independent duty cycles, LED status; run in parallel for crossfades. Water management: shared pump to base-fill and execute purge cycles (pre/post-flush volumes in the plan). Controls & IO: stepper drivers, endstops, flow timing, physical E-STOP wired to cut motors and pumps. Key design choice: one plan file. It simplifies reproducibility, debugging, and testing: if you can render the JSON and the timeline, you can reason about the performance."},{"heading":"Challenges we ran into","content":"Drop repeatability: Oil viscosity and temperature made early dosing inconsistent. We switched to microliter units, shortened tubing, tuned actuator speeds, and added pre/post-flush in the plan. Cross-contamination: Even tiny residue ruins the next blend. We added a purge-then-diffuse cycle and guarded it in the state machine. Time sync: Keeping A/B in lockstep required a centralized scheduler with drift checks and a ±1 s correction window. Explaining A vs B: People don’t intuit “two atomizers.” The timeline visualization (A=blue, B=amber) and short stage labels fixed comprehension."},{"heading":"Accomplishments that we're proud of","content":"A working duet rig that reads a prompt and performs a 20–60 min scent with clean start and finish. The single-file compiler (plan.json) that covers both recipe and playback. Precision microliter dosing and reliable purge → fresh blend. A safety-first UX (allergy filter, intensity cap, E-STOP) baked into both software and hardware."},{"heading":"What we learned","content":"Smell is slow but powerful—designing evolutions (intro → build → finish) matters more than raw intensity. Mechanical tolerances dominate user experience; 1–2 µL errors are noticeable over time. LLMs shine when you give them tight schemas and constraints; “one plan” beats multiple partial outputs. Visualizing the plan (the A/B timeline) is the fastest path to user trust."},{"heading":"What's next for A Printer… for Smell — (AI “Scent Songs”)","content":"Local/offline path: package the compiler for edge devices once memory is available (Ollama/LM Studio configs in the repo). Closed-loop control: add VOC/airflow sensing to auto-adjust duty cycles for room size and ventilation. More voices: quartet mode (A/B/C/D) and spatial choreography across multiple atomizers. Creators’ API: OSC/MIDI/WebSocket so musicians and game engines can “play the air.” Cartridges & maintenance: quick-swap oil trays and automated cleaning reports. Mini user studies: comfort, alertness, and preference scores across presets (focus, calm, sleep)."},{"heading":"Built With","content":"chatgpt-oss fast-api javascript nextjs python react"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"RoboChef: GPT-OSS Powered Kitchen Assistant","project_url":"https://devpost.com/software/robochef-gpt-oss-powered-kitchen-assistant","tagline":"Ask for a dish, get it done. Our GPT-OSS kitchen assistant turns natural language into robot actions, orchestrating Isaac GR00T with a live UI that tracks progress and next steps.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"Best Overall | Sponsored by Hugging Face"}],"team_members":[],"built_with":[{"name":"gpt-oss","url":null},{"name":"gr00t","url":null},{"name":"huggingface","url":null},{"name":"lerobot","url":null},{"name":"love","url":"https://devpost.com/software/built-with/love"},{"name":"ollama","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/alex-luci/RoboChef"},{"label":"huggingface.co","url":"https://huggingface.co/datasets/VoicAndrei/so100_kitchen_hackathon"}],"description_sections":[{"heading":"Inspiration","content":"We wanted to push the boundaries of what open models can do in robotics. Kitchens are complex environments with many objects, actions, and sequences. A true assistant needs reasoning, planning, and physical execution, so we combined GPT-OSS with NVIDIA Isaac GR00T and the SO-100 robotic arm to bring the idea of an AI-powered robotic chef to life."},{"heading":"What it does","content":"RoboChef takes natural language instructions like “make me a pineapple smoothie” and breaks them down into executable steps. GPT-OSS generates the sequence of actions needed, expressed in natural language (e.g., “open the cabinet door”, “pick up the pineapple”, “place it in the pot”). These instructions are passed to the execution tool, which drives Isaac GR00T and the SO-100 robotic arm to carry them out.\n\nWhile RoboChef is executing, a live UI keeps the user informed: showing the current kitchen state, the action in progress, and what’s coming next. By blending reasoning, robotics, and feedback, the system feels like a true assistant."},{"heading":"How we built it","content":"GPT-OSS handles reasoning and generates flexible natural-language actions. A single execution tool takes any action GPT-OSS outputs and passes it to the robotics layer. Isaac GR00T translates these instructions into precise motions for the SO-100 robotic arm . A real-time UI visualizes progress, the current state of the kitchen, and the next action. Isaac GR00T was fine-tuned to our embodiment (SO-100 robotic arm) for reliable, precise control, using training data that we collected by teleoperating the robot. Explore our dataset on Hugging Face (first 1200 episodes kept private)"},{"heading":"Challenges we ran into","content":"Handling uncertainty and variability in the kitchen environment. Designing a UI that feels intuitive while hiding technical complexity under the hood."},{"heading":"Accomplishments that we're proud of","content":"Built an end-to-end pipeline from natural language to robotic execution. Successfully demonstrated multi-step cooking tasks like preparing a pineapple smoothie. Integrated open models with state-of-the-art robotics (GPT-OSS + Isaac GR00T + SO-100 arm). Designed a clean, informative UI for real-time kitchen updates. Ran the entire pipeline locally on a single RTX 4090 (24GB VRAM), proving that advanced reasoning and robotics can work without cloud dependency."},{"heading":"What we learned","content":"GPT-OSS is powerful at reasoning through multi-step tasks when paired with a general execution interface. Robotics requires a balance between high-level reasoning and low-level motor control. Clear user feedback builds trust in autonomous systems. Hackathons are great for stress-testing ambitious integrations across AI, robotics, and UI."},{"heading":"Why end-to-end AI models for controlling the robot instead of classical inverse kinematics?","content":"Classical control methods like inverse kinematics excel in highly controlled environments such as factories, where every object's position, shape, and condition are predetermined. But home kitchens are unpredictable, objects may be placed differently, containers vary in size, and conditions change.\n\nWith an end-to-end vision-language-action (VLA) model like Isaac GR00T , our system adapts to these dynamic environments. Instead of relying on rigid control pipelines, RoboChef can interpret instructions and act flexibly in real-world settings, making it far better suited for personal assistants in everyday homes."},{"heading":"What's next for RoboChef: GPT-OSS Powered Kitchen Assistant","content":"Expand the action set to cover more cooking scenarios and recipes. Improve the UI with voice feedback and multimodal updates (text + vision). Experiment with a humanoid platform such as Unitree G1 , exploring how RoboChef could operate in a form factor closer to a human assistant."},{"heading":"Built With","content":"gpt-oss gr00t huggingface lerobot love ollama python"},{"heading":"Try it out","content":"github.com huggingface.co"}]},{"project_title":"bota","project_url":"https://devpost.com/software/bota","tagline":"An autonomous Dota 2 agent powered by GPT-OSS-20B and vLLM","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/723/318/datas/medium.png","prizes":[{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"Wildcard—best unexpected use | Sponsored by vLLM"}],"team_members":[],"built_with":[{"name":"gpt-oss-20b","url":null},{"name":"lua","url":"https://devpost.com/software/built-with/lua"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"vllm","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/AmandineFlachs/bota"}],"description_sections":[{"heading":"Inspiration","content":"In 2019 we flew from London to San Francisco for the OpenAI Five Finals , the first competition between professional Dota 2 players and the AI that OpenAI trained on Dota 2. Inspired by OpenAI's work we built bota: an autonomous Dota 2 agent powered by GPT-OSS-20B and vLLM."},{"heading":"What it does","content":"Our bota agent plays Dota 2 in 1v1, which is a mode typically used for players to develop their skills on new heroes and to practice combat scenarios. Our agent plays the game similarly to a human being and displays the model reasoning in the in-game chat. The agent can move the hero, attack, use items as well as abilities."},{"heading":"How we built it","content":"Our agent is written in Python and interacts with Dota 2 via 2 mechanisms: a socket to receive the observed game state and the LUA bot API to execute actions. We use the vLLM inference engine to run GPT-OSS-20B locally. The model is used without fine-tuning, in combination with a custom prompt to generate actions that the LUA bot API understands.\n\nBe sure to check out our GitHub for more details and a deep dive into the code: https://github.com/AmandineFlachs/bota"},{"heading":"Challenges we ran into","content":"We ran into multiple challenges to interact with Dota 2. First, the game needs to be run with a variety of custom - sometimes undocumented - arguments. Analyzing the game state serialized with Protocol Buffer sent on a socket was also challenging as a lot of things are undocumented and there is no reference implementation. Using the LUA bot API to send actions to the game is also constraining. Any mistakes meant you had to stop and restart the game, which is quite time-consuming, especially in such a competition where time is precious. We encountered a lot of bugs and edge cases in Dota 2 because implementing bots for this game is not very common. The public documentation for the bot API is outdated and new features implemented in the game are not supported by the bot API."},{"heading":"Accomplishments that we're proud of","content":"We made it work even though Dota 2 was very reluctant to let our bot play the game. Scoping a project for a hackathon can be an art, and while there is still a lot of work to be done to cover all aspects of the game, the heroes, abilities and actions we choose to focus on for the hackathon have been successfully implemented."},{"heading":"What we learned","content":"GPT-OSS-20B was very simple to steer to play the game the way we wanted and followed instructions well compared to other models we tried out of curiosity. It was very interesting to examine the reasoning traces generated by the model, as well as the mistakes the model sometimes makes."},{"heading":"What's next for bota","content":"We would like to see if increasing the reasoning effort of the model would improve bota's performance. Unfortunately, as it is a synchronous system, when we tried with our RTX 5090 we encountered very slow inference time, which meant the agent was not reactive enough to play the game. We would also like to expand Bota to more Dota 2 heroes and set up a livestream, similar to ClaudePlaysPokemon , where viewers can watch the agent’s reasoning unfold in real time and even interact with the game to guide its decisions."},{"heading":"Built With","content":"gpt-oss-20b lua python vllm"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Dental Assessment GPT","project_url":"https://devpost.com/software/dental-assessment-gpt","tagline":"Dental GPT that generates evidence-based dental assessments and treatment plans from case data. Treatment planning is critical for patient care provided + revenue generation for dental clinics.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/735/020/datas/medium.png","prizes":[{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"Most Useful Fine-Tune | Sponsored by LM Studio"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"shell","url":"https://devpost.com/software/built-with/shell"}],"external_links":[{"label":"huggingface.co","url":"https://huggingface.co/Wildstash/dental-gpt-oss-20b"}],"description_sections":[{"heading":"Inspiration","content":"My partner (Erika) is a dental professional who has seen first hand the impacts undetected issues and subsequently poor treatment planning can have on patient health. A common issue new dentists face is evaluating all possible problems and required treatment based on a case file. Secondly, several dentists in lower-socio economic areas administer care with limited knowledge and understanding. Thirdly, dentists are facing significant burn out and dental practices are just getting busier. We hope this GPT helps them diagnose and provide essential care to their communities."},{"heading":"What it does","content":"The Dental Assessment GPT generates an evidence-based dental assessment & plan from a structured case, and qualitatively grade the assistant’s output against clinical principles.\n\nThe GPT takes inputs such as Patient data, oral problems, patient medical history, clinical findings, radiographs, current medications and habits."},{"heading":"How we built it","content":"We built this using a mix of data collected from dentists via surveys + publicly available data and specialised problem/treatment dental documentation.\n\n1. Dataset Processing Pipeline\n\n⁠ Initial Data Audit\n\nFound 2494 total cases in dataset ⁠ Placeholder patterns (e.g., “Patient reports symptoms began approximately 2 weeks ago”) dominated content\n\nClinical Enhancement\n\nEnriched cases with demographics (age 18–75, gender balance, diverse occupations) Inserted condition-specific findings : Caries vs. periodontal disease vs. cysts vs. mucosal lesions Matching radiographic findings (periapical radiolucency, bone loss patterns, cystic expansion, etc.) Urgency levels (0 = elective, 1 = moderate, 2 = urgent)\n\n⁠ Full Pipeline Application\n\nApplied continous refinements and enhancements to get the final 2494 cases . Have gone through expert feedback and llm as a judge fine tuning to get robust causal links\n\n⁠ Quality Control Gates\n\nJSON schema validation (ensured ⁠ diagnosis ⁠, ⁠ etiology ⁠, ⁠ urgency ⁠, ⁠ management ⁠, ⁠ abx ⁠, ⁠ follow_up ⁠, ⁠ counseling ⁠, ⁠ guideline ⁠) Internal checks for: missing values, duplicate patient profiles, inconsistent urgency assignments\n\n2. Expert Validation Process\n\nDentist Grading via Typeform\n\nPracticing dentists graded sample cases for clinical plausibility and completeness Scores used to refine enhancement logic Typeform: https://form.typeform.com/to/RFEHs2Xy\n\nAgent Mode Research\n\nRan 40+ structured Agent Mode queries (e.g., “How would a periodontist classify this?”) Extracted literature-backed treatment pathways.\n\nAI Cross-Comparison\n\nBenchmarked random cases against ChatGPT-5 “thinking” mode outputs Flagged inconsistencies between enhanced cases vs. gold-standard reasoning\n\nStructured Input–Output Linking\n\nBuilt causal mapping: Demographics + findings → Risk assessment Risk assessment + urgency → Management plan Management plan + systemic signs → Antibiotic indication"},{"heading":"Challenges we ran into","content":"Data Issues\n\n⁠ Duplication Noise – 95% of data was placeholders ⁠ Template Lock-In – Generic time markers (\"2 weeks ago\") everywhere ⁠ Missing Clinical Context – No age, gender, or systemic history initially ⁠ Radiographic Gaps – No condition-specific images described Flat Urgency Levels – Every case looked the same complexity-wise Dentist buy in – Typeform required about 45mins of dentists time which was hard to get in this short window. Over-Representation of Healthy Cases – Dataset skewed toward low-complexity “routine” or “checkup” visits, underrepresenting challenging pathologies. Ambiguity in Diagnoses – Some cases were vague or combined multiple possible conditions, creating fuzzy labels for model training.\n\nTechnical Problems\n\nPython Failures – Syntax errors during JSONL transformation ⁠ Scope Creep – Accidentally generated “healthy check-up” patients instead of pathology-driven cases ⁠ Expert Coordination – Dentists flagged inconsistencies requiring multiple feedback loops ⁠ AI Variability – ChatGPT-5 outputs differed across sessions even with structured prompts"},{"heading":"Accomplishments that we're proud of","content":"We have learnt a lot about the dental space over the last 2 weeks and the long term impacts poor dental treatments can have on people. We are most pleased that dentists feel heard when we talk to them and are able to create something that can alleviate some of the pressures they face. Alongside this holistic accomplishment we also are proud of:\n\nData & Clinical\n\nDeduplication First – prevents amplifying noise ⁠- Schema Discipline – enforce required fields early ⁠ Scope Boundaries – healthy cases ≠ pathology training data Urgency Calibration – cases must span elective to urgent for realism Causal Pathways – clinical data must logically connect to management\n\nTechnical\n\n⁠ JSONL Handling – strict error checks & rollback points ⁠ Template Detection – auto-flag placeholder cases pre-enhancement ⁠ Version Control – multiple checkpoints during processing ⁠ Iterative Sampling – small test batches before scaling full dataset\n\nExpert Validation\n\n⁠ Typeform Feedback Loop – dentist ranked scores provided measurable quality signal. We had to make the typeform ourselves and let dentist to rank out of 4 answers. Here is the form: https://form.typeform.com/to/RFEHs2Xy ⁠ Agent Mode Testing – revealed weak spots where AI diverged from expert consensus. good for rapid dataset production. ⁠ Cross-AI Comparison – Over the span of several days we continuously sampled, compared and refined the training data and graded each sample out of 100. ChatGPT-5 thinking mode was vastly more superior in understanding input output pairs than ChatGPT normal mode (which seems to give a relatively higher score on training data when using it to compare samples)."},{"heading":"What we learned","content":"Despite enhancements, the final dataset skewed heavily toward “healthy” patient cases , limiting its usefulness for training pathology-focused dental AI. This underlined the need for domain-specific expert tuned validation criteria :\n\nMinimum pathology density Balanced representation of caries, periodontal, mucosal, and radiographic conditions ⁠- Urgency distribution reflecting real-world triage ability to capture nuances is only possible with scale. Instead of 2494 high quality gold outputs, more samples curated and reviewed by experts helps understand nuance a lot better."},{"heading":"What's next for Dental Assessment GPT","content":"We have a strong feeling that this GPT can become the spine/brains for several Dental AI agents if further refinement can be provided. We plan on getting dentist buy in and refining this model further with RLHF on the fine tuned outputs. At inference we hope to implement RAG in context and longer token sequences to make model fully context aware. In line with recent breakthroughs in model hallucinations, we also plan to improve its ability to reject prompts and not solely aim for the best answer, instead improve its \"i dont know\" quotient for multivariate dental cases."},{"heading":"Built With","content":"python shell"},{"heading":"Try it out","content":"huggingface.co"}]},{"project_title":"Memory Palace","project_url":"https://devpost.com/software/memory-palace-qyat7g","tagline":"An offline-first AI companion that helps those with memory loss reconnect with their memories and their families.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/702/981/datas/medium.jpg","prizes":[{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"Best Local Agent | Sponsored by NVIDIA"},{"hackathon_name":"OpenAI Open Model Hackathon","hackathon_url":"https://openai.devpost.com/","prize_name":"For Humanity | Sponsored by OpenAI"}],"team_members":[],"built_with":[{"name":"apple-vision-framework","url":null},{"name":"axios","url":null},{"name":"better-sqlite3","url":null},{"name":"blip","url":null},{"name":"coqui-tts","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"face-recognition","url":null},{"name":"fastapi","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"gpt-oss-20b","url":null},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"ollama","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react.js","url":null},{"name":"sentence-transformers","url":null},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"swiftui","url":null},{"name":"transformers","url":null},{"name":"vscode","url":null},{"name":"websocket","url":null},{"name":"whisper","url":null},{"name":"xcode","url":"https://devpost.com/software/built-with/xcode"}],"external_links":[],"description_sections":[{"heading":"Built With","content":"apple-vision-framework axios better-sqlite3 blip coqui-tts express.js face-recognition fastapi git github gpt-oss-20b ios javascript node.js ollama python pytorch react.js sentence-transformers sqlite swift swiftui transformers vscode websocket whisper xcode"}]}],"generated_at":"2026-02-18T16:41:05.563313Z"}}