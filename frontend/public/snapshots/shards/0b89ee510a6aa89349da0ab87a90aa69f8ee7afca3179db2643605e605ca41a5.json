{"version":"v1","hackathon_url":"https://pinecone-hackathon.devpost.com","generated_at":"2026-02-18T16:40:49.754958Z","result":{"hackathon":{"name":"Pinecone Hackathon","url":"https://pinecone-hackathon.devpost.com","gallery_url":"https://pinecone-hackathon.devpost.com/project-gallery","scanned_pages":6,"scanned_projects":123,"winner_count":3},"winners":[{"project_title":"PatentBot","project_url":"https://devpost.com/software/patentbot","tagline":"Patent creation can a be complex process often requiring legal expertise. To streamline this process, we created PatentBot, an NLP project designed to automate the creation of patent documentation.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/525/586/datas/medium.png","prizes":[{"hackathon_name":"Pinecone Hackathon: Jumpstart Real-World AI Applications","hackathon_url":"https://pinecone-hackathon.devpost.com/","prize_name":"Second place"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"chainlit","url":null},{"name":"chatgpt","url":null},{"name":"cohere","url":null},{"name":"huggingface","url":null},{"name":"langchain","url":null},{"name":"openai","url":null},{"name":"pinecone","url":null}],"external_links":[{"label":"3.89.59.211","url":"http://3.89.59.211:8000/"},{"label":"github.com","url":"https://github.com/factoredai/pinecone-hackaton/tree/main"}],"description_sections":[{"heading":"Inspiration","content":"Patents play a crucial role in protecting and incentivizing innovation by granting exclusive rights to inventors. However, the process of creating and documenting patents can be time-consuming and complex, often requiring legal expertise and specialized knowledge. The inspiration behind creating PatentBot stemmed from recognizing the challenges and complexities involved in the patenting process. Several factors contributed to the development of PatentBot:\n\nTime-consuming and labor-intensive nature: Drafting patent documentation requires significant time and effort, involving technical expertise, legal knowledge, and adherence to specific guidelines. The manual process often leads to delays and increases costs for inventors and organizations. Vast amount of information: Patent offices receive a tremendous volume of patent applications and scientific publications, making it a challenge to sift through this vast amount of information to identify prior art that may be relevant to a particular invention, Evolving technology landscape: Technological advancements are rapidly changing and expanding across multiple domains. Keeping up with the latest developments and ensuring comprehensive coverage in the search for prior art becomes increasingly challenging. Legal expertise requirement: Patent law can be intricate, and understanding the legal nuances and requirements is crucial for successful patent applications. Many inventors and small businesses lack access to specialized legal counsel, making the process even more daunting. Complexity of patent language: Patent documents often use complex technical and legal jargon, making them challenging for inventors, researchers, and even legal professionals to comprehend fully. Clear and concise communication of inventions becomes crucial to ensure accurate representation.\n\nPatentBot harnesses the power of NLP, a branch of artificial intelligence (AI) that focuses on the interaction between computers and human language, to extract and analyze information from various sources and generate patent documentation efficiently. By employing advanced algorithms and machine learning techniques, PatentBot aims to significantly reduce the manual effort and time required for patent drafting, while ensuring accuracy, consistency, and compliance with legal requirements."},{"heading":"What it does","content":"Automated Drafting: Leveraging its language understanding capabilities, PatentBot can automatically generate patent claims, abstracts, descriptions, and other sections of the patent document. This automation frees inventors and patent attorneys from the tedious task of manual drafting, allowing them to focus on the core aspects of their inventions. Prior Art Search: PatentBot utilizes NLP techniques to conduct comprehensive searches for prior art references, which are crucial to assess the novelty and inventiveness of a patent. By analyzing vast repositories of patents databases, PatentBot assists in identifying potential conflicts or similar inventions, enhancing the quality of the patent documentation. Patent Comparison: The comparison results generated by PatentBot are presented in a clear and concise manner, allowing users to visualize the relationships between patents and identify areas of overlap or potential conflicts."},{"heading":"How we built it","content":"PatentBot is an advanced patent documentation tool that integrates various technologies to streamline the patenting process. Let's dive into how PatentBot utilizes each component:\n\nHuggingFace Dataset: PatentBot leverages the HuggingFace dataset, specifically the HUPD (HuggingFace Unified Patent Dataset) , which provides a vast collection of patent documents. This dataset serves as a valuable resource for training and fine-tuning PatentBot's language understanding capabilities. Cohere for Embeddings: PatentBot utilizes Cohere, an AI platform, to generate embeddings for patent documents. By using the CohereEmbeddings API, PatentBot can convert patent texts into numerical representations that capture the semantic information and context of the documents. These embeddings capture the essence of the patents and form the basis for similarity calculations. Pinecone Vector DB and Indexing: PatentBot stores the generated embeddings in Pinecone, a vector database. Pinecone enables efficient storage and retrieval of high-dimensional vectors, making it suitable for managing the embeddings generated by Cohere. PatentBot creates an index in Pinecone based on the embeddings, enabling fast and accurate retrieval of similar patents during search and comparison operations. LangChain for Conversational Bot: PatentBot integrates with LangChain, a conversational AI platform, to handle chat interactions. LangChain provides the necessary interfaces to connect with OpenAI's ChatGPT API, allowing PatentBot to generate responses and engage in interactive conversations with users. This enables PatentBot to provide real-time assistance, answer questions, and guide users through the patenting process in a conversational manner. ChainLit for Frontend Integration: To tie all the components together and create a user-friendly frontend, PatentBot utilizes ChainLit, a frontend development framework. ChainLit enables seamless integration of the various technologies and components into a cohesive and intuitive user interface. It facilitates the design and implementation of the frontend, allowing users to interact with PatentBot effortlessly. AWS for Deployment: PatentBot is deployed on the cloud infrastructure provided by Amazon Web Services (AWS). AWS offers a scalable and reliable cloud environment that ensures PatentBot's availability, performance, and security. By leveraging AWS services, PatentBot can handle varying user loads, scale resources as needed, and provide a seamless experience for users. ChatGPT for Documentation Creation: PatentBot also incorpores ChatGPT, a powerful language model developed by OpenAI, to assist in creating the project documentation. ChatGPT leverages its language generation capabilities to generate clear and concise descriptions of its own functionalities."},{"heading":"Challenges we ran into","content":"There were two main challenges we ran into when developing PatentBot:\n\nEvaluating the quality of the generated embeddings. There are multiple ways to generate embeddings however evaluating their suitability for an specific task and quantify can be challenging. We approached this problem with a categorization metric in which we used CPCs as labels and using a reference text for each CPC we query the most similar embeddings in the database compare their CPC to the one of the text used as reference. Hence, we can quantify it as a classification task. Nevertheless, due to issues with ID for each patent and lack of metadata we decided to migrate from Big Patent as primary dataset to The Harvard USPTO Patent Dataset changing the requirements for evaluation and the proper definition of a categorization metric. Proper source referencing for the tasks of the PatentBot was also a challenge since during some stages of the development process, id referencing in our bot was inaccurate and in sometimes retrieved ids not present in the dataset."},{"heading":"Accomplishments that we're proud of","content":"We build a Bot with multiple capabilities able to tackle three main common challenges when developing a patent: the search for prior works, patent drafting and comparing. The automation of these tasks can help reduce costs and increase efficiency in patent processes. Our bot has the potential of becoming an scalable app with high impact, that in this first version accomplished great results."},{"heading":"What we learned","content":"By integrating the HuggingFace dataset, Cohere for embeddings, Pinecone for efficient storage and retrieval, LangChain for conversational capabilities, and ChatGPT for documentation creation, PatentBot revolutionizes the patenting process. It simplifies the exploration of prior art, assists in assessing novelty, and provides a comprehensive solution for generating accurate and high-quality patent documentation."},{"heading":"What's next for PatentBot","content":"The main functionality that considered could be added to the PatentBot is the possibility of having users with different permissions and capabilities inside the application. This will increase robustness of the app and secure flow of information.\n\nIn addition, a further research with attorneys and eventual users of the application can give us ideas on what are the key points to enhance and functionalities to add so that PatentBot can tackle as many issues in the patent processes as possible."},{"heading":"Built With","content":"amazon-web-services chainlit chatgpt cohere huggingface langchain openai pinecone"},{"heading":"Try it out","content":"3.89.59.211 github.com"}]},{"project_title":"Real time disaster imagery annotation and search analytics","project_url":"https://devpost.com/software/real-time-disaster-imagery-annotation","tagline":"Using advanced state of art AI capabilities to perform real time disaster imagery annotation and search to accelerate the disaster response during the \"Golden Hour\" to save lives and minimize damage.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/525/111/datas/medium.gif","prizes":[{"hackathon_name":"Pinecone Hackathon: Jumpstart Real-World AI Applications","hackathon_url":"https://pinecone-hackathon.devpost.com/","prize_name":"First place"}],"team_members":[],"built_with":[{"name":"amazon-dynamodb","url":"https://devpost.com/software/built-with/amazon-dynamodb"},{"name":"amazon-lambda","url":null},{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"hugging-face","url":null},{"name":"langchain","url":null},{"name":"openai","url":null},{"name":"pinecone","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sagemaker","url":"https://devpost.com/software/built-with/sagemaker"},{"name":"streamlit","url":null},{"name":"transformers","url":null}],"external_links":[{"label":"geospatial-disaster-image-search.streamlit.app","url":"https://geospatial-disaster-image-search.streamlit.app"}],"description_sections":[{"heading":"Inspiration","content":"It is estimated that over the past decade (2010s), natural disasters have caused an average of roughly 60,000 deaths per year globally. During disaster events, the \"Golden Hour\" emphasizes importance of rapid response in disaster situations, as the first hour following such an event is often the most critical for saving lives, mitigating further damage, and setting the stage for successful recovery efforts. With availability of huge corpus of imagery data collected in real time from UAV's, Aerial surveliance vehicle and Satellite images, provide us with a rich information about the extent of the damage and disaster conditions. Harnessing this information and providing it to the ground first responder teams in real time give a tremendous value saving the lives and mitigating further damages. This was our key inspiration to build project Golden Hour - Real time disaster imagery annotation and search to accelerate disaster response."},{"heading":"What it does","content":"Live dashboards of the disasters mapped on to the geospatial dashboard Geospatial search of the disaster images powered by the natural language querying Life safety navigation guidance to the first responder and disaster relief teams\n\nAreas of Application\n\nDisaster Image Search and impact analysis Satellite/UAV Image Search Wild Fire detection Threat detection and monitoring for defense applications"},{"heading":"How we built it","content":"Building Blocks of the Solution\n\nImage Annotation to identify text features and annotate disaster images. Extract metadata from the images, such as Geospatial data. Centralized content repository to store image files in segregated folders. Index the features and metadata to categorize and catalog the images Search the features and metadata to find the images with the relevant features and filter with metadata Natural Language Understanding to identify the user intent and derive the semantics for response generation.\n\nTech Stack AWS : end-to-end solution: Includes Amazon SageMaker, S3, DynamoDB, Lambda, SQS, API GW, IAM Pinecone : centralized repository to store the features data (embeddings) and metadata of the Images. Open AI : semantic processing of the user query â€“ NLU and the CLIP framework to train the image, text pairs. Hugging Face : transformer architecture, the base model uses a ViT-B/32 Transformer Architecture. We fine-tuned the CLIP Network from OpenAI with satellite images and captions from the LADI dataset LangChain : prompt engineering and orchestration of the end-to-end query processing. Streamlit : interactive user interface and user state management."},{"heading":"Challenges we ran into","content":"Training domain specific AI model. It is easy to distinguish between a lake and flood. But when you are looking at an aerial photograph factors like angle, altitude, cloud cover and context make the task more difficult. We augmented the images with the text that is generated from the labels that are captured as part of the human annotations and generated relevant captions for images that are used to prevent the underfitting of the model."},{"heading":"Accomplishments that we're proud of","content":"Now we have a live dashboard that can map the disaster images along with the map location on the geospatial maps in real time. Geospatial search of the disaster images powered by the natural language querying and can be done at damage specific and location specific."},{"heading":"What's next for Real time disaster imagery annotation and search analytics","content":"Extending this solution to provide navigation guidance avoiding the disaster areas (Ex: Avoiding the Bridge that was damaged or avoiding the road that is washed out due to landslide)"},{"heading":"Built With","content":"amazon-dynamodb amazon-lambda amazon-web-services hugging-face langchain openai pinecone python sagemaker streamlit transformers"},{"heading":"Try it out","content":"geospatial-disaster-image-search.streamlit.app"}]},{"project_title":"BIAS","project_url":"https://devpost.com/software/bias","tagline":"Arguments extraction, analysis & more","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/519/083/datas/medium.png","prizes":[{"hackathon_name":"Pinecone Hackathon: Jumpstart Real-World AI Applications","hackathon_url":"https://pinecone-hackathon.devpost.com/","prize_name":"Third place"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"cohere","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"pinecone","url":null},{"name":"raiden.ai","url":null},{"name":"vercel","url":null},{"name":"zapier","url":"https://devpost.com/software/built-with/zapier"}],"external_links":[{"label":"bias.raiden.ai","url":"https://bias.raiden.ai"}],"description_sections":[{"heading":"Inspiration","content":"The BIAS project stems from a belief in better-informed, potent and healthier debate and decision-making contexts, for everybody"},{"heading":"What it does","content":"Navigate the flow of information on issues you are interested in, with the help of AI :\n\nAutomated analysis, extraction of arguments and counter-arguments from any source Support for webpages, documents, audio, video, youtube Browse analyzed and extracted arguments using a smooth and intuitive interface Arguments clustering for better discovery Source-backed conversational features to enquire more Agent-based debate simulations in a chatbot-like interface Data exports for further usage and analysis Detailed PDF report generations on issues of interest"},{"heading":"How we built it","content":"We used the following tools to build the BIAS projects:\n\nRAIDEN AI\n\nWe signed up for this hackaton as a startup to put our infrastructure, RAIDEN AI , to use in a real-life application, and potentially generate interest in our platform from users (and investors)\n\nRAIDEN AI is used:\n\nTo process, index and retrieve sources, in a project-scoped environment, with all the required features out-the-box (feature analysis, embeddings generations Pinecone indexing, ...), accessible with a single API call and no-config, in addition to automating adding sources from search queries in a single request To instantly deploy automation pipelines to production. Here we used 5 pipelines, all of which make use of LLMs : analyze-sources : extract arguments & counter-arguments related to given statement, from a source simulate-debate : agent-based debate simulation suggest-searches : on project creation, suggest search queries to be used for source retrieval cluster-title : generate titles for clusters of arguments report-generation : write a source-backed report related to the given statement\n\nPINECONE\n\nPinecone is used to pretty much connect everything:\n\nSources content blocks are indexed (using generated embeddings) alongside analyzed features as metadata, and project-scoped with namespaces. This allows for: Semantic search across projects Source-backed conversational chatbots Generation of a bibliography section when generating project reports Arguments & counter-arguments that are extracted from the source analysis pipeline are indexed in Pinecone as well. This allows for: Deep project search to be performed on atomic arguments in addition to source texts Relevant list for each argument : on the BIAS web platform, tapping an argument retrieves and displays list of closest arguments cards (both opposing and agreeing arguments) Generating clusters of arguments for a better exploration of topics Debate simulation : agents can make use of Pinecone as a memory bank to generate pertinent debate replies\n\nOPENAI\n\nWe used:\n\ntext-ada-002 to generate embeddings before storing them in Pinecone, for: sources text blocks analyzed arguments gpt-3.5-turbo-0613 in all pipelines to generate texts and objects\n\nWe highlight the usage of the new functions addition, which makes workflows simply perfect for further automation.\n\nCOHERE\n\nWe use Cohere's classification feature in order to analyze each argument, in order to attribute it one of 3 labels:\n\nOptimistic Tone Pessimistic Tone Neutral Tone\n\nWe believe the argument Tone feature will be useful further down the line in analyzing/shaping conversations related to pertinent topics, as the sentiment/tone of the message is as important as the reasoning dimension.\n\nZAPIER\n\nZapier is used to email users at the completion of 2 tasks, for which we believe users would like to preserve a copy:\n\nData export - a CSV file Report generation - a PDF file\n\nThis is accomplished in 3 steps :\n\nOn processing completion : upload file to storage Catch Hook in Webhooks by Zapier : call webhook on upload completion, object with target user email and email content Send Outbound Email in Email by Zapier : configured on data from previous step\n\nWe highlight the attachment feature of Send Outbound Email in Email by Zapier which lessens the time required by lengthy configurations for email attachments :\n\nA simple url string is supplied instead (the signed url of the data file uploaded on our storage), and the Zap takes care of the rest\n\nAWS\n\nAmazon Textract is used a one of the document extraction engines. It is especially useful in cases where documents have:\n\nScanned formats; i.e. user take pictures of their notes and use them as source Non-standard structure; i.e. multi-column structures that do not follow a strict template\n\nVERCEL\n\nUsing Vercel to deploy everything frontend related, whether in this project or any other:\n\nInstant deployment of our webapp, written in SvelteKit Instant subdomain configuration : bias.raiden.ai"},{"heading":"Challenges we ran into","content":"Conceptualizing the web platform interface : Required multiple design attempts and restarting from scratch before figuring out what to use Systems adapted to long processing times : As the processing tasks can be lengthy regarding the features we wanted to implement, there was a need to come up with models that emphasize intermittent updates; i.e. in Debate Simulation, instead of running the entire simulation and then displaying results, had to remodel the entire process, opted for a chatbot-like interface with step by step updates the same approach would be useful in future contexts where deep, nested processing is required by automations. i.e. prioritize the use of streams instead of responses on full completions Debate simulation : We needed to have opposing agents debate a given statement, using the analyzed arguments as context. Required lengthy testing of numerous sets of prompts to reach an acceptable model. Note : Challenges typically faced by teams according to Discord messages seemed to be related to document processing, extraction and storing/indexing in Pinecone. Using RAIDEN AI took care of all that without configuration needed."},{"heading":"Accomplishments that we're proud of","content":"Built & deployed everything that was set to be accomplished in the BIAS project, both in backend and frontend, strictly within the 1 week hackaton."},{"heading":"What we learned","content":"New tools to integrate in upcoming projects Approaches to modelling logical reasonning methods in a LLM context Better prompt techniques, especially in relation the GPT functions feature Insights on AI and development from discussing with other hackaton participants"},{"heading":"What's next","content":"for the BIAS project:\n\nPotential usage in education platforms: A start in responding to the discruption and vacuum created in education by recent AI developments. A new type of newsfeed: Can be deployed either as: an independent new type of platform, in which case, lengthy processes will be continuously running in the background and results presented as a news feed adapted to the users' areas of interest an integration with social media platforms (i.e. an augmentation of features such as twitter's community notes) a chatbot tailored to users' interests\n\nfor RAIDEN AI:\n\nThe BIAS project highlights promises outlined by the RAIDEN AI infrastructure in terms of speeding up the development of AI apps, through a zero-config Knowledge Base Layer and a very powerful API-first Automation Layer, to build + deploy + scale AI apps instantly We are looking for partnerships and investments for RAIDEN AI , reach out! If you happen to be in contact with the openAI team, please help reach back on quota increases requests we filled numerous times"},{"heading":"Credits","content":"Made for the BIAS BY RAIDEN AI team\n\nTry It Out\n\nYou can try it out at bias.raiden.ai\n\nnote : Login might be temporarily disabled to balance out incoming requests and credits usage. If it is the case, just let me know so i can open logins for you :)"},{"heading":"Built With","content":"amazon-web-services cohere firebase node.js openai pinecone raiden.ai vercel zapier"},{"heading":"Try it out","content":"bias.raiden.ai"}]}],"generated_at":"2026-02-18T16:40:49.754958Z"}}