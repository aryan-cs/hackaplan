{"version":"v1","hackathon_url":"https://hpaistudio.devpost.com","generated_at":"2026-02-18T16:53:35.287163Z","result":{"hackathon":{"name":"HP & NVIDIA Developer Challenge","url":"https://hpaistudio.devpost.com","gallery_url":"https://hpaistudio.devpost.com/project-gallery","scanned_pages":1,"scanned_projects":23,"winner_count":10},"winners":[{"project_title":"Document Analysis Toolset","project_url":"https://devpost.com/software/document-analysis-toolset","tagline":"The Document Analysis Toolset uses custom trained vision models to analyze PDF files and extract signatures, detect forgery, show detailed version history, and identify manipulated areas of documents.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/434/403/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Second Place"}],"team_members":[],"built_with":[{"name":"hp-ai-studio","url":null},{"name":"mlflow","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"streamlit","url":null}],"external_links":[{"label":"doctools.streamlit.app","url":"https://doctools.streamlit.app/"},{"label":"github.com","url":"https://github.com/msamylea/hprepo"}],"description_sections":[{"heading":"Inspiration","content":"Document fraud costs organizations billions annually, with forged signatures and manipulated PDFs becoming increasingly sophisticated. Traditional manual verification methods are time-consuming, subjective, and often miss subtle digital alterations.\n\nI was inspired by real-world cases where: - Legal contracts were invalidated due to undetected signature forgeries - Financial institutions lost money to document manipulation - Court cases hinged on proving document authenticity\n\nMy goal: democratize advanced document forensics using AI."},{"heading":"What it does","content":"The Document Analysis Toolset is a comprehensive AI-powered platform that provides three core forensic capabilities with models trained using HP AI Studio on NVIDIA GPUs.\n\nThe system automatically detects and extracts signatures from PDF documents using YOLO-based object detection, processes multi-page documents in seconds, and allows users to download individual signature crops for further analysis.\n\nFor signature verification, the deep learning classifier compares two signature images to determine authenticity with confidence scoring, providing side-by-side visual comparisons and expert-level accuracy in distinguishing genuine signatures from forgeries. The manipulation detection module utilizes Error Level Analysis (ELA) to identify digitally altered areas in PDF documents, highlighting suspicious regions and tracking version changes to reveal tampering attempts that would be invisible to the naked eye.\n\nThe platform processes documents in real-time, supports multiple file formats, and presents results through clean, professional visualizations that make complex forensic analysis accessible to non-technical users."},{"heading":"How we built it","content":"I built the system using a modern AI-first architecture. For the hosted web interface, I chose Streamlit for its rapid prototyping capabilities and clean presentation of complex data visualizations. The signature detection pipeline leverages a custom-trained YOLO model exported to ONNX format for optimized inference, while the forgery classification uses a fine-tuned Vision Transformer (ViT) model hosted on Hugging Face for the web versions, but both are also deployable for inference through HP AI Studio.\n\nMy development workflow utilized HP AI Studio for model experimentation and training, with an NVIDIA GPU (RTX 4070 Ti Super) providing the computational power needed for deep learning model development. I implemented MLFlow for experiment tracking and model versioning, ensuring reproducible results and systematic performance improvements. The manipulation detection module integrates PDF2Image for document conversion and PIL for advanced image processing operations.\n\nKey technical decisions included using PyTorch with ONNX export for production deployment, implementing caching decorators for model loading optimization, and designing modular components that could be easily extended. I structured the codebase with separate pages for each analysis type, shared utility functions for common operations, and custom CSS styling for a professional user experience."},{"heading":"Challenges we ran into","content":"One of the biggest challenges was optimizing model inference speed while maintaining accuracy. Initial implementations were too slow for real-time use, leading me to experiment with model quantization, ONNX optimization, and strategic caching. I also struggled with memory management when processing large PDF documents, ultimately implementing streaming approaches and efficient image handling.\n\nThe signature detection model required extensive data preprocessing and augmentation to handle the wide variety of signature styles, document qualities, and scanning artifacts found in real-world documents."},{"heading":"Accomplishments that we're proud of","content":"I successfully created a production-ready document forensics platform that rivals expensive specialized software, making advanced AI accessible through a simple web interface. The signature detection achieves over 95% precision across diverse document types, while the forgery classification model demonstrates over 90% accuracy in distinguishing authentic from forged signatures.\n\nThe technical architecture is both scalable and maintainable, with clean separation of concerns and modular components that facilitate future development.\n\nThe Error Level Analysis implementation successfully identifies subtle digital manipulations that would be missed by manual inspection, providing forensic investigators with a powerful new tool for document authentication. The entire system processes documents in real-time, making it practical for high-volume use cases in professional environments.\n\nPerhaps most importantly, this demonstrates that advanced AI capabilities can be democratized - bringing laboratory-grade document forensics to any professional who needs it, regardless of technical background or budget constraints."},{"heading":"What we learned","content":"This project taught me valuable lessons about productionizing machine learning models and the importance of user-centered design in AI applications. I learned that model accuracy is only one piece of the puzzle - inference speed, memory efficiency, and result interpretability are equally crucial for real-world deployment."},{"heading":"What's next for Document Analysis Toolset","content":"My immediate roadmap includes expanding the signature verification model to handle a broader range of signature styles and document types, implementing ensemble methods to combine multiple detection approaches for higher accuracy, and adding support for additional document formats beyond PDF."},{"heading":"Built With","content":"hp-ai-studio mlflow python streamlit"},{"heading":"Try it out","content":"doctools.streamlit.app github.com"}]},{"project_title":"AffectLink","project_url":"https://devpost.com/software/affectlink","tagline":"AffectLink provides real-time multimodal emotion insights for tele-health with HP AI Studio. Uncovers hidden cues via facial, audio, & text analysis, boosting clinician understanding & patient care.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/467/413/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"First Place"}],"team_members":[],"built_with":[{"name":"deepface","url":null},{"name":"hp-ai-studio","url":null},{"name":"hugging-face-transformers","url":null},{"name":"librosa","url":null},{"name":"mlflow","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"openai-whisper","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"scipy","url":"https://devpost.com/software/built-with/scipy"},{"name":"sounddevice","url":null},{"name":"soundfile","url":null},{"name":"streamlit","url":null},{"name":"swagger","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/kingkw1/AffectLink"}],"description_sections":[{"heading":"Inspiration","content":"The rapid shift to tele-health has brought convenience but also a significant challenge: how do clinicians truly understand a patient's emotional state when nuanced non-verbal cues are often lost through a screen? Traditional tele-health lacks the rich emotional context present in in-person interactions. This inspired AffectLink , a project aimed at empowering mental health professionals with deeper, data-driven insights into patient emotions during virtual sessions, ultimately improving care."},{"heading":"What it does","content":"AffectLink is a real-time, local-first AI application designed to provide comprehensive emotional analysis by processing three key modalities: facial expressions, speech tone (audio emotion), and spoken content (text emotion) . Our core innovation, the Emotional Consistency Index (ECI) , synthesizes these independent emotional streams, highlighting when a patient's verbal message might diverge from their non-verbal cues. For example, a patient might say \"I'm fine,\" but their facial expression or tone suggests underlying distress. AffectLink surfaces these critical discrepancies, acting as a powerful tool for clinicians to identify unexpressed emotions and guide their therapeutic approach. All processing occurs securely and locally on the HP workstation, ensuring patient privacy and HIPAA compliance."},{"heading":"How we built it","content":"AffectLink was built from the ground up to showcase the power and security of HP AI Studio for real-time, on-device AI inference. Our technical workflow is as follows:\n\nData Capture: Live video and audio streams are captured from the user's webcam and microphone. Orchestration ( main_processor.py ): This central component manages the overall pipeline, distributing data to specialized AI modules and aggregating their results. Facial Emotion Analysis: Video frames are sent to a Video Emotion Processor which utilizes DeepFace (running locally). While we aimed for full AI Studio integration, due to specific complexities encountered during the hackathon timeframe, DeepFace is currently executed directly via its Python library to detect and classify facial expressions in real-time on the local machine. Audio Processing: Audio chunks are sent to an Audio Pre-processor . This module prepares the audio for the deployed AI models. HP AI Studio Integration (Swagger API): This is a core highlight. Our Whisper (ASR) model for speech-to-text transcription, our Speech Emotion Recognition (SER) model for vocal tone analysis, and our Text Emotion model (for analyzing transcribed text) are all successfully registered and deployed locally via HP AI Studio's Swagger API . This ensures secure, high-performance, and on-device inference without cloud dependency for these critical AI components. Multimodal Fusion: The main_processor.py receives results from all modalities (facial, audio emotion, text emotion, and transcription). It then calculates the Emotional Consistency Index , providing a unified view of the patient's emotional state. Data Visualization (Streamlit): All processed data, including live video frames and emotion metrics, are written to local temporary files ( affectlink_emotion.json , affectlink_frame.jpg ). This method of inter-process communication was chosen for efficient real-time updates. The AffectLink Streamlit UI dashboard then reads these files, providing an intuitive, interactive display for clinicians. MLflow Tracking: Throughout the development process, MLflow was leveraged within HP AI Studio to track experiments, manage model versions, and log metrics, ensuring full reproducibility and control over our AI models."},{"heading":"Challenges we ran into","content":"Real-time Performance: Balancing complex multimodal AI models with real-time performance on local hardware was a significant challenge. We optimized frame processing rates and experimented with different model sizes (e.g., Whisper 'tiny' or 'base'). Synchronization: Ensuring accurate synchronization between video frames, audio chunks, and their corresponding emotional analyses was crucial for the ECI. We implemented robust timestamping and buffering mechanisms. DeepFace Integration with AI Studio Deployment: We encountered integration complexities with the DeepFace library within the hackathon's compressed timeline. This led us to handle facial emotion analysis locally via direct Python library calls rather than deploying it via AI Studio's Swagger API like our other models. HP AI Studio Deployment: Configuring and deploying other models (ASR, SER, Text Emotion) to the local Swagger API within HP AI Studio required careful attention to environment dependencies and API endpoint definitions."},{"heading":"Accomplishments that we're proud of","content":"Successful Local-First AI Deployment with HP AI Studio: We successfully registered and deployed our Whisper (ASR), Speech Emotion Recognition (SER), and Text Emotion models locally via HP AI Studio's Swagger API, demonstrating secure, high-performance, and on-device inference without cloud dependency. Novel Emotional Consistency Index (ECI): Developed and implemented the ECI to synthesize multimodal emotional streams, providing unique insights into discrepancies between verbal and non-verbal cues. Robust Real-time Multimodal Pipeline: Built a functioning real-time pipeline integrating facial expressions, speech tone, and spoken content analysis on local hardware. Effective Inter-process Communication: Implemented efficient local file-based communication for seamless data flow between AI processing modules and the Streamlit UI, achieving real-time visualization. Strong Privacy and HIPAA Focus: Delivered a \"local-first\" design directly addressing ethical concerns in healthcare AI by ensuring no sensitive patient data leaves the device. MLflow Integration: Leveraged MLflow within HP AI Studio for comprehensive experiment tracking, model versioning, and metric logging, ensuring reproducibility."},{"heading":"What we learned","content":"Diverse Model Packaging Requirements: The DeepFace integration challenge reinforced our understanding of diverse model packaging requirements and the importance of a clear, iterative development process within HP AI Studio to overcome such hurdles. HP AI Studio Deployment Best Practices: We learned best practices for packaging models and managing their lifecycle within the platform, including the efficiency gains from consolidating audio-related models into a single environment. Optimization for Local Performance: Gained insights into optimizing frame processing rates and model sizes for real-time performance on local hardware. Ethical AI Design: Reinforced the importance of a \"local-first\" design as a key strategy for ensuring patient privacy and HIPAA compliance in sensitive AI applications like healthcare."},{"heading":"What's next for AffectLink","content":"AffectLink has the potential to significantly impact the healthcare industry by providing clinicians with a vital \"sixth sense\" in tele-health. Beyond mental health, the multimodal analysis framework could be adapted for sales training, customer service, or even remote education, providing real-time feedback on emotional engagement and comprehension. Its secure, local deployment model makes it highly attractive for sensitive enterprise applications. Future enhancements could include exploring more advanced fusion techniques for the ECI, integrating more deeply with Electronic Health Records (EHR) systems, and potentially expanding to other biometric signals."},{"heading":"Built With","content":"deepface hp-ai-studio hugging-face-transformers librosa mlflow numpy openai-whisper opencv python pytorch scipy sounddevice soundfile streamlit swagger"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Sophia","project_url":"https://devpost.com/software/sophia-76ueiq","tagline":"Meet Sophia: Complete AI-powered crypto intelligence platform featuring fine-tuned models, news RAG, trading agents, deep research, daily summaries/podcasts, and hyper-personalized smart chat contexts","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/467/686/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Best in Gen AI"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"llm","url":null},{"name":"milvus","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/prasaddpathak/hp-hackathon"},{"label":"www.sophiainvest.com","url":"https://www.sophiainvest.com/"}],"description_sections":[{"heading":"Inspiration","content":"The cryptocurrency market moves fast, and retail investors struggle to keep up with the constant stream of news, sentiment shifts, and market data. We saw an opportunity to democratize sophisticated crypto analysis by building an AI-powered platform that combines real-time market intelligence with personalized investment guidance. Our goal was to create a tool that both novice and experienced crypto traders could use to make more informed decisions."},{"heading":"What it does","content":"Sophia is a comprehensive cryptocurrency intelligence platform that provides:\n\nAI-powered conversations using a fine-tuned Phi-3-Mini model specialized for financial discussions Real-time market analysis with live price data and portfolio tracking for paper trading Sentiment analysis across 100+ cryptocurrencies by processing news from major crypto media outlets RAG-enhanced news insights using vector search to provide context-aware responses about market events Multiple chat modes including Deep Research for enhanced analysis and Agent mode for portfolio operations Automated daily podcast generation summarizing market trends and news\n\nUsers can chat with Sophia to get market insights, track their portfolio performance, analyze sentiment trends, and execute paper trades in a risk-free environment."},{"heading":"How we built it","content":"AI/ML Stack: (HP AI Studio)\n\nFine-tuned Microsoft Phi-3-Mini-128k on a Finance-Instruct-500k dataset using LoRA for efficient training Implemented 8-bit quantization to optimize model performance on GPU infrastructure Built sentiment analysis pipeline using VADER and TextBlob on scraped news articles Created vector embeddings with Milvus database for RAG functionality\n\nApplication Architecture:\n\nReact + TypeScript frontend with Tailwind CSS for responsive design Node.js + Express backend with SQLite for user management and rate limiting Docker-based infrastructure with Milvus vector database and MLFlow model serving RSS scraper collecting articles from CoinDesk, Cointelegraph, BeInCrypto, and Decrypt\n\nIntegrations:\n\nCoinGecko API for real-time cryptocurrency market data OpenAI-compatible wrapper for seamless model integration Google OAuth for authentication with development bypass mode"},{"heading":"Challenges we ran into","content":"Model Deployment: Getting HP AI Studio's model serving to work seamlessly with our web application required building a custom OpenAI-compatible wrapper due to inference performance limitations. We solved this by creating a Flask server that translates between HP AI Studio's API and standard OpenAI endpoints.\n\nVector Database Setup: Configuring Milvus for RAG functionality with proper embedding generation and similarity search took significant optimization. We had to fine-tune the indexing strategy and embedding model selection for optimal news retrieval performance.\n\nReal-time Data Integration: Synchronizing cryptocurrency market data with user portfolios while maintaining fast response times required careful API rate limiting and caching strategies.\n\nContext Length Management: Balancing detailed financial conversations with model context limitations required implementing intelligent context truncation and conversation threading."},{"heading":"Accomplishments that we're proud of","content":"Successfully fine-tuned a specialized financial AI model using LoRA that understands financial terminology and market dynamics Built a fully functional paper trading interface with real-time market data and portfolio analytics Implemented sophisticated sentiment analysis across 11,000+ news articles covering 100+ cryptocurrencies Created a seamless user experience that combines multiple AI capabilities (chat, sentiment, RAG, agents) in a single platform Deployed a complete production-ready application with proper authentication, rate limiting, and security measures Generated daily crypto podcasts that summarize market trends and news"},{"heading":"What we learned","content":"Technical Learning:\n\nFine-tuning techniques with LoRA and quantization for deployment efficiency Vector database optimization for semantic search in financial contexts Implementing LangChain Agents customized for financial use case Building OpenAI-compatible APIs for custom model deployment Integrating multiple AI services into a cohesive user experience\n\nDomain Learning:\n\nSentiment analysis techniques for financial news The importance of context length in financial AI applications\n\nDevelopment Insights:\n\nThe value of containerized development for complex AI applications Balancing model performance with user experience requirements The complexity of building production-ready AI applications beyond basic demos"},{"heading":"What's next for Sophia","content":"Advanced Trading Automation:\n\nAuto Rebalancing : Automatic portfolio rebalancing with customizable frequencies (monthly, quarterly, yearly) Stop Loss & Take Profit : Automated risk management with intelligent order execution Advanced Order Types : Limit orders, trailing stops, and conditional trading strategies\n\nEmotional Intelligence & Behavioral Coaching:\n\nTrading Psychology Analysis : Advanced psychological insights to help users make better financial decisions Behavioral Pattern Recognition : AI-powered detection of emotional trading pitfalls Personalized Coaching : Tailored recommendations based on individual trading psychology profiles\n\nComprehensive Alert System:\n\nPortfolio Alerts : Milestone notifications for portfolio value thresholds ($10K, $25K, $50K, $100K+) Price Movement Alerts : Customizable notifications for portfolio coins and watchlist items (Â±5%, Â±10%, Â±20%) Market Intelligence : Breaking news alerts with priority settings (immediate, important only, daily digest)\n\nWallet & Exchange Integrations:\n\nDeFi Wallet Connections : MetaMask and Phantom wallet integrations for seamless DeFi portfolio tracking Exchange API Integration : Direct connections to Binance, Coinbase, and other major exchanges Unified Portfolio View : Consolidated tracking across all wallets and exchange accounts Real Money Trading : Transition from paper trading to live trading with connected accounts\n\nEnhanced News & Research:\n\nReal-time News Processing : Instant sentiment analysis and market impact assessment Custom News Categories : Personalized feeds for market analysis, regulatory updates, and technology developments Research Reports : AI-generated deep-dive analyses on specific cryptocurrencies and market trends Social Sentiment Tracking : Integration with social media sentiment analysis for comprehensive market mood tracking\n\nThese features will transform Sophia from a cryptocurrency intelligence platform into a complete digital investment advisor, providing users with institutional-grade tools and insights for making informed investment decisions."},{"heading":"Built With","content":"ai docker llm milvus node.js python react"},{"heading":"Try it out","content":"github.com www.sophiainvest.com"}]},{"project_title":"Mira3D","project_url":"https://devpost.com/software/mirai-5d98z3","tagline":"Turn your surroundings into immersive 3D scenes using AI-powered Gaussian splatting","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/468/324/datas/medium.jpeg","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Third Place"}],"team_members":[],"built_with":[{"name":"birefnet","url":null},{"name":"brush","url":null},{"name":"colmap","url":null},{"name":"hpaistudio","url":null},{"name":"mlflow","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reactthreefiber","url":null},{"name":"swagger","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ankithreddypati/mira3d"}],"description_sections":[{"heading":"Inspiration","content":"We live in a 3D world, yet most of our digital interactions are still two-dimensional. Whether it's online shopping, memory capture, or education, we're often stuck with flat media. Mira3D was built to change that. By enabling users to turn short 2D videos into realistic, interactive 3D models, we make high-quality 3D content creation fast, local, and intuitiveâ€”without relying on the cloud."},{"heading":"What it does","content":"Mira3D transforms 2D video footage into interactive 3D point clouds using a combination of AI-driven photogrammetry and real-time Gaussian splatting rendering. The platform operates entirely on local hardware through HP AI Studio, offering privacy, speed, and control.\n\nKey capabilities include:\n\nAI-based background removal (Birefnet) COLMAP-based 3D reconstruction with CUDA acceleration Real-time rendering with the Brush engine In-browser 3D model viewing and editing On-prem, secure pipeline execution with MLflow tracking and export"},{"heading":"How It is built","content":"Preprocessing : Background removal using Birefnet (ONNX) 3D Reconstruction : COLMAP with CUDA for Structure-from-Motion and Multi-View Stereo Rendering : Gaussian splatting via the Brush engine (Vulkan-based) Frontend : React with Three Fiber for the 3D interactive editor Deployment : MLflow and Swagger within HP AI Studio containers Persistence : Exported binaries and artifacts tracked using MLflow"},{"heading":"Challenges I ran into","content":"Integrating CUDA-enabled COLMAP within containerized HP AI Studio environments Ensuring Vulkan GPU support inside the container for hardware-accelerated rendering Managing VRAM and compute limits on mid-range GPUs Making binary dependencies persistent across container restarts Debugging gsplat compatibility issues with newer GPU architectures (Blackwell)"},{"heading":"What I learned","content":"How to deeply integrate MLflow for model/artifact tracking and UI deployments The trade-offs and practical limits of combining classical photogrammetry with neural rendering Containerization strategies for GPU-heavy workloads (Vulkan, CUDA, ONNXRuntime) The nuances of running low-level graphics APIs (like Vulkan) in remote and containerized environments"},{"heading":"What's next for Mira3D","content":"Texture generation from prompts using models like Trellis and Hunyuan Migration back to gsplat for faster CUDA-accelerated Gaussian splatting Advanced UI features for interactive editing of splats and model refinement Optimized low-VRAM workflows for wider hardware support Streamlined deployment via packaged binaries for non-technical users"},{"heading":"Built With","content":"birefnet brush colmap hpaistudio mlflow python reactthreefiber swagger"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"OpenMed","project_url":"https://devpost.com/software/openmed-gtp792","tagline":"A modular AI-driven diagnostic system that intelligently selects specialized models for each case, offering clear, interpretable insights to foster greater trust and transparency in medical AI.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/456/727/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Best in Enterprise Computer Vision"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"hp-ai-studio","url":null},{"name":"openai","url":null},{"name":"openwebui","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"}],"external_links":[{"label":"github.com","url":"https://github.com/sriharshapy/openMed"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for OpenMed came from witnessing the critical challenges facing healthcare systems worldwide. Medical professionals are overwhelmed with increasing patient loads while facing pressure to make accurate, timely diagnoses. We observed that:\n\nAI mistrust in healthcare stems from \"black box\" systems that provide no explanation for their decisions Diagnostic delays can be life-threatening, especially in emergency settings Medical imaging expertise is scarce in underserved regions Human error in medical diagnosis affects millions of patients annually Second opinions are often unavailable or delayed in critical situations\n\nThe Interpretability Crisis in Medical AI\n\nA pivotal moment in our inspiration came from conversations with radiologists who expressed deep skepticism about existing AI diagnostic tools. They shared frustrating experiences with AI systems that would flag potential diseases but provide no explanation of their reasoning. One radiologist told us: \"I can't stake my medical license on a system that won't show me why it thinks there's a problem. If I can't understand the AI's reasoning, how can I trust it with my patients' lives?\"\n\nThis highlighted a fundamental barrier to AI adoption in healthcare:\n\nTrust requires transparency : Medical professionals need to understand AI decision-making processes Clinical accountability : Doctors remain legally responsible for diagnoses, requiring explainable AI recommendations Educational value : Interpretable AI can serve as a teaching tool for medical students and residents Bias detection : Visual explanations help identify when AI models focus on irrelevant image artifacts Quality assurance : Interpretability enables validation that AI is \"looking\" at clinically relevant anatomical regions\n\nOpenMed's Answer: Visual AI Explanations\n\nOur GradCAM visualization technology addresses the interpretability crisis by showing exactly which brain regions influenced the AI's tumor detection decision. The heat map overlay allows radiologists to validate that the AI is focusing on medically relevant anatomical structures, building trust through transparency.\n\nWe were particularly moved by stories of:\n\nRural hospitals lacking radiologists for urgent chest X-ray interpretations Developing countries with limited access to specialized medical imaging expertise Medical students and residents needing better training tools for pattern recognition Patients waiting weeks for specialist consultations that could be expedited with AI assistance\n\nThe vision was clear : Create an AI system that doesn't replace medical professionals but empowers them with intelligent, interpretable, and trustworthy assistance. We wanted to democratize access to advanced medical imaging analysis while maintaining the highest standards of clinical accuracy and transparency.\n\nOur Interpretability-First Approach\n\nWe recognized that for medical AI to gain widespread adoption, interpretability couldn't be an afterthoughtâ€”it had to be foundational. Our mission became:\n\nBuild trust through transparency : Every AI decision comes with visual explanations showing which image regions influenced the diagnosis Enable clinical validation : Provide tools for medical professionals to verify that AI reasoning aligns with medical knowledge Foster AI-human collaboration : Create a partnership where AI augments human expertise rather than obscuring it Accelerate medical education : Transform AI explanations into powerful learning tools for the next generation of healthcare providers Ensure ethical AI deployment : Maintain accountability and prevent algorithmic bias through interpretable systems\n\nThis interpretability-first philosophy guided every design decision, from our GradCAM visualization system to our conversational AI interface that explains medical reasoning in plain language."},{"heading":"What it does","content":"OpenMed is a comprehensive AI-powered medical imaging analysis platform that revolutionizes how healthcare professionals approach diagnostic imaging. The system provides:\n\nCore Capabilities\n\nMulti-Disease Detection : Automatically detects and classifies pneumonia, tuberculosis, and brain tumors from medical images Intelligent Conversational Interface : OpenAI-powered agent that understands natural language queries about medical images Visual Explanations : GradCAM-based interpretability showing exactly which regions influenced the AI's decision Confidence Scoring : Provides uncertainty quantification to help clinicians make informed decisions Web-Based Interface : User-friendly platform accessible from any device with internet connectivity\n\nReal-World Application Scenarios\n\nEmergency Department Triage : Rapid pneumonia screening from chest X-rays during busy shifts Telemedicine Support : Remote consultation assistance for rural healthcare providers Medical Education : Interactive learning platform for students and residents Second Opinion Services : Automated preliminary analysis before specialist review Quality Assurance : Continuous monitoring and validation of diagnostic accuracy\n\nSupported Medical Conditions\n\nDisease Model Classes Test Accuracy Data Type Pneumonia ResNet50 2 (Normal, Pneumonia) 96.49% Chest X-rays Tuberculosis ResNet50 2 (Normal, TB) 98.65% Chest X-rays Brain Tumor ResNet50 3 (Glioma, Meningioma, Tumor) 97.21% Brain MRI\n\nDetailed Capabilities:\n\nPneumonia Detection : Binary classification (Normal vs. Pneumonia) from chest X-rays with 96.49% accuracy Tuberculosis Screening : Early detection of TB patterns in chest radiographs with 98.65% accuracy Brain Tumor Classification : Multi-class identification of glioma, meningioma, and other tumor types from MRI scans with 97.21% accuracy\n\nThe platform seamlessly integrates into existing clinical workflows while providing the transparency and interpretability essential for medical decision-making."},{"heading":"How we built it","content":"Building OpenMed required integrating cutting-edge AI technologies with robust software engineering practices and deep understanding of medical requirements.\n\nTechnical Architecture\n\nThe OpenMed platform architecture showcases the integration of multiple AI models, intelligent agent system, and user interfaces working together to provide comprehensive medical imaging analysis.\n\n1. Deep Learning Foundation\n\nModel Selection : Chose ResNet50 architecture for its proven performance in medical imaging Wightman et al., 2021 Transfer Learning : Leveraged pre-trained ImageNet weights and fine-tuned on medical datasets Multi-Model Approach : Separate specialized models for each medical condition Vision Transformers : Implemented advanced transformer architecture for enhanced feature extraction\n\nMedical Datasets Used:\n\nBrain Cancer MRI Dataset : Open source brain cancer MRI dataset from Kaggle @https://www.kaggle.com/datasets/orvile/brain-cancer-mri-dataset Tuberculosis Chest X-ray Dataset : Open source tuberculosis chest X-ray dataset from Kaggle @https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset Chest X-Ray Images (Pneumonia) Dataset : Open source pneumonia chest X-ray dataset from Kaggle @https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n\n2. Interpretability Layer\n\nGradCAM Integration : Built custom visualization pipeline to generate attention maps based on Gradient-weighted Class Activation Mapping (Grad-CAM) methodology Selvaraju et al., 2016 Feature Attribution : Implemented methods to trace decision pathways back to input regions Confidence Calibration : Developed uncertainty quantification techniques for clinical reliability\n\n3. Intelligent Agent System\n\nOpenAI Integration : Built conversational interface using GPT models for natural language understanding Intent Classification : Developed medical intent recognition to route queries appropriately Context Management : Implemented multi-turn conversation handling for complex medical discussions\n\n4. Backend Infrastructure\n\nFastAPI Framework : RESTful API design with OpenAI-compatible endpoints Microservices Architecture : Separate services for feature extraction, classification, and visualization MLflow Integration : Comprehensive experiment tracking and model management Docker Containerization : Consistent deployment across different environments\n\n5. Frontend Experience\n\nOpenWebUI Integration : Modern, responsive web interface Real-time Processing : Asynchronous image upload and analysis Interactive Visualizations : Dynamic GradCAM overlays and confidence displays\n\nDevelopment Methodology\n\nAgile Development : Iterative development with continuous stakeholder feedback Test-Driven Development : Comprehensive testing suite for medical-grade reliability Clinical Validation : Collaboration with medical professionals for accuracy verification Regulatory Compliance : Built-in HIPAA compliance and FDA guideline adherence\n\nDevelopment Infrastructure: HP AI Studio\n\nModel Training and Tracking with HP AI Studio\n\nHP AI Studio provided the comprehensive development environment for training, tracking, and managing our medical AI models with MLflow integration.\n\nComprehensive Metrics Tracking and Monitoring\n\nOur MLflow integration within HP AI Studio enabled comprehensive tracking of model performance metrics including accuracy, loss, F1 score, sensitivity, and specificity across all medical AI models. This systematic approach to metrics monitoring ensured consistent model performance validation and enabled data-driven optimization decisions throughout the development lifecycle.\n\nWe leveraged HP AI Studio as our primary development and deployment infrastructure, which proved instrumental in building OpenMed:\n\nTraining Environment:\n\nGPU-Accelerated Training : Utilized HP AI Studio's high-performance computing resources for efficient deep learning model training Experiment Tracking : Integrated MLflow within HP AI Studio to track model experiments, hyperparameters, and performance metrics Model Registry : Centralized model versioning and artifact management for all medical AI models Data Pipeline Management : Streamlined data preprocessing and augmentation workflows\n\nDeployment Infrastructure:\n\nTerminal-Based Deployment : Used HP AI Studio's terminal environments to deploy frontend, middleware, and backend services Containerized Services : Deployed Docker containers for consistent environment management across development and production Scalable Computing : Leveraged elastic compute resources for handling variable workloads Integrated Development : Seamless integration between model development, testing, and deployment phases\n\nKey Benefits:\n\nUnified Platform : Single environment for the entire ML development lifecycle Resource Efficiency : On-demand scaling of computational resources based on training requirements Collaboration : Shared environments enabling team collaboration and code sharing Production Readiness : Smooth transition from development to production deployment\n\nTechnology Stack\n\nBackend : Python, FastAPI, PyTorch, OpenAI API Frontend : OpenWebUI, HTML/CSS/JavaScript Database : SQLite for development, PostgreSQL for production Monitoring : MLflow, custom logging and analytics Deployment : Docker, cloud-ready infrastructure\n\nKey Dependencies\n\n1. OpenWebUI\n\nDescription : User-friendly AI interface that supports multiple AI providers including OpenAI API Role : Provides the frontend web interface for OpenMed's conversational medical AI Repository : Open WebUI GitHub Benefits : Streamlined chat interface, image upload capabilities, and seamless integration with FastAPI backends\n\n2. FastAPI\n\nDescription : Modern, fast web framework for building APIs with Python based on standard Python type hints Role : Powers OpenMed's backend services and provides OpenAI-compatible API endpoints Benefits : High performance, automatic API documentation, and easy integration with AI models\n\n3. Python 3.8+\n\nDescription : Core programming language for the entire OpenMed platform Role : Foundation for all machine learning, API development, and data processing components Benefits : Rich ecosystem for AI/ML development, extensive medical imaging libraries, and robust deployment tools\n\n4. PyTorch\n\nDescription : Open source machine learning framework for deep learning and neural network development Role : Training and inference engine for all medical AI models including ResNet50 architectures Benefits : Dynamic computation graphs, excellent GPU acceleration, and strong medical imaging community support\n\n5. HP AI Studio\n\nDescription : Comprehensive AI development and deployment platform providing GPU resources and MLOps capabilities Role : Primary development infrastructure for model training, experiment tracking, and deployment Benefits : Unified development environment, scalable computing resources, and integrated model management\n\n6. MLflow\n\nDescription : Open source platform for managing the complete machine learning lifecycle Role : Experiment tracking, model versioning, and performance monitoring for all medical AI models Benefits : Comprehensive model management, reproducible experiments, and seamless deployment workflows"},{"heading":"Pre-trained Models and Accessibility","content":"ðŸš€ Ready-to-Use Models on HuggingFace Hub\n\nTo make OpenMed accessible to the broader healthcare and research community, we've published all our trained models on HuggingFace Hub:\n\nRepository : https://huggingface.co/hitmanonholiday/openmed-medical-imaging-models\n\nThis repository contains production-ready, pre-trained ResNet50 models for immediate use, eliminating the need for users to train models from scratch.\n\nModel Availability\n\nModel Purpose HuggingFace Path Local Checkpoint Name Brain Tumor Classifier 3-class brain tumor detection resnet50_brain_tumor_full/pytorch_model.bin best_resnet50_brain_tumor_full_trained.pth Tuberculosis Detector TB screening from chest X-rays resnet50_tb_full/pytorch_model.bin best_resnet50_tb_full_trained.pth Pneumonia Detector Pneumonia detection from chest X-rays resnet50_pneumonia_full/pytorch_model.bin best_resnet50_pneumonia_full_trained.pth\n\nQuick Setup Instructions\n\nAutomated Setup (Recommended) :\n\n# Install required dependency pip install huggingface-hub # Run one-time setup script python -c \" from huggingface_hub import hf_hub_download import os, shutil # Create checkpoint directories dirs = ['checkpoints/resnet50_brain_tumor_full', 'checkpoints/resnet50_tb_full', 'checkpoints/resnet50_pneumonia_full'] for d in dirs: os.makedirs(d, exist_ok=True) # Download and place models models = [ ('resnet50_brain_tumor_full/pytorch_model.bin', 'checkpoints/resnet50_brain_tumor_full/best_resnet50_brain_tumor_full_trained.pth'), ('resnet50_tb_full/pytorch_model.bin', 'checkpoints/resnet50_tb_full/best_resnet50_tb_full_trained.pth'), ('resnet50_pneumonia_full/pytorch_model.bin', 'checkpoints/resnet50_pneumonia_full/best_resnet50_pneumonia_full_trained.pth') ] for hf_path, local_path in models: print(f'Downloading {hf_path}...') downloaded = hf_hub_download(repo_id='hitmanonholiday/openmed-medical-imaging-models', filename=hf_path) shutil.copy2(downloaded, local_path) print(f'âœ… Saved to {local_path}') print('ðŸŽ‰ All models ready for use!') \"\n\nManual Setup :\n\nVisit: https://huggingface.co/hitmanonholiday/openmed-medical-imaging-models Download the pytorch_model.bin files from each model directory Rename and place them in the correct checkpoints/ subdirectories as shown in the table above\n\nExpected Directory Structure\n\nAfter setup, your project should have:\n\nopenMed/ â”œâ”€â”€ checkpoints/ â”‚ â”œâ”€â”€ resnet50_brain_tumor_full/ â”‚ â”‚ â””â”€â”€ best_resnet50_brain_tumor_full_trained.pth â”‚ â”œâ”€â”€ resnet50_tb_full/ â”‚ â”‚ â””â”€â”€ best_resnet50_tb_full_trained.pth â”‚ â””â”€â”€ resnet50_pneumonia_full/ â”‚ â””â”€â”€ best_resnet50_pneumonia_full_trained.pth â””â”€â”€ ... (other project files)\n\nCustom Training Option\n\nWhile pre-trained models provide immediate functionality, users can also train their own models:\n\nBenefits of Custom Training :\n\nðŸŽ¯ Domain Adaptation : Fine-tune on institution-specific datasets ðŸ”§ Hyperparameter Optimization : Customize for specific hardware configurations ðŸ“Š Population-Specific Models : Adapt to specific patient demographics or imaging protocols ðŸš€ Research Applications : Experiment with different architectures or training strategies\n\nTraining Commands :\n\ncd src/rd # Train custom models python resnet50_pneumonia_full.py # Pneumonia detection python resnet50_tb_full.py # Tuberculosis detection python resnet50_brain_tumor_full.py # Brain tumor classification\n\nDeployment Flexibility\n\nThe availability of pre-trained models enables multiple deployment scenarios:\n\nImmediate Deployment : Use pre-trained models for instant setup Fine-tuning : Start with pre-trained weights and adapt to specific datasets Research & Development : Use as baseline models for academic research Educational Use : Enable medical students and researchers to explore AI interpretability\n\nThis approach democratizes access to advanced medical AI, enabling healthcare institutions and researchers worldwide to benefit from state-of-the-art medical imaging analysis without requiring extensive computational resources for training."},{"heading":"Challenges we ran into","content":"Technical Challenges\n\n1. Medical Dataset Quality and Diversity\n\nChallenge : Medical imaging datasets often have inconsistent quality, limited diversity, and potential biases Solution : Implemented robust data preprocessing pipelines, augmentation techniques, and multi-dataset validation Impact : Achieved more reliable and generalizable model performance across different patient populations\n\n2. Model Interpretability vs. Performance Trade-off\n\nChallenge : Making models interpretable without sacrificing diagnostic accuracy Solution : Developed custom GradCAM implementation optimized for medical imaging, integrated attention mechanisms Impact : Maintained high accuracy while providing clinically meaningful explanations\n\n3. Real-time Processing Requirements\n\nChallenge : Medical applications require fast response times for emergency scenarios Solution : Optimized model inference, implemented efficient caching, and used GPU acceleration Impact : Reduced average processing time to under 3 seconds per image\n\n4. OpenAI API Integration Complexity\n\nChallenge : Creating seamless integration between medical AI models and conversational AI Solution : Built custom middleware layer with intelligent routing and context management Impact : Enabled natural language interaction with complex medical AI systems\n\nDomain-Specific Challenges\n\n5. Clinical Validation and Trust\n\nChallenge : Gaining trust from medical professionals skeptical of AI systems Solution : Implemented comprehensive interpretability features, extensive validation studies, and transparent performance reporting Impact : Achieved clinical acceptance through evidence-based validation and clear explanations\n\n6. Regulatory and Compliance Requirements\n\nChallenge : Navigating complex medical device regulations and privacy requirements Solution : Built-in HIPAA compliance, implemented audit trails, and designed for FDA pathway compliance Impact : Created a system ready for clinical deployment and regulatory approval\n\n7. Multi-Disease Model Management\n\nChallenge : Managing and coordinating multiple specialized AI models Solution : Developed unified API layer with intelligent model routing and version management Impact : Seamless user experience despite complex backend architecture\n\nInfrastructure Challenges\n\n8. Scalability and Performance Optimization\n\nChallenge : Ensuring system performance under high concurrent load Solution : Implemented microservices architecture, load balancing, and efficient resource management Impact : System capable of handling multiple simultaneous medical image analyses"},{"heading":"Accomplishments that we're proud of","content":"Technical Achievements\n\nModel Performance Summary\n\nOur ResNet50-based medical AI models achieved exceptional test accuracies across all supported medical conditions:\n\nMedical Condition Model Architecture Classification Type Test Accuracy Dataset Size Pneumonia Detection ResNet50 Binary (Normal/Pneumonia) 96.49% Chest X-rays Tuberculosis Detection ResNet50 Binary (Normal/TB) 98.65% Chest X-rays Brain Tumor Classification ResNet50 Multi-class (3 types) 97.21% Brain MRI\n\n1. High-Accuracy Medical AI Models\n\nAchieved 96.49% accuracy on pneumonia detection and 98.65% accuracy on tuberculosis detection Developed robust brain tumor classification with 97.21% multi-class accuracy Successfully validated models on diverse, real-world medical imaging datasets\n\n2. Breakthrough in Medical AI Interpretability\n\nCreated clinically meaningful GradCAM visualizations that highlight anatomically relevant regions Developed natural language explanation system that provides medical reasoning in plain English Integrated confidence scoring that helps clinicians make informed decisions\n\n3. Seamless Human-AI Integration\n\nBuilt conversational interface that understands complex medical queries Created workflow integration that enhances rather than disrupts clinical practice Developed real-time processing capabilities suitable for emergency medical scenarios\n\nInnovation Highlights\n\n4. Novel Multi-Modal Approach\n\nSuccessfully combined deep learning, computer vision, and conversational AI Integrated multiple specialized models under a unified, intuitive interface Created first-of-its-kind medical imaging platform with natural language interaction\n\n5. Open Source Contribution\n\nDeveloped reusable components for medical AI interpretability Created comprehensive documentation and tutorials for the medical AI community Built extensible architecture that supports addition of new medical conditions\n\n6. Clinical Impact Potential\n\nDesigned system that can democratize access to advanced medical imaging analysis Created educational tools that can enhance medical training and knowledge transfer Developed technology that can significantly reduce diagnostic delays in underserved regions\n\nEngineering Excellence\n\n7. Production-Ready System\n\nBuilt robust, scalable architecture ready for clinical deployment Implemented comprehensive monitoring, logging, and error handling Created extensive test suites ensuring medical-grade reliability\n\n8. User Experience Innovation\n\nDesigned intuitive interface that requires minimal training for medical professionals Created responsive, accessible web platform compatible with hospital IT systems Developed mobile-friendly interface for point-of-care usage"},{"heading":"What we learned","content":"Technical Insights\n\n1. Medical AI Requires Different Standards We learned that medical AI applications demand significantly higher standards than traditional AI systems:\n\nInterpretability is non-negotiable : Medical professionals need to understand AI reasoning to trust and validate decisions Failure modes must be predictable : Systems must fail gracefully and indicate uncertainty clearly Performance consistency is critical : Models must perform reliably across diverse patient populations and imaging conditions\n\n2. The Power of Transfer Learning in Medical Imaging Pre-trained models provided an excellent foundation, but medical domain fine-tuning was crucial:\n\nImageNet features translate well to medical imaging tasks Domain-specific augmentation significantly improves generalization Multi-dataset training is essential for robust performance across different hospitals and imaging protocols\n\n3. Integration Complexity Building a cohesive system from multiple AI components presented unique challenges:\n\nAPI design matters : Consistent interfaces enable seamless integration State management is crucial : Medical conversations require context preservation Error propagation : Failures in one component can cascade through the entire system\n\nDomain Knowledge Acquisition\n\n4. Healthcare Workflow Understanding Working on medical AI taught us the complexity of healthcare systems:\n\nClinical workflows are intricate : AI must integrate seamlessly without disrupting established practices Stakeholder diversity : Different users (doctors, nurses, administrators) have different needs and perspectives Regulatory landscape : Medical AI faces complex approval processes and compliance requirements\n\n5. The Importance of Clinical Partnership Collaboration with medical professionals was invaluable:\n\nDomain expertise is irreplaceable : Technical excellence alone isn't sufficient for medical applications Validation requires clinical input : Performance metrics must align with clinical relevance User feedback drives improvement : Iterative development with clinical partners led to significant enhancements\n\nProject Management Lessons\n\n6. Agile Development in Regulated Domains Traditional agile methodologies needed adaptation for medical AI:\n\nDocumentation is critical : Regulatory compliance requires extensive documentation from the start Validation takes time : Clinical validation cycles are longer than typical software testing Change management : Modifications require careful impact assessment in medical contexts\n\n7. The Value of Interpretability Investing in interpretability features proved more valuable than initially anticipated:\n\nTrust building : Interpretable AI gained faster acceptance from medical professionals Debugging capabilities : Visual explanations helped identify and fix model issues Educational value : Interpretability features became powerful teaching tools"},{"heading":"What's next for OpenMed","content":"Immediate Roadmap (Next 6 Months)\n\n1. Clinical Validation Studies\n\nPartner with hospitals for prospective clinical trials Conduct reader studies comparing AI performance with radiologists Gather real-world performance data across diverse patient populations Publish peer-reviewed validation studies in medical journals\n\n2. Regulatory Pathway\n\nSubmit FDA 510(k) pre-submission for pneumonia detection module Implement additional compliance features for medical device classification Develop quality management system for medical device standards Create comprehensive clinical evaluation protocols\n\n3. Enhanced Interpretability Features\n\nCounterfactual Explanations : \"What would need to change for a different diagnosis?\" Uncertainty Visualization : Advanced confidence mapping and error bounds Temporal Analysis : Track disease progression over multiple imaging studies Comparative Analysis : Side-by-side comparison with historical cases\n\nMedium-term Expansion (6-18 Months)\n\n4. New Medical Conditions\n\nCardiovascular Disease : ECG analysis and cardiac imaging interpretation Orthopedic Imaging : Fracture detection and musculoskeletal analysis Dermatology : Skin lesion classification and melanoma screening Ophthalmology : Diabetic retinopathy and glaucoma detection\n\n5. Advanced AI Capabilities\n\nMulti-modal Integration : Combine imaging with electronic health records and lab results Predictive Analytics : Risk assessment and prognosis prediction Personalized Medicine : Patient-specific risk factors and treatment recommendations Federated Learning : Collaborative model training while preserving patient privacy\n\n6. Platform Enhancements\n\nMobile Applications : Native iOS and Android apps for point-of-care usage PACS Integration : Direct integration with hospital picture archiving systems Voice Interface : Speech-to-text capabilities for hands-free operation Workflow Automation : Integration with hospital information systems\n\nLong-term Vision (18+ Months)\n\n7. Global Health Impact\n\nTelemedicine Platform : Comprehensive remote diagnostic capabilities Developing World Deployment : Offline-capable versions for resource-limited settings Medical Education Integration : Curriculum integration with medical schools Public Health Applications : Population-level screening and surveillance\n\n8. Research & Development\n\nNovel Architecture Exploration : Investigation of cutting-edge AI architectures Biomarker Discovery : AI-driven identification of novel imaging biomarkers Drug Development Support : Imaging endpoints for clinical trials Precision Medicine : Genomics-imaging integration for personalized healthcare\n\n9. Ecosystem Development\n\nDeveloper Platform : APIs and SDKs for third-party medical AI applications Marketplace Model : Curated marketplace for specialized medical AI models Research Collaboration : Partnerships with academic medical centers and research institutions Open Source Initiative : Release of core components to accelerate medical AI research\n\nSustainability and Impact Goals\n\n10. Business Model Maturation\n\nSaaS Platform : Subscription-based model for healthcare institutions Per-Study Pricing : Flexible pricing for smaller practices and telemedicine providers Partnership Revenue : Revenue sharing with integrated healthcare platforms Training and Consulting : Professional services for AI implementation\n\n11. Social Impact Objectives\n\nHealthcare Democratization : Make advanced diagnostic AI accessible globally Medical Education Enhancement : Transform how medical professionals learn and practice Research Acceleration : Contribute to faster medical research and discovery Health Equity : Reduce healthcare disparities through accessible AI technology\n\nOpenMed represents just the beginning of a healthcare revolution. Our vision extends beyond diagnostic assistance to comprehensive AI-powered healthcare support that maintains human expertise at its center while dramatically expanding access to quality medical care worldwide.\n\nOpenMed: Where artificial intelligence meets human compassion in healthcare."},{"heading":"Built With","content":"fastapi hp-ai-studio openai openwebui python pytorch"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Medical Report Analyzer & Healthy Habits Recommender","project_url":"https://devpost.com/software/medical-report-analyzer-healthy-habits-recommender","tagline":"This application provides a simple web interface to generate summaries, health suggestions, and food recommendations from medical documents (PDFs or images)","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/468/396/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Honorable Mention: Healthcare"}],"team_members":[],"built_with":[{"name":"llama","url":null},{"name":"llm","url":null},{"name":"openai","url":null},{"name":"prompt","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/nobelchowdary/hp-ai-hackathon-health-habit-recommender-app"}],"description_sections":[{"heading":"Inspiration","content":"Every year during my annual hospital checkups, I receive multiple test reports. Some values are normal, while others need improvement. But after a few days, I tend to forget about them. I wanted a solution that could analyze my reports, remind me of areas to improve, and suggest actionable health habits so I can stay on track year-round.\n\nThis project was inspired by that needâ€”to have an intelligent assistant that turns my medical data into daily, personalized health advice."},{"heading":"What it does","content":"Accepts medical documents (PDFs, scans, images) Uses OCR to extract structured medical data Summarizes doctorâ€™s notes and prescriptions using GPT Generates personalized: Health summaries Few health habit suggestions Few food/nutrition suggestions Stores data per user profile and avoids duplicate document processing Recommends habits based on entire user history, not just one report"},{"heading":"How we built it","content":"Backend: Python Frontend: HTML UI OCR: Tesseract for extracting text from images AI Engine: OpenAI GPT/Llama2 local model for summarization and suggestions Model Management: MLflow for model versioning and deployment Platform: Deployed and tested on HP AI Studio\n\nThe pipeline automatically connects uploaded files â†’ OCR â†’ GPT prompt creation â†’ response processing â†’ user-specific output generation."},{"heading":"Challenges we ran into","content":"Parsing inconsistent formats across medical reports Handling poor-quality scanned images for OCR Designing prompts that produce medically accurate summaries and suggestions Avoiding redundant processing of the same report multiple times Deployment issues with GitHub and HP AI Studio integration"},{"heading":"Accomplishments that we're proud of","content":"Fully functional end-to-end system: upload â†’ analyze â†’ actionable insights Automatically detects duplicate uploads to save compute and time Personalized health & food suggestions based on real medical context Intuitive and clean interface with minimal setup Packaged model using MLflow for reproducibility"},{"heading":"What we learned","content":"Effective ways to combine OCR + LLMs for real-world document processing How to build self-adaptive prompts for variable medical data Streamlining model lifecycle with MLflow Leveraging HP AI Studioâ€™s deployment stack for app hosting and testing Importance of user-centric design for medical applications"},{"heading":"What's next for Medical Report","content":"Add multi-language support for reports in regional languages Enable continuous habit tracking and notifications via email or mobile app Integrate basic health visualizations (charts/trends) Include more advanced medical models for deeper insights Add voice-based interaction for elderly or visually impaired users"},{"heading":"Built With","content":"llama llm openai prompt python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Codence","project_url":"https://devpost.com/software/codence","tagline":"Codence is an AI agent that turns meeting decisions into enforceable specs by syncing Jira tickets and validating GitLab code against agreed rules.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/467/130/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Best in Enterprise Data Analysis"}],"team_members":[],"built_with":[{"name":"ai-studio","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"hp","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/ChaoKuan-Lin/HP-hackathon#"}],"description_sections":[{"heading":"ðŸ§  About the Project","content":"What inspired us\n\nModern product development moves fast, but alignment often lags behind. Now, with AI tools writing code faster than ever, weâ€™re seeing an ironic gap: speed has increased â€” but product quality hasnâ€™t. Teams can generate code in seconds, but if that code doesnâ€™t reflect the latest meeting decisions, it's just fast wrong work. We were inspired by this growing mismatch between decision-making and implementation .\n\nPMs spend hours manually checking Jira tickets. Developers unknowingly violate specs that were never updated. We asked: What if an AI could watch over these processes and enforce alignment? Thatâ€™s how Codence was born â€” to make sure product consensus becomes code reality.\n\nWhat we learned\n\nWe learned that:\n\nMeeting notes are messy, but full of hidden structure. Developers donâ€™t ignore specsâ€”they often never see them. The gap between decisions and delivery is often process, not intent.\n\nWe also realized how powerful LLMs can be when paired with source-of-truth systems like Jira and GitLab.\n\nHow we built it\n\nFrontend : A minimal web interface for uploading meeting notes and reviewing suggestions. Backend : Python + FastAPI with GitLab webhook integration. AI : GPT-based models extract decisions and rules from unstructured meeting notes. DevOps : GitLab Webhook triggers Codence to validate PRs against extracted rules. Jira Integration : Syncs ticket updates through REST APIs after human approval.\n\nChallenges we faced\n\nParsing casual, non-standard meeting notes into structured intent. Mapping natural language decisions to specific Jira tickets and code actions. Avoiding false positives in code rule checks while keeping responses fast. Making the AI explain why something violates a rule in human-readable terms.\n\nDespite the constraints of time and scope, weâ€™re proud that Codence brings alignment closer to automation."},{"heading":"Built With","content":"ai-studio docker flask hp openai python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"QuantumFinance AI","project_url":"https://devpost.com/software/quantumfinance-ai","tagline":"Your AI copilot for the stock market. Get instant analysis, news summaries, and trading strategies to make smarter, faster decisions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/465/101/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Honorable Mention: Finance"}],"team_members":[],"built_with":[{"name":"hp-ai-studio","url":null},{"name":"mlflow","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"quantum-finance-ai-deploy.vercel.app","url":"https://quantum-finance-ai-deploy.vercel.app"},{"label":"github.com","url":"https://github.com/Husky-AI9/QuantumFinanceAI"}],"description_sections":[{"heading":"Inspiration","content":"The world of finance is flooded with data, but the tools to make sense of it all are often too complex or expensive for the average person. I wanted to change that. My goal was to build a single, easy to use platform that uses AI to break down complex market information into clear, actionable insights. I built QuantumFinance AI to give every investor a real strategic advantage."},{"heading":"What it does","content":"QuantumFinance AI is a complete financial analysis platform powered by Large Language Model AI. Here's are the features:\n\nMain Dashboard: Shows you the hot topics driving a stock sector, which companies are gaining momentum, and the overall market mood. AI Stock Research: Gives you a full picture of any stock by creating a single story from many web sources and modeling its future projections. LLM Powered Stock Strategy: Creates trading strategies based on a stock's history. You can get ideas for intraday, weekly, or monthly timeframes. Synthesized News: Keeps you ahead with quick summaries of important news for any company or theme you're watching. Market Sentiment Analysis: Helps you understand the feeling of the market and what's driving it for specific stocks. Automated Risk Identification: Lays out the key risks for a company or sector so you can make safer decisions. Stock Assist: This is your personal finance expert. You can ask it specific questions like, \"Summarize Apple's latest earnings report,\" and get a straight answer, fast."},{"heading":"How we built it","content":"I built QuantumFinance AI with a local first, security focused approach. That's why I chose HP AI Studio to be the foundation of my work.\n\nBackend and Development: The entire backend, built with Python and FastAPI, was developed inside HP AI Studio's containerized environment. This gave me a stable and ready to use workspace with NVIDIA's libraries already set up, which saved a huge amount of time. Deployment and Version Control: I used MLflow inside HP AI Studio for local deployment and to keep track of my models. This made it easy to test new ideas and roll back to an older version if something didn't work. For the live version, the application is containerized, so it's ready for cloud deployment. AI and Models: I used Perplexity Large Language Model API for the momentum dashboard, stock assist, news summary , sentiment analysis, and risk features. This allow me to incorporate A.I with web searching for latest data My Stock Research feature uses Perplexity Deep Research Model pipeline to generate a comprehensive report on requested company using the most up to date information. The Stock Strategy feature uses an Perplexity Deep Reasoning Model with historical market data from AlphaAdvantage API to come up with trading signals.\n\nHow HP AI Studio Helped\n\nSolving a Real World Problem: I used HP AI Studio to build a serious FinTech application from the ground up, showing how it can handle a complex, real world project. A Better Technical Workflow: My process was made much easier by HP AI Studio. I used its containers for stable development and MLflow to manage all my AI models, all while using the power of NVIDIA GPUs. Key Features I Used: Local First Security: Building the tool locally was essential for handling financial data safely and privately. Efficiency and Repeatable Results: The built in tools made my development cycle faster and meant I could easily reproduce my results. NVIDIA GPU Power: Having direct access to GPU acceleration was critical for training my models and making the platform fast."},{"heading":"Challenges we ran into","content":"My main challenge was not training models, but orchestrating a suite of powerful external APIs to work together seamlessly. Getting a specialized model like Perplexity to provide precise, reliable financial insights requires more than a simple query; it demands meticulous prompt engineering . Crafting and refining the perfect prompts to get structured, accurate data for each feature was a huge task.\n\nThis is where HP AI Studio became indispensable. I ran my entire FastAPI backend within its stable, containerized environment. This allowed me to rapidly test and debug countless API calls and data pipelines without ever worrying about system conflicts. As I iterated through hundreds of prompt variations, especially for the nuanced sentiment and risk analysis features, keeping track of what worked was critical. HP AI Studio's built-in MLflow integration was a lifesaver. It allowed me to log each prompt version and its resulting output, creating a clear, versioned history that made the entire process manageable and prevented me from losing progress. Without this structured environment, managing the complexity would have been overwhelming."},{"heading":"Accomplishments that we're proud of","content":"My proudest accomplishment is the sophisticated architecture of QuantumFinance AI, which I was able to design and build as a solo developer entirely within HP AI Studio . The platform acts as a central hub, intelligently routing user requests to the correct specialized AI model and then synthesizing the responses into a simple, clear format. Building this complex system was only possible because of the streamlined and organized workflow the studio provided."},{"heading":"What we learned","content":"This project taught me a lot about applying AI in the real world of finance. I learned how important a good development environment like HP AI Studio is for keeping a complex project on track. I also got great hands on experience fine tuning language models for finance and building a solid RAG pipeline. Most importantly, I learned that building a tool that people will actually find useful means focusing on the user from start to finish."},{"heading":"What's next for QuantumFinance AI","content":"I believe QuantumFinance AI is just getting started. Here's what I have planned next:\n\nMore Data: I plan to add support for international markets, cryptocurrencies, and other alternative data sources. Smarter AI: I will keep improving the models and plan on exploring NVIDIA's NeMo framework to build custom language models for even better financial insights. Mobile App: I will be developing native apps for both iOS and Android."},{"heading":"Built With","content":"hp-ai-studio mlflow python"},{"heading":"Try it out","content":"quantum-finance-ai-deploy.vercel.app github.com"}]},{"project_title":"TRYON","project_url":"https://devpost.com/software/tryon-woi0bh","tagline":"Try before you buy â€“ virtually using GenAI. Our app lets users try on clothes using just their image, all processed locally for complete privacy and instant results.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/468/450/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Honorable Mention: Retail"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"hp","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"studio","url":null},{"name":"transformers","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/krishnaadithya/tryon"}],"description_sections":[{"heading":"Inspiration","content":"Shopping online often lacks the personal touch of trying on clothes. Many users hesitate to purchase because theyâ€™re unsure how the outfit will look on them. At the same time, concerns over data privacy make uploading personal images to the cloud a non-starter. We wanted to solve this by offering a fast, private, and fun virtual try-on experienceâ€”right from the userâ€™s device."},{"heading":"What it does","content":"TryOn allows users to virtually try on clothes using their own photo, without uploading any personal data to external servers. The application uses a body segmentation model to isolate the userâ€™s silhouette and a powerful inpainting diffusion model to realistically overlay the selected clothing item. All processing happens locally on the user's device, ensuring full privacy."},{"heading":"How we built it","content":"We combined:\n\nA body segmentation model to accurately detect and segment the user's figure GenAI - An inpainting diffusion based model generative AI model to generate a realistic try-on image by seamlessly blending the garment with the user's photo A locally hosted inference pipeline, optimized to run on standard consumer devices without requiring internet or cloud support A simple, intuitive user interface that lets users upload their image, select a clothing item, and preview the try-on instantly"},{"heading":"Challenges we ran into","content":"Deployment hurdles â€” setting up the application for local use involved troubleshooting vague error messages and failure logs. Lack of clear diagnostics made it difficult to pinpoint issues during runtime, which significantly slowed down the initial deployment process. Managing memory and compute constraints for real-time local inference, compared to just hosting FASTAPI. When not hosting API endpoint I can offload and spin up models based on requirements but here all endpoints that is needed to run the solution should be up and running, which does not help in GPU constraint situations."},{"heading":"Accomplishments that we're proud of","content":"Successfully deploying a fully offline virtual try-on system Maintaining a high standard of visual realism while keeping the app lightweight Enabling complete user privacyâ€”images never leave the device Making it easy and intuitive for anyone to try on clothes virtually -Registered the model on AI Studio"},{"heading":"What we learned","content":"Gained hands-on experience in registering and managing models in AI Studio, which helped streamline model versioning and deployment Learned to leverage HP AI Studio to assist with deploying the virtual try-on pipeline"},{"heading":"What's next for TRYON","content":"Include a text to image search that allows users to search through different clothing items"},{"heading":"Built With","content":"ai hp python studio transformers"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Orpheus Engine","project_url":"https://devpost.com/software/orpheus-engine","tagline":"The Orpheus Engine Workspace, the AI-powered DAW for audio restoration and analysis, offering seamless cross-device use and diverse plugin support for music, gaming, and data science!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/466/276/datas/medium.png","prizes":[{"hackathon_name":"HP & NVIDIA Developer Challenge","hackathon_url":"https://hpaistudio.devpost.com/","prize_name":"Bonus Honorable Mention: Technology"}],"team_members":[],"built_with":[{"name":"graphql","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/jhead12/orpheus-engine"}],"description_sections":[{"heading":"Inspiration","content":"The Orpheus Engine Workspace is a Digital Audio Workstation that enables users to perform audio restoration and analysis with the assistance of an AI agent. The app is designed to work seamlessly within any environment, allowing users to continue their research in the browser, on desktops, and mobile devices. The use of various data technologies designed as plugins for diverse audio exports that include metadata or analysis results critical for audio applications. The OEW has input with various audio devices, nRF (Bluetooth), audio interfaces, and MADI. The use of the HP Ai Studio can leverage the CPU and GPU, enabling the use of more dynamic and pro-level processing plugins, which can be utilized for Gaming, music, and data science projects."},{"heading":"What it does","content":"An audio professional will use Orpheus Engine Workspace to edit, analyze, and export data, while having an AI companion for research and the audio analysis from the visual process. With the recording capabilities, this allows a user to record from Bluetooth devices into the DAW, where the agent rag can analyze the audio data, identify the audio recordings for specific needs or suggestions of the Audio engineer. If the audio is a voice or speech, the AI will be able to conduct actions such as audio transcriptions, slice the sections of the audio when the user searches. To later be exported with metadata, or the transcript to be used for further development."},{"heading":"How we built it","content":"We built this App using available TypeScript components and the demos of the HP Studio to help design the MLFlow and the Jupyter notebooks. A lot of the design and implementation was done using AI. The approach began with the backend, but I had to adjust and redesign the frontend to work within a browser, where the components were only displayed in Electron. This itself posed many challenges."},{"heading":"Challenges we ran into","content":"The main challenge was fixing all the dependency issues with the components. So I designed a new test suite and refactored all the code to work seamlessly with the browser. This was one of the many other challenges. I began using a model from HuggingFace, later utilized the model within the HP AI Studio."},{"heading":"Accomplishments that we're proud of","content":"Got it to work! Due to this project's complexity, there have been drawbacks; however, to get it working, many of the features I wanted to incorporate are only mockups. The Plugin Management systems and some of the uses of the nRF devices are not fully tested."},{"heading":"What we learned","content":"I learned how to prioritize the usage of the App rather than how it is built. With larger projects that have a UI with many components, operating front-end testing suites and MLFlow helps keep track of the development processes."},{"heading":"What's next for Orpheus Engine","content":"I want to see the Orpheus Engine functioning as a dDapp. I would love for this DAW to be available on AI platforms, repositories, as a decentralized source for exporting audio projects to Web3. With the framework of the plugin store system, OEW can allow other developers to construct apps for processes such as Minting, saving data to a Vector database, and IP plugins ( using Story for securing the rights of the audio) to a plugin. OEW multiple input plugin (VST, Python, Typscript, dApp) design allows an audio pro to use their preferred DAW, and this tool will be the bridge to exporting audio files to Web3."},{"heading":"Built With","content":"graphql python typescript"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-18T16:53:35.287163Z"}}