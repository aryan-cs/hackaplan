{"version":"v1","hackathon_url":"https://treehacks-2024.devpost.com","generated_at":"2026-02-17T18:19:37.154901Z","result":{"hackathon":{"name":"TreeHacks 2024","url":"https://treehacks-2024.devpost.com","gallery_url":"https://treehacks-2024.devpost.com/project-gallery","scanned_pages":14,"scanned_projects":330,"winner_count":60},"winners":[{"project_title":"Good Samaritan - Vision Pro Assisted First-Aid Care","project_url":"https://devpost.com/software/good-samaritan-29zp5x","tagline":"Allowing bystanders to provide informed first-aid care to those experiencing medical emergencies with the assistance of mixed reality (looking years in advance when the tech is commonplace).","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/774/496/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Most Impactful Hack (4x Apple Watch Series 9)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"TerraAPI: Health Prize (4x Meta x Ray-Ban)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"BGB Group: Informed Physicians Prize (4x $500 Gift Card + Internship Interview)"}],"team_members":[],"built_with":[{"name":"applevisionpro","url":null},{"name":"mediapipe","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"realitydeepcomposer","url":null},{"name":"swiftui","url":null},{"name":"terraapi","url":null}],"external_links":[{"label":"github.com","url":"http://github.com/rayhotate/tree"}],"description_sections":[{"heading":"The Problem:","content":"Inspired by our teammates' lack of knowledge in first aid administration, we address a gap in emergency medical response time for remote and underserviced locations. Currently, it takes 7 minutes for emergency medical services (EMS) to reach the scene, with the time in rural areas doubling to 14 minutes. For critical medical emergencies, such as cardiac arrest, every minute the chances of survival decrease by 10% making immediate intervention necessary. This is where the most crucial element to save lives comes in: the bystander. In fact, the intervention of bystanders can triple the chances of survival in instances of cardiac arrest.\n\nHere is where another issue arises. Only 6 out of 10 people feel comfortable even attempting to perform CPR for someone in cardiac arrest. This stems from only 3.5% of people in the United States being trained in first-aid procedures such as cardiac arrest, and an irrational fear the bystander will inflict further damage to the victim."},{"heading":"The Solution:","content":"The report below follows the assumption that in the next 5-10 years VR technology will be readily adopted into everyday life, by everyone. Strides in hardware will make the technology as sleek as a pair of glasses, and as common as a smartphone.\n\nOur novel VR and AI-based pipeline equips bystanders with the knowledge and visual guidance to perform life-saving procedures with confidence and precision. At Treehacks, we focused on creating guidance for a situation where a bystander witnesses someone seizing, which eventually escalates into cardiac arrest. Using Good Samaritan, the device passively detects a medical emergency and guides the bystander on how to care for the victim until emergency medical services arrive on the scene in order to give the victim the best chances of survival.\n\nWe provide in-depth, real-time visuals that detail:\n\nReal-time data streamed from a Fitbit of the victim to monitor vitals (TerraAPI) Facilitate proper “log-roll technique” to move an injured person The correct safe orientation for someone having a seizure so they don’t choke on their saliva The proper supporting of the head and neck to prevent paralysis Instruction for proper CPR with visuals on the victim’s body for where to place hands, what pace to perform compressions at, and when to give mouth-to-mouth breaths A live progress bar displaying the time until emergency medical services arrive on the scene\n\nThis immersive experience ensures that, without immediate professional help, the affected individuals receive the best possible care, increasing their chances of survival and recovery."},{"heading":"What we are proud of, and how we built it:","content":"Wow, we created a novel dynamic application that has the potential to save millions of lives in the future.\n\nWe programmed computer vision-based models using MediaPipe and OpenCV to perform pose detection and joint detection. We then performed linear transformations in a 3D vector space to identify and anchor the points in the Apple Vision Pro’s virtual space using real-time video from our computer vision script built onto VisionOS."},{"heading":"Business Model:","content":"$14.6 Billion Market Cap -Work with public health departments to include the app in rural or underserved areas. The government could fund the deployment of the app as part of their mandate to improve public health infrastructure. -Work with emergency services to integrate the app into their response protocols, providing first responders with additional information or assisting in situations where they can't reach the scene immediately. -Apply for government grants aimed at technological innovations that improve public safety and health. -Educational Programs: Integrate the app into educational programs, such as school safety initiatives or community health workshops, funded or supported by local or national government agencies."},{"heading":"The team (4 people, 4 schools represented):","content":"Ray- Stanford, specializes in AI and product structure Shutaro- Columbia, specializes in immersive technologies Shloak- UCLA, specializes in vision Yash- Georgia Tech, specializes in medicine"},{"heading":"Next Steps:","content":"This is one tangible application for Good Samaritan, though in the future we plan to have similar guiding procedures for: -Anaphylactic shock -Lacerations where bleeding must be managed -Stroke -AED’s -Other emergency medical complications that benefit from the interference of a bystander"},{"heading":"Challenges we ran into:","content":"Our expertise lay in Unity; however, Apple Vision Pro was only accessible with Unity Pro ($2,000), so we pivoted to and learned Swift. We ran into errors while translating CV’s 2D data into a 3D environment; we made use of anchoring techniques to pin the z-dimension while using the xy-dimension from CV."},{"heading":"What we learned:","content":"A ton! Applying CV’s 2D data into a 3D space, programming VR and AR on the Apple Vision Pro, and using Swift UI to develop VisionOS applications! Our pipeline is built ground up and novel—we figured it out along the way with little documentation to lean on!"},{"heading":"Built With","content":"applevisionpro mediapipe opencv python realitydeepcomposer swiftui terraapi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Baymax, Your Personal Healthcare Companion","project_url":"https://devpost.com/software/baymax-your-personal-healthcare-companion","tagline":"Our AI-powered robot arm is a personalized solution to restore physically impaired and elderly individuals’ ability to interact with their environment.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/775/885/datas/medium.jpeg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Grand Prize ($10k Cash)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Reazon Holdings: Best Senior-Focused Hack (4x Round Trip Tickets to Tokyo)"}],"team_members":[],"built_with":[{"name":"dynamixel-sdk","url":null},{"name":"google-mediapipe","url":null},{"name":"matplotlib","url":null},{"name":"mediapipe","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"openai","url":null},{"name":"openai-gpt4-api","url":null},{"name":"openai-whisper","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"pyaudio","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"vlc","url":null},{"name":"vlc-sdks:-dynamixel-sdk-apis:-openai-gpt-4-model-calls","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Jaybear411/VisArm"}],"description_sections":[{"heading":"Inspiration","content":"40 million people in the world are blind, including 20% of all people aged 85 or older. Half a million people suffer paralyzing spinal cord injuries every year. 8.5 million people are affected by Parkinson’s disease, with the vast majority of these being senior citizens. The pervasive difficulty for these individuals to interact with objects in their environment, including identifying or physically taking the medications vital to their health, is unacceptable given the capabilities of today’s technology.\n\nFirst, we asked ourselves the question, what if there was a vision-powered robotic appliance that could serve as a helping hand to the physically impaired? Then we began brainstorming: Could a language AI model make the interface between these individual’s desired actions and their robot helper’s operations even more seamless? We ended up creating Baymax—a robot arm that understands everyday speech to generate its own instructions for meeting exactly what its loved one wants. Much more than its brilliant design, Baymax is intelligent, accurate, and eternally diligent.\n\nWe know that if Baymax was implemented first in high-priority nursing homes, then later in household bedsides and on wheelchairs, it would create a lasting improvement in the quality of life for millions. Baymax currently helps its patients take their medicine, but it is easily extensible to do much more—assisting these same groups of people with tasks like eating, dressing, or doing their household chores."},{"heading":"What it does","content":"Baymax listens to a user’s requests on which medicine to pick up, then picks up the appropriate pill and feeds it to the user. Note that this could be generalized to any object, ranging from food, to clothes, to common household trinkets, to more. Baymax responds accurately to conversational, even meandering, natural language requests for which medicine to take—making it perfect for older members of society who may not want to memorize specific commands. It interprets these requests to generate its own pseudocode, later translated to robot arm instructions, for following the tasks outlined by its loved one. Subsequently, Baymax delivers the medicine to the user by employing a powerful computer vision model to identify and locate a user’s mouth and make real-time adjustments."},{"heading":"How we built it","content":"The robot arm by Reazon Labs, a 3D-printed arm with 8 servos as pivot points, is the heart of our project. We wrote custom inverse kinematics software from scratch to control these 8 degrees of freedom and navigate the end-effector to a point in three dimensional space, along with building our own animation methods for the arm to follow a given path. Our animation methods interpolate the arm’s movements through keyframes, or defined positions, similar to how film editors dictate animations. This allowed us to facilitate smooth, yet precise, motion which is safe for the end user.\n\nWe built a pipeline to take in speech input from the user and process their request. We wanted users to speak with the robot in natural language, so we used OpenAI’s Whisper system to convert the user commands to text, then used OpenAI’s GPT-4 API to figure out which medicine(s) they were requesting assistance with.\n\nWe focused on computer vision to recognize the user’s face and mouth. We used OpenCV to get the webcam live stream and used 3 different Convolutional Neural Networks for facial detection, masking, and feature recognition. We extracted coordinates from the model output to extrapolate facial landmarks and identify the location of the center of the mouth, simultaneously detecting if the user’s mouth is open or closed.\n\nWhen we put everything together, our result was a functional system where a user can request medicines or pills, and the arm will pick up the appropriate medicines one by one, feeding them to the user while making real time adjustments as it approaches the user’s mouth."},{"heading":"Challenges we ran into","content":"We quickly learned that working with hardware introduced a lot of room for complications. The robot arm we used was a prototype, entirely 3D-printed yet equipped with high-torque motors, and parts were subject to wear and tear very quickly, which sacrificed the accuracy of its movements. To solve this, we implemented torque and current limiting software and wrote Python code to smoothen movements and preserve the integrity of instruction.\n\nControlling the arm was another challenge because it has 8 motors that need to be manipulated finely enough in tandem to reach a specific point in 3D space. We had to not only learn how to work with the robot arm SDK and libraries but also comprehend the math and intuition behind its movement. We did this by utilizing forward kinematics and restricted the servo motors’ degrees of freedom to simplify the math. Realizing it would be tricky to write all the movement code from scratch, we created an animation library for the arm in which we captured certain arm positions as keyframes and then interpolated between them to create fluid motion.\n\nAnother critical issue was the high latency between the video stream and robot arm’s movement, and we spent much time optimizing our computer vision pipeline to create a near instantaneous experience for our users."},{"heading":"Accomplishments that we're proud of","content":"As first-time Hackathon participants, we are incredibly proud of the incredible progress we were able to make in a very short amount of time, proving to ourselves that with hard work, passion, and a clear vision, anything is possible. Our team did a fantastic job embracing the challenge of using technology unfamiliar to us, and stepped out of our comfort zones to bring our idea to life. Whether it was building the computer vision model, or learning how to interface the robot arm’s movements with voice controls, we ended up building a robust prototype which far surpassed our initial expectations. One of our greatest successes was coordinating our work so that each function could be pieced together and emerge as a functional robot. Let’s not overlook the success of not eating our hi-chews we were using for testing!"},{"heading":"What we learned","content":"We developed our skills in frameworks we were initially unfamiliar with such as how to apply Machine Learning algorithms in a real-time context. We also learned how to successfully interface software with hardware - crafting complex functions which we could see work in 3-dimensional space. Through developing this project, we also realized just how much social impact a robot arm can have for disabled or elderly populations."},{"heading":"What's next for Baymax","content":"Envision a world where Baymax, a vigilant companion, eases medication management for those with mobility challenges. First, Baymax can be implemented in nursing homes, then can become a part of households and mobility aids. Baymax is a helping hand, restoring independence to a large disadvantaged group.\n\nThis innovation marks an improvement in increasing quality of life​ for millions of older people, and is truly a human-centric solution in robotic form."},{"heading":"Built With","content":"dynamixel-sdk google-mediapipe matplotlib mediapipe numpy openai openai-gpt4-api openai-whisper opencv pyaudio python vlc vlc-sdks:-dynamixel-sdk-apis:-openai-gpt-4-model-calls"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"dockerc","project_url":"https://devpost.com/software/dockerc","tagline":"Software distribution solved.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Most Technically Complex Hack (4x Mac Mini)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Bun: Best Use of Zig (4x iPad Pro + Interview with Bun Founder)"}],"team_members":[],"built_with":[{"name":"zig","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/NilsIrl/dockerc/releases/tag/treehacksv0"},{"label":"github.com","url":"https://github.com/NilsIrl/dockerc"}],"description_sections":[{"heading":"Inspiration","content":"Distributing software is difficult. A lot of projects end up getting their users to run docker run to make things simpler. However relying on docker makes things complicated. Users need to have docker installed, users need to have the correct docker version. Installing docker is often a pain, you need to setup permissions correctly (docker needs to run as root).\n\nThere are other ways of distributing software but they require a lot more investment. Writing Dockerfile s is just so easy!\n\nDockerc solves this problem:\n\nBuilders can continue writing Dockerfile s Users can just download a single executable and run it, no need to have 10 thousand setup steps"},{"heading":"What it does","content":"Dockerc takes existing docker images and a single executable file from them which can be distributed to your users. For 0 setup, you get the best possible form of software distribution: a single file to download."},{"heading":"How we built it","content":"The compiler and runtime are written in zig.\n\nIn order to have incredibly fast startup times no copying is done at runtime. The image is stored as a squashfs file and so the startup is near instant. (a squashfs file is a file that represents a file system and that can be mounted in the directory structure)"},{"heading":"Challenges we ran into","content":"Avoiding the copying of data was a difficult task. Most docker files are hundreds of megabytes large so we cannot afford to copy! The most difficult part was finding a way to mount the squashfs because it required identifying the offset of the squashfs file within a file that the squashfs-mounter could use.\n\nThings I tried:\n\nUsing the /proc filesystem to identify how the executable was mapped into memory and to then deduce the location in the physical file based on that. (/proc/self/maps) Using /proc/self/mem so that I can just directly use the pointer to the embedded file. Unfortunately, I didn't realize that processes couldn't access each other's memory until I implemented this...\n\nOverall the biggest challenge was using Zig. I had never used the language before and because the language has so little documentation it made things very challenging."},{"heading":"What's next for dockerc","content":"Make a paid web platform on which people can generate executables with the click of a button! Add MacOS support using QEMU Add support for ARM64 (currently a lot of the binaries are just hardcoded)"},{"heading":"What we learned","content":"Zig How containers work internally\n\nExample projects that make use of docker for distribution:\n\nhttps://github.com/oven-sh/bun https://github.com/shepherdjerred/macos-cross-compiler"},{"heading":"Built With","content":"zig"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"Show and Tell - Capturing Emotion in Sign Language","project_url":"https://devpost.com/software/show-and-tell-capturing-emotion-in-sign-language","tagline":"Enabling emotion and expression for the hard of hearing. Through innovative wearable technology and LLM integration, Show and Tell tackles the issue of accessibility for the hard of hearing.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/772/210/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Most Creative Hack (4x Nintendo Switches)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Human Capital: Memorial Partnership Journey (Box @ Warriors Game)"}],"team_members":[],"built_with":[{"name":"esp32","url":null},{"name":"humeai","url":null},{"name":"imu","url":null},{"name":"openai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/raviriley/aidactyl"}],"description_sections":[{"heading":"Inspiration","content":"Our team focuses extensively on opportunities to ignite change through innovation. In this pursuit, we began to investigate the impact of sign-language glove technology on the hard of hearing community. In this, we learned that despite previous efforts to enhance accessibility, feedback from deaf advocates highlighted a critical gap: earlier technologies often facilitated communication for hearing individuals with the deaf, rather than empowering the deaf to communicate on their terms. After discussing this problem with a friend of ours who faces disabilities related to her hearing, we realized that this problem impacts many people's daily lives, significantly affecting their ability to engage with those around them. By focusing on human centered design and integrating feedback presented in numerous journals, we solve these problems by developing an accessible, easy to use interface that enables the hard of hearing and mute to converse seamlessly. Through integration of a wearable component and a sophisticated LLM, we completely change the landscape of interpersonal communications for the hard of hearing."},{"heading":"What it does","content":"Our solution consists of two components, a wearable glove and a mobile video call interface. The wearable glove is meant to be utilized by a deaf or hard of hearing individual when conversing with another person. This glove, fitted with numerous flex sensors and an Inertial Measurement Unit (IMU), can discern what gloss (term for a word in ASL) the wearer is signing at any moment in time. From here, the data moves to the second component of the solution - the mobile video call interface. Here, the user's signs are converted into both text and speech during a call. The text is displayed on the screen while the speech is stated, ensuring to include emotional cues as picked up by the integrated computer vision model. This effectively helps users communicate with others, especially loved ones, in a manner that accurately represents their intent and emotions. This experience is one that is currently not offered anywhere else on the market. In tandem, both of these technologies enable us to understand body language, emotion, and signs from a user, and also help vocalize the feelings of a person who is hard of hearing."},{"heading":"How we built it","content":"Two vastly different components call for drastically different approaches. However, we needed to ensure that these two approaches still stayed true to the same intent. We first began by identifying our design strategy, based in our problem statement and objective. From here, we moved forward with set goals and milestones.\n\nOn the hardware side of things, we spent an extensive amount of time in the on-site lab fabricating our prototype. In order to ensure the validity of our design, we researched circuit diagrams and characteristics, ultimately building our own. We performed a variety of tests on this prototype, including practical use testing by taking it around campus while interacting with others. The glove withstood numerous handshakes and even a bit of rain!\n\nOn the software side, we also had two problems to face - interfacing with the glove, and creating the mobile application. To interface with the glove, we began with the Arduino IDE for testing. After we ensured that our design was functional and gained test data, we moved to a python implementation that sends sensed words up to an API, which can later be accessed by the mobile application.\n\nMoving to the mobile application, we utilized SwiftUI for our design. From there, we used the StreamAPI to build a FaceTime style infrastructure. We prototyped and integrated between Figma designs and our prototype to best understand where we could increase capabilities and improve the user experience."},{"heading":"Challenges we ran into","content":"This project was ambitious, and as such, was also chock full of complications. Initially, we faced extensive challenges on the hardware side. Due to the nature of the design, we have many components that are trying to draw power or ground from the same source. This provided increased complexity in our manufacturing process, as we had to come up with an innovative solution to a sleek design that maintained functionality. Even after we found our first solution, our prototype was inconsistent due to manufacturing flaws. On the last day, 2 hours before the submission deadline, we completely disassembled and rebuilt our prototype using a new methodology. This proved to be successful, minimizing the issues seen previously and resulting in an amazing product.\n\nOn the software side, we also pursued ambitious desires that didn't distinctly align with our team's expertise. Due to this, we faced great difficulty when troubleshooting the numerous errors we faced in initial implementation. This set us back quite extensively, but we were able to successfully recover."},{"heading":"Accomplishments that we're proud of","content":"We are proud of the magnitude of success we were able to show in the short frame of this hackathon. We came in knowing that we had ambitious and lofty goals, but were unsure if we would truly be able to achieve them. Thankfully, we complete this hackathon with a functional, viable MVP that clearly represents our goals and desires for this project."},{"heading":"What we learned","content":"Because of the cross discipline nature of this project. All of our team members got the opportunity to explore new spaces. Through collaboration, we all learned about these fields and technologies from and with each other and how we can integrate them into our systems in the future. We also learned about best practices for manufacturing in general. Additionally, we were able to become more comfortable with SwiftUI and creating our own APIs for our video calling component. These valuable skills shaped our experiences at TreeHacks and will stick with us for many years to come."},{"heading":"What's next for Show and Tell - Capturing Emotion in Sign Language","content":"We hope to continue to pursue this idea and bring independence to the hard of hearing population worldwide. In a market that has underserved the deaf population, we see Show and Tell as the optimal solution for accessibility. In the future, we want to flesh out the hardware prototype further by investing in custom PCB's, streamlining the production process and making it much more professional. Additionally, we want to build out functionality within the video calling app, adding in as many helpful features as possible."},{"heading":"Built With","content":"esp32 humeai imu openai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TherapEase.ai","project_url":"https://devpost.com/software/therapease-ai","tagline":"Expanding access to physical therapy and critical healthcare services through AI-enabled telehealth.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/772/268/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Health Grand Prize (4x Oura Ring)"}],"team_members":[],"built_with":[{"name":"bootstrap","url":"https://devpost.com/software/built-with/bootstrap"},{"name":"css3","url":"https://devpost.com/software/built-with/css3"},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"pose-estimation","url":null},{"name":"predictionguard","url":null},{"name":"remote-photophlethysmography","url":null},{"name":"webrtc","url":"https://devpost.com/software/built-with/webrtc"}],"external_links":[{"label":"github.com","url":"https://github.com/anshgupta1234/TherapEase/"}],"description_sections":[{"heading":"Inspiration","content":"One of our team members, Aditya, has been in physical therapy (PT) for the last year after a wrist injury on the tennis court. He describes his experience with PT as expensive and inconvenient. Every session meant a long drive across town, followed by an hour of therapy and then the journey back home. On days he was sick or traveling, he would have to miss his PT sessions.\n\nAnother team member, Adarsh, saw his mom rushed to the hospital after suffering from a third degree heart block. In the aftermath of her surgery, in which she was fitted with a pacemaker, he noticed how her vital signs monitors, which were supposed to aid in her recovery, inhibited her movement and impacted her mental health.\n\nThese insights together provided us with the inspiration to create TherapEase.ai. TherapEase.ai uses AI-enabled telehealth to bring affordable and effective PT and contactless vital signs monitoring services to consumers, especially among the elderly and disabled communities . With virtual sessions, individuals can receive effective medical care from home with the power of pose correction technology and built-in heart rate, respiratory, and Sp02 monitoring. This evolution of telehealth flips the traditional narrative of physical development—the trainee can be in more control of their body positioning, granting them greater levels of autonomy."},{"heading":"What it does","content":"The application consists of the following features: Pose Detection and Similarity Tracking Contactless Vital Signs Monitoring Live Video Feed with Trainer Live Assistant Trainer Chatbot\n\nOnce a PT Trainer or Medical Assistant creates a specific training room, the user is free to join said room. Immediately, the user’s body positioning will be highlighted and compared to that of the trainer. This way the user can directly mimic the actions of the trainer and use visual stimuli to better correct their position. Once the trainer and the trainee are aligned, the body position highlights will turn blue, indicating the correct orientation has been achieved.\n\nThe application also includes a live assistant trainer chatbot to provide useful tips for the user, especially when the user would like to exercise without the presence of the trainer.\n\nFinally, on the side of the video call, the user can monitor their major vital signs: heart rate, respiratory rate, and blood oxygen levels without the need for any physical sensors or wearable devices. All three are estimated using remote Photoplethysmography: a technique in which fluctuations in camera color levels are used to predict physiological markers."},{"heading":"How we built it","content":"We began first with heart rate detection. The remote Photoplethysmography (rPPG) technique at a high level works by analyzing the amount of green light that gets absorbed by the face of the trainee. This serves as a useful proxy as when the heart is expanded, there is less red blood in the face, which means there is less green light absorption. The opposite is true when the heart is contracted. By magnifying these fluctuations using Eulerian Video Magnification, we can then isolate the heart rate by applying a Fast Fourier Transform on the green signal.\n\nOnce the heart rate detection software was developed, we integrated in PoseNet’s position estimation algorithm, which draws 17 key points on the trainee in the video feed. This lead to the development of two-way video communication using webRTC, which simulates the interaction between the trainer and the trainee. With the trainer’s and the trainee’s poses both being estimated, we built the weighted distance similarity comparison function of our application, which shows clearly when the user matched the position of the trainer.\n\nAt this stage, we then incorporated the final details of the application: the LLM assistant trainer and the additional vital signs detection algorithms. We integrated Intel’s Prediction Guard , into our chat bot to increase speed and robustness of the LLM. For respiratory rate and blood oxygen levels, we integrated algorithms that built off of rPPG technology to determine these two metrics."},{"heading":"Challenges we ran into (and solved!)","content":"We are particularly proud of being able to implement the two-way video communication that underlies the interaction between a patient and specialist on TherapEase.ai. There were many challenges associated with establishing this communication. We spent many hours building an understanding of webRTC, web sockets, and HTTP protocol. Our biggest ally in this process was the developer tools of Chrome, which we could use to analyze network traffic and ensure the right information is being sent.\n\nWe are also proud of the cosine similarity algorithm which we use to compare the body pose of a specialist/trainer with that of a patient. A big challenge associated with this was finding a way to prioritize certain points (from posnet) over others (e.g. an elbow joint should be given more importance than an eye point in determining how off two poses are from each other). After hours of mathematical and programming iteration, we devised an algorithm that was able to weight certain joints more than others leading to much more accurate results when comparing poses on the two way video stream. Another challenge was finding a way to efficiently compute and compare two pose vectors in real time (since we are dealing with a live video stream). Rather than having a data store, for this hackathon we compute our cosine similarity in the browser."},{"heading":"What's next for TherapEase.ai","content":"We all are very excited about the development of this application. In terms of future technical developments, we believe that the following next steps would take our application to the next level.\n\nPeak Enhancement for Respiratory Rate and SpO2 Blood Pressure Contactless Detection Multi-channel video Calling Increasing Security"},{"heading":"Built With","content":"bootstrap css3 html5 node.js opencv pose-estimation predictionguard remote-photophlethysmography webrtc"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Nab 3D","project_url":"https://devpost.com/software/nab-3d","tagline":"Nab any object straight out of the real world and into your browser. Video to 3D web component -- all in one line of code.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/767/855/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Beginner Hack (4x Razer Keyboard + Mouse)"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"bun","url":null},{"name":"cloudflare","url":"https://devpost.com/software/built-with/cloudflare"},{"name":"ffmpeg","url":"https://devpost.com/software/built-with/ffmpeg"},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"},{"name":"vite","url":null},{"name":"webassembly","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/soapantelope/nab"}],"description_sections":[{"heading":"Inspiration","content":"Nab streamlines the process of bringing real-world objects into the digital sphere by utilizing video transformations, photogrammetry, providing a custom API to host and display them.\n\nSophia is well acquainted with photogrammetry, having used it to create models of locations around Seattle. But the process is nothing short of terrible :\n\nTake 30 - 250 images of your object. Manually find and run a good photogrammetry model (or spend 10-20 hours 3D modeling your object on Blender) Process and export .glb Edit model in Blender and re-export Import .glb to project and set up a scene with Three.js (which usually takes at least 20+ lines of complicated, buggy 3js code)\n\nAnd, you have to repeat the entire process all over again for each model you want to add!"},{"heading":"Functionality","content":"Nab 3D, however, introduces a web component that does all of this in one line of code— in a matter of seconds. It revolutionizes this intricate process: It's as easy as... 1. Input a video of the object. 2. Paste our web component URL in your website. With Nab 3 D\n\nThe user does not have to touch a single line of Three.js or deal with any aspect of the photogrammetry/modeling process!"},{"heading":"How did we build Nab 3D?","content":"Filled with excitement for the photogrammetry process, we dove headfirst into development. The idea of creating 3D models from videos was incredibly captivating. Yet, as we immersed ourselves in our project, we found ourselves unsure of exactly how to realize the idea. So, midway through the hackathon, we decided to pause and reassess our value proposition. With the growing importance of 3D modeling in e-commerce, our mission became clear: make 3D modeling accessible. We want Nab 3D to truly stand out as a user-friendly solution in the world of 3D modeling. We used Bun, React, Vite, and Three.js for frontend, Cloudflare for the database, Apple Object Capture API/Swift for backend, and FFmpeg for video parsing."},{"heading":"Challenges faced","content":"We spent the first half of the hackathon building our app around Autodesk’s Reality Capture API and fighting authorization bug after authorization bug, only to realize that you can't use their API until you contact their associates — weekdays only! We struggled to find another photogrammetry API and ended up using Apple’s Object Capture API with Swift— a language none of us had developed in before. Although each and every part of our pipeline had its own set of roadblocks, almost to the point of pivoting, we think sticking with the idea paid off."},{"heading":"What We Learned","content":"Besides learning classic soft skills like ideation and resilience, we also had an opportunity to learn completely new skills, frameworks, and even languages. In fact, two of us were beginners in any kind of web dev/JavaScript, one of us had never touched APIs and databases before, and all of us had never worked in Swift and Bun before (even though a large part of our app ended up being written in Swift!). Everyone on the team is coming out of this hackathon with not only an entirely new stack but also new friends and hilarious sleep-deprived memories.\n\nOur team members have spent countless hours ripping our hair out trying to bring 3D scans of objects into the web. For something this relevant to the future of e-commerce, advertising, and media (especially considering the rise of AR), it was surprising how slow and laborious the process was. We realized that the process did not need to be this painful – in fact, it could be simplified down to one line of code, served by an API with the help of photogrammetry."},{"heading":"What's next?","content":"3D e-commerce, advertising, & media are coming – and Nab's goal is to make that world accessible to everyone. Furthermore, photogrammetry lends itself to the demand for an easy-to-use scan to 3D asset pipeline, which will grow as AR/VR becomes a dominant technology and focus for advertising. The global 3D e-commerce market size was USD 11.11 Billion in 2023, with a projected 40.0% compound annual growth rate in 2024-2030. We plan to further develop this idea, with anything as big as a foray into AR/VR or as specific as potential implementations in food marketplaces like Doordash, or small businesses on Etsy.\n\nThanks for reading!"},{"heading":"Built With","content":"api bun cloudflare ffmpeg git javascript react swift three.js vite webassembly"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Recollect: Leave No Book Behind","project_url":"https://devpost.com/software/recollect-ytn0bk","tagline":"Our robot scans, digitizes, and analyzes books with zero intervention.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/770/663/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Hardware Hack (4x Prusa 3D Printers)"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"autodesk-fusion-360","url":"https://devpost.com/software/built-with/autodesk-fusion-360"},{"name":"bun","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"google-cloud-vision-api","url":null},{"name":"gpt-4","url":null},{"name":"next.js","url":null},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"supabase","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null}],"external_links":[{"label":"recollect-knowledge.vercel.app","url":"https://recollect-knowledge.vercel.app"},{"label":"github.com","url":"https://github.com/Scott-Hickmann/Book-Scanner"}],"description_sections":[{"heading":"Why build this?","content":"Only 12% of all published books have been digitized. Historical records, ancient manuscripts, rare collections. Family photo albums, old journal entries, science field notes. Without digitization, centuries of accumulated wisdom, cultural treasures, personal narratives, and family histories threaten to be forever lost to time.\n\nLarge-scale digitization currently requires highly specialized equipment and physical personnel to manually flip, scan, and process each page. Oftentimes, this is simply not practical, resulting in many books remaining in undigitized form, which necessitates careful, expensive, and unsustainable transportation across various locations for analysis."},{"heading":"How we built it","content":"Hardware:\n\nRecollect was made with easy-to-fabricate material including 3D printed plastic parts, laser-cut acrylic and wood, and cheap, and off-the-shelf electronics. A book rests at a 160-degree angle, optimal to hold the book naturally open while minimizing distortions. The page presser drops onto the book, flattening it to further minimize distortions. After the photo is taken, the page presser is raised, then a two-degree-of-freedom robotic arm flips the page. A lightly adhesive pad attaches to the page, and then one of the joints rotates the page. The second joint separates the page from the adhesive pad, and the arm returns to rest. The scanner was designed to be adaptable to a wide range of books, up to 400mm tall and 250 mm page width, with easy adjustments to the arm joints and range of motion to accommodate for a variety of books.\n\nSoftware:\n\nImage processing:\n\nOn the backend, we leverage OpenCV to identify page corners, rescale images, and sharpen colors to produce clear images. These images are processed with pre-trained Google Cloud Vision API models to enable optical character recognition of handwriting and unstructured text. The data are saved into a Supabase database to allow users to access their digital library from anywhere.\n\nWebpage and cloud storage:\n\nThe front end is a Vercel-deployed web app built with Bun, Typescript, Chakra, Next.js, and React.js."},{"heading":"Challenges we ran into","content":"We ran into challenges involving getting the perfect angle for the robotic arm to properly stick to the page. To fix this, we had to modify the pivot point of the arm’s base to be in line with the book’s spine and add a calibration step to make it perfectly set up for the book to be scanned. Our first version also used servo motors with linkages to raise the acrylic page presser up and down, but we realized these motors did not have enough torque. As a result, we replaced them with DC motors and a basic string and pulley system which turned out to work surprisingly well."},{"heading":"Accomplishments that we're proud of","content":"This project was a perfect blend of each team member’s unique skill sets: Lawton, a mechanical engineering major, Scott, an electrical and systems engineer, Kaien, an AI developer, and Jason, a full-stack developer. Being able to combine our skills in this project was amazing, and we were truly impressed by how much we were able to accomplish in just 24 hours. Seeing this idea turn into a physical reality was insane, and we were able to go beyond what we initially planned on building (such as adding summarization, quotation, and word cloud features as post-processing steps on your diary scans). We’re happy to say that we’ve already digitized over 100 pages of our diaries through testing."},{"heading":"What we learned","content":"We learned how to effectively divide up the project into several tasks and assign it based on area of expertise. We also learned to parallelize our work—while parts were being 3D-printed, we would focus on software, design, and electronics."},{"heading":"What's next for Recollect","content":"We plan to improve the reliability of our system to work with all types of diaries, books, and notebooks, no matter how stiff or large the pages are. We also want to focus on recreating PDFs from these books in a fully digital format (i.e. not just the images arranged in a PDF document but actual text boxes following the formatting of the original document). We also plan to release all of the specifications and software publicly so that anyone can build their own Recollect scanner at home to scan their own diaries and family books. We will design parts kits to make this process even easier. We will also explore collaborating with Stanford libraries and our close communities (friends and family). Thanks to Recollect, we hope no book is left behind."},{"heading":"Built With","content":"arduino autodesk-fusion-360 bun google-cloud google-cloud-vision-api gpt-4 next.js openai react supabase typescript vercel"},{"heading":"Try it out","content":"recollect-knowledge.vercel.app github.com"}]},{"project_title":"SkySplat","project_url":"https://devpost.com/software/skysplat","tagline":"drones + gaussian splatting = automated, AI-powered 3D modeling of any environment for disaster recovery, emergency response, and infrastructure inspections","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/776/173/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Sustainability Grand Prize (4x Patagonia Down Jackets)"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"css3","url":"https://devpost.com/software/built-with/css3"},{"name":"cuda","url":"https://devpost.com/software/built-with/cuda"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gaussian-splatting","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"gpu","url":null},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"olympe","url":null},{"name":"parrot","url":"https://devpost.com/software/built-with/parrot"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"sphinx","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"ubuntu","url":"https://devpost.com/software/built-with/ubuntu"},{"name":"vite","url":null},{"name":"webgl","url":"https://devpost.com/software/built-with/webgl"}],"external_links":[{"label":"github.com","url":"https://github.com/Hoponga/splatting/tree/main"}],"description_sections":[{"heading":"Inspiration","content":"In 2010, Haiti faced a magnitude 7.0 earthquake, which remains to this day one of the most devastating natural disasters of our century. An estimated 220,000 individuals lost their lives, with an additional 1.5 million losing their homes. At the center of the tragedy was poor building construction, practices, and materials. More than 208,000 buildings were damaged, half of which were fully destroyed, severely crippling the nation’s infrastructure. Two questions stuck out to us: How could 3D models have been used to gauge the degree of damage, stability, and safety of infrastructure? If a natural disaster occurred today, how could an improved understanding of building layouts help first responders provide aid more quickly?\n\nOne of our main goals was to utilize state of the art technology with a potential for powerful impact. Drawing from some of our members’ past work with Neural Radiance Fields (NeRFs) and other generative CV techniques, we wanted to help solve these questions by fusing drones and AI."},{"heading":"What it does","content":"Introducing SkySplat, an AI-powered drone software to perform automated 3D captures of any building or infrastructure asset. Here are some sample applications: Helping first responders deeply understand the layouts of crippled buildings before performing emergency response. Empowering governments to maintain detailed 3D models of infrastructure without the need of specialized mapping equipment. Enabling technicians to perform inspections on structures that would be impossible to reach without extensive equipment and risk of personal injury. Allowing property owners to provide 3D models of properties for customers to explore at their own convenience.\n\nCurrently, we are focused on the Parrot drone suite. Our software seamlessly connects to your drone based on your IP. In our web app, you can decide when to begin and complete your data recording session. Immediately upon finishing the data collection, a video is automatically sent to our Google Cloud backend for preprocessing. The video is split into a set frames, optimizing for both efficiency and accuracy, which is then passed into our COLMAP pipeline. Once COLMAP completes, our software automatically trains the Gaussian Splatting models, uploads the results into a Google Cloud bucket, which is then forwarded to our frontend. Finally, a user can effortlessly view their model in high resolution 3D from our webapp."},{"heading":"How we built it","content":"For our frontend and splatting visualization, we used Vite, Typescript, and three.js (a wrapper on top of WebGL). Additionally, we used CSS for styling.\n\nAs some context, the Gaussian Splatting models cannot directly take in video streams. We have to preprocess them using COLMAP, a software that intelligently figures out the relative position of each frame in 3D space (known as a structure from motion point cloud computation). After optimizing the COLMAP parameters, we moved onto the training of our Gaussian Splatting models.\n\nDue to the intensive nature of Gaussian Splatting models, we set up virtual machines on Google Cloud with CUDA-accelerated runtimes. Due to our lack of budget, we were highly limited in the number of and models of GPUs we could use. The custom models themselves are based in PyTorch, derived from the paper 3D Gaussian Splatting for Real-Time Radiance Field Rendering. Each scene has its own model, ensuring a highly specialized understanding of spatial relations.\n\nFinally, we used third-party open-source libraries (shout-out to gsplat.js & HuggingFace!) to efficiently render our .ply point-clouds into WebGL textures (3D -> 2D projection) to visualize on our webapp without exorbitant amounts of compute.\n\nAcross the stack, we used Google Cloud buckets to store models, renders, and video files."},{"heading":"Challenges we ran into","content":"Difficulties we ran into were primarily related to the usage of the Olympe SDK for the simulation and drone movement and SuGaR (Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering) for the fine tuning of the mesh.\n\nWhile using the Olympe SDK, it was difficult to build our complete pipeline which started from a video taken of the drone’s movements in Olympe (which also required using Ubuntu, which came with its own limitations) since Olympe had very specific requirements with regards to threading and lack thereof to produce a working drone simulation with movement.\n\nFor SuGaR, we dealt with long training times and diminishing marginal returns on the improvement in visualization of point clouds. For example, it took multiple hours to produce a more refined mesh and optimize given a 40-second video as input, and the produced output was only slightly better than what we had produced without using SuGaR.\n\nFundamentally, beyond these two specific cases, it was a difficult learning curve to understand how Gaussian Splatting works mathematically, and why it can be considered today’s cutting-edge software for 3D-Renders built from video inputs. Gaussian Splatting as a concept becomes very math-heavy, but we were able to eventually identify the most useful distinguishing features and the salient similarities it shares with other more familiar tools we’ve worked with before, such as NeRFs (Neural Radiance Fields)."},{"heading":"Accomplishments that we're proud of","content":"We’re very proud of being able to utilize a technique that’s very cutting-edge and mostly only known to research communities in an applicable & impactful setting such as drone view reconstruction. We were also very proud to be able to do so with minimal compute resources, given that Gaussian splatting models are known to need very heavy amounts of GPU acceleration for good results (the entirety of our 3D reconstructions were done using cloud T4 GPUs on free Google Cloud credits). To get good models from this approach, we had to get a better understanding of hyperparameter tuning & also what entails good data collection in terms of video-taking.\n\nGiven that our project also combined many diverse moving parts (from GCS buckets to WebGL frontend), we were very proud that we were able to string everything together and get a working pipeline in such a short time!"},{"heading":"What we learned","content":"We learned a lot about modern computer vision research through looking at models like NERFs and Gaussian splatting (as well as optimizations to Gaussian splatting such as SuGaR). We learned a lot about computer vision algorithms such as structure from motion and multiview geometry. On the other hand, we learned quite a bit about the software that goes behind operating drones through Parrot’s SDK, Sphinx, and Olympe. All in all, we also learned a lot about software development and learning how to merge many different frameworks into one application!"},{"heading":"What's next for SkySplat","content":"We hope to get a better understanding of how parameters can be tuned based on what kind of video-type we’re trying to map, since the optimization of parameters (such as our loss function, number of iterations, and frame rate decomposition from videos into image sets) makes a large difference in the quality of our renders. Additionally, we hope to look more into how parallel computing can help us take advantage of the fine-surface meshes that SuGaR can produce.\n\nPictures don’t tell the full story. Our 3D models are lightweight, explorable, and powerful. Whenever you want to freeze a moment in time, SkySplat has you covered!"},{"heading":"Built With","content":"css css3 cuda flask gaussian-splatting google-cloud gpu html5 javascript olympe parrot python pytorch sphinx typescript ubuntu vite webgl"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Study Suite","project_url":"https://devpost.com/software/study-suite","tagline":"Research shows students learn 20% more content through the active recall method. Our project converts lectures into quizzes that help student retain and understand the information they learn best.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/788/022/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Fetch.ai: Beyond Boundaries AI Agent Prize (4x Engraved iPads + 1v1 with CEO)"}],"team_members":[],"built_with":[{"name":"database","url":null},{"name":"fetch.ai","url":null},{"name":"llm","url":null},{"name":"model","url":null},{"name":"openai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Daniel-Moraes1/UnifyAI"}],"description_sections":[{"heading":"Inspiration","content":"Inspired by a shared vision for transformative education, our team is embarking on a project to enhance learning from recorded lectures. We believe that integrating interactive quizzes will foster active participation, deepen understanding, and improve retention for students. Through this initiative, we aim to revolutionize the learning experience by empowering students to engage with the material in a more dynamic and effective way."},{"heading":"What it does","content":"Through a series of meticulously designed agents, we facilitate every step of the process. Initially, our system effortlessly transforms video content into detailed lecture transcripts, ensuring accessibility and clarity. Next, these transcripts are seamlessly converted into interactive quizzes, engaging students in active learning. Our quiz auto-grader feature streamlines assessment, providing instant feedback and allowing students to revisit incorrect responses for further understanding. Moreover, our system offers valuable suggestions, pinpointing timestamps for review based on quiz mistakes, and recommending related lectures to deepen comprehension. By integrating these functionalities, we aim to optimize learning outcomes and empower students to navigate their educational pursuits with confidence and efficiency."},{"heading":"How we built it","content":"Agents is the whole idea of the project, so we needed a way to create responsive agents,and make them communicate with each other.\n\nOur project was constructed with a sophisticated blend of cutting-edge technologies. Leveraging Fetch.ai 's framework, uagents, we orchestrated a network of AI agents to execute various tasks seamlessly. For the conversion of video to text transcripts, we harnessed the power of advanced APIs dedicated to this purpose, ensuring accuracy and efficiency in the process. Additionally, we integrated Language Model APIs, such as LLM's, to dynamically generate quizzes from the extracted transcripts, enabling personalized and contextually relevant assessment content. Through the strategic amalgamation of these tools and frameworks, we engineered a robust and adaptable system that streamlines the creation of interactive learning materials while harnessing the capabilities of artificial intelligence to enhance the educational experience for students."},{"heading":"Challenges we ran into","content":"Undertanding the different ways agents, protocols, and delta v communicate is our main struggle. We needed to understand the concepts from the network including Context, Protocol, Mailbox, and other things."},{"heading":"Accomplishments that we're proud of","content":"We're proud to have created an end-to-end pipeline using AI agents, leveraging cutting-edge technologies to enhance the learning experience. Our seamless integration of advanced tools transforms video content into detailed transcripts and generates personalized quizzes, resulting in a dynamic and engaging platform. This accomplishment showcases our commitment to innovation and our dedication to empowering students with effective educational solutions."},{"heading":"What we learned","content":"Throughout this project, we've learned to harness Fetch.ai's uagents for building interactive AI agents and effectively utilize the latest text generation APIs. This journey has honed our skills in deploying uagents for seamless communication and collaboration while also mastering the capabilities of text generation APIs to dynamically create learning materials. This experience has broadened our technical expertise and highlighted the transformative potential of AI in education."},{"heading":"What's next for Study Suite","content":"Different format of questions like - Truth or False and even open ended questions add more agents to increase the functionality of the agents. Integrating wispr with the app to make a just voice integrated agent."},{"heading":"Built With","content":"database fetch.ai llm model openai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Zorg - Health Records Reimagined","project_url":"https://devpost.com/software/zorg-7ubd4x","tagline":"Automated healthcare with Zorg: secure, decentralized storage and instant access to encrypted patient records, ensuring life-saving info is available in emergencies.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/779/733/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Blockchain & Security Grand Prize (4x JBL Partybox)"}],"team_members":[],"built_with":[{"name":"blockchain","url":"https://devpost.com/software/built-with/blockchain"},{"name":"bun","url":null},{"name":"data","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"hardhat","url":null},{"name":"intersystems","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"next","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"shad-cn","url":null},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"zig","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/rohan-patra/Zorg"}],"description_sections":[{"heading":"Inspiration","content":"Last week, one of our team members was admitted to the hospital with brain trauma. Doctors hesitated to treat them because of their lack of insight into the patient’s medical history. This prompted us to store EVERYONE’s health records on a single, decentralized chain. The catch? The process is end-to-end encrypted, ensuring only yourself and your designated providers can access your data."},{"heading":"How we built it","content":"Zorg was built across 3 verticals: a frontend client, a backend server, and the chain.\n\nThe frontend client…\n\nWe poured our hearts and ingenuity into crafting a seamless user interface, a harmonious blend of aesthetics and functionality designed to resonate across the spectrum of users. Our aim? To simplify the process for patients to effortlessly navigate and control their medical records while enabling doctors and healthcare providers to seamlessly access and request patient information. Leveraging the synergy of Bun, React, Next, and Shadcn, we crafted a performant portal. To safeguard privacy, we fortified client-side interactions with encryption, ensuring sensitive data remains inaccessible to central servers. This fusion of technology and design principles heralds a new era of secure, user-centric digital healthcare record keeping.\n\nThe backend server…\n\nThe backend server of Zorg is the crux of our mission to revolutionize healthcare records management, ensuring secure, fast, and reliable access to encrypted patient data. Utilizing Zig for its performance and security advantages, our backend encrypts health records using a doctor's public key and stores them on IPFS for decentralized access. These records are then indexed on the blockchain via unique identifiers (CIDs), ensuring both privacy and immutability.\n\nUpon request, the system retrieves and decrypts the data for authorized users, transforming it into a vectorized format suitable for semantic search. This process not only safeguards patient information but also enables healthcare providers to efficiently parse through detailed medical histories. Our use of Zig ensures that these operations are executed swiftly, maintaining our commitment to providing immediate access to critical medical information while prioritizing patient privacy and data integrity.\n\nThe chain…\n\nThe chain stores the encrypted key and the CID, allowing seemless access to a patient’s file stored on decentralized storage (IPFS). The cool part? The complex protocols and keys governing this system is completely abstracted away and wrapped up modern UI/UX, giving easy access to senior citizens and care providers."},{"heading":"Challenges we ran into","content":"Our biggest challenges were during integration, near the end of the hackathon. We had divided the project, with each person focusing on a different area—machine learning and queries, blockchain and key sharing, encryption and IPFS, and the frontend design. However, when we began to put things together, we quickly realized that we had failed to communicate with each other the specific details of how each of our systems worked. As a result we had to spend a few hours just tweaking each of our systems so that they could work with each other.\n\nAnother smaller (but enjoyable!) challenge we faced was learning to use a new language (Zig!). We ended up building our entire encryption and decryption system in Zig (as it needed to be incredibly fast due to the potentially vast amounts of data it would be processing) and had to piece together both how to build these systems in Zig, and how to integrate the resulting Zig binaries into the rest of our project."},{"heading":"What's next for Zorg","content":"In the future, we hope to devise a cryptographically sound way to revoke access to records after they have been granted. Additionally, our system would best benefit the population if we were able to partner with the government to include patient private keys in something everyone carries with them like their phone or ID so that in an emergency situation, first responders can access the patient data and identify things like allergies to medications."},{"heading":"Built With","content":"blockchain bun data docker express.js flask hardhat intersystems javascript machine-learning next python react shad-cn solidity typescript zig"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Lingua Franca","project_url":"https://devpost.com/software/lingua-franca-6xovi3","tagline":"Immersive VR language learning experience powered by AI.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/773/249/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Education Grand Prize (4x Quest 3)"}],"team_members":[],"built_with":[{"name":"oculus","url":"https://devpost.com/software/built-with/oculus"},{"name":"together.ai","url":null},{"name":"unity","url":"https://devpost.com/software/built-with/unity"},{"name":"wit.ai","url":"https://devpost.com/software/built-with/wit-ai"}],"external_links":[{"label":"github.com","url":"https://github.com/ShounakRay/TreeHacks2024"}],"description_sections":[{"heading":"Inspiration","content":"Conventional language learning apps like Duolingo don’t offer the ability to have freeform and dynamic conversations. Additionally, finding a language partner can be difficult and costly.\n\nLingua Franca tackles this head-on by offering intermediate to advanced language learners an immersive, interactive experience.\n\nAlthough other apps exist that try to do the same thing, their interaction topics are hard-coded, meaning that you encounter yourself in the same dialogue over and over again. By leveraging LLMs, we’re able to ensure that no two experiences are the same!"},{"heading":"What it does","content":"You stumble into a foreign land and must communicate with the townsfolk in order to get by. As you talk with them, you must reply by recording yourself speaking in their language. Aided by LLMs, their responses dynamically change depending on what you say. Additionally, at some points in the conversation, they will give you checkpoints that you must accomplish, which encourages you to talk to other villagers.\n\nAfter each of your responses, you can also see alternative phrases you could’ve said in response to the villager. Seeing these alternative responses can aid in learning vocabulary, grammar, and can help the user branch outside of their usual go-to phrases in the language they are learning.\n\nNot only can you guide the conversation to whatever topic you’d like to practice, but to keep the user engaged, we’ve also added backstory to the characters in the village. Each time you talk with them, you can learn something more about their relationship with others in the village!"},{"heading":"How we built it","content":"Development was done in Unity3D. We used Wit.ai to capture and transcribe the user’s recorded responses. Those transcribed responses were then fed into an LLM from Together.ai, along with extra information to give context and guide the LLM to prompt the user to complete checkpoints. The response from the LLM becomes the villager’s response to the player. We created the world using assets from Unity Asset store, and the character models are from Mixamo."},{"heading":"What we learned","content":"Developing in VR was new to all team members, so developing for the Oculus Quest and using Unity3D was a great learning experience. LLMs aren’t perfect, and working to mitigate poor, harmful, or unproductive responses is difficult. However, we took this challenge seriously while working on this app and carefully tuned our prompts to give the model the context it needed to avoid these situations."},{"heading":"What's next for Lingua Franca","content":"The next steps for this app include: Adding more languages adding audio feedback from the villagers as an addition to text responses adding new locations, characters, and worlds for more variation in the experience."},{"heading":"Built With","content":"oculus together.ai unity wit.ai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"3dReal","project_url":"https://devpost.com/software/3dreal","tagline":"Your memories, but realer","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/772/324/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Entertainment Grand Prize (4x 32in Curved Gaming Monitors)"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"figma","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"nerf","url":null},{"name":"nvidia","url":"https://devpost.com/software/built-with/nvidia"},{"name":"xcode","url":"https://devpost.com/software/built-with/xcode"}],"external_links":[{"label":"github.com","url":"https://github.com/alexkranias/3dReal"}],"description_sections":[{"heading":"Inspiration","content":"Metaverse, vision pro, spatial video. It’s no doubt that 3D content is the future. But how can I enjoy or make 3d content without spending over 3K? Or strapping massive goggles to my head? Let's be real, wearing a 3d vision pro while recording your child's birthday party is pretty dystopian. And spatial video only gets you so far in terms of being able to interact, it's more like a 2.5D video with only a little bit of depth.\n\nHow can we relive memories in 3d without having to buy new hardware? Without the friction?\n\nMeet 3dReal, where your best memories got realer. It's a new feature we imagine being integrated in BeReal, the hottest new social media app that prompts users to take an unfiltered snapshot of their day through a random notification. When that notification goes off, you and your friends capture a quick snap of where you are! The difference with our feature is based on this idea where if you have multiple images of the same area ie. you and your friends are taking BeReals at the same time, we can use AI to generate a 3d scene. So if the app detects that you are in close proximity to your friends through bluetooth, then you’ll be given the option to create a 3dReal."},{"heading":"What it does","content":"With just a few images, the AI powered Neural Radiance Fields (NeRF) technology produces an AI reconstruction of your scene, letting you keep your memories in 3d. NeRF is great in that it only needs a few input images from multiple angles, taken at nearly the same time, all which is the core mechanism behind BeReal anyways, making it a perfect application of NeRF.\n\nSo what can you do with a 3dReal?\n\nView in VR, and be able to interact with the 3d mesh of your memory. You can orbit, pan, and modify how you see this moment captures in the 3dReal Since the 3d mesh allows you to effectively view it however you like, you can do really cool video effects like flying through people or orbiting people without an elaborate robot rig. TURN YOUR MEMORIES INTO THE PHYSICAL WORLD - one great application is connecting people through food. When looking through our own BeReals, we found that a majority of group BeReals were when getting food. With 3dReal, you can savor the moment by reconstructing your friends + food, AND you can 3D print the mesh, getting a snippet of that moment forever."},{"heading":"How it works","content":"Each of the phones using the app has a countdown then takes a short 2-second \"video\" (think of this as a live photo) which is sent to our Google Firebase database. We group the videos in Firebase by time captured, clustering them into a single shared \"camera event\" as a directory with all phone footage captured at that moment. While one camera would not be enough in most cases, by using the network of phones to take the picture simultaneously we have enough data to substantially recreate the scene in 3D. Our local machine polls Firebase for new data. We retrieve it, extract a variety of frames and camera angles from all the devices that just took their picture together, use COLMAP to reconstruct the orientations and positions of the cameras for all frames taken, and then render the scene as a NeRF via NVIDIA's instant-ngp repo. From there, we can export, modify, and view our render for applications such as VR viewing, interactive camera angles for creating videos, and 3D printing."},{"heading":"Challenges we ran into","content":"We lost our iOS developer team member right before the hackathon (he's still goated just unfortunate with school work) and our team was definitely not as strong as him in that area. Some compromises on functionality were made for the MVP, and thus we focused core features like getting images from multiple phones to export the cool 3dReal. There were some challenges with splicing the videos for processing into the NeRF model as well."},{"heading":"Accomplishments that we're proud of","content":"Working final product and getting it done in time - very little sleep this weekend!"},{"heading":"What we learned","content":"A LOT of things out of all our comfort zones - Sunny doing iOS development and Phoebe doing not hardware was very left field, so lots of learning was done this weekend. Alex learned lots about NeRF models."},{"heading":"What's next for 3dReal","content":"We would love to refine the user experience and also improve our implementation of NeRF - instead of generating a static mesh, our team thinks with a bit more time we could generate a mesh video which means people could literally relive their memories - be able to pan, zoom, and orbit around in them similar to how one views the mesh.\n\nBeReal pls hire 👉👈"},{"heading":"Built With","content":"ai figma firebase nerf nvidia xcode"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"gradeAI","project_url":"https://devpost.com/software/gradeai-s9ptdc","tagline":"Efficient teaching, effective learning","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/772/044/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Fetch.ai: Smartest AI Agent Prize (Top 2 Teams - $3k Cash)"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"fetch.ai","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/justbustin/gradeAI"}],"description_sections":[{"heading":"Inspiration","content":"As engineering students who attend public universities, class sizes of 300 are no rarity. Sadly, that also means that oftentimes, our most important classes can take weeks or months to supply us with feedback for our assignments, hamstringing the learning process and creating a larger disconnect between the professor and the student. A quarter system is 10 weeks long, and having 3 weeks of ungraded assignments means learning 30% of the class without any feedback.\n\nWe're also keenly aware that now more than ever, teachers are overworked. The average teacher logs upwards of 180 hours per year on grading alone, making it the most worked-on task outside of actual teaching inside the classroom. The solutions they have right now are few. Hiring extra teaching assistants to aid with grading is an extensive process that demands thorough interviewing to ensure the applicants actually know the material. Simultaneously, money is something that most universities can't spare on extra graders, meaning those that are qualified face heavily inflated workloads. At the moment, it seems there's no feasible out. With less time spent grading, there's more time spent with students.\n\nEnter gradeAI, our AI-based tool that aims to streamline grading by automatically processing assignments and evaluating them based on a given rubric. Our goal isn't to replace the teacher or the TA, but rather to appraise assignments quickly, let educators review and alter judgements made, and expedite grading large volumes of similar assignments."},{"heading":"What it does","content":"It utilizes AI Agents from Fetch.ai to break down multiple steps of grading assignments. First on the frontend, the teacher can create an assignment in a course, then name it and provide a rubric. As students submit their homework, the assignment will start getting graded after the due date. We have multiple AI Agents with the first being locally run and ported publicaly using Fetch.AI's mailbox. This first agent is responsible for pulling submitted homework files and the associated rubric from our Google Cloud Platform storage. This agent also processes the text and is the handler for sending homework to be graded in our pipeline, and eventually write the processed grades to our database. Next in the process is our pipeline of 4 agents on the Agentverse! The first agent is responsible for sending all of the processed text in a clean format to our next agent which parses the homework into readable chunks. The second agent-let's name it Parser-takes each problem number and associated work with it, and puts them all into an array! Parser does this for both the solution and homework submitted! This agent then sends this data to our third AI Agent and let's name this one Solver! This is where all of the grading happens. This AI Agent cross examines each problem's solution and attempt one at a time, which increases accuracy of grading. We ask this AI Agent to return data in a JSON format with the grade, confidence level, summary, and details for each problem. From there, Solver sends all of the graded problems that it did one by one to ensure accuracy to our last AI Agent on the Agentverse! This last agent aggregates all of the data and sends it back to our first agent being locally ran! Our locally ran agent then writes everything to our database, where we show it on our frontend!"},{"heading":"How we built it","content":"We used the Reflex framework and Fetch.AI to accomplish all of our goals. This proved to be both difficult and convenient, as we never used these technologies before, but everything was written in Python! Thanks to Reflex, we built our entire website in Python, and Fetch.AI is all in python, which was great. In terms of Fetch.AI, we used many of their products such as the Agentverse, mailbox, template agents, and a little bit of DeltaV! We utilized the Agentverse, as it provided a convenient way to deploy our agents and have them run 24/7, which allowed us to keep a constant pipeline! The integrated development environment was also helpful in debugging and creating agents from scratch. Mailbox provided by Fetch.AI allowed our locally ran AI Agent to communicate with all of the deployed agents. Using a combination of all of these tools, we were able to piece together multiple agents with using a combination of openAI's API and OCR. Reflex was also a great tool and we utilized it's documentation and ability to wrap React code to create nice and complex components with functionality."},{"heading":"Challenges we ran into","content":"We took a big challenge in using technology we've never touched before! Reflex and Fetch.AI were the two major components of our project, and there was a big learning curve. One of the issues we ran into was a lack of documentation. Being a relatively newer product that also had a recent name change, Reflex didn't have as many resources as many of the tools we were more accustomed to. As a result, it took us a while to get the hang of what we were doing, and complex issues such as managing routes and backend integration with GCP were made even more difficult as we searched for solutions. Simpler issues like embedding PDF views of files was also made demanding, and our time frame for design was expanded due to underestimation of what we thought wouldn't be issues."},{"heading":"What we learned","content":"In terms of our web framework, we chose to use Reflex to build out our web app that incorporated Fetch.ai, FAST APIs, and OpenAI APIs. For Reflex, we learned how to develop the architecture of our application by fully understanding the Reflex documentation, and of course, reaching out to the Reflex developers for assistance. Additionally, we learned to link the various pages together and to incorporate components into our pages in order to fully immerse the user(s) and to provide a platform for the students and teachers to view the graded papers and their respective results."},{"heading":"What's next for gradeAI","content":"Given a time frame of 36 hours, our team had many more features planned than we were able to execute. Primarily, we'd love to add a plagiarism checker, LLM integration for student follow-ups, and textbook breakdowns for relevant questions in the homework. If gradeAI were a product in wide circulation, plagiarism would undoubtedly be the number one threat to its efficacy; as such, this implement demands our most immediate attention. Textbook breakdowns would also be a great quality of life feature we'd love to be able to tackle, but in truth, the feature we'd most look towards would be the LLM integration. Our undertaking of this project was meant to challenge us from the very beginning, and LLM is something we're all excited about and ready to learn and apply.\n\nOutside of features, we're looking to simply run and test a wider variety of materials so gradeAI can be the product it was meant to be. We've been nothing but thoroughly impressed with what we've been able to make, and we hope educators will be too.\n\nThere's a long road to come for gradeAI, but whatever comes our way, we're confident we have the tools to ace it."},{"heading":"Built With","content":"fastapi fetch.ai google-cloud openai python reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Healthiator","project_url":"https://devpost.com/software/healthiator","tagline":"Easily compare medical costs across hospitals, tailored to your condition, insurance plan vs out-of-pocket payment with Healthiator. Uncover savings and take control of your healthcare journey.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/767/931/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Fetch.ai: Smartest AI Agent Prize (Top 2 Teams - $3k Cash)"}],"team_members":[],"built_with":[{"name":"convex","url":null},{"name":"fetch.ai","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"together.ai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/elenaxx2022/healthiator"},{"label":"pitch.com","url":"https://pitch.com/v/healthiator-g9crp5"},{"label":"github.com","url":"https://github.com/elenaxx2022/healthiators"}],"description_sections":[{"heading":"Inspiration","content":"As a patient in the United States you do not know what costs you are facing when you receive treatment at a hospital or if your insurance plan covers the expenses. Patients are faced with unexpected bills and left with expensive copayments. In some instances patients would pay less if they cover the expenses out of pocket instead of using their insurance plan."},{"heading":"What it does","content":"Healthiator provides patients with a comprehensive overview of medical procedures that they will need to undergo for their health condition and sums up the total costs of that treatment depending on which hospital they go-to, and if they pay the treatment out-of-pocket or through their insurance. This allows patients to choose the most cost-effective treatment and understand the medical expenses they are facing. A second feature healthiator provides is that once patients receive their actual hospital bill they can claim inaccuracies. Healthiator helps patients with billing disputes by leveraging AI to handle the process of negotiating fair pricing."},{"heading":"How we built it","content":"We used a combination of Together.AI and Fetch.AI. We have several smart agents running in Fetch.AI each responsible for one of the features. For instance, we get the online and instant data from the hospitals (publicly available under the Good Faith act/law) about the prices and cash discounts using one agent and then use together.ai's API to integrate those information in the negotiation part."},{"heading":"Ethics","content":"The reason is that although our end purpose is to help people get medical treatment by reducing the fear of surprise bills and actually making healthcare more affordable, we are aware that any wrong suggestions or otherwise violations of the user's privacy have significant consequences. Giving the user as much information as possible while keeping away from making clinical suggestions and false/hallucinated information was the most challenging part in our work."},{"heading":"Challenges we ran into","content":"Finding actionable data from the hospitals was one of the most challenging parts as each hospital has their own format and assumptions and it was not straightforward at all how to integrate them all into a single database. Another challenge was making various APIs and third parties work together in time."},{"heading":"Accomplishments that we're proud of","content":"Solving a relevant social issue. Everyone we talked to has experienced the problem of not knowing the costs they're facing for different procedures at hospitals and if their insurance covers it. While it is an anxious process for everyone, this fact might prevent and delay a number of people from going to hospitals and getting the care that they urgently need. This might result in health conditions that could have had a better outcome if treated earlier."},{"heading":"What we learned","content":"How to work with convex fetch.api and together.api."},{"heading":"What's next for Healthiator","content":"As a next step, we want to set-up a database and take the medical costs directly from the files published by hospitals."},{"heading":"Built With","content":"convex fetch.ai javascript python react together.ai"},{"heading":"Try it out","content":"github.com pitch.com github.com"}]},{"project_title":"Pawsome","project_url":"https://devpost.com/software/pawsome-nk68yv","tagline":"We envision a world where every pet enjoys a happy, healthy life. Our goal is to prioritize pets' health and wellness, offering accessible, reliable solutions for their care.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/771/037/datas/medium.PNG","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Fetch.ai: Smartest AI Agent Prize (Top 2 Teams - $3k Cash)"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"fetch.ai","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"next.js","url":null},{"name":"postman","url":"https://devpost.com/software/built-with/postman"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"together.ai","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/mounika2000/Treehacks"}],"description_sections":[{"heading":"About Pawsome","content":"Pawsome isn't just a project—it's a mission to ensure the well-being of pets worldwide. Our platform offers tools and resources for pet owners to maintain and improve their pet's health and fitness, featuring interactive consultations and tailored fitness schedules.\n\nOur Vision\n\nWe envision a world where every pet enjoys a happy, healthy life. Our goal is to prioritize pets' health and wellness, offering accessible, reliable solutions for their care."},{"heading":"Technology Stack","content":"To deliver a seamless, efficient user experience, Pawsome leverages a comprehensive and robust stack:\n\nFrontend : React: A JavaScript library for building user interfaces. Next.js: A React framework for production; it makes building scalable applications easier. JavaScript: A programming language that allows you to implement complex features on web pages. TypeScript: A typed superset of JavaScript that compiles to plain JavaScript. HTML: The standard markup language for documents designed to be displayed in a web browser. CSS: The language used to describe the presentation of a document written in HTML. Tailwind CSS: A utility-first CSS framework for rapidly building custom designs. Backend : Flask: A micro web framework written in Python. Python: A programming language that lets you work quickly and integrate systems more effectively. Integrations : Fetch.ai: An artificial intelligence lab working on creating a decentralized digital world.\n\nAgent 1 Using Fetch AI\n\nAgent 2 Using Fetch AI\n\nAgent 3 Using Fetch AI\n\nTogether.ai: A platform that provides AI-based solutions. Postman: An API platform for building and using APIs. Postman simplifies each step of the API lifecycle and streamlines collaboration. Bun: Bun is an all-in-one JavaScript runtime & toolkit designed for speed, complete with a bundler, test runner, and Node.js-compatible package manager. It served as a JavaScript runtime optimizer for our project.\n\nThese technologies are carefully chosen to ensure that Pawsome is not only cutting-edge but also robust and scalable, catering to the dynamic needs of pet owners and their beloved pets.\n\nThis tech combination enables dynamic, responsive interactions and complex data processing across our platform."},{"heading":"Features","content":"Interactive Pet Wellness AI\n\nPawsome features a chat button for users to interact with our pet wellness AI. Depending on user needs, it offers appointment booking, medicine orders, or grooming session scheduling.\n\nCustomized Fitness Center\n\nOur Fitness Center allows pet owners to input details like breed, weight, and health, creating a custom fitness plan tailored to their pet's needs, and promoting a holistic wellness approach.\n\nCustom Push Notifications\n\nTo ensure pets and their owners stay on track with their wellness routines, Pawsome sends custom push notifications via email, reminding users of upcoming activities and schedules."},{"heading":"Future Enhancements","content":"We're committed to continuously improving Pawsome with several key initiatives:\n\nData-Driven Personalization : By gathering more data, we aim to make even more personalized recommendations, enhancing the effectiveness of wellness plans. E-commerce Platform : Developing a one-stop destination for all pet needs, from food to toys, ensuring pet owners have access to high-quality products. Network of Veterinarians : Establishing a network of doctors available for consultations and emergencies, providing peace of mind for pet owners."},{"heading":"Getting Started","content":"Clone repo git clone https://github.com/mounika2000/treehacks.git cd Treehacks yarn Install dependencies npm install 3.Then, you can run locally in development mode with live reload: yarn dev"},{"heading":"Contributing","content":"Contributions are welcome! Whether you're a developer, designer, or pet wellness enthusiast, there are many ways to contribute to Pawsome. Please see our contributing guidelines for more details."},{"heading":"Contact Us","content":"Sai Mounika Peteti: saimounika.peteti@sjsu.edu Chinmay Nilesh Mahagaonkar: chinmaynilesh.mahagaonkar@sjsu.edu Sanket Kulkarni: sanket.kulkarni@sjsu.edu Tanay Godse: tanay.godse@sjsu.edu"},{"heading":"Built With","content":"bun fetch.ai flask next.js postman python react sqlite together.ai typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TrustFile","project_url":"https://devpost.com/software/trustfile","tagline":"Nearly 48,000 Canadian children and ten times as many in the US lack stable document access in foster care. TrustFile offers a blockchain-based solution for secure document storage.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Ava Labs / Avalanche: Best Consumer Use Case for Blockchain ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"ipfs","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"web3.js","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Matt711/TrustFile"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1d0DE6LQQSvilpXDNuZuupJ1cHLfB-h06K011YSW1-r0/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Our journey with TrustFile began with a deep-seated concern for the welfare of children in foster care systems. Upon learning about the staggering number of nearly 48,000 children in Canada and ten times more in the US facing instability in document access, we were deeply moved to take action. This ignited our inspiration to develop a solution that could alleviate their challenges. Throughout our journey, we gained invaluable insights into the complexities of document management within foster care systems. We learned about the profound impact that document loss can have on a child's life, affecting their ability to enroll in school, receive necessary medical care, secure employment, and establish their identity. Witnessing these challenges firsthand motivated us to explore innovative approaches to address this critical issue."},{"heading":"What it does","content":"TrustFile, a decentralized document storage application leveraging blockchain technology. TrustFile empowers users to securely upload and retrieve their documents, ensuring their accessibility and security. By utilizing blockchain's immutable and transparent nature, TrustFile mitigates the risk of document loss and provides foster children with a reliable solution to manage their essential records effectively. So that children have all their necessary documents once they are 18."},{"heading":"How we built it","content":"Our project, TrustFile, harnesses cutting-edge technologies to provide a secure and decentralized solution for document management. Leveraging the capabilities of the Avalanche Blockchain on the Fuji testnet, we implemented a Smart Contract that facilitates the creation and management of Non-Fungible Tokens (NFTs).\n\nThe process begins with users uploading their documents to the platform, which triggers the creation of unique NFTs associated with each document. These NFTs are then transferred to the users' wallets, providing them with a secure and verifiable ownership mechanism.\n\nBehind the scenes, we utilize the IPFS (InterPlanetary File System) to store the documents securely. Each document uploaded by the user is stored on the IPFS file system, ensuring durability and accessibility while maintaining the decentralized nature of the platform.\n\nWhen a user needs to retrieve a document, they simply present the corresponding NFT associated with that document. Through the Smart Contract's functionality, the NFT is authenticated, allowing the user to access and retrieve their document from the IPFS file system.\n\nThis seamless integration of blockchain technology, NFTs, and IPFS enables TrustFile to offer a robust and user-friendly solution for document management, empowering users with secure and decentralized access to their important records."},{"heading":"Challenges we ran into","content":"Our journey with TrustFile presented several challenges, especially considering it was our team's first foray into blockchain technology. One of the most significant hurdles we encountered revolved around the implementation of IPFS libraries.\n\nNavigating the intricacies of integrating IPFS into our project proved to be a formidable task. The complexity of the IPFS ecosystem, coupled with the novelty of blockchain development, posed challenges in understanding and effectively utilizing IPFS libraries within our application.\n\nWe encountered issues with library compatibility, documentation discrepancies, and unfamiliar concepts inherent to decentralized file storage systems. As a result, the process of implementing IPFS functionality into TrustFile required extensive research, experimentation, and troubleshooting."},{"heading":"Accomplishments that we're proud of","content":"Successful Integration of Blockchain Technology: As a team new to blockchain development, successfully integrating blockchain technology into TrustFile was a significant achievement. We overcame the steep learning curve associated with blockchain development and gained valuable insights into its potential applications.\n\nCreation of a Decentralized Document Storage Platform: Developing TrustFile as a decentralized document storage platform represents a significant accomplishment. We built a solution that leverages the benefits of blockchain and IPFS to provide users with secure, transparent, and accessible document management.\n\nImplementation of Non-Fungible Tokens (NFTs): Integrating NFTs into TrustFile to provide users with verifiable ownership of their documents was a notable accomplishment. By leveraging NFTs, we created a unique and secure mechanism for document authentication and retrieval.\n\nSuccessful Deployment on Avalanche Blockchain: Deploying TrustFile on the Avalanche Blockchain, specifically on the Fuji testnet, was a significant milestone. This achievement validated our ability to work with blockchain networks and showcased our proficiency in deploying decentralized applications.\n\nUser-Centric Design and Functionality: Designing TrustFile with a user-centric approach and intuitive functionality was an accomplishment we're proud of. We prioritized user experience and usability throughout the development process, ensuring that TrustFile meets the needs of its users effectively."},{"heading":"What we learned","content":"Throughout the development journey, we encountered various technical challenges, from blockchain integration to IPFS implementation. Successfully overcoming these challenges demonstrated our team's resilience, problem-solving skills, and commitment to delivering a high-quality product. By developing TrustFile, we contributed to addressing real-world challenges faced by children in foster care systems. This social impact aspect of our project is a source of pride for our team, as it reflects our commitment to leveraging technology for positive change."},{"heading":"What's next for TrustFile","content":"Our journey with TrustFile has been just the beginning of our mission to revolutionize document management and make a positive impact in the world. Moving forward, we are committed to further developing and expanding TrustFile into a full-fledged decentralized application (DApp) that can address novel causes and serve a broader user base."},{"heading":"Built With","content":"css html ipfs javascript solidity typescript web3.js"},{"heading":"Try it out","content":"github.com docs.google.com"}]},{"project_title":"Meshworks - NLP LoRa Mesh Network for Emergency Response","project_url":"https://devpost.com/software/meshworks-nlp-lora-mesh-network-for-emergency-response","tagline":"Imagine an earthquake disabling all telecommunication. We combine mesh-based radio technology with cutting-edge AI processing to build a resilient information network for emergency responders.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/776/028/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Intel: Best Use of Intel Developer Cloud ($10k Credits + 4x Lenovo AI PC [1st] & $4k Credits [2nd] & $2k Credits [3rd])"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"image-augmentation","url":null},{"name":"intel-development-cloud","url":null},{"name":"intel-extension-for-pytorch-(model-optimization)","url":null},{"name":"lora","url":null},{"name":"predicitionguardllm","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"whisper-(speech-to-text)","url":null},{"name":"wio-terminal","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/AWESDUDEtheCOOL/Treehacks2024"}],"description_sections":[{"heading":"Inspiration","content":"In times of disaster, the capacity of rigid networks like cell service and internet dramatically decreases at the same time demand increases as people try to get information and contact loved ones. This can lead to crippled telecom services which can significantly impact first responders in disaster struck areas, especially in dense urban environments where traditional radios don't work well. We wanted to test newer radio and AI/ML technologies to see if we could make a better solution to this problem, which led to this project."},{"heading":"What it does","content":"Device nodes in the field network to each other and to the command node through LoRa to send messages, which helps increase the range and resiliency as more device nodes join. The command & control center is provided with summaries of reports coming from the field, which are visualized on the map."},{"heading":"How we built it","content":"We built the local devices using Wio Terminals and LoRa modules provided by Seeed Studio; we also integrated magnetometers into the devices to provide a basic sense of direction. Whisper was used for speech-to-text with Prediction Guard for summarization, keyword extraction, and command extraction, and trained a neural network on Intel Developer Cloud to perform binary image classification to distinguish damaged and undamaged buildings."},{"heading":"Challenges we ran into","content":"The limited RAM and storage of microcontrollers made it more difficult to record audio and run TinyML as we intended. Many modules, especially the LoRa and magnetometer, did not have existing libraries so these needed to be coded as well which added to the complexity of the project."},{"heading":"Accomplishments that we're proud of:","content":"We wrote a library so that LoRa modules can communicate with each other across long distances We integrated Intel's optimization of AI models to make efficient, effective AI models We worked together to create something that works"},{"heading":"What we learned:","content":"How to prompt AI models How to write drivers and libraries from scratch by reading datasheets How to use the Wio Terminal and the LoRa module"},{"heading":"What's next for Meshworks - NLP LoRa Mesh Network for Emergency Response","content":"We will improve the audio quality captured by the Wio Terminal and move edge-processing of the speech-to-text to increase the transmission speed and reduce bandwidth use. We will add a high-speed LoRa network to allow for faster communication between first responders in a localized area We will integrate the microcontroller and the LoRa modules onto a single board with GPS in order to improve ease of transportation and reliability"},{"heading":"Built With","content":"flask image-augmentation intel-development-cloud intel-extension-for-pytorch-(model-optimization) lora predicitionguardllm python pytorch whisper-(speech-to-text) wio-terminal"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ScratchML","project_url":"https://devpost.com/software/scratchml","tagline":"The no-code platform for ML designed for students in the age of AI.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/775/975/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Intel: Best Use of Intel Developer Cloud ($10k Credits + 4x Lenovo AI PC [1st] & $4k Credits [2nd] & $2k Credits [3rd])"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Warp: Best Developer Tool by Warp (4x Keytron Keyboards + Warp Tumblers)"}],"team_members":[],"built_with":[{"name":"chakra","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"predictionguard","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"scikit-learn","url":"https://devpost.com/software/built-with/scikit-learn"},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/theDubW/ScratchML"}],"description_sections":[{"heading":"Inspiration","content":"We’re students ranging in various levels of machine learning experience, but we all started the same way; working through infamous online courses and filling out starter code with lines we don’t understand. Although we eventually were able to mature our understanding of machine learning, through ScratchML, we are changing the ML education paradigm from our shared experiences and struggles to creating a platform where kids can develop intuition on data analysis and machine learning that abstracts away the parts that make it difficult. Instead of “learning” by filling out lines of pytorch code that aren’t self evident, we want kids to start thinking about the richness of data around us, what it can be used for, and how to leverage models to infer things about the world around us. We are from the generation that was inspired by Scratch, where drag-and-drop code helped us to make toy games and develop our intuition to code even before we learned our first programming language. ScratchML aims to take a similar approach, creating a learning platform that's easy-to-use and develops our way of thinking and intuition behind the scenes."},{"heading":"What it does","content":"Our platform is designed to teach machine learning and data analysis principles. The challenge is two-fold: making the learning fun and engaging while also providing a high quality curriculum developed around experiential learning. ScratchML delivers on both aspects by providing a reliable drag-and-drop interface used just like Scratch for model development and data experimentation, and through engineered datasets where the student are guided through exploring the data and reporting findings, students will be much more engaged through trying out a bunch of different approaches to accomplish a mission.\n\nEach lesson comes with a workspace, where students can drag-and-drop models and other blocks to create a no-code, data analysis pipeline. A personalized tutor system is designed to guide the learning process, offering explanations and tips to guide the learning process. This system leverages the Prediction Guard LLM API to provide real-time insights into user decisions and outcomes within lessons. It employs state management to ensure continuous progress while fostering a sandbox-style learning environment."},{"heading":"How we built it","content":"The tech stack for the project consisted of: React for the frontend, utilizing the Chakra UI component library and Tailwind CSS for styling. A Flask server that runs and trains models based on the layout specified by the user. We also used Intels extension for Scikit-learn/PyTorch to deliver faster training and inference time critical for making the user experience on the site seamless. Firebase database for storing user data and storage of models that can be evaluated on the fly. APIs: Prediction Guard LLM API to provide personalized real-time feedback using Neural-Chat-7B. Models: Scikit-learn and PyTorch Dev tools: Intel's Developer Cloud for constructing and testing the sandbox model, leveraging the PyTorch optimizations from Intel."},{"heading":"Challenges we ran into","content":"The largest challenge was to constantly rework the design to build a more intuitive, highly-functional interface. We are software developers by training; it was difficult to settle on a UI/UX design that achieves all of our priorities. Prior approaches for no-code machine learning are primarily designed for use in industry. Although they work well for older users who need to model data, at each stage we were super focused on whether or not each of the design elements were good for students. Furthermore, with this being our first or second hackathon experience, we worked hard to coordinate distributing work, ideating what could be feasibly accomplished in 36 hours, and ensuring that we accomplish our tasks in a timely manner. A final challenge we faced was simple endurance - the last few hours of development were extremely difficult given the sleep deprivation we all suffered from - overcoming this challenge was simply a matter of willpower and perseverance."},{"heading":"Accomplishments that we're proud of","content":"As amateur hackers, we are proud of what we accomplished this weekend - from building a helpful and innovative product from scratch to just the sheer amount of hard work we exhibited - this weekend proved to us that we are each capable of much more than we originally thought. Our team has limited hackathon experience, and we went in with the approach of not compromising on even our most ambitious ideas. We hacked together the base form of ScratchML, which supports all of the critical features that we set out to do at the beginning of the hackathon, and we are super excited to continue to work at the idea and think creatively and collaboratively on ways we can improve the learning experience for students in the future."},{"heading":"What we learned","content":"One of the largest takeaways from this weekend was simply that we are capable of much more than we originally believed. Getting together a team of passionate and driven individuals with aligned goals is a powerful tool to create and build.\n\nAnother lesson we learned in hindsight is the importance of sleep. Sometimes sacrifice can be beneficial, but it’s likely that our excitement in the earlier stages of the weekend came back to bite us during the final stretch. Finally, this challenge asked all of us to wear a wide variety of hats, working with technologies and frameworks that we have limited experiences with. We learned a variety of different tools and also learned how to quickly rise to the occasion and accomplish the needs of the team."},{"heading":"What's next for ScratchML","content":"We plan on making the UI more intuitive, adding more lessons, increasing the number of blocks available in the sandbox, and increasing the number of datasets users can play with. It is also imperative that we continue to think of creative ways to encourage learning. Our vision for the future of education, shared with many leaders in the space, is turning the classroom into a laboratory, where students can experiment and grow through trial-and-error. This requires coordinated collaboration and a lot of learning on our end as well, and we are eager to innovate and learn from innovators in the educational space to grow ScratchML into the go-to platform for young students trying to learn machine learning and data science principles."},{"heading":"Built With","content":"chakra firebase flask javascript llm predictionguard python pytorch scikit-learn tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Sonoverse","project_url":"https://devpost.com/software/sonoverse","tagline":"Protecting small artists' work through a decentralized on-chain ML-driven platform, integrating automatic DMCA claims.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/771/750/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Taisu Ventures: Creative Web3 Platform Award ($500 Cash)"}],"team_members":[],"built_with":[{"name":"caldera","url":null},{"name":"cohere","url":null},{"name":"crossmint","url":null},{"name":"ethers","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"next.js","url":null},{"name":"pinata","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/gashon/treehacks-2024"}],"description_sections":[{"heading":"TL; DR","content":"Music piracy costs the U.S. economy $12.5 billion annually . Independent artists are the fastest growing segment in the music industry , yet lack the funds and reach to enforce the Digital Millennium Copyright Act (DMCA). We let artists OWN their work (stored on InterPlanetary File System) by tracking it on our own Sonoverse Ethereum L2 chain (powered by Caldera). Artists receive Authenticity Certificates of their work in the form of Non-Fungible Tokens (NFTs), powered by Crossmint’s Minting API. We protect against parodies and remixes with our custom dual-head LSTM neural network model trained from scratch which helps us differentiate these fraudulent works from originals. We proactively query YouTube through their API to constantly find infringing work. We’ve integrated with DMCA Services , LLC. to automate DMCA claim submissions.\n\nInterested? Keep reading!"},{"heading":"Inspiration","content":"Music piracy, including illegal downloads and streaming, costs the U.S. economy $12.5 billion annually. Independent artists are the fastest growing segment in the music industry, yet lack the funds to enforce DMCA.\n\nWe asked “Why hasn’t this been solved?” and took our hand at it. Enter Sonoverse, a platform to ensure small musicians can own their own work by automating DMCA detection using deep learning and on-chain technologies."},{"heading":"The Issue","content":"Is it even possible to automate DMCA reports? How can a complex piece of data like an audio file be meaningfully compared? How do we really know someone OWNS an audio file? and more...\n\nThese are questions we had too, but by making custom DL models and chain algorithms, we have taken our hand at answering them."},{"heading":"What we’ve made","content":"We let artists upload their original music to our platform where we store it on decentralized storage (IPFS) and our blockchain to track ownership . We also issue Authenticity Certificates to the original artists in the form of Non-Fungible Tokens.\n\nWe compare uploaded music with all music on our blockchain to detect if it is a parody, remix, or other fraudulent copy of another original song, using audio processing and an LSTM deep learning model built and trained from scratch.\n\nWe proactively query YouTube through their API for “similar” music (based on our lyric hashes , frequency analysis , and more) to constantly find infringing work. For detected infringing work, we’ve integrated with DMCA Services, LLC. to automate DMCA claim submissions ."},{"heading":"How we built it","content":"All together, we used…\n\nNextJS Postgres AWS SES AWS S3 IPFS Caldera Crossmint AssemblyAI Cohere YouTube API DMCA Services\n\nIt’s a lot , but we were able to split up the work between our team. Gashon built most of the backend routes, an email magic link Auth platform, DB support, and AWS integrations.\n\nAt the same time, Varun spent his hours collecting hours of audio clips, training and improving the deep LSTM model, and writing several sound differentiation/identification algorithms. Here’s Varun’s explanation of his algorithms: “To detect if a song is a remix, we first used a pre-trained speech to text model to extract lyrics from mp3 files and then analyzed the mel-frequency cepstral coefficients, tempo, melody, and semantics of the lyrics to determine if any songs are very similar. Checking whether a song is a parody is much more nuanced, and we trained a dual-head LSTM neural network model in PyTorch to take in vectorized embeddings of lyrics and output the probability of one of the songs being a parody of the other.”\n\nWhile Varun was doing that, Ameya built out the blockchain services with Caldera and Crossmint, and integrated DMCA Services. Ameya ran a Ethereum L2 chain specific for this project (check it out here ) using Caldera. He built out significant infrastructure to upload audio files to IPFS (decentralized storage) and interact with the Caldera chain. He also created the Authenticity Certificate using Crossmint that’s delivered directly to each Sonoverse user’s account.\n\nAmeya and Gashon came together at the end to create the Sonoverse frontend, while Varun pivoted to create our YouTube API jobs that query through recently uploaded videos to find infringing content."},{"heading":"Challenges we overcame","content":"We couldn’t find existing models to detect parodies and had to train a custom model from scratch on training data we had to find ourselves. Of course, this was quite challenging, but with audio files each being unique, we had to create a dataset of hours of audio clips.\n\nAnd, like always, integration was difficult. The power of a team was a huge plus, but also a challenge. Ameya’s blockchain infrastructure had Solidity compilation challenges when porting into Gashon’s platform (which took some precious hours to sort out). Varun’s ML algorithms ran on a Python backend which had to be hosted alongside our NextJS platform. You can imagine what else we had to change and fix and update, so I won’t bore you.\n\nAnother major challenge was something we brought on ourselves, honestly. We set our aim high so we had to use several different frameworks, services, and technologies to add all the features we wanted. This included several hours of us learning new technologies and services, and figuring out how to implement them in our project."},{"heading":"Accomplishments that we're proud of","content":"Blockchain has a lot of cool and real-world applications, but we’re excited to have settled on Sonoverse. We identified a simple (yet technically complex) way to solve a problem that affects many small artists. We also made a sleek web platform, in just a short amount of time, with scalable endpoints and backend services.\n\nWe also designed and trained a deep learning LSTM model to identify original audios vs fraudulent ones (remixes, speed ups, parodies, etc) that achieved 93% accuracy ."},{"heading":"What we learned","content":"About DMCA\n\nWe learned how existing DMCA processes are implemented and the large capital costs associated with them. We became experts on digital copyrights and media work!\n\nBlockchain\n\nWe learned how to combine centralized and decentralized infrastructure solutions to create a cohesive end-to-end project."},{"heading":"What's next for Sonoverse","content":"We're looking forward to incorporating on-chain royalties for small artists by detecting when users consume their music and removing the need for formal contracts with big companies to earn revenue. We’re excited to also add support for more public APIs in addition to YouTube API!"},{"heading":"Built With","content":"caldera cohere crossmint ethers flask next.js pinata python pytorch react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"zKnowledgeBase","project_url":"https://devpost.com/software/zknowledge-base","tagline":"The world's research, embedded, immortalized and decentralized.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/775/197/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Ava Labs / Avalanche: Best Decentralized Application ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"chromadb","url":null},{"name":"css3","url":"https://devpost.com/software/built-with/css3"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"ipfs","url":null},{"name":"next.js","url":null},{"name":"pinata","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"together.ai","url":null},{"name":"zig","url":null},{"name":"zkp","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"One of my team members, Thor, was looking for citations for his psychology research paper while on the bus ride up from SoCal. He had a psychology paper due and as he scrambled to finish the assignment, he struggled to find articles to cite due to the fragmented landscape of paywalled academic journals and lackluster indexing. We looked into his problem and realized that research is very unfriendly to those without the means.\n\nAccess to Academic journals are expensive (>$500) for one person and the biggest commercial indexers such as Web of Science or Scopus will charge you for using their search. Open source alternatives such as IEEEExplore and arXiv either do not have the breadth of research or lack vetting as preprints. A further exigence is the detrimental nature of science journalism that fuels a cycle of misleading publications. When scientists are at the mercy of research journals to expose their work for grants and journals need clicks to drive revenue, the scientific community as a whole pays the price. This felt like an issue that needed to be tackled.\n\nzKnowledgeBase is a decentralized research platform that eliminates paywalls, enables free sharing of verified research without third-party control, and mitigates censorship risks - empowering academics with open and unbiased access to knowledge."},{"heading":"What it does","content":"We built a decentralized web platform allowing users to search and store research articles, immutably and forever. Users can upload research PDFs and search for articles using our vector-embedded search. We secure our articles with a Merkle Tree, where the root is publicly available on the Avalanche Blockchain."},{"heading":"How we built it","content":"We allow users to upload research papers in PDF form. We store their submissions on IPFS, a distributed ledger designed for file storage, allowing for reliable uptime and free access. At the same time, we chunk and vector embed the paper, using LangChain and together.ai to render the paper into a multiple hundred dimensional vector, stored in ChromaDB's vector database. When the user searches with our platform, we vector embed their search query and use cosine similarity to compare the search vector to the stored vectors in the vector database. We then present the most similar papers in a scrollable format. Finally, we used a Merkle tree built with Zig for submission security and to verify that papers retrieved from IPFS came from our uploads."},{"heading":"Challenges we ran into","content":"We used a lot of new technologies for the first time including Merkle Trees, Zig, Vector Databases, and we had to read a lot of documentation and learn quickly to finish on time. The integration of the front end and backend took some time."},{"heading":"Accomplishments that we're proud of","content":"We used Zig for the first time and managed to code a complicated Delta Merkle Proof quickly and correctly despite time constraints. We were able to follow a plan from ideation to submission."},{"heading":"What we learned","content":"We learned that it is important to budget your time wisely and spend time with system design."},{"heading":"What's next for zKnowledge Base:","content":"RICHER METADATA: Publication, # of Citations, etc INCENTIVIZE UPLOADS: Spread the word and get more engagement TOKENS: Distribute tokens for decentralized governance"},{"heading":"Built With","content":"chromadb css3 flask ipfs next.js pinata python together.ai zig zkp"}]},{"project_title":"EstiMate","project_url":"https://devpost.com/software/estimate-0nqzfx","tagline":"Friends Fueled Fun: Betting Made Easy and Secure","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/769/523/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Taisu Ventures: Amazing Web3 Gaming Award ($500 Cash)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Caldera: Best Use of Caldera ($1k ETH)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Ava Labs / Avalanche: Best Consumer Use Case for Blockchain ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"anvil","url":null},{"name":"apns","url":null},{"name":"caldera","url":null},{"name":"convex","url":null},{"name":"ethers","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"forge","url":"https://devpost.com/software/built-with/forge"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"oauth","url":"https://devpost.com/software/built-with/oauth"},{"name":"signinwithapple","url":null},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"swiftui","url":null},{"name":"togetherai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/underthestars-zhy/EstiMate"},{"label":"github.com","url":"https://github.com/ryantinder/treehacks-2024"},{"label":"www.figma.com","url":"https://www.figma.com/file/k6sasibE1cMsBNcvwQRvyb/App?type=design&node-id=27%3A1644&mode=design&t=shxCAvK8Vy8jImc7-1"}],"description_sections":[{"heading":"Inspiration 🌱","content":"We got this idea after noticing a common annoyance—tracking casual bets was a mess. Whether it was predicting the snow day or daring each other over who could do the craziest things, our bets were often forgotten, leaving behind a trail of \"who said what.\" We created EstiMate , an app that makes betting with friends easy and secure. Our aim is to solidify those playful moments into lasting memories.\n\nCheck out the fun video that inspired us!"},{"heading":"What it does 💡","content":"EstiMate lets you set up bets and share them with your friends, whether it's a one-on-one challenge or a wager within your group, club, or frat. After creating a bet, you can pick how long the bet lasts, how much is at stake, and which side you're on. Inviting friends is super easy – just send a link or hold your phones close to each other to share with AirDrop. EstiMate is built on blockchain, allowing for seamless, protected, and transparent transactions."},{"heading":"How we built it 🛠️","content":"In EstiMate, when a user logs in using their Apple ID , they are taken to their dashboard where all ongoing bets are displayed. After creating a bet, the frontend sends a request to our backend, which is built on Bun-powered Express , with the necessary details including userId, bet name, bet amount, and timeframe. This request is processed by our backend, which interacts with the Convex database to register the bet and retrieve the user's private key.\n\nNext, we use Solidity for writing smart contracts that handle the logic of holding bets on the Caldera chain. These smart contracts are crucial for ensuring the integrity and security of each transaction. Using Ethers , we obtain private keys from Convex to broadcast transactions from clients' accounts on the Caldera chain."},{"heading":"Challenges we ran into 🌃","content":"One of the main technical challenges we faced was managing the complexity of integrating multiple components, including iOS for the frontend, express.js and ethers.js for the backend, and on-chain technology for the blockchain functionality. Coordinating these distinct areas required careful planning and collaboration among team members specialized in each domain.\n\nAdditionally, understanding user needs and behaviors presented another hurdle. To address this, we conducted market validation surveys, interviewing over 10 individuals. This effort helped us segment our target market—college students—more effectively, allowing us to tailor EstiMate to better meet their preferences and betting habits."},{"heading":"What we learned 📝","content":"While developing EstiMate, we learned the critical importance of adopting a user-centered approach. Transforming a traditional, pen-and-paper practice into a digital social application highlighted the need to understand and prioritize user needs and behaviors — ensuring the app's design and functionality are both intuitive and engaging.\n\nAdditionally, given the diverse technological backgrounds of our team members and the variety of APIs and development environments we utilized, meshing these different elements helped to hone both our technical and teamwork skills."},{"heading":"What's next? 🚀","content":"Looking ahead, we plan to expand EstiMate's capabilities by supporting multichain blockchain technology. We aim to transform the platform into a public marketplace for campus and institution-wide betting, allowing us to leverage tokenomics within the app. Additionally, our next steps include evaluating partnerships with campus organizations like SideChat and Fizz to enhance the social aspect of creating and sharing bets."},{"heading":"Built With","content":"anvil apns caldera convex ethers express.js forge node.js oauth signinwithapple solidity swiftui togetherai"},{"heading":"Try it out","content":"github.com github.com www.figma.com"}]},{"project_title":"FediFi","project_url":"https://devpost.com/software/fedifi","tagline":"Ad platforms like Google make $10+ a click. None of that makes it to you. Fedifi: the first-ever decentralized financial infra for web3, built with automated smart contracts to cut out the middleman.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/770/266/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Taisu Ventures: Innovative DeFi Award ($500 Cash)"}],"team_members":[],"built_with":[{"name":"activitypub","url":null},{"name":"airtable","url":"https://devpost.com/software/built-with/airtable"},{"name":"bun","url":null},{"name":"bun.js","url":null},{"name":"caldera","url":null},{"name":"ethereum","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"next.js","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"ruby-on-rails","url":"https://devpost.com/software/built-with/ruby-on-rails"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"}],"external_links":[{"label":"github.com","url":"https://github.com/tonyjiang02/FediFi-Pixel"},{"label":"github.com","url":"https://github.com/tonyjiang02/FediFi-Social"},{"label":"github.com","url":"https://github.com/tonyjiang02/FediFi-Dashboard"},{"label":"github.com","url":"https://github.com/AdvitDeepak/FediFi-Controller"}],"description_sections":[{"heading":"Inspiration","content":"Current advertising platforms like Google make up to $10 a click. None of that makes it to the users. These traditional web2 advertising methods also face intense resistance by the web3 community, and thus monetization methods should be reimagined to fit the new decentralized format of social media.\n\nPlatforms like Threads, BlueSky, and Mastodon have entered into the Fediverse (decentralized social media platforms). They also have brought along with millions of users. FediFi helps ethically monetize web3 stakeholders with a user-centric and privacy focused approach."},{"heading":"What it does","content":"FediFi allows instance owners and clients to install and display monetization modules quickly, making it as simple as just 'npm install'.\n\nIn web3, users reward good content. FediFi allows users to support content and creators with donations and advertisements. Creators, Instances, and Advertisers bid to find an optimal pay structure, which is then automatically executed by a smart contract.\n\nBy using Caldera and smart contracts, users can take out their money at any time and not be bound to a biweekly payment schedule of web2 platforms. Caldera's layer-2 transaction layer also acts as protection against gas price spikes. 100% of revenue goes towards the intended recipient.\n\nInstance moderators can also according choose which ads that best fit their community.\n\nFurther web3 social monetization features will be added such as super-likes, creator shops, and more.\n\nUsers can view dashboards to see advertising and post view analytics. Developers can access our documentation to and landing page to answer any questions they may have."},{"heading":"How we built it","content":"Our tooling consists of many interconnected components:\n\nFediFi-Module. A React component developers can easily add to their platforms. Integrate with any instance with npm install. Resembles Google Ad’s Pixel*, and enables creation of autonomous smart contracts to manage transactions. FediFi-Demo. Sample client with FediFi-Module installed. We chose to build a proof of concept on top of Mastodon, but it could be anything. FediFi-Dashboard. Custom Next.js webpage; A one-stop-shop for both advertisers and creators to view advertising, traffic analytics, bidding, and transactions in real time. FediFi-Controller. Bun.js backend with SQLite database. Automatically writes and deploys Smart Contracts to the Caldera chain. Boosted with Caldera's rollups. FediFi-LandingPage. Deployed with Vercel to show documentation and context behind the creation of FetiFi for instance owners to learn more."},{"heading":"Challenges we ran into/Accomplishments that we're proud of","content":"Our team is really proud to have tackled a difficult project with many complex interacting systems, while having limited exposure to the world of web3. We are proud to have learned so much about web3 technologies and the Fediverse. So much of what we worked on was either in codebases that we did not personally write, or unfamiliar technologies with little documentation.\n\nSome of the biggest challenges we faced were:\n\nDefining the initial limited feature set because there were so many different avenues that we could have gone down to implement within 36 hours. Lack of documentation with new technologies, where it was up to a lot of (educated) trial and error to finally debug and figure out what was going wrong. Complex interacting systems, as we have 4 or 5 separate components that are all communicating with each other and working together to act as a monetization system. We had to implement smart contracts, backend, frontend, analytics dashboard, and more."},{"heading":"What we learned","content":"Concretely there were a ton of new technologies that we were all exposed to such as Solidity, Bun.js, and more.\n\nHowever, we also learned a lot about planning out software systems and drawing system diagrams to make sure we didn't waste much time when we jumped in to code. We also had the opportunity to improve our communication, as everyone was working on separate components in tandem before eventually integrating our components together."},{"heading":"What's next for FediFi","content":"We have many features that we would love to implement if we were given more time. For example:\n\nDirect revenue share with fans. We could allow creators to automatically give away percentages of their revenue to people who consume their content. This could be an interesting and new way for creators to grow their audiences and give back to the community. Zero Knowledge Proofs in advertising. Currently we use context based advertising in order to preserve user privacy, but with ZK proofs we could have targeted advertising based on the users interest while still maintaining privacy. Automated Micro Payments. Our users right now need to decide for themselves when to cash out, but eventually we could support an instant deposit into their account per view of their content. We could form a DAO as a decentralized governing entity to ensure a higher level of transparency and trust amongst the community, and allow the community to help make decisions for the future of FetiFi."},{"heading":"Built With","content":"activitypub airtable bun bun.js caldera ethereum javascript next.js node.js python ruby-on-rails solidity sqlite"},{"heading":"Try it out","content":"github.com github.com github.com github.com"}]},{"project_title":"MemoryLane","project_url":"https://devpost.com/software/memorylane-5aon4r","tagline":"Brighten your golden years with MemoryLane! Activate the mind, connect hearts, and revive cherished memories—all with just a tap. Bond over shared stories and sharpen your mind, effortlessly.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/772/722/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"InterSystems: Best Use of GenAI using InterSystems IRIS Vector Search ($2k Cash [1st] & $1.5k Cash [2nd] & $1k Cash [3rd])"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"intersystems","url":null},{"name":"iris","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex.dev","url":null},{"name":"together.ai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/gracelliu/TreeHacksX"}],"description_sections":[{"heading":"Inspiration","content":"In many developed countries across the world, the population is rapidly aging. This poses a variety of issues to senior citizens, including social isolation, an overburdened healthcare system unable to meet their needs, and the widespread effects of neurodegenerative conditions. We aimed to build a solution which would address all three of these issues in a way which is easily accessible and empowering to senior citizens."},{"heading":"What it does","content":"MemoryLane allows senior citizens to relive and share their cherished memories. The web application combines three main functionalities, which include a journaling and recall feature for important memories, an AI-powered match and chat system for users to discuss their experiences which are shared with other users, and an analytics dashboard which can be used by healthcare professionals to track key indicators of neurodegenerative conditions. Overall, MemoryLane allows users to not only keep their memories fresh but also weave a tapestry of connections with others with similar life experiences."},{"heading":"How we built it","content":"In order to develop a clean and responsive front-end and versatile back-end, we used Reflex.dev to develop entirely in Python. We also used the InterSystems IRIS database to easily perform vector search as well as other database operations to support the backend functionalities required by MemoryLane. Additionally, we made use of the Together.AI inference API to generate embeddings to match users based on shared experiences, perform sentiment analysis to find trends within memory recall data, and to create sample data to test our web app with. Finally, we used Google Cloud to implement speech-to-text functionality to increase ease of access to our platform for senior citizens. The majority of our app was built with Python, with a little JavaScript."},{"heading":"Challenges we ran into","content":"As 2 of our team members had never done full-stack dev before and one was attending his first hackathon, learning the nuances of new frameworks was initially a challenge, especially getting our environments set up. We’re incredibly grateful to the supportive mentors and sponsors for helping us get unstuck when we ran into issues, which indubitably helped us build our final product."},{"heading":"Accomplishments that we're proud of","content":"We’re very proud of our clean, intuitive UI which aims to make the product as accessible as possible to our target audience, senior citizens. Additionally, we believe that MemoryLane is a truly unique product which fills a niche which hasn’t been focused on before social media for the elderly, especially in combination with its potential benefits of improving the healthcare industry by aggregating data about the elderly. Also, half of our team was able to go from near-zero web dev knowledge to familiarity with important tools and techniques, which we thought was very representative of the spirit of hackathons – coming together to meet new people and learn new things in a fast-paced creative environment."},{"heading":"What we learned","content":"Our journey with MemoryLane has been an enlightening dive into several new technologies. We harnessed the power of Reflex.dev for frontend and full stack development, explored the nuances in our data with InterSystems IRIS’s vector search on text embeddings from TogetherAI, and learned how to bring text to life with Google Cloud. Together AI has also become our ally in understanding our users' needs and narratives with natural language processing."},{"heading":"What's next for MemoryLane","content":"Looking to the horizon, we are definitely looking into expanding MemoryLane’s reach. Our roadmap includes scaling our solution and refining our data model to improve performance, and looking into business models which are sustainable and align with our mission. We envision forming partnerships with healthcare providers, memory care centers, and senior living communities. Integrating IoT could also redefine ease of use for seniors. Keeping innovation in mind, we'll dive deeper into Reflex's capabilities and explore bespoke AI models with Together AI. We aim to improve the technical aspects of our platform as well, including venturing into voice tone analysis to add another layer of emotional intelligence to our app. We believe that MemoryLane is not just a walk in the past – it's a stride into the future of senior healthcare."},{"heading":"Built With","content":"fastapi google-cloud intersystems iris javascript python reflex.dev together.ai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ephemeral","project_url":"https://devpost.com/software/invisible-me","tagline":"always helping but you never notice","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/775/062/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Together.ai: Best Use of Together API ($2.5k Credits + 1x AirPods Max)"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"together-ai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pgasawa/ephemeral"}],"description_sections":[{"heading":"Inspiration","content":"Have you ever wished you had…another you?\n\nThis thought has crossed all of our heads countless times as we find ourselves swamped in too many tasks, unable to keep up in meetings as information flies over our head, or wishing we had the feedback of a third perspective.\n\nOur goal was to build an autonomous agent that could be that person for you — an AI that learns from your interactions and proactively takes actions , provides answers , offers advice, and more, to give back your time to you."},{"heading":"What it does","content":"Ephemeral is an autonomous AI agent that interacts with the world primarily through the modality of voice . It can sit in on meetings, calls, anywhere you have your computer out.\n\nIt’s power is the ability to take what it hears and proactively carry out repetitive actions for you such as be a real-time AI assistant in meetings, draft emails directly in your Google inbox, schedule calendar events and invite attendees, search knowledge corpuses or the web for answers to questions, image generation, and more.\n\nMultiple users (in multiple languages!) can use the technology simultaneously through the server/client architecture that efficiently handles multiprocessing."},{"heading":"How we built it","content":"Languages : Python ∙ JavaScript ∙ HTML ∙ CSS\n\nFrameworks and Tools : React.js ∙ PyTorch ∙ Flask ∙ LangChain ∙ OpenAI ∙ TogetherAI ∙ Many More\n\n1. Audio to Text\n\nWe utilized OpenAI’s Whisper model and the python speech_recognition library to convert audio in real-time to text that can be used by downstream functions.\n\n2. Client → Server via Socket Connection\n\nWe use socket connections between the client and server to pass over the textual query to the server for it to determine a particular action and action parameters. The socket connections enable us to support multiprocessing as multiple clients can connect to the server simultaneously while performing concurrent logic (such as real-time, personalized agentic actions during a meeting).\n\n3. Neural Network Action Classifier\n\nWe trained a neural network from scratch to handle the multi-class classification problem that is going from text to action (or none at all). Because the agent is constantly listening, we need a way to efficiently and accurately determine if each transcribed chunk necessitates a particular action (if so, which?) or none at all (most commonly).\n\nWe generated data for this task utilizing data augmentation sources such as ChatGPT (web).\n\n4. LLM Logic: Query → Function Parameters\n\nWe use in-context learning via few-shot prompting and RAG to query the LLM for various agentic tasks. We built a RAG pipeline over the conversation history and past related, relevant meetings for context. The agentic tasks take in function parameters, which are generated by the LLM.\n\n5. Server → Client Parameters via Socket Connection\n\nWe pass back the function parameters as a JSON object from the server socket to the client.\n\n6. Client Side Handler: API Call\n\nA client side handler receives a JSON object that includes which action (if any) was chosen by the Action Planner in step 3, then passes control to the appropriate handler function which handles authorizations and makes API calls to various services such as Google’s Gmail Client, Calendar API, text-to-speech, and more.\n\n7. Client Action Notifications → File (monitored by Flask REST API)\n\nAfter the completion of each action, the client server writes the results of the action down to a file which is then read by the React Web App to display ephemeral updates on a UI, in addition to suggestions/answers/discussion questions/advice on a polling basis.\n\n8. React Web App and Ephemeral UI\n\nTo communicate updates to the user (specifically notifications and suggestions from Ephemeral), we poll the Flask API for any updates and serve it to the user via a React web app. Our app is called Ephemeral because we show information minimally yet expressively to the user, in order to promote focus in meetings."},{"heading":"Challenges we ran into","content":"We spent a significant amount of our time optimizing for lower latency, which is important for a real-time consumer-facing application. In order to do this, we created sockets to enable 2-way communication between the client(s) and the server. Then, in order to support concurrent and parallel execution, we added support for multithreading on the server-side. Choosing action spaces that can be precisely articulated enough in text such that a language model can carry out actions was a troublesome task. We went through a lot of experimentation on different tasks to figure out which would have the highest value to humans and also the highest correctness guarantee."},{"heading":"Accomplishments that we're proud of","content":"Successful integration of numerous OSS and closed source models into a working product, including Llama-70B-Chat, Mistral-7B, Stable Diffusion 2.1, OpenAI TTS, OpenAI Whisper, and more. Integration of real actions that we can see ourselves directly using was very cool to see go from a hypothetical to a reality. The potential for impact of this general workflow in various domains is not lost on us, as while the general productivity purpose stands, there are many more specific gains to be seen in fields such as digital education, telemedicine, and more!"},{"heading":"What we learned","content":"The possibility of powerful autonomous agents to supplement human workflows signals the shift of a new paradigm where more and more our imprecise language can be taken by these programs and turned into real actions on behalf of us."},{"heading":"What's next for Ephemeral","content":"An agent is only constrained by the size of the action space you give it. We think that Ephemeral has the potential to grow boundlessly as more powerful actions are integrated into its planning capabilities and it returns more of a user’s time to them."},{"heading":"Built With","content":"flask openai python pytorch react together-ai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Ekko","project_url":"https://devpost.com/software/ekko-v9wkgy","tagline":"Elevate your foreign language fluency with tailored guidance and individualized verbal conversation practice on Ekko.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/775/860/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Together.ai: Best Project Built on Open-Source AI Programs and Models ($2.5k Credits)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Bun: Best Use of Bun ($2k Cash + Interview with Bun Founder)"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"convex","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/chroline/treehacks2024.git"},{"label":"ekko-app.vercel.app","url":"https://ekko-app.vercel.app"}],"description_sections":[{"heading":"Inspiration","content":"Multilingualism is in. In today’s interconnected world, the ability to communicate in multiple languages is not only valuable, but imperative to personal and professional success. Whether it be conversing with business professionals during international commerce exchanges, preparing for standardized school language exams, serving as a journalist or diplomat on the international stage, or simply wanting to converse more fluently with your loved ones, everybody can benefit from conversing in a foreign language. However, traditional curriculum-based language learning methods rely on repetitive exercises and lack personalization, overlooking verbal fluency markers such as verbal precision, language variation, and personal goals. A key indication of an advanced speaker is the ability to engage in debates and convey subtle shades of meaning effectively, a skill that cannot be developed solely through the use of apps centered around memorization.\n\nAs a 2nd generation Mandarin and Cantonese speaker, one of our team members realized firsthand how difficult it was to maintain fluency of foreign languages in university. Additionally, while visiting her grandmother in the ICU at a reputable San Francisco hospital, another team member noticed that it was frustrating—and potentially life threatening—for non-English speaking patients to communicate their needs to care providers since these care providers are not trained conversationally since they received very rudimentary academic foreign language training. Although these bilingual care providers were technically licensed to care for non-English speaking patients, most of them who learned the foreign language as a second language were unable to demonstrate spoken proficiency and cultural awareness outside of a classroom context. These experiences all led us to develop Ekko.\n\nWhether you’re a busy professional looking to enhance your global marketability or a student aiming to broaden your cultural horizons during study abroad, Ekko enables you to access verbal language practice anytime, anywhere, offering you the personalization and flexibility to learn at your own pace and develop as a global citizen."},{"heading":"What it does","content":"Introducing Ekko : a personalized real time AI vocal chatbot that assesses your vocal language fluency.\n\nWith Ekko, you only talk about what you actually want to talk about. Once you enter your basic onboarding information into Ekko such as your learning goals and interests, the app will then prompt you to a simple user interface where you can start your conversation. After each response, Ekko will then give personalized feedback on your conversational performance by catching your errors and providing you with an ACTFL based proficiency level. Conversations and feedback are personalized to your learning goals; for example, if you are using Ekko to prepare for career oriented work purposes, Ekko would generate prompts that you’d likely encounter in the workplace and the feedback would likely be centered around making your diction more formal. Similarly, if you were simply using Ekko to converse with friends and family, conversation topics and corrections provided by the chatbot would be more casual.\n\nEkko saves your speaking errors and transfers those language specific content errors to tailor feedback to your language learning goals. For example, if you were to say “me llamo es Cole” as opposed to the correct version: “me llamo Cole” , Ekko would save that error to check in the future. Using this unique feature, Ekko also makes connections between your learning language of interest and languages you currently speak (inputted during the onboarding process), drawing parallels between the two.\n\nSimilarly, if you were to consecutively respond with singular word responses, Ekko would suggest that you vary your sentence structure to maximize the effectiveness of the conversation.\n\nUnlike pre-existing language learning applications such as Duolingo, Ekko is not based on a curriculum, meaning that you take full reign of the conversation and practice."},{"heading":"How we built it","content":"To make Ekko as capable as possible, we used a combination of many AI and machine learning technologies—most of which we had never used before.\n\nBecause Ekko’s main value proposition is its conversational aspect, it was important that conversing with the platform is as natural as possible. This included using a state-of-the-art text-to-speech model, powered by ElevenLabs, as well as speech-to-text, powered by Deepgram. The combination of these two technologies made natural conversation on Ekko a seamless experience.\n\nProcessing speed was also of utmost importance to us to make the conversations feel natural. Hence, the obvious choice for us was to power our backend using Bun. Specifically, we’re running an Elysia.js server to interface with our ML and large language models for incredibly fast performance. This strategic choice contributed to Ekko's impressive performance and responsiveness during interactions.\n\nRegarding large language models, Ekko chose to go full open-source thanks to Together.AI. We’re using the \"NousResearch/Nous-Hermes-2-Yi-34B\" model to generate responses from the AI agent, as well as \"togethercomputer/m2-bert-80M-32k-retrieval\" for text embeddings. These models were blazingly fast and out-performed the multitude of other models we tested for these purposes.\n\nTo store the data we collected, we chose to use the Convex.dev platform. We’re leveraging their database and authentication services, as well as function calling and vector database. Using Convex enabled us to build a complex platform with many simultaneous and interconnected processes in such a limited time span.\n\nIn order to classify the user’s proficiency, we built a text classification model using scikit learn. To train this model, we generated a synthetic dataset of hypothetical conversations that corresponded to specific ACTFL Proficiency guidelines using Together.AI’s \"NousResearch/Nous-Hermes-2-Yi-34B\" model. This model, hosted on GCP Vertex AI platform, enables us to specifically denote the user’s progress as they reach fluency.\n\nAltogether, Ekko's development is characterized by a comprehensive integration of state-of-the-art technologies. The emphasis on natural conversation, swift processing, open-source language models, efficient data handling through Convex.dev, and a proficiency-classifying text model collectively contribute to Ekko's prowess as an advanced conversational language learning platform powered by frontier tech."},{"heading":"Business model","content":"In regards to our business model, we initially looked into adopting a Freemium model, but ultimately steered away from that inclination due to not wanting to exacerbate accessibility issues in the edtech space. For now, we intend for all Ekko features to be free of charge, and eventually rely on community partnerships and sponsorships with relatively small organizations such as out patient clinics in order to cover costs. In the future during the reiteration phase, we also plan on hosting a donation platform to raise money for our developing team, as well as to purchase technology in underprivileged schools so that students worldwide can use Ekko. We also want to look into partnerships with larger organizations that would benefit from improved language fluency services such as hotels and universities."},{"heading":"Challenges we ran into","content":"One of the major challenges we encountered was finding an adequate fluency metric to score user responses. While percentages and other numerical metrics seemed like an obvious choice, this would also mean that the longer a user were to maintain a conversation (typically holding a longer conversation is a good thing when practicing foreign languages), the higher percent error they’d receive, thus deterring users from talking for longer periods of time. We eventually settled on the idea of using qualitative feedback based on the well-established ACTFL Language Speaking category rankings that contained specific comprehension and fluency requirements under each conversation difficulty level. The scoring would be based off the average ACTFL score of the five most recent responses provided.\n\nOur team also raised larger scale questions pertaining to stuttering and speech impediments, as many fluent speakers often naturally stutter while talking and the STT model could interpret that as lack of fluency. Moreover, usage of slang is also something that we need to look into a bit further, as the current system struggles to interpret colloquial vernacular."},{"heading":"What we learned","content":"Through developing Ekko, we learned that different languages present different challenges with TTS that we must reconsider when building past our MVP.\n\nWe also learned the major advantages that active conversation has over repetitive exercises when practicing a foreign language, as active conversation provides learners the opportunity to contextual learning in practical and authentic situations through immediate correction while repetitive exercises solely focus on reinforcing specific patterns and foundational drills and lack spontaneity of real-life language."},{"heading":"What's next for Ekko","content":"During our next iteration of Ekko, we hope to also launch a real-time typing version of our personalized chatbot that would simulate your ideal interlocutor in both content and formality. We are also looking to implement a feature that encourages users to utilize figurative language in their speech. For example, if the user were using Ekko to improve their English proficiency and they told our chatbot \" It’s raining very hard outside, ” Ekko would highlight that sentence and perhaps suggest: “ It’s raining cats and dogs ” or the more casual “ It’s pouring. \" Another feature we are looking to implement post-MVP is a time suggestion feature, as it is a useful skill to show a comprehensive understanding of the other person’s input while also keeping responses pertinent and cutting off unnecessary fluff. This feature would especially come in handy to those preparing for professional interviews.\n\nNow more on Ekko’s social component. Our team would like to develop a social component to Ekko integrating a gamified social element that allows students to build profiles, connect with other students, and compare streaks with friends. We would also consider gamifying the conversations with fun interaction challenge modes simulating Heads Up or Hot Seat. This would not only incentivize users to practice their verbal communication even more, but would also be especially helpful to those using Ekko to prepare for less formal conversational settings.\n\nIn regards to getting the word about Ekko out there, our team would launch a guerilla marketing campaign, pushing out content on all social media platforms and attending in-person hackathons and conventions to get initial feedback during beta testing.\n\nIn addition to partnerships with small clinics and healthcare providers, we would also gradually partner with more outside organizations such as university residential housing and career services, refugee councils helping young asylum seekers, and larger international corporations to implement Ekko into their daily regimens.\n\nLastly, we would also like to launch a donation platform to support the developing team and generate donations for tech for underprivileged schools so they can continue using Ekko."},{"heading":"Ethical Discussion","content":"First and foremost, the responsible use of large language models (LLMs) has engendered immense ethical debate, as they can inadvertently perpetuate representation biases from the data used to train them, thereby amplifying existing linguistic and cultural prejudice. Thus, it is imperative that developers—especially developers of language learning applications—stress the importance of cultural sensitivity, empathy, and respect for linguistic diversity in language learning communities. LLMs also pose security risks that must be mitigated through robust cybersecurity measures, and as with any web application, concerns regarding data privacy and security raise concerns about the safeguarding of personal user information within educational platforms. However, we see Ekko being a safer alternative to similar platforms such as TalkAbroad, as users are chatting verbally with our chatbot rather than on live video call with an individual they are unfamiliar with.\n\nAdditionally, another key factor in promoting inclusivity in educational technology is ensuring accessibility to technologies and devices. Since we intend on Ekko being used in underserved school communities where access to devices is not always guaranteed, in the future, we want to collect donations and sponsors to purchase devices for underprivileged schools so they can continue using Ekko.\n\nFurthermore, current speech-to-text platforms overlook individuals impacted by speech impediments, creating potential barriers to participation. Through rounds of reiteration and beta testing, we hope to eventually develop a version of Ekko that accounts for individuals with speech and learning disabilities.\n\nAdditionally, the development of Ekko could financially impact those who rely on virtual conversation exchange provider services such as TalkAbroad for supplementary income.\n\nLastly, monetization strategies and pricing models such as the popular Freemium model can exacerbate educational inequities and access. Though our team has collectively decided to offer all our services free of charge, that does put us at a stalemate when discussing how we will monetize. Although the potential positive impact of Ekko is rife, it is still crucial that we are diligent in navigating these complexities. It is imperative for us to address these issues conscientiously, ensuring that educational language fluency technologies remain accessible, equitable, and respectful of diverse linguistic and cultural backgrounds."},{"heading":"Citations","content":"https://arxiv.org/pdf/2307.06435.pdf"},{"heading":"Built With","content":"bun convex react typescript"},{"heading":"Try it out","content":"github.com ekko-app.vercel.app"}]},{"project_title":"Memory Playground","project_url":"https://devpost.com/software/memory-playground","tagline":"Build a digital Memory Playground to help boost memory recall and retention.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/771/464/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Intel: Best Use of Intel Developer Cloud ($10k Credits + 4x Lenovo AI PC [1st] & $4k Credits [2nd] & $2k Credits [3rd])"}],"team_members":[],"built_with":[{"name":"chatgpt","url":null},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"togetherai","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"According to the Alzheimer’s Association, 6.7 million Americans age 65 and older are living with Alzheimer's in 2023. Alzheimer’s is a gradually progressive brain disorder involving memory loss. It is the most common form of dementia; people forget who their loved ones are and cannot carry out daily tasks anymore. Alzheimer’s and dementia is not only a personal health crisis but impacts family, friends, and caregivers. It is important to focus on the prevention and slow progression of symptoms related to memory loss. How can we use the Memory Palace – a psychology-based technique where people can associate mnemonic images in their mind to places they know – to help prevent and ease the lives of those with Alzheimer’s and dementia?\n\nThere is a lot of psychology research focused on memory and learning that can be used to guide technical applications and enhance memory performance. Our team is very interested in memory and learning mechanisms, which have inspired our idea to use scientific background to improve memory."},{"heading":"What it does","content":"Memory Playground is a web application that helps boost memory recall and retention through the Memory Palace technique, especially for senior citizens and those with Alzheimer’s and dementia. The application allows users to pick a setting/environment and list out words that are related. Then, we create broad yet distinct categories for the words. From here, we have users practice classifying objects, allowing them to create visual mappings of images physically and mentally. This allows them to strengthen memory connections and enhance memory performance. We also give them other words that fall into those categories to expand on the established mental connections.\n\nMemory Playground also uses an integration of zero-knowledge proofs. It stores uploaded data securely on servers and allows users to anonymously interact with the application without revealing personally identifiable information, which is ideal for those concerned about privacy."},{"heading":"How we built it","content":"We used the OpenAI API to prompt their GPT-4 model for category groupings and new objects. Then we used Together.AI's stable diffusion model for image generation. From there we connected the Python components to the web side using Fetch API. We built the web application with React, HTML, CSS, and JS. We used Flask to integrate the Python backend with the frontend."},{"heading":"Challenges we ran into","content":"1) Working with multiple servers 2) Integrating Flask with React 3) Learning to use multiple APIs to integrate various services 4) Implementing drag and drop functionality using use states 5) Limited credit and GPU usage 6) Utilizing multimodal machine learning models"},{"heading":"Accomplishments that we're proud of","content":"We are proud of how we efficiently and quickly were able to understand and implement new technologies and concepts. We were new to Flask and a lot of the recent AI technology. We are also proud of how we worked as a team, ideation stages to product creation. Additionally, we are proud of how we were able to integrate multiple varying technologies."},{"heading":"What we learned","content":"We developed a lot of technical skills involving using APIs, prompt engineering, model optimization, Flask, managing multiple server applications, and web development. We also learned a lot about the practical applications of AI in healthcare towards a potential treatment of Alzheimer's and dementia. It is critical for us as a society to consider how we can prevent , not just treat, such diseases. We also learned a lot about the engineering design process. Throughout the hackathon we went through research, ideation, designing, prototyping, and building phases. We gained a lot of skills through this process that helped us as we adapted to new technologies and grew our knowledge base."},{"heading":"What's next for Memory Playground","content":"1) Scaling: We are looking to make our application public and available to the community. This would involve cloud data storage (ex. Firestore) and increased efficiency to manage larger requests. 2) Speech-to-text recognition: We would like to implement a speech to text recognition model so that we can utilize verbal connections and improve accessibility."},{"heading":"Built With","content":"chatgpt openai react togetherai"}]},{"project_title":"Kaleido","project_url":"https://devpost.com/software/insight-critter","tagline":"Kaleido illuminates the colorful spectrum of bias in the news, like peering through a kaleidoscope.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/773/064/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"InterSystems: Best Use of GenAI using InterSystems IRIS Vector Search ($2k Cash [1st] & $1.5k Cash [2nd] & $1k Cash [3rd])"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"elysia","url":null},{"name":"elysiajs","url":null},{"name":"intersystems","url":null},{"name":"iris","url":null},{"name":"plasmo","url":null},{"name":"pnpm","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/austinchen12/insight"}],"description_sections":[{"heading":"Inspiration","content":"In a world saturated with information and misinformation, the upcoming 2024 presidential election stands as a testament to our need for clarity and truth. This is where Kaleido steps in. Inspired by the vision of bringing light to the unseen, our mission goes beyond merely summarizing news content. We dive deeper, revealing not just what you're reading, but also what you're missing out on. In addition, by assessing bias and sentiment across related articles, Kaleido provides a fuller, more balanced view of every story.\n\nOur approach is grounded in the belief that understanding the full spectrum of information is crucial in navigating the complexities of today's world. Kaleido is not just a tool; it's a movement towards informed, critical thinking and a beacon for those who seek to understand beyond the surface."},{"heading":"What Kaleido Does","content":"Kaleido is a Chrome extension designed to seamlessly integrate into your browsing and reading experience. Kaleido works in the background as you explore news articles, leveraging vector embedding search technology to analyze the content of your current article. After identifying similar articles discovered by other users through embeddings, Kaleido offers a unique comparative analysis of the article at hand.\n\nThe core functionality of Kaleido is twofold:\n\nComparative Analysis : Kaleido enables users to compare the bias and sentiment of the current article with a wide array of other similar articles identified by its vector embedding search. This feature allows for an in-depth understanding of where the article stands, in bias and sentiment, within a broader spectrum of perspectives and analyses. Idea Aggregation and Analysis : The second key feature distills the essence of an article into multiple points or ideas, and each point is then embedded in vector space. This process constructs a vast network or a \"superset\" of ideas shared across articles, identified as clusters of similar thoughts in the vector-embedded space. Through this approach of embedding points across articles, Kaleido surfaces significant, overarching takeaways from these groups of articles, offering a comprehensive view that goes beyond the surface level.\n\nAdditionally, Kaleido aggregates crucial insights by comparing the focal points of an article against others, enriching the user's understanding with enhanced data on embeddings, sentiment, and bias. This not only broadens the perspective of readers but also deepens their engagement with content, fostering a more informed and critical approach to information consumption."},{"heading":"Why We Built Kaleido","content":"Since our team's convergence at TreeHacks, the driving force behind Kaleido has been a deep-seated interest in education and politics, with a spotlight on the impending 2024 election. This particular moment in time underscores the paramount importance of clarity, truth, and context in media consumption. Our decision to bring Kaleido to life stemmed from a desire to make a meaningful impact within the media space, guided by the principle that the quality of input fundamentally shapes the quality of output.\n\nIn crafting Kaleido, our ambition was to elevate the caliber and context of the information we consume. We recognized that in an era where information is both weapon and tool, enhancing the input quality—by providing a more nuanced, comprehensive understanding of the news—can significantly influence our perceptions, decisions, and, ultimately, our societal output. Our hope with Kaleido was to optimize this input process, fostering a more informed, discerning, and engaged populace, especially in the context of critical events like elections, where the stakes are incredibly high."},{"heading":"How We Built Kaleido","content":"From the start, we knew that building Kaleido would be a complex adventure with a lot of moving parts. In fact, the project ended up being particularly complicated, especially with the employment of multiple embeddings and clustering. However, although Kaleido embeddings seem particularly scary, we were very lucky to have sponsored technology that significantly streamlined our development process.\n\nUnified Database Solution: Perhaps one of the most significant achievements in our development process was the integration of a vector database that allowed us to consolidate our data storage needs into a single, versatile database, provided by InterSystems IRIS Database. This eliminated the necessity for multiple databases, streamlining our data management and enhancing the efficiency of our operations.\n\nInterSystem's multi-paradigm database capabilities allowed us to not only store vector information efficiently, but also to leverage SQL for our database needs. This dual use case was crucial for managing the vector data for our embedding and clustering functionalities. The ability for us to conduct vector embedding search and store data in the same database was an advantage whose importance could not be understated.\n\nFront-End and Back-End Synergy: The architectural foundation of Kaleido was a harmonious blend of front-end and back-end technologies. Our Chrome extension, developed using Plasma and React, was intricately designed to offer a user-friendly interface and a responsive user experience. The backend, powered by Bun, facilitated a REST API in a monorepo setup, enabling seamless type sharing with the front-end and ensuring a cohesive development environment.\n\nAPI Development: To facilitate seamless communication between our Chrome extension and the backend, we opted for Bun with Elysia. Elysia is a highly ergonomic web framework, with an API like express, for building backend servers, but specifically designed to run in the Bun runtime.\n\nWhile Bun, unfortunately, does not fully support a Turborepo monorepo setup, we actually were able to still use Bun for everything else (because Bun is both a package manager and runtime). By using pnpm as a package manager, we realized we could still use Bun as a runtime. This choice proved instrumental in developing a high-performance, type-safe API. Elysia, being a drop-in replacement for Express, offered us the ease of building our API with enhanced efficiency and reliability, ensuring our backend infrastructure was robust and scalable."},{"heading":"Challenges we ran into","content":"From our initial ideation, we realized quickly that our application was nowhere near as simple as we initially thought and we brainstormed a lot to think and address these complexities before coding.\n\nOne significant hurdle was the process of acquiring and scraping articles, a task made increasingly difficult by the recent rise of anti-scraping measures implemented by many websites.\n\nHowever, the true test of our resolve was the spontaneous decision to incorporate clustering into Kaleido. This idea, born from the brainstorming sessions aimed at addressing our project's growing complexity, introduced a new level of challenge since nobody on our team had any familiarity with how to do it. Achieving proficiency required not just technical acumen but a willingness to dive into uncharted waters."},{"heading":"Accomplishments that we're proud of","content":"Similar to the previous sections about the challenges we ran into, we're quite proud of the challenges that we have overcome. Particularly, during our brainstorming session, we went from a half-baked idea with very little development into one which had many moving parts and complexity, and all thanks to a few hours of brainstorming that we set aside at the very beginning of the weekend. In addition, we are particularly happy about how we had spontaneously stumbled upon the idea of clustering—until then, nobody in our group had done it before, but it just felt like the right word describing what we wanted to do. Finally, we're very happy with how the logo and the user interface turned out. It turned out to be far better than we ever would have imagined."},{"heading":"What we learned","content":"The importance of brainstorming and thinking things through before implementation. Although our team was formed relatively late, we were able to avoid a lot of bumps by taking an hour at the beginning to think and brainstorm and concretely write down our implementations before coding them.\n\nAdditionally, we really enjoyed engaging with new technologies and learning how to cluster!"},{"heading":"Conclusion","content":"Developing Kaleido was a pleasure, and our team really enjoyed working together, learning new technologies, and making new friends.\n\nNow, we hope you will join us in our quest to illuminate the unseen, enhance your reading experience, and equip you with the insights needed to face the misinformation challenge head-on. Together, let's create a more informed society ready to make educated decisions for the future."},{"heading":"Built With","content":"bun elysia elysiajs intersystems iris plasmo pnpm python react tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MediFlow","project_url":"https://devpost.com/software/mediflow","tagline":"Automating healthcare's tedious administrative tasks to save hospitals $265.6 billion yearly and prevent the closure of over 30% of rural hospitals in America, allowing them to focus on patient care.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/774/166/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Convex: Best Use of Convex Features ($1.25k Cash + 6mo Pro [1st] & $500 Cash + 3mo Pro [2nd] & $250 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"clerk","url":"https://devpost.com/software/built-with/clerk"},{"name":"convex","url":null},{"name":"monsterapi","url":null},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null},{"name":"whispir","url":"https://devpost.com/software/built-with/whispir"}],"external_links":[{"label":"mediflow.tech","url":"http://mediflow.tech"}],"description_sections":[{"heading":"Inspiration","content":"We have firsthand knowledge from our close family and friends who are doctors serving in rural communities across America. They have shared with us their experiences of burnout due to the extensive manual back-office tasks they are required to handle, such as patient pre-authorization claims (PAs), in addition to their primary responsibility of serving their patients. Upon conducting further research, we have discovered that this is not just an isolated issue within our circle but rather a systemic problem that plagues the entire hospital industry.\n\nToday, administrative complexity is costing the healthcare industry a staggering $265.6 billion annually. Furthermore, over 30% of the country's rural hospitals are at risk of closure due to financial instability, with a projected increase of 16% in the year 2023. If these hospitals close down, over ~57M Americans will be without care. Moreover, according to an AMA 2022 Physician Survey, 88% of physicians say burnout associated with PA is extremely high, and 34% report that PAs led to serious adverse events for patient care due to insurance rejections preventing needed care."},{"heading":"What it does","content":"MediFlow automates back-office administrative work for hospitals, focusing on workflow experience and exacerbated in rural hospital environments. MediFlow seeks to be both a core system of record for electronic health records and a vertical AI tool that leverages proprietary hospital data to enhance. In this vein, MediFlow currently offers tools to automate 3 hospital administration processes. Prior Authorization. MediFlow can shorten the process of requesting prior authorization from insurance companies, i.e. approval for treatment/pharmaceutical coverage, from weeks to minutes by reducing the error rate of this manual process. By taking in information from the EHR and using LLMs to generate likely treatment codes, a process that normally takes staffers 45 minutes to even complete can be done in <3 minutes. Patient-Meeting Summaries. Using OpenAI’s Whisper, MediFlow can summarize doctor meetings and store summaries in an object store for future use when initiating prior authorization requests. Patient Intake and Client Relationship Management. MediFlow enables easy patient intake for new and recurring patients as well as a top-down view of a hospital’s clientele."},{"heading":"How we built it","content":"We used Convex as our database and its serverless functions to build the backend for our app. We finetuned llama’s 13bn parameter LLM model with 1,200 examples of doctor-patient summaries with MonsterAPI’s ML fine-tuning suite. We also used MonsterAPI’s hosted Whisper endpoint for voice transcription. To handle unstructured data for prior authorization, we used OpenAI’s GPT3 API and prompt engineering to retrieve siloed information from noisy data. We used React and TypeScript in a node environment to build out our friend end with Turbo as a tool to manage our mono repo. Finally, we used Clerk for user authentication and log in our platform."},{"heading":"Challenges we ran into","content":"Defining the specific products we wanted to build was challenging as the medical space was large and something we were unfamiliar with. We ran into issues configuring our convex environment initially and found working with different insurance health codes extremely difficult and confusing. Additionally, it was difficult to find datasets to fine-tune our models and ensure high accuracy in our highly-context dependent environment."},{"heading":"Accomplishments that we're proud of","content":"We’re proud of building out multiple features instead of the initial tool we set out to build. We’re proud of building a multi-modal AI solution on top of a traditional enterprise software platform and effectively resolving a challenge faced by rural hospitals."},{"heading":"What we learned","content":"We learned how to fine-tune models, how voice diffusion and transformer models work, and the medical workflow process for doctors and rural hospitals."},{"heading":"What's next for MediFlow","content":"Convex Cron functions Go multiproduct"},{"heading":"Built With","content":"clerk convex monsterapi openai react typescript vercel whispir"},{"heading":"Try it out","content":"mediflow.tech"}]},{"project_title":"proofpix","project_url":"https://devpost.com/software/proofpix","tagline":"attested image ZK library + verified image iPhone app","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/769/253/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"QED Protocol: Best Application of Zero Knowledge Proofs ($5k Cash)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Caldera: Most Creative On-Chain Hack ($2.5k ETH)"}],"team_members":[],"built_with":[{"name":"qed","url":null},{"name":"rust","url":"https://devpost.com/software/built-with/rust"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"sp1","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"zero-knowledge","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/kayleegeorge/attested-images"},{"label":"github.com","url":"https://github.com/Sofianel5/ProofPix"}],"description_sections":[{"heading":"Inspiration","content":"In the past, trust in the digital world was simple: a picture was a snapshot of reality. Today, that's no longer true. Generative AI is completely disrupting our relationship with authentic content. In today's digital world, anything is possible: from dogs hosting podcasts on mountains and the pope wearing puffers . Just this past week, we've seen the hyper realistic videos Sora is capable of producing; people are already falling victim to deepfakes . Soon, it will be impossible to distinguish what is reality and what is fiction.\n\nIt's time to separate what's real from what's fake. We need a solution that is practical, uncensorable, and unforgeable. That's where the the power of zero-knowledge and blockchains come in. Cryptography is the only way to ensure that the content we see in the digital world is real. We want to live in a future rooted in truth , not trust .\n\nBut this is a difficult task. Signing images with a cryptographic signature is a promising way to verify that content is taken from a camera and not created by AI: Cameras have a private key in their hardware that signs an image when it is taken. However, there are only a few cameras on the market with the necessary hardware to support these cryptographic signatures — not to mention that they cost upwards of $9,000 . Furthermore, the signature becomes invalid when the photo is transformed in anyway. This is a huge problem for media organizations who want to crop an image, photographers who want to prove the veracity of their pictures, sites that need to compress image files, or just everyday people who want to brighten their portrait a touch. Finally, it is currently impossible for any individual to simply verify the authenticity of an image, as the public keys signing images are not in a publicly accessible space."},{"heading":"What it does","content":"We built a more practical way to verify that an image was taken by a real camera (and not AI-generated), using next-generation cryptography to create the first end-to-end verifiable image pipeline. From the point at which an image is captured on your iphone, through any image edits, and finally when the image is posted somewhere in the digital world, anyone can verify that the image is real using the power of cryptographic signatures and zero-knowledge proofs. These cryptographic signatures serve as certificates that content is authentic, i.e. that an image was taken by a camera, and cannot be forged.\n\n*We have included a \"Crypto FAQ\" down below to define cryptography terms."},{"heading":"How we built it","content":"The three main components of our project were verifiable image capture, image transformation, and the image's existence in the world.\n\n(1) Image Capture : We created a POC iphone app that uses Apple's Secure Enclave Storage API to generate a private key to cryptographically sign images taken with the iphone, which serves as an attestation that the image was taken by a hardware device. This private key cannot be extracted from the Secure Enclave so it is the most secure way to verify an image's authenticity. Our app then uploads the image, the signature, and any transformations to our ZK prover node to compute and store a zero-knowledge proof of verifiability. We store all registered iphone public keys in a merkle tree and post a merkle root of these public keys so that anyone can verify the authenticity of the image source without revealing which device actually took the image.\n\n(2) Image Transformation : We created an image editor library that verifies the initial signature as well as allows the authenticity of an image to be verified even through (reasonable) edits. We use the power of zero-knowledge cryptography to allow the image authenticity to be verified without revealing the actual edits performed / having to replicate them yourself. We created the first-of-its-kind Rust library that performs the image transformations in the state-of-the-art zkVM (SP1) and produces a ZK proof along with the edited image. In order to ensure that someone doesn't just edit every pixel of the image, we defined a set of reasonable image transformations (e.g. rotate, brighten, crop) that image-editor library supports. We also verify the image's ECDSA signature in the circuit before applying transformations.\n\n(3) Image Existence : When an image is uploaded to the internet, it includes metadata that contains the image signature. To ensure that both the iphone that took the image is verifiable (to ensure that a camera took the image, not AI-generated) and that the ZK proof outputted by the image transformations is verifiable, we store a merkle tree of registered iphone public keys and the ZK proofs on-chain. Blockchains are quite important as a storage mechanism here because they are transparent, uncensorable, and decentralized. Anyone can verify the published signatures /proofs and no one can tamper with them (decentralization is important in the presence of adversarial centralized parties for example)."},{"heading":"Challenges we ran into","content":"(1) Apple hardware: The only way to truly securely authenticate a public key is from an iPhone is for Apple to store a private key in the hardware's secure enclave and publish a Merkle root of iPhone public keys to a public database. However, Apple does not do these things. This is why we used Apple's secure storage API to generate a private signing key and uploaded the public key onto the blockchain. However, Apple does authenticate code that runs on the iPhone using its private keys so we do have some security guarantees that people are unable to interfere with the protocol.\n\n(2) Rust: The ZK image verification and transformation library was difficult to build in such a short timeframe. This was our first time developing something in Rust that wasn't printing \"Hello World.\" We had to go through quite a bit of the official Rust documentation in order to build out the library, as well as deeply understand the inner-workings of the zkVM.\n\n(3) Newly released zkVM: Because the SP1 zkVM was released just this past week (Feb. 14th), we ran into a few challenges while building our image-editor Rust library. For example, for one of the Rust crates we were using, image-rs (handles image processing), there were a few functions that the crate used that wasn't supported in the zkVM, such as \"round\" and \"roundf\". Thus, we had to implement those functions ourselves and make the compiler use our local implementations of those functions.\n\nBecause SP1 and the whole ZK community is quite open source forward, we were able to add PRs to the SP1 repo that helped address some of the challenges we faced while using the zkVM / docs — which we hope will help future developers in the ZK community :)"},{"heading":"Accomplishments that we're proud of","content":"We were honestly quite surprised that we were able to create a whole ZK library in just 36 hours — as well as integrate the library into a compatible iPhone app, launch a ZK prover node, and create accompanying smart contracts to add the keys and proofs on-chain. As a team who is generally very excited about crypto, we were excited to make use of cutting-edge developer tools in the space (like SP1 and QED) and participate in the open-source dev culture.\n\nWe also didn't think it was possible to sign images taken by an iPhone (since it's dependent on the iPhone's private key), so we are really happy that we were able to find an actually cryptographically secure and verifiable solution!"},{"heading":"What we learned","content":"We had some theoretical cryptography background coming into TreeHacks, but in order to develop the image verifier library using ZK in such a short time, we decided to use the new state-of-the-art zkVM, SP1 (announced just this past week)! Since this was our first time using Rust, we learned a lot about the nitty gritties of the language but came out as proud Rustaceans.\n\nThis whole experience has made us very hopeful about the future of verifiable computation and the open source ZK community. A big thank you to SP1 and QED protocol specifically for their contributions to the ZK developer space!!"},{"heading":"What's next","content":"Verifiability, transparency, and security are of utmost importance in today's digital world. We hope that our project is a call to action for companies like Apple to add hardware that support signatures on images (and eventually video). We are excited to see what is next for attested images with zero-knowledge proofs! We have already made the crate available for the open source community."},{"heading":"Crypto FAQ","content":"Q: What is a cryptographic signature? A: Cryptographic digital signatures use public key algorithms to provide data integrity. When you sign data with a digital signature, someone else can verify the signature, and can prove that the data originated from you and was not altered after you signed it.\n\nQ: What is a public key and a private key? A: A private-public key pair is a pair of cryptographic keys assigned to an entity where the private key is not known to anyone and the public key is known to everyone. The private key is used to sign data to produce a digital signature and the public key is used to verify the signature.\n\nQ: What is a zero-knowledge proof? A: A zero-knowledge proof is a way to mathematically prove a claim in constant time without revealing any evidence for that claim. In our case, we prove that an edited image is authentic without revealing the initial image or the edits. Zero knowledge proofs form the basis for new blockchain scaling technology (zk rollups) such as QED protocol.\n\nQ: What is a merkle tree? A: A merkle tree is a data structure frequently used in cryptography to prove that an element is part of a set without revealing the contents of the set (via a merkle proof). This is encoded in a merkle root, which is cheap to publish to the blockchain."},{"heading":"Built With","content":"qed rust solidity sp1 swift zero-knowledge"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"Flipside","project_url":"https://devpost.com/software/flipside-rz9i1v","tagline":"News reimagined: Swipe to see every side of the story.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/769/560/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"InterSystems: Best Use of GenAI using InterSystems IRIS Vector Search ($2k Cash [1st] & $1.5k Cash [2nd] & $1k Cash [3rd])"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"deep-learning","url":null},{"name":"fetch.ai","url":null},{"name":"fine-tuning","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"huggingface","url":null},{"name":"intersystems","url":null},{"name":"iris","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"large-language-model","url":null},{"name":"llama-index","url":null},{"name":"newsapi","url":null},{"name":"newspaper3k","url":null},{"name":"openai","url":null},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"sqlalchemy","url":"https://devpost.com/software/built-with/sqlalchemy"},{"name":"together.ai","url":null},{"name":"twitter","url":"https://devpost.com/software/built-with/twitter"}],"external_links":[{"label":"github.com","url":"https://github.com/robbym-dev/Flipside-AI"}],"description_sections":[{"heading":"Inspiration","content":"The way we consume news is often one-dimensional, confined within our own echo chambers without even knowing how to find opposing viewpoints. Don't you want to know what the other side thinks?\n\nAs bot-infested and hate-speech-filled social media platforms become primary news sources, students fall victim to misinformation and hidden agendas. We must prevent the spread of extremism to our generation by providing students with a safe space to stay informed.\n\nOur mission is not just to redefine the news experience, but to bring people together by fostering understanding and discourse between polarized groups."},{"heading":"What Flipside does","content":"Flipside breaks through echo chambers and creates a safe space for students to discover the world through multiple lenses.\n\nGlide through bite-sized news stories in a sleek interface. Start with an AI-generated, unbiased article, then swipe right to explore contrasting viewpoints sourced from real people on online forums, Twitter posts and YouTube comment sections. If the story captures your imagination, enter the 'Rabbithole' and deepen your understanding through a guided conversation with our purpose-built AI agent that provides personalized and safe answers."},{"heading":"Key Features","content":"1. Bite-sized News: Swipe up and down to scroll through story summaries, swipe left/right to dig deeper into a story that catches your eye. Kinda like Tinder 👀\n\n2. Explore Narratives: Read thoughts, reactions and opinions on any story sourced from Reddit, Twitter and YouTube. A place for every perspective.\n\n3. Enter The Rabbithole: Ever been down a Wikipedia rabbit hole? Yeah, it's pretty addicting. Our rabbit hole features an AI-generated wiki fine-tuned on the topic with a trove of real-time information on the news story. You can also chat with a personalized AI agent to learn more.\n\n4. Subscribe to Stories, not Sources: Keep yourself out of the echo chamber by following news stories instead of sources. As soon as there's an update to your favorite news story, we'll send you a notification."},{"heading":"How we built it","content":"1. Automatic Data Curation: Our Mailman Agent This agent is the crux of our app. It’s tasked to periodically fetch latest news to keep our app on top of the headlines while also notifying users if they might be interested in the update. Our Fetch AI agent uses time and geolocation to fetch the top headlines from the NewsAPI and process it through our extensive Together.ai GenAI content generation pipeline and publish it on our app. During this process, it uses llama_index from InterSystems IRIS to generate keywords through a RAG model, fed into Twitter Search API and webscrapers to extract relevant discussions. All data is automatically stored on and retrieved from Firebase Firestore.\n\n2. Flipside Mobile App We build our platform using React Native and Swift for iOS. We used complex gesture handling to enable an intuitive and addictive swipe & scroll based user experience. We set up a Flask server to communicate with the various systems deployed. On top of this, we built a beautiful UI to ensure user experience was exactly how we imagined it.\n\n3. Rabbithole Mode This agent provides the seamless replies in the Rabbithole mode. The agent is tasked to complete a whole AI pipeline to curate the best reply semantically to the context and accurately to the query. We utilized Fetch.ai’s versatile AI agent framework in Python to retrieve requests from our React Native app containing our query and a Together AI fine-tuned model. This is further processed using llama_index integration with the IRIS container by InterSystems for a RAG model powered by our context file and query intertwined with an engineered prompt. We utilize certain criteria such as query relevance, frequency, distinctiveness and contextual relevance. These keywords are fed into NewsAPI to retrieve the most similar articles which are added as context to generate valid reliable information.\n\n4. Vector Search We utilize a semantic SQL vector search powered by InterSystems at the heart of our prediction tasks throughout the app. There were two main tasks with this foundation:\n\n1. Subscriptions : We maintain a database of subscribed articles of each user. We take in a new article fetched by our Mailman Agent powered by Fetch.ai and generate embeddings of the fetched article and all subscribed articles using HuggingFace’s all-mpnet-base-v2. We then calculate a normalized vector dot product, sorted in decreasing order, and choose the top 5 users above a 0.75 threshold. This informs us that these users are most-likely to be interested in the new updates and we send them a notification.\n\n2. Flipside Companion : We utilize the same process in our Chrome Extension to give a bit-sized summary of the article while providing the different narratives of the current article by choosing the top two articles from our database, ordered by a vector dot product (SQL semantic search) by InterSystems of the current article and our database.\n\n5. Prompt Engineering: The Magic"},{"heading":"Challenges we ran into","content":"Jaisal: AI agents, vector databases, and the world's most powerful LLMs; one would assume that swiping would be the easiest feature to implement - wrong. Why React why? Why did this take 8 hours to build?\n\nRobby: I spent quite a while trying to get an LLM to respond to my prompts accurately. In my exasperation, I asked it to take a deep breath and then it started working!\n\nAyaan: Found a weird bug while implementing the InterSystems IRIS SQL Search and spent 4 hours with the InterSystems team to figure out this never-seen-before bug!\n\nAnant: Going from a sleep-deprived midterm week to a sleep-deprived hackathon weekend :)"},{"heading":"Accomplishments that we're proud of","content":"Ayaan: I'm so proud of Robby for taking ownership of some of the most tedious and technically difficult tasks we had, plus for being a fantastic group DJ.\n\nRobby: I'm so proud of Ayaan for debugging one of our most complex modules, and as we hit enter in the terminal to test it...his laptop died!\n\nJaisal: I'm so proud of Anant for coming up with great ideas for Flipside's features, combined with some really catchy names to make our project memorable.\n\nAnant: I'm so proud of Jaisal for managing to build out the entire UI, creating the technical outline for our app and doing so much more while helping the rest of us through our first hackathon. Truly incredible."},{"heading":"What we learned","content":"36 hours is a LOT of time wow... We actually built one of our most complex projects yet - even calling it full stack won't cut it.\n\nWe learnt the important balance of compromise: with dozens of ideas we had to prioritize the ones which were important (healthy arguments!), settle for imperfect implementations, and think about the tech, the business, the moral implications, and most importantly, the user experience. These are all skills that we don't get the practice in our daily lives. So, Treehacks was the perfect place to get exposed to what we would say is a more realistic representation of the real world.\n\nAnd we developed newfound appreciation for each other. The heart-to-heart conversations you have on 2 hours of sleep go a long way."},{"heading":"What's next for Flipside","content":"News on the go : We already have AI generated images (did you notice? :P). Next it's time for AI podcasts, weekly summaries and rabbithole-y stories - all automatically and perfectly generated.\n\nHighlight and Reply: A lot of users are refering to a small piece of text within the article when the make comments. Why not add the option to quote pieces of text? See which parts are interesting to others. Kind of like the YouTube \"Most Watched Part\" feature."},{"heading":"Built With","content":"css deep-learning fetch.ai fine-tuning firebase flask html huggingface intersystems iris javascript large-language-model llama-index newsapi newspaper3k openai pandas python react-native sqlalchemy together.ai twitter"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"VeriVote","project_url":"https://devpost.com/software/verivote-t42xoi","tagline":"Empowering democracy through QED Zero-Knowledge Proofs in order to improve, secure, and bring transparency one vote at a time!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/774/145/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"QED Protocol: Best Application of Zero Knowledge Proofs ($5k Cash)"}],"team_members":[],"built_with":[{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rust","url":"https://devpost.com/software/built-with/rust"}],"external_links":[{"label":"github.com","url":"https://github.com/certifiedp/VeriVote"}],"description_sections":[{"heading":"Inspiration","content":"Voting systems fail to adopt new technology and address systematic concerns. Blockchain, having the ability to create transparency and decentralize centralized powers, seemed like a plausible solution to this issue. Seeing the QED workshop and its utilization of ZKPs, we knew that would be a great opportunity to create a scalable, decentralized, and extremely secure voting platform."},{"heading":"What it does","content":"This platform uses the QED protocol as a backend to verify state changes to proposals that are part of VeriVote using ZKPs. This means that users in the network are empowered with the ability to vote on policies and verify the legitimacy of the election/system."},{"heading":"How we built it","content":"We used the QED Protocol coded in Rust to create a series of functions that allow users to vote and propose on different policies posed on our platform. We specifically created functions on the server side that helped list proposals, vote on them, and finalize them. To help run our application, we created a Python client and found ways to execute API calls from our server."},{"heading":"Challenges we ran into","content":"Our team did not have as much exposure to ZKPs which resulted in us spending the majority of the hackathon learning and building levels of abstraction for the protocol. This was a significant challenge but through cross-collaboration and help from online resources, we were able to learn and create a product we're proud of!"},{"heading":"Accomplishments that we're proud of","content":"We're proud of creating a back-end that uses QED and works with Rust to create ZKPs with the potential for real application. Our team adapted to using a new language and learned more about a field that is constantly developing."},{"heading":"What we learned","content":"Our team learned more about zero-knowledge proofs, contemporary solutions that are being implemented to address the Blockchain trilemma, and how to code a robust back-end in Rust. We plan on learning and diving more into open-source projects related to ZKPs and decentralized applications."},{"heading":"What's next for VeriVote","content":"Our next steps include:\n\nAdding a delegation feature that allows people to delegate their votes to certain proposals/users in the network Implement a front end to make the application more functional/accessible (i.e. votes sidebar) Create a means of robust authentication to ensure that people on the network are unique."},{"heading":"Built With","content":"json python rust"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Spark","project_url":"https://devpost.com/software/spark-mhxso9","tagline":"Empowering Passion, Sparking Change, Inspiring Impact: Connecting Sustainable Hearts with Purposeful Projects.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/769/220/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Convex: Best Use of Convex Features ($1.25k Cash + 6mo Pro [1st] & $500 Cash + 3mo Pro [2nd] & $250 Cash [3rd])"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Stanford Ecopreneurship: Best Prototyping Process ($1k Cash)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Cotopaxi: Most Innovative Sustainability Hack (4x Allpha 35L Backpack)"}],"team_members":[],"built_with":[{"name":"ant-design","url":null},{"name":"convex","url":null},{"name":"figma","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"next.js","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"vscode","url":null}],"external_links":[{"label":"spark-neon.vercel.app","url":"https://spark-neon.vercel.app/"}],"description_sections":[{"heading":"Inspiration","content":"As we began to look at the TreeHacks 10 tracks, all team members were immediately drawn to the sustainability track. In a world with increasing temperatures, excessive greenhouse gas emissions, biodiversity loss, and pollution, among numerous other ecological challenges, we know we all have an individual responsibility to help preserve and revitalize our environment. As a result, we began brainstorming how we could individually help contribute to a more sustainable future. Our first thoughts centered around how we could encourage contributions to environmental nonprofits. Still, we struggled to name localized organizations that could impact on an individual scale.\n\nWith three of us originally from Iowa, we did a quick Google search to find potential organizations whose mission aligned with our goal and found over 20 (including 3 within 20 minutes of our hometown) around the state that could utilize resources from people in various ways. The contributions they were seeking primarily consisted of people volunteering and monetary donations. If this was the case in Iowa, we knew most other states would likely have even more available opportunities. But how could we make people aware of them? Looking at the communities of people we know, it’s clear there is no shortage of people interested in environmental sustainability. But just being passionate about an issue doesn’t lead to improvement. A streamlined way to identify tangible ways to catalyze change, though? That is what’s needed to bridge the gap between someone’s desire to make change and their ability to follow through. We realized our platform’s goal: to allow organizations to make themselves known to those people who already have a planted spark and want to help preserve their environment for future generations."},{"heading":"What it does","content":"Your spark can create change.\n\nSpark is a platform that allows environmental organizations to create a campaign outlining their mission, vision, and goals to encourage people with an existing spark who don’t know what to do with their desire to make a difference to join their projects. Our platform works in two parts. First, organizations post their campaign, which is then added to a database holding all posted campaigns. Next, contributors can browse available campaigns to find one(s) that resonate with their goals. Once they identify organizations that do so, they can identify which of the organization’s goals they are inspired to contribute to and gain spark points. These spark points work to (1) allow contributors to see the tangible impact they are having as a continuous endeavor and (2) motivate these individuals to continue their contributions with more organizations."},{"heading":"How we built it","content":"Iterations:\n\nWe started with a basic outline of listing an organization and its needs and allowing an individual to sign up to help. To explore the broader stakeholders beyond just contributors, we spoke to an Executive Director at a nonprofit local to us (someone who may make a campaign page). We learned what features would make this platform more useful for them: “Because it is so hard for nonprofits to receive funding [as the application process is often long and rarely fruitful because of the number of competitors], individual contributions go a long way,” so we made the monetary donation aspect the first built-out type of contribution with future plans to build out a page showing all volunteering opportunities Within the organization’s dashboard view, they should be able to view and manage all of their own campaigns, so we added this functionality “Nonprofits benefit greatly from being able to receive feedback from participants,” so in a future iteration, we hope to allow some form of communication between participants and the organization (if valuable and often not used, this may be required for someone to earn their spark points) We then showed our product to a hackathon mentor to gain more feedback on how to address the pain points of a potential user She suggested the usefulness of being able to “visually” observe opportunities “physically nearby.” We used this feedback and incorporated the Google Maps API to display the physical locations of the opportunities. She also noted this would remind users “how accessible” it is to make change. After speaking to friends (people who would hold a future contributor role), we added in a few more features that could make our platform better encourage people: A link to the non-profit website, if applicable, to allow for deeper learning. To encourage an easy-to-use UI (especially for those more tech-averse), we wanted to avoid a cluttered card and instead redirect contributors to the organization's website. The ability to favorite a non-profit for future engagements, which we hold as a goal for a future iteration Rotating information on our home page to serve as motivation for contributors (also not yet implemented but planned for next iteration)\n\nTechnology Used: (1) We used Next.js to build and host the front-end portion of our application. This decision allowed us to scale easily with a growing user base. Next.js is a very popular framework with a lot of open-source support that made our ability to build a website quickly. (2) We use Convex for our API and backend. We enjoyed their presentation during the opening ceremony, which convinced us to use its extensive functionality. Its lightweight nature helped us develop much more quickly than what would’ve been required with other software. (3) Ant Design is a very popular UI framework and made it easy to translate our Figma designs into our final product with a clean, modern interface. (4) Visual Studio Code + Extensions to make development environment easier"},{"heading":"What makes us different","content":"The main features of Spark that set it apart from existing services can be grouped into three main parts:\n\nThe focus on individual contributions to environmental challenges We couldn’t find any existing websites that were focused on sustainability. Many environmental organizations had their donation and/or funding pages in hard to find places and with little tangible impact associated with it. The motivation through point earning This feature is a unique motivator we didn’t find in other platforms. People like to do things when they feel like it is worthwhile. Providing an ability to track the “significance” of their cumulative – not just one-time – contributions does precisely that. Allowing individuals to see that their monetary or physical donations are tied to specific goals, not just a cause By having organizations outline why they’re asking for contributions in a certain manner, people are more aware of their individual part in these large-scale problems. A general donation fund or volunteering list is less valuable for contributing individuals."},{"heading":"Challenges we ran into","content":"Configuration and integration challenges like getting things to talk to each other, installing libraries Agreeing with design and idea choices like UI, brand, and features Working through exhaustion, stress, and frustration at times Navigating a new environment of learning and networking"},{"heading":"Accomplishments that we're proud of","content":"Our app successfully makes roundtrips! Data is rendered from the database to our front end, and we can successfully demonstrate our MVP 3/4 of our team's first hackathon project! We met new people with great ideas and enjoyed sharing them throughout the weekend We got the chance to learn and experiment with technologies new to us (Next and Convex) and were successful in making them work We had fun!!! We were a successful team and enjoyed collaborating together :D"},{"heading":"What we learned","content":"About Sustainability:\n\n(1) What do nonprofits and organizations need when looking for support?\n\nAccess to a large user base: This can be especially key for smaller organizations and the funding of their projects Passionate contributors: they are the key to spreading ideas through word of mouth, and this is our target demographic for our platform\n\n(2) How beneficial individual change can be\n\nEcological organizations have already done the research: they know what needs to be done to improve our environments. Once they’ve identified useful ways to use people, getting people to them is the new important goal. The more involved people are individually, the better equipped they are to elect representatives that can further change on a more national and even global scale. Individuals spark greater contributions.\n\nAbout Technology:\n\n(3) The web development space is constantly evolving\n\nMany tools out there are robust for scaling applications with growing users. Frameworks for the backend like Convex make spinning up a cloud server a breeze, with frameworks like Next.js ensuring that front-end applications are production-ready. It’s crucial that before starting a project, the needs of the project are evaluated to find the right tech stack to handle them. Additionally, when encountering bugs or issues, evaluate the simplest potential culprits first, as the technologies being used are well-tested and are unlikely to be the issue."},{"heading":"What's next for Spark","content":"We want to see Spark develop into a general platform for all kinds of organizations and allow them to receive support in ways not currently built into the website. While our motivation started out with achieving improvements in ecology, we learned that many nonprofits and small organizations also struggle with their day-to-day costs for small things, even as simple as needed plates or cups. Schools have underprivileged students who struggle to receive the school supplies they need. Both of these cases could be solved by having an additional contribution mode: purchasing individual items.\n\nSpark has the potential to become _ the _ platform for social good: you want to help a specific industry, that industry, and your ability to contribute is _ literally _ at your fingertips.\n\nOnce this generalization is implemented, we hope to add a recommendation feature that will allow contributors to be matched with projects that they will likely find fulfilling based on their previous interests and engagements."},{"heading":"Broader Stakeholders, Context, and Ethicality","content":"Accessibility:\n\nWhile this is hosted online, the requirements of people are nothing other than their time or money. While money can be a barrier to contributing to projects one resonates with, our platform encourages people to give their time if that is more accessible to them. While not everyone has access to the internet from their homes, they can easily access this platform from a public source (ex., library), allowing them to make individual contributions in whichever way they see as most suitable to their desire and ability.\n\nOur “add a campaign” process is extremely simple for an organization that may not have tech-savvy employees. Organizations have to provide as little information as they want to, and in a few simple clicks, they will be listed with minimal technology required. Additionally, existing organizations may see Spark as a competition. Still, Spark works to elevate the organization’s existing issues to be helped by a broader range of people looking to better their community and environment.\n\nContributor Motivations:\n\nA potential unintended consequence may be a motivation surrounding the gamification of the process through spark points and hours, but regardless of motivation, contributions are impactful. If people begin to look for ways to maximize their points or hours, they will ultimately create more change for the better.\n\nOther Considerations and Research\n\nOur largest stakeholders outside contributors are the organizations that post to Spark. By speaking with someone so involved with a non-profit, its funding, and difficulties finding volunteers and donors, we better understood the pain points of potential users on both ends of the platform.\n\nAddressing environmental issues is the responsibility of all people. For one, poor environmental conditions disproportionately harm marginalized individuals. These communities are more likely to be exposed to lead, air pollution, hazardous waste, and extreme temperatures. We have individual responsibilities to improve the state of our environment to minimize this disproportionate impact, and it starts with awareness and individual contributions. Secondly, there is a moral obligation to leave the world livable for future generations. Without intervention and prioritizing of these projects, we don’t follow this ethical duty.\n\nProjects identified by local organizations often occur where the need for better conditions is very visible: beach or park cleanups, invasive species removal, recycling, or food waste minimization, to name a few. They can also work to improve these places through projects such as tree planting or beautification in neglected neighborhoods. Improving living conditions is a social issue that can help decrease the disproportionate impacts faced by those living in areas identified by environmentally focused organizations.\n\nThere are potential ethical concerns that we also must consider with the creation of our platform:\n\nA bias in which organizations are displayed for prospective contributors. To combat this, we want to incorporate technology that cycles organizations' recommendations to individuals. Especially in larger cities, we wouldn’t want small organizations to lose their ability to gain contributors at the expense of larger organizations on name alone. The risk of greenwashing. Often, organizations looking to gain participation may falsely indicate an interest in ecological progress and then take away attention from organizations focused on improving sustainable practices. This would need to be handled delicately because if a vetting process to allow organizations to add a campaign is added, there may be bias in which types of organizations are filtered out or find the additional steps technologically challenging. Community displacement. Projects that take place in a community may displace the residents of that area. To prevent this, there may be terms and conditions requiring organizations to ensure that their projects meet specific standards that don’t cause issues in the areas they are working with. Largely, though, ecological organizations are very aware of the footprint they leave in places where they work and are careful to be considerate of these communities.\n\nSources: https://www.apha.org/Topics-and-Issues/Environmental-Health/Environmental-Justice\n\nhttps://iep.utm.edu/envi-eth/"},{"heading":"Built With","content":"ant-design convex figma javascript next.js react vscode"},{"heading":"Try it out","content":"spark-neon.vercel.app"}]},{"project_title":"LangUR","project_url":"https://devpost.com/software/polyaiglot","tagline":"LangUR: Your language learning companion. Effortless fluency with personalized articles, translations, and progress tracking. Join us today, and master languages at your leisure!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/774/318/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Vespa.ai: Best Hack using a Large Language Model (4x Bose Headphones)"}],"team_members":[],"built_with":[{"name":"astro","url":null},{"name":"bun","url":null},{"name":"chromavectordb","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gpt","url":null},{"name":"monsterapi","url":null},{"name":"postmanapi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/langur-treehacks/langur"}],"description_sections":[{"heading":"Learning languages is one thing, but what does it take to be fluent in them?","content":"LangUR is a groundbreaking project which bridges the difficult phases of learning languages. In a diverse business world with hundreds of languages being used in high commerical settings, modern applications serve to teach proficiency in langauges, but per a fluency study of Duolingo patrons learning French and Spanish, only 52.94% of 102 French learners reached a pre-intermediate level, and 66.03% of 156 Spanish speakers reached a similar level. (Jiang X, 2021)"},{"heading":"Efficiency. Scalability. Practicality.","content":"Key Features:\n\nUses a grounded evaluation metric to measure your performance (LIX scoring algorithm) Application learns and adapts to individual learner's abilities Learning of language conveniently incorporated into daily workflow Reinforced learning of queries words through review quizzes\n\nDevelopment Tools:\n\nPostmanAPI: Testing and Integrating API Endpoints Between Frontend and Backend-> enabled synchronous development by different teams Testing of MonsterAPI's inference engine-> enable quick iteration and testing of various baseline models and their associated hyperparameters Facilitated collaboration between Frontend and Backend Team with a shareable link detailing communication format Bun: Leveraging Bun as a JavaScript ecosystem for web deployment ultra fast and all in one bundler, runtime and package manager that saved us a lot of time MonsterAPI: Fine tuning the baseline models and using the inference engines easy to access deployment and fine-tuning platform greatly reduced the learning curve round-clock support by staff made development easier Chroma Vector DB: Quick and reliable retrieval of semantic lists and articles for rapid processing with the LLM Engines.\n\nTech Stack:"},{"heading":"Research","content":"The project's premise and continuity relies very heavily on social research. When Robin was learning his languages, he found that maintaining continuous performance on a daily basis aided his performance in learning a language, and such a trend is commonly correlated with higher testing proportions among students that are fed information on a consistent daily basis when studying, as demonstrated by the American Psychological Society (Mawhinney et al., 1971).\n\nMoreover, the concept of integrating language learning seamlessly into daily routines aligns with principles of habit formation and behavioral psychology. By embedding language practice within the natural flow of a user's day, LangUR capitalizes on the psychological phenomenon of habit stacking. This approach leverages existing habits as anchors for new behaviors, making language learning feel less like a burdensome task and more like an integrated aspect of daily life. Stacking habits is quintessential as evidenced for learning, so, how would one be able to consider a new approach to structuring a language learning app based off of this? Let's look back at the presented graph.\n\nIn any instance, Distributed Practice and Practice Testing appear to be the largest factors associated with higher testing in general study areas, where we based our project idea off of: a gradual but slow streamline of language implementation, albeit slowly and consistently. By being passive and seamless, LangUR has the capability to gradually streamline language learning modes into a user's daily workflow, providing an excellent UI, with a diverse array of features such as translation, suggested articles based off of past history, and progressively improving the user's ability to take in the language with the readability algorithm.\n\nWe often encounter the issue that learning a language is daunting, requiring continuous effort, where a lot of people simply don't have that time to invest, whereas the practicality of being fluent in languages has a high yield in business returns by eliminating barriers between multinational individuals and corporations. The idea behind LangUR had to critically emphasize the parallel and efficient nature of learning, which other applications failed to consider. Giving users a comfortable experience, automatically determining skill level through a customized LIX Algorithm, and structuring the application to cater to their learning pace was imperative during the building phase.\n\nIn conclusion, taking into account human study patterns, the demanding needs of learning a language and daily time constraints, LangUR was built with the mindfulness that dedication is always on the users side, but we can do our job to make it as seamless as possible, accessible, and catered to the users."},{"heading":"What's next for LangUR","content":"LangUR's potential is unbelievable. It has the ability to drive a stake in the educational world, collectivizing language learners, teachers, students, and anyone through simplicity. As the project's development continues, more functions will be added such as social integrations, connecting learners across the world, and more advanced algorithms will be put in place to create more personalized suggestions, helping people learn their new languages."},{"heading":"Built With","content":"astro bun chromavectordb flask gpt monsterapi postmanapi python tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"DermaDetect","project_url":"https://devpost.com/software/skin-ai-zlwdsy","tagline":"DermaDetect leverages AI for accessible skin health checks, focusing on affordability and privacy for underserved populations.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/773/797/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Convex: Best Use of Convex Features ($1.25k Cash + 6mo Pro [1st] & $500 Cash + 3mo Pro [2nd] & $250 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"clerk-oauth","url":null},{"name":"convex","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"framer-motion","url":null},{"name":"infobip-twillio-like","url":null},{"name":"intel-cloud","url":null},{"name":"keras","url":null},{"name":"llm","url":null},{"name":"matplotlib","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"predictionguard","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"shadcn","url":null},{"name":"sklearn","url":null},{"name":"tensorflow","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vite","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/abccodes/Treehacks2024"}],"description_sections":[{"heading":"Inspiration","content":"DermaDetect was born out of a commitment to improve healthcare equity for underrepresented and economically disadvantaged communities, including seniors, other marginalized populations, and those impacted by economic inequality.\n\nRecognizing the prohibitive costs and emotional toll of traditional skin cancer screenings, which often result in benign outcomes, we developed an open-source AI-powered application to provide preliminary skin assessments.\n\nThis innovation aims to reduce financial burdens and emotional stress, offering immediate access to health information and making early detection services more accessible to everyone, regardless of their societal status."},{"heading":"What it does","content":"AI-powered analysis: Fine-tuned Resnet50 Convolutional Neural Network classifier that predicts skin lesions as benign versus cancerous by leveraging the open-source HAM10000 dataset. Protecting patient data confidentiality: Our application uses OAuth technology (Clerk and Convex) to authenticate and verify users logging into our application, protecting patient data when users upload images and enter protected health information (PHI). Understandable and age-appropriate information: Prediction Guard LLM technology offers clear explanations of results, fostering informed decision-making for users while respecting patient data privacy. Journal entry logging: Using the Convex backend database schema allows users to make multiple journal entries, monitor their skin, and track moles over long periods. Seamless triaging: Direct connection to qualified healthcare providers eliminates unnecessary user anxiety and wait times for concerning cases."},{"heading":"How we built it","content":"Machine learning model TensorFlow, Keras: Facilitated our model training and model architecture, Python, OpenCV, Prediction Guard LLM, Intel Developer Cloud, Pandas, NumPy, Sklearn, Matplotlib\n\nFrontend TypeScript, Convex, React.js, Shadcn (Components), FramerMotion (Animated components), TailwindCSS\n\nBackend TypeScript, Convex Database & File storage, Clerk (OAuth User login authentication), Python, Flask, Vite, InfoBip (Twillio-like service)"},{"heading":"Challenges we ran into","content":"We had a lot of trouble cleaning and applying the HAM10000 skin images dataset. Due to long run times, we found it very challenging to make any progress on tuning our model and sorting the data. We eventually started splitting our dataset into smaller batches and training our model on a small amount of data before scaling up which worked around our problem. We also had a lot of trouble normalizing our data, and figuring out how to deal with a large Melanocytic nevi class imbalance. After much trial and error, we were able to correctly apply data augmentation and oversampling methods to address the class imbalance issue. One of our biggest challenges was setting up our backend Flask server. We encountered so many environment errors, and for a large portion of the time, the server was only able to run on one computer. After many Google searches, we persevered and resolved the errors."},{"heading":"Accomplishments that we're proud of","content":"We are incredibly proud of developing a working open-source, AI-powered application that democratizes access to skin cancer assessments. Tackling the technical challenges of cleaning and applying the HAM10000 skin images dataset, dealing with class imbalances, and normalizing data has been a journey of persistence and innovation. Setting up a secure and reliable backend server was another significant hurdle we overcame. The process taught us the importance of resilience and resourcefulness, as we navigated through numerous environmental errors to achieve a stable and scalable solution that protects patient data confidentiality. Integrating many technologies that were new to a lot of the team such as Clerk for authentication, Convex for user data management, Prediction Guard LLM, and Intel Developer Cloud. Extending beyond the technical domain, reflecting a deep dedication to inclusivity, education, and empowerment in healthcare."},{"heading":"What we learned","content":"Critical importance of data quality and management in AI-driven applications. The challenges we faced in cleaning and applying the HAM10000 skin images dataset underscored the need for meticulous data preprocessing to ensure AI model accuracy, reliability, and equality. How to Integrate many different new technologies such as Convex, Clerk, Flask, Intel Cloud Development, Prediction Guard LLM, and Infobip to create a seamless and secure user experience."},{"heading":"What's next for DermaDetect","content":"Finding users to foster future development and feedback. Partnering with healthcare organizations and senior communities for wider adoption. Continuously improving upon data curation, model training, and user experience through ongoing research and development."},{"heading":"Built With","content":"clerk-oauth convex flask framer-motion infobip-twillio-like intel-cloud keras llm matplotlib numpy opencv pandas predictionguard python react shadcn sklearn tensorflow typescript vite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Aros","project_url":"https://devpost.com/software/aros-prove-your-images-are-real","tagline":"Aros is an app to verify that an image is real and not AI-generated. Aros uses hardware security and cryptography to prove that an image was clicked on your iPhone.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/769/695/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"QED Protocol: Best Application of Zero Knowledge Proofs ($5k Cash)"}],"team_members":[],"built_with":[{"name":"cryptography","url":null},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next","url":null},{"name":"rust","url":"https://devpost.com/software/built-with/rust"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"zero-knowledge","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/siddhantsharma301/aros/"},{"label":"aros-dashboard.vercel.app","url":"https://aros-dashboard.vercel.app/"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1KGelkGELRfa3LHfVj24Bu3TszH5IC1EkGX0pIwa9OfY/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"With the rise of AI-generated content and DeepFakes, it's hard for people to identify what's real and what's fake. This leads to fake news and abuse. After seeing the launch of OpenAI's Sora model this week, we decided to build a solution to verify whether an image is real or AI-generated."},{"heading":"What it does","content":"Aros is an iOS app that allows you to verify that an image is real and not AI-generated . It does this by cryptographically proving that you clicked an image on your iPhone, which means that the image is real.\n\nThis is how it works:\n\nWhen you click a photo using the Aros camera app, Aros uses your iPhone's Secure Enclave to cryptographically sign this image. This signature is posted to the online Aros registry. Anyone can use this signature and your public key to verify that the photo was clicked on your iPhone, and not generated using AI.\n\nWe also built a zero-knowledge prover that verifies the signature on your image within a ZK circuit. This allows any blockchain to easily verify that an image is real."},{"heading":"How we built it","content":"This is a system architecture diagram for Aros:\n\nSecure Enclave\n\nWe create a cryptographic key pair in your iPhone's Secure Enclave to rely on hardware security and ensure that your private keys are never leaked outside your iPhone. Aros uses these keys to sign your photos to prove and verify that you clicked them on your iPhone.\n\nZero-Knowledge\n\nTo easily verify the image signatures on a blockchain, we decided to build a ZK verifier for this. We used state-of-the-art cryptographic systems like the SP1 RISC-V prover from Succinct Labs to verify the image signatures within a Plonky3 circuit .\n\niOS App and Web Registry\n\nWe built the iOS app using Swift .\n\nThe Aros registry is used to store each image's hash and signature, along with users' public keys. It doesn't store the raw image data so we can protect privacy. We built the Aros registry using Next.js, Typescript, and Tailwind CSS. We deployed the registry dashboard and registry API using Vercel ."},{"heading":"Challenges we ran into","content":"The Secure Enclave in the iPhone uses the P-256 elliptic curve but we found it hard to find a verifier ZK circuit for this curve within Circom or Halo2. So, we decided to use the SP1 RISC-V prover from Succinct Labs to verify the image signatures and generate a Plonky3 circuit. We faced challenges with base64 encoding and decoding the public key. However, we realized that we could use the base64EncodedString function in Swift to help with this."},{"heading":"Accomplishments that we're proud of","content":"It was our first time developing on iOS and using Swift , so there was a pretty steep learning curve on the first day. We're really happy that we were able to learn Swift and iOS development over the weekend and successfully build this project. It was a stretch goal for us to build a zero-knowledge verifier of the P256 signature verification. We're proud that we were able to build this, and now anyone can efficiently verify that an image is real on any blockchain as well."},{"heading":"What we learned","content":"In terms of technologies, we learned iOS development, Swift, and SwiftUI, and we also learned how to work with RISC-V ZK proving systems like the SP1 prover. We learned about hardware security, specifically how to protect private keys using the Secure Enclave on iPhones."},{"heading":"What's next for Aros","content":"We want to extend this technology beyond just images, to prove that audio and video is real and not AI-generated. We have some ideas for this and we are excited to try these out soon! We plan to deploy a verifier smart contract for the ZK circuit on Ethereum. We hope to work with social media platforms to try to integrate our system since we think fake news and images are most prevalent on social media, and Aros can help reduce misinformation online."},{"heading":"Built With","content":"cryptography ios mongodb next rust swift typescript zero-knowledge"},{"heading":"Try it out","content":"github.com aros-dashboard.vercel.app docs.google.com"}]},{"project_title":"VR-TINES <3","project_url":"https://devpost.com/software/vr-tines","tagline":"Bridging hearts across miles with immersive VR—experience shared moments, tastes, and realistic presence! Your solution to overcoming distance—because love knows no bounds, and now, neither do you!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/771/468/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Otsuka/Valuenex: Best Hack to Connect with Others through Food (4x $500 DoorDash + 4x $250 Spa Giftcards)"}],"team_members":[],"built_with":[{"name":"android","url":"https://devpost.com/software/built-with/android"},{"name":"c#","url":"https://devpost.com/software/built-with/c--2"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"unity","url":"https://devpost.com/software/built-with/unity"}],"external_links":[{"label":"github.com","url":"https://github.com/kmay23/VR-TINES"}],"description_sections":[{"heading":"⭐Key Experience Points","content":"Collaborative Scrapbooking : Couples can work together on a scrapbook, flipping through pages and dragging in photos and various decorative elements. This scrapbook feels like you’re actually working on it in the real world due to its alignment with 3d surfaces and interactive elements of flipping pages and dragging in elements. By having your partner right next to you, it’s like you’re working on it together in this shared space. Love to reflect on the mems :) Shared Taste : Get a themed-meal at the same time! Here, we use the two users’ favorite boba orders. Using the Doordash API, we synchronize the delivery of identical beverages or snacks to both partners during their VR date, which we track in the Doordash delivery simulator. Enhanced Realism with Live Webcam Feed : People typically use passthrough with VR headsets to feed what’s “real” into their experiences. We take advantage of this idea to stream a live webcam feed into the VR-tines experience, so it feels like your partner is actually sitting right next to you (and you see them in passthrough) as you do activities together! Tab Bar Navigation : We support toggling through the three main options: scrapbooking, Doordash, and home. Social Impact : VR-tines is about bringing people together. Our project has the potential to significantly reduce the emotional distance in long-distance relationships, fostering stronger bonds and happier couples.\n\nThis problem is also important to us because all of us are in long-distance relationships! However, we notice many similar issues that arise in all kinds of relationships in life, such as family, friends, work, etc due to the difficulty of being far apart. Therefore, we sought to solve this real user problem faced every day and create a better solution than current methods of call communication to create more seamless, immersive experiences to feel closer to our loved ones.\n\nAs first-time hackers and Stanford freshmen, the concept of home stretches across oceans to Myanmar and Vietnam, where our families reside. The yearning for a deeper connection with our loved ones, despite the geographical miles that separate us, sparked not just a need but a personal quest. Facing this hackathon, our greatest challenge was not the complexity of the technology or the novelty of the concept, but the mental hurdle of believing we could make a significant impact. This project became more than a hack; it evolved into a journey of discovery, learning, and overcoming, driven by our shared experiences of longing and the universal desire to feel closer to those we hold dear. It’s a testament to our belief that distance shouldn't dim the bonds of love and friendship but rather, with the right innovation, can be bridged beautifully."},{"heading":"Built With","content":"android c# python unity"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Project Horus","project_url":"https://devpost.com/software/project-horus-prejc4","tagline":"Civilians accounted for 95% of all cluster munition casualties in 2022. We developed an autonomous system for detecting cluster munitions in post-war zones using a Parrot Drone and a custom CNN.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Parrot Drones: Best Use of Drone Technology ($4.5k Anafi AI Drone)"}],"team_members":[],"built_with":[{"name":"parrot","url":"https://devpost.com/software/built-with/parrot"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"tensorflow","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Driven by the harrowing reality that civilians comprised a staggering 95% of all cluster munition casualties in 2022, we were impassioned to devise a solution that could confront this pressing humanitarian crisis head-on. With up to 40% of these munitions failing to explode upon impact, the threat to civilian lives persists long after conflict ceases. In regions like Laos, where between 1964 and 1973, 260 million cluster bomblets were dropped, and a chilling 80 million failed to detonate, the urgency of our mission became abundantly clear. Inspired by the imperative to safeguard innocent lives in post-war zones, we embarked on the development of Project Horus."},{"heading":"What it does","content":"Project Horus utilizes cutting-edge technology to detect and localize unexploded cluster munitions in conflict-affected areas. Using a Parrot drone equipped with a custom-trained Convolutional Neural Network (CNN), our system autonomously scans vast territories, identifying potential threats. The drone's onboard Inertial Measurement Unit (IMU) aids in precisely localizing the detected munitions, even in electronic warfare or GPS-denied environments. Results are visualized in real-time as a heatmap, providing actionable insights to demining teams and enabling targeted removal of these deadly remnants of war."},{"heading":"How we built it","content":"We started by creating a robust training dataset by printing out images of cluster munitions, placing them on the ground, and capturing a video to partition into frames. Our CNN, built using TensorFlow, achieved an impressive 98% validation accuracy after rigorous training. Integration with the Parrot drone involved deploying grid search algorithms to autonomously search for munitions, leveraging the drone's capabilities for real-time bomb detection. Overcoming challenges such as uploading custom assets into the simulator and optimizing the model to run efficiently on non-hardware accelerated systems, we created a streamlined solution ready for deployment in the field."},{"heading":"Challenges we ran into","content":"Throughout the development process, we encountered several challenges, including difficulties uploading custom assets into the simulator and optimizing the CNN to run efficiently on hardware-constrained systems. Despite these hurdles, we adapted our approach, moving the computer vision processing off the drone and onto a separate system, ensuring our solution remained viable and effective."},{"heading":"Accomplishments that we're proud of","content":"We're proud to have successfully implemented grid search and bomb-detection capabilities on the Parrot drone, as well as leveraging the IMU for precise localization of munitions. Additionally, visualizing the detection results in a heatmap provides actionable intelligence for demining teams, marking a significant achievement in our mission to save civilian lives."},{"heading":"What's next for Project Horus","content":"Moving forward, we aim to enhance the robustness of our model by incorporating synthetic and real-world data for fine-tuning. Furthermore, we envision implementing multi-agent reinforcement learning techniques to enable collaborative scanning by fleets of drones, enhancing efficiency in covering large areas. Ultimately, we aspire to deploy Project Horus alongside demining teams in regions such as Ukraine, Israel, Afghanistan, and Southeast Asia, contributing to the safe removal of cluster munitions and the protection of civilian populations."},{"heading":"Built With","content":"parrot python tensorflow"}]},{"project_title":"vly.ai: the bubble.io killer","project_url":"https://devpost.com/software/vly-ai-the-bubble-io-killer","tagline":"Generate Full-Stack SaaS Apps with just 1 click using AI.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/775/579/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Reflex: Best Use of Reflex ($2k Cash)"}],"team_members":[],"built_with":[{"name":"gpt","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"tailwind","url":null},{"name":"vly","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/victorxheng/treehacks"}],"description_sections":[{"heading":"THE PROBLEM: Building a SaaS web application is hard.","content":"If you were trying to build one, here are your options:\n\nHiring a developer or agency: a. $10,000-$100,000, 2-6 months b. Expensive to iterate, prone to miscommunication Developing an app on your own: a. Free + Software costs, 2-12 months b. Requires significant amounts of personal time, not many people can do it Using a no-code tool like bubble.io: a. Free + Software costs, 1-6 months b. Highly restrictive and limited, and learning curve requiring lots of time Hiring a no-code developer or agency: a. $5,000-$50,000, 1-4 months b. Still highly restrictive and limited, prone to miscommunication"},{"heading":"THE SOLUTION: vly.ai","content":"A component-based natural language framework that takes specific saas project ideas and turns them into code. Through our framework, we are able to laser focus on specific features exactly the way the user wants it to be.\n\nThese features can be produced reliably through our component-based system.\n\nEssentially: we abstract layer by layer. This allows us to not rely on AI to produce bottom-level code (it can be inaccurate).\n\nFinally: our abstractions and context system makes it very easy for AI to understand the code base and locate edits."},{"heading":"What sets us apart","content":"Here is how we stand compared to other tools out there trying to solve the same thing.\n\nReliable and robust without the technical issues Scales effectively in size and complexity Significantly faster load times and optimizations not possible in no-code More flexibility and complexity in terms of what can be produced Faster integration of external APIs and capabilities Automatic generation of front end and back end code not possible in no-code Ownership of code and the ability to export and expand\n\nOur solution also maintains the benefits of a no-code system:\n\nAutomatic deployment and hosting on the web for both front and back end Automatic scaling and optimization User-friendly environment for interacting with data Ability to make changes quickly and re-deploy instantly"},{"heading":"Our solution builds what a no-code tool can build, but in hours instead of months:","content":"You can build enterprise-grade software with unlimited features custom to exactly what you need:\n\nCRM systems Custom Internal Tools Niche platforms Dashboards and client portals Interfaces on data GPT integrations and wrappers on custom data Marketplaces, web apps, and more SaaS applications for founders to launch and make money\n\nWe also allow business owners to create custom software at dirt cheap costs. So instead of paying for 5 subscriptions to manage your business, you can now combine everything into 1 super-app, such as the following:\n\nManaging Employee Payroll Tracking hours POS and inventory tracking Front site and processing orders\n\nThe Technology built at Treehacks: breakdown\n\nHere is how our treehacks project operates differently"},{"heading":"attention is all you need","content":"We mean context. You can't just say: \"build me a full stack blogging site\" and expect the AI to produce the next Medium.\n\nYou need to be specific. As specific as possible. You need to describe every page, feature, and component, or else it may not give you what you want, and most of the times, the AI doesn't have the capacity to build out this much logic on its own.\n\nSo, here's the process:\n\nUser enters a broader prompt The AI conversationally details the prompt and reviews it with the user This cycle repeats for a description and a list of features & user flow Then, the AI helps build out in natural language the database schema Process continues to write descriptions for each page, then all the way down to a component level\n\nEventually, you create one giant configuration file split up from the top (being more context related) to the bottom (being more specific to exact operations).\n\nWhat you have at the bottom isn't too far off from just using Reflux and calling pre-built components with pre-defined parameters. This form of abstraction makes it much more straightforward for the AI."},{"heading":"The .vly programming language","content":"We standardized the format of this config file and optimized its AI-friendliness to where it's pretty much become it's own language. It's now called the vly programming language (with .vly file extensions) that uses natural language arranged in an intuitive format.\n\nThis allows people with no programming knowledge to write out what they want in immense detail for our AI to implement. This specificity is required to ensure that the level of depth and complexity desired is reached; something current AI code generators lack.\n\nThis also is how we seperate ourselves in comprehensiveness."},{"heading":"AI-Agent specific capabilities","content":"Due to the specificity of the reflex framework, we need to prompt engineer different AI agents for each step of the process to specifically produce what we need. For example, we have one agent for configuring specifically the database based on the .vly framework. We have others for different parts of the process, from selecting and implementing components in our library.\n\nAlso, the limited component library requires the generation of the vly language to choose from the existing library, fed through directly into the system message. The AI then knows the exact way to implement each library, which is in the form of a function with parameters."},{"heading":"Component Library","content":"The limitation is that we rely on pre-built components rather than components built on the fly. We chose this route because we didn't want to rely on AI to write bottom-level code; you definitely don't want the AI to be writing your stripe payments scripts."},{"heading":"Integrated SaaS Specific Technologies","content":"We reliably integrate Stripe, email, text, and more into our tech stack.\n\nThe vertical approach we take allows us to ensure correctness in our work and creates reliability albeit sacrificing flexibility in the short term."},{"heading":"Use of Reflex and language abstraction","content":"Our full-stack web app uses reflex to allow for simple intuitive abstraction of web applications. Rather than verbose systems seen in next.js, reflex abstracts all of it for us already so that the AI can have a lot more focus on the higher level operations rather than the specific syntax.\n\nThe future of vly.ai: committed to becoming a startup\n\nWe are highly committed to turning this venture into a startup company. We have contacted clients already to build projects for to raise funds and expand our component library.\n\nWe hope to someday be able to generate software cheaper, faster, and more reliable than other companies to bring consumers technologies fit for their needs that don't cost large sums of money."},{"heading":"Expanding library of components","content":"We are actively working on expanding the capabilities of the AI to be able to tackle more and more components and external specific features based on demand from clients.\n\nThis could mean instant implementation of large-level features that often replace existing tools.\n\nAll potential prize money will go towards funding this project and the extension."},{"heading":"Built With","content":"gpt javascript openai python reflex tailwind vly"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Vision Mama: LLMs + Vision Pro + Agents = Cooking Magic","project_url":"https://devpost.com/software/vision-mama-llm-vision-pro-agents-fun-learning","tagline":"Remember Cooking Mama? We turned it into a Conversational Agent for Vision Pro that teaches cooking! We pretrained & fine-tuned recipe making LLMs, made a food ordering agent, semantic recipes,...","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/770/151/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Codegen: Best Use of AI Agents ($3k Cash)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Use of Monster Generative AI APIs (4x XBox Series S [1st] & 1 million Monster API credits [2nd] & $400 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"cursor","url":null},{"name":"finetuning","url":null},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"gpt","url":null},{"name":"intersystems","url":null},{"name":"iris","url":null},{"name":"jupyternotebook","url":null},{"name":"lora","url":null},{"name":"mistral","url":null},{"name":"mixedreality","url":null},{"name":"monsterapi","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"replicate","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"vision","url":null},{"name":"visionos","url":null},{"name":"visionpro","url":null},{"name":"vscode","url":null},{"name":"xcode","url":"https://devpost.com/software/built-with/xcode"}],"external_links":[{"label":"recipes.reflex.run","url":"https://recipes.reflex.run/"},{"label":"github.com","url":"https://github.com/andrewgcodes/treehacks2024"}],"description_sections":[{"heading":"The future of computing 🍎 👓 ⚙️ 🤖 🍳 👩‍🍳","content":"How could Mixed Reality, Spatial Computing, and Generative AI transform our lives? And what happens when you combine Vision Pro and AI? (spoiler: magic! 🔮)\n\nOur goal was to create an interactive VisionOS app 🍎 powered by AI. While our app could be applied towards many things (like math tutoring, travel planning, etc.), we decided to make the demo use case fun.\n\nWe loved playing the game Cooking Mama 👩‍🍳 as kids so we made a voice-activated conversational AI agent that teaches you to cook healthy meals, invents recipes based on your preferences, and helps you find and order ingredients.\n\nOverall, we want to demonstrate how the latest tech advances could transform our lives. Food is one of the most important, basic needs so we felt that it was an interesting topic. Additionally, many people struggle with nutrition so our project could help people eat healthier foods and live better, longer lives."},{"heading":"What we created","content":"Conversational Vision Pro app that lets you talk to an AI nutritionist that speaks back to you in a realistic voice with low latency. Built-in AI agent that will create a custom recipe according to your preferences, identify the most efficient and cheapest way to purchase necessary ingredients in your area (least stores visited, least cost), and finally creates Instacart orders using their simulated API. Web version of agent at recipes.reflex.run in a chat interface InterSystems IRIS vector database of 10k recipes with HyDE enabled semantic search Pretrained 40M LLM from scratch to create recipes Fine-tuned Mistral-7b using MonsterAPI to generate recipes"},{"heading":"How we built it","content":"We divided tasks efficiently given the time frame to make sure we weren't bottlenecked by each other. For instance, Gao's first priority was to get a recipe LLM deployed so Molly and Park could use it in their tasks. While we split up tasks, we also worked together to help each other debug and often pair programmed and swapped tasks if needed. Various tools used: Xcode, Cursor, OpenAI API, MonsterAI API, IRIS Vector Database, Reflex.dev, SERP API,...\n\nVision OS\n\nTalk to Vision Mama by running Whisper fully on device using CoreML and Metal Chat capability powered by GPT-3.5-turbo, our custom recipe-generating LLM (Mistral-7b backbone), and our agent endpoint. To ensure that you are able to see both Vision Mama's chats and her agentic skills, we have a split view that shows your conversation and your generated recipes Lastly, we use text-to-speech synthesis using ElevenLabs API for Vision Mama's voice\n\nAI Agent Pipeline for Recipe Generation, Food Search, and Instacart Ordering\n\nWe built an endpoint that we hit from our Vision Pro and our Reflex site. Basically what happens is we submit a user's desired food such as \"banana soup\". We pass that to our fine-tuned Mistral-7b LLM to generate a recipe. Then, we quickly use GPT-4-turbo to parse the recipe and extract the ingredients. Then we use the SERP API on each ingredient to find where it can be purchased nearby. We prioritize cheaper ingredients and use an algorithm to try to visit the least number of stores to buy all ingredients. Finally, we populate an Instacart Order API call to purchase the ingredients (simulated for now since we do not have actual partner access to Instacart's API)\n\nPre-training (using nanogpt architecture):\n\nCreated large dataset of recipes. Tokenized our recipe dataset using BPE (GPT2 tokenizer) Dataset details (9:1 split): train: 46,826,468 tokens val: 5,203,016 tokens\n\nTrained for 1000 iterations with settings: layers = 12 attention heads = 12 embedding dimension = 384 batch size = 32\n\nIn total, the LLM had 40.56 million parameters! It took several hours to train on an M3 Mac with Metal Performance Shaders.\n\nFine-tuning\n\nWhile the pre-trained LLM worked ok and generated coherent (but silly) English recipes for the most part, we couldn't figure out how to deploy it in the time frame and it still wasn't good enough for our agent. So, we tried fine-tuning Mistral-7b, which is 175 times bigger and is much more capable. We curated fine-tuning datasets of several sizes (10k recipes, 50k recipes, 250k recipes). We prepared them into a specific prompt/completion format:\n\nYou are an expert chef. You know about a lot of diverse cuisines. You write helpful tasty recipes.\\n\\n###Instruction: please think step by step and generate a detailed recipe for {prompt}\\n\\n###Response:{completion}\n\nWe fine-tuned and deployed the 250k-fine-tuned model on the MonsterAPI platform, one of the sponsors of TreeHacks. We observed that using more fine-tuning data led to lower loss, but at diminishing returns.\n\nReflex.dev Web Agent\n\nMost people don't have Vision Pros so we wrapped our versatile agent endpoint into a Python-based Reflex app that you can chat with! Try here Note that heavy demand may overload our agent.\n\nIRIS Semantic Recipe Discovery\n\nWe used the IRIS Vector Database, running it on a Mac with Docker. We embedded 10,000 unique recipes from diverse cuisines using OpenAI's text-ada-002 embedding . We stored the embeddings and the recipes in an IRIS Vector Database. Then, we let the user input a \"vibe\", such as \"cold rainy winter day\". We use Mistral-7b to generate three Hypothetical Document Embedding (HyDE) prompts in a structured format. We then query the IRIS DB using the three Mistral-generated prompts. The key here is that regular semantic search does not let you search by vibe effectively. If you do semantic search on \"cold rainy winter day\", it is more likely to give you results that are related to cold or rain, rather than foods. Our prompting encourages Mistral to understand the vibe of your input and convert it to better HyDE prompts.\n\nReal example: User input: something for a chilly winter day Generated Search Queries: {'queries': ['warming winter dishes recipes', 'comfort food recipes for cold days', 'hearty stews and soups for chilly weather']} Result: recipes that match the intent of the user rather than the literal meaning of their query"},{"heading":"Challenges we ran into","content":"Programming for the Vision Pro, a new way of coding without that much documentation available Two of our team members wear glasses so they couldn't actually use the Vision Pro :( Figuring out how to work with Docker Package version conflicts :(( Cold starts on Replicate API A lot of tutorials we looked at used the old version of the OpenAI API which is no longer supported"},{"heading":"Accomplishments that we're proud of","content":"Learning how to hack on Vision Pro! Making the Vision Mama 3D model blink Pretraining a 40M parameter LLM Doing fine-tuning experiments Using a variant of HyDE to turn user intent into better semantic search queries"},{"heading":"What we learned","content":"How to pretrain LLMs and adjust the parameters How to use the IRIS Vector Database How to use Reflex How to use Monster API How to create APIs for an AI Agent How to develop for Vision Pro How to do Hypothetical Document Embeddings for semantic search How to work under pressure"},{"heading":"What's next for Vision Mama: LLM + Vision Pro + Agents = Fun & Learning","content":"Improve the pre-trained LLM: MORE DATA, MORE COMPUTE, MORE PARAMS!!! Host the InterSystems IRIS Vector Database online and let the Vision Mama agent query it Implement the meal tracking photo analyzer into VisionOs app Complete the payment processing for the Instacart API once we get developer access"},{"heading":"Impacts","content":"Mixed reality and AI could enable more serious use cases like:\n\nAssisting doctors with remote robotic surgery Making high quality education and tutoring available to more students Amazing live concert and event experiences remotely Language learning practice partner"},{"heading":"Concerns","content":"Vision Pro is very expensive so most people can't afford it for the time being. Thus, edtech applications are limited. Data privacy\n\nThanks for checking out Vision Mama!"},{"heading":"Built With","content":"cursor finetuning github gpt intersystems iris jupyternotebook lora mistral mixedreality monsterapi openai python reflex replicate swift vision visionos visionpro vscode xcode"},{"heading":"Try it out","content":"recipes.reflex.run github.com"}]},{"project_title":"MockupMagic","project_url":"https://devpost.com/software/xxx-23s7u1","tagline":"Think. Click. Prototype. MockupMagic seamlessly turns paper napkin ideas into fully functioning prototypes.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/771/123/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Neo: Most Likely to Become a Business (Airfare + Accommodation at Summer Retreat)"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"gpt-api","url":null},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"monsterapi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null}],"external_links":[{"label":"www.youtube.com","url":"https://www.youtube.com/watch?v=G1kWbLUU82s"},{"label":"github.com","url":"https://github.com/Nsilswal/MockupMagic"}],"description_sections":[{"heading":"Inspiration","content":"Our journey to creating this project stems from a shared realization: the path from idea to execution is fraught with inefficiencies that can dilute even the most brilliant concepts. As developers with a knack for turning visions into reality, we've faced the slow erosion of enthusiasm and value that time imposes on innovation. This challenge is magnified for those outside the technical realm, where a lack of coding skills transforms potential breakthroughs into missed opportunities. Harvard Business Review and TechCrunch analyzed Y Combinator startups and found that around 40% of founders are non-technical.\n\nDrawing from our experiences in fast-paced sectors like health and finance, we recognized the critical need for speed and agility. The ability to iterate quickly and gather user feedback is not just beneficial but essential in these fields. Yet, this process remains a daunting barrier for many, including non-technical visionaries whose ideas have the potential to reshape industries.\n\nWith this in mind, we set out to democratize the development process. Our goal was to forge a tool that transcends technical barriers, enabling anyone to bring their ideas to life swiftly and efficiently. By leveraging our skills and insights into the needs of both developers and non-developers alike, we've crafted a solution that bridges the gap between imagination and tangible innovation, ensuring that no idea is left unexplored due to the constraints of technical execution.\n\nThis project is more than just a tool; it's a testament to our belief that the right technology can unlock the potential within every creative thought, transforming fleeting ideas into impactful realities."},{"heading":"What it does","content":"Building on the foundation laid by your vision, MockupMagic represents a leap toward democratizing digital innovation. By transforming sketches into interactive prototypes, we not only streamline the development process but also foster a culture of inclusivity where ideas, not technical prowess, stand in the spotlight. This tool is a catalyst for creativity, enabling individuals from diverse backgrounds to participate actively in the digital creation sphere.\n\nThe user can upload a messy sketch on paper to our website. MockupMagic will then digitize your low-fidelity prototype into a high-fidelity replica with interactive capabilities. The user can also see code alongside the generated mockups, which serves as both a bridge to tweak the generated prototype and a learning tool, gently guiding users toward deeper technical understanding. Moreover, the integration of a community feedback mechanism through the Discussion tab directly within the platform enhances the iterative design process, allowing for real-time user critique and collaboration.\n\nMockupMagic is more than a tool; it's a movement towards a future where the digital divide is narrowed, and the translation of ideas into digital formats is accessible to all. By empowering users to rapidly prototype and refine their concepts, we're not just accelerating the pace of innovation; we're ensuring that every great idea has the chance to be seen, refined, and realized in the digital world."},{"heading":"How we built it","content":"Conceptualization: The project began with brainstorming sessions where we discussed the challenges non-technical individuals face in bringing their ideas to life. Understanding the value of quick prototyping, especially for designers and founders with creative but potentially fleeting ideas, we focused on developing a solution that accelerates this process.\n\nResearch and Design: We conducted research to understand the needs of our target users, including designers, founders, and anyone in between who might lack technical skills. This phase helped us design a user-friendly interface that would make it intuitive for users to upload sketches and receive functional web mockups.\n\nTechnology Selection: Choosing the right technologies was crucial. We decided on a combination of advanced image processing and AI algorithms capable of interpreting hand-drawn sketches and translating them into HTML, CSS, and JavaScript code. We leveraged and finetuned existing AI models from MonsterAPI and GPT API and tailored them to our specific needs for better accuracy in digitizing sketches.\n\nDevelopment: The development phase involved coding the backend logic that processes the uploaded sketches, the AI model integration for sketch interpretation, and the frontend development for a seamless user experience. We used the Reflex platform to build out our user-facing website, capitalizing on their intuitive Python-like web development tools.\n\nTesting and Feedback: Rigorous testing was conducted to ensure the accuracy of the mockups generated from sketches. We also sought feedback from early users, including designers and founders, to understand how well the tool met their needs and what improvements could be made."},{"heading":"Challenges we ran into","content":"We initially began by building off our own model, hoping to aggregate quality training data mapping hand-drawn UI components to final front-end components, but we quickly realized this data was very difficult to find and hard to scrape for. Our model performs well for a few screens however it still struggles to establish connections between multiple screens or more complex actions."},{"heading":"Accomplishments that we're proud of","content":"Neither of us had much front-end & back-end experience going into this hackathon, so we made it a goal to use a framework that would give us experience in this field. After learning about Reflex during our initial talks with sponsors, we were amazed that Web Apps could be built in pure Python and wanted to jump right in. Using Reflex was an eye-opening experience because we were not held back by preconceived notions of traditional web development - we got to enjoy learning about Reflex and how to build products with it. Reflex’s novelty also translates to limited knowledge about it within LLM tools developers use to help them while coding, this helped us solidify our programming skills through reading documentation and creative debugging methodologies - skills almost being abstracted away by LLM coding tools. Finally, our favorite part about doing hackathons is building products we enjoy using. It helps us stay aligned with the end user while giving us personal incentives to build the best hack we can."},{"heading":"What we learned","content":"Through this project, we learned that we aren’t afraid to tackle big problems in a short amount of time. Bringing ideas on napkins to full-fledged projects is difficult, and it became apparent hitting all of our end goals would be difficult to finish in one weekend. We quickly realigned and ensured that our MVP was as good as it could get before demo day."},{"heading":"What's next for MockupMagic","content":"We would like to fine-tune our model to handle more edge cases in handwritten UIs. While MockupMagic can handle a wide range of scenarios, we hope to perform extensive user testing to figure out where we can improve our model the most. Furthermore, we want to add an easy deployment pipeline to give non-technical founders even more autonomy without knowing how to code. As we continue to develop MockupMagic, we would love to see the platform being used even at TreeHacks next year by students who want to rapidly prototype to test several ideas!"},{"heading":"Built With","content":"css gpt-api html javascript monsterapi python reflex"},{"heading":"Try it out","content":"www.youtube.com github.com"}]},{"project_title":"Synthesis","project_url":"https://devpost.com/software/synthesis-ag1djb","tagline":"News that promotes personalization and prevents polarization.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/772/474/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Coframe: Best AI Hack (NVIDIA RTX 4080 GPU)"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"algorithms","url":null},{"name":"gpt-4","url":null},{"name":"llms","url":null},{"name":"natural-language-processing","url":"https://devpost.com/software/built-with/natural-language-processing"},{"name":"next.js","url":null},{"name":"pinecone","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"supabase","url":null},{"name":"vectordbs","url":null},{"name":"webscraping","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ronnachum11/synthesis.git"},{"label":"synthe.vercel.app","url":"https://synthe.vercel.app/"}],"description_sections":[{"heading":"Built With","content":"agents algorithms gpt-4 llms natural-language-processing next.js pinecone python react supabase vectordbs webscraping"},{"heading":"Try it out","content":"github.com synthe.vercel.app"}]},{"project_title":"dialogixAI","project_url":"https://devpost.com/software/dialogixai","tagline":"Revolutionize language learning with our AI-driven app: Engage in real conversations, receive instant feedback, and master languages naturally. Beyond rote, towards fluency.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/771/205/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Wispr.ai: Best Voice Experience (4x Interview with Wispr founders and visit to Wispr HQ)"}],"team_members":[],"built_with":[{"name":"elevenlabs","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/cherylwu834/dialogixAI"},{"label":"www.loom.com","url":"https://www.loom.com/share/0b783dc893924cd9b2ccb0c7bd231967?sid=a7b22f3b-5661-4b0f-944e-4d72690d3caf"}],"description_sections":[{"heading":"Inspiration","content":"Our journey with language learning in class and on apps revealed their limitations—rote memorization and the absence of real-life conversations. Other language learners I met through Toastmasters Club, a public speaking club, echoed the same sentiment. Recognizing the importance of practical speaking skills and interactive learning, we set out to create DialogixAI, a platform that simulates conversational experiences with AI, making language learning more natural, engaging, and effective."},{"heading":"What It Does","content":"DialogixAI bridges the gap between short phrases or words learned on other language platforms and real-life conversations by enabling users to engage in voice-based conversations with an AI. The AI bot listens, responds, and provides instant feedback on grammar and expression for the user. This real-time feedback enhances speaking skills and boosts confidence."},{"heading":"How We Built It","content":"Leveraging cutting-edge AI technologies, including large language models (LLMs), DialogixAI processes user speech, generates contextually relevant responses, and evaluates language use for constructive feedback. We utilized open-source libraries and APIs, ensuring robust performance and accelerating development."},{"heading":"Challenges We Ran Into","content":"Developing an AI that can handle the nuances of human language, including accents, slang, and idiomatic expressions, was challenging. Ensuring the AI's responses feel natural and engaging required intricate integration. Technical hurdles surfaced in seamlessly combining various AI components and ensuring global scalability and accessibility."},{"heading":"Accomplishments That We're Proud Of","content":"We're proud of our clean UI and conquering the complexities of integrating the AI component into our web app."},{"heading":"What We Learned","content":"This project deepened our understanding of AI's educational potential. We gained insights into language processing complexities and the importance of user experience design in educational technologies. The journey taught us teamwork, problem-solving, and the iterative design and development process."},{"heading":"What's Next for DialogixAI","content":"Looking ahead, we aim to expand DialogixAI's capabilities to include more languages and dialects, making it accessible to a broader audience. We also plan to include an AI generated human avatar to simulate the interaction with a human in the future."},{"heading":"Built With","content":"elevenlabs javascript llm openai python react"},{"heading":"Try it out","content":"github.com www.loom.com"}]},{"project_title":"Shard","project_url":"https://devpost.com/software/shard-i03hcs","tagline":"A distributed system to train models across heterogeneous computers","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/775/984/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Pear VC: Most Likely to Get Funded ($1.5k Cash)"}],"team_members":[],"built_with":[{"name":"data-parallel","url":null},{"name":"distributed-systems","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"parallel-processing","url":null},{"name":"pipeline-parallel","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"web-sockets","url":null},{"name":"zmq","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration:","content":"The single biggest problem and bottleneck in training large AI models today is compute. Just within the past week, Sam Altman tried raising an $8 trillion dollar fund to build a large network of computers for training larger multimodal models."},{"heading":"Built With","content":"data-parallel distributed-systems javascript numpy pandas parallel-processing pipeline-parallel python react web-sockets zmq"}]},{"project_title":"TruckrZzz","project_url":"https://devpost.com/software/truckrzzz","tagline":"An all-in-one smart solution for early drowsiness detection amongst truck drivers, providing smart solutions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/773/192/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Reflex: Best Use of Reflex ($2k Cash)"},{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"TerraAPI: Most Commercially Viable Hack (weekly 1v1 product sessions)"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"terraapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pnavab/TruckrZzz"},{"label":"github.com","url":"https://github.com/banyar-shin/TruckrZzz-iOS-App"}],"description_sections":[{"heading":"Inspiration","content":"News frequently reports on vehicle crashes and accidents, with one statistic highlighting the prevalence of heavy truck accidents caused by driver fatigue. Truck drivers endure long hours on the road, delivering shipments nationwide, contributing to the tiredness that can lead to accidents. According to the National Transportation Safety Board, nearly 40% of heavy truck accidents originate from fatigue. In response, we pushed to develop a system capable of monitoring both facial expressions and heartbeats to detect early signs of fatigue among drivers."},{"heading":"What it does","content":"Our web app boasts two features aimed at improving driver safety: one harnesses computer vision technology to track the driver's face, effectively detecting signs of drowsiness, while the other streams the driver's heartbeat in real-time, providing an additional layer of drowsiness detection. Accessible through our web app is a dedicated page for viewing the webcam feed, which ideally can be monitored via personal devices like smartphones. Should the webcam detect the driver falling asleep, it triggers an alert with flashing lights and a sound to awaken the driver. Additionally, our dashboard feature enables managers to monitor their drivers and their respective drowsiness levels. We've incorporated a graphing feature within the dashboard that dynamically turns red when a selected driver's drowsiness level drops below the acceptable threshold, providing a clear visual indication of potential fatigue."},{"heading":"How we built it","content":"By combining Reflex and TerraAPI, as well as a companion mobile app in Swift, we were able to create a solution all within our ecosystem. The TerraAPI provided the crucial heartrate data in real time, which we livestreamed through a webhook that our Reflex website could read. The Reflex website also contains a manager-style dashboard for viewing several truckers and collect their unique data all at the same time. As a demo for future mobile usage, we also included a facial recognition and landmarking model to detect drowsiness and alert the user if they are falling asleep. The Swift app also provided additional information such as the heartrate in real time and establishing the connection to the webhook from the wearable device."},{"heading":"Challenges we ran into","content":"In order to construct the complex data flow of our project, we had to learn several new technologies along the way. It started with developing on a new wearable device with limited documentation and support only through a Swift iOS app, which none of us had experience with. With Reflex, we also encountered some bugs, which all had workarounds, and the difficulties that come with developing any website."},{"heading":"Accomplishments that we're proud of","content":"We're proud of being able to integrate such complex technologies and orchestrate them in a seamless way. At times, we were afraid that our product wouldn't come together since all the components depended on each other and we needed to complete all of them. However, our team made everything work in the end."},{"heading":"What we learned","content":"Many of the technologies we worked with during TreeHacks were new and had a large learning curve in order to build our end goal. Along this journey, our team picked up valuable skills in Swift, Python, computer vision, web development, and how to work on 2 hours of sleep."},{"heading":"What's next for TruckrZzz","content":"We hope to broaden our target audience and not only apply these technologies for truck drivers, but also every day drivers that might need some extra assistance staying awake on the road."},{"heading":"Built With","content":"python reflex swift terraapi"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"Bino-SoRAs","project_url":"https://devpost.com/software/watchdog-onxyge","tagline":"AI-generated video detection using interpolated next-frame-prediction and Fréchet inception distance to generate 92% accurate zero-shot classification. Novel Binocular method applied to OpenAI's SORA.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/002/772/341/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"HRT: Best Use of Data (4x HRT Keyboard + Mouse)"}],"team_members":[],"built_with":[{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"intel","url":null},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"scikit-learn","url":"https://devpost.com/software/built-with/scikit-learn"},{"name":"tensorflow","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Willy-Chan/Bino-SoRAs"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1ooeldYN2sxcl4dXyJdPyVUl5kpRcJfgBAUEyNJS1O6o/edit#slide=id.g269774b2551_0_111"}],"description_sections":[{"heading":"The Gist","content":"We combine state-of-the-art LLM/GPT detection methods with image diffusion models to accurately detect AI-generated video with 92% accuracy."},{"heading":"Inspiration","content":"As image and video generation models become more powerful, they pose a strong threat to traditional media norms of trust and truth. OpenAI's SORA model released in the last week produces extremely realistic video to fit any prompt, and opens up pathways for malicious actors to spread unprecedented misinformation regarding elections, war, etc."},{"heading":"What it does","content":"BinoSoRAs is a novel system designed to authenticate the origin of videos through advanced frame interpolation and deep learning techniques. This methodology is an extension of the state-of-the-art Binoculars framework by Hans et al. (January 2024), which employs dual LLMs to differentiate human-generated text from machine-generated counterparts based on the concept of textual \"surprise\".\n\nBinoSoRAs extends on this idea in the video domain by utilizing Fréchet Inception Distance (FID) to compare the original input video against a model-generated video. FID is a common metric which measures the quality and diversity of images using an Inception v3 convolutional neural network. We create model-generated video by feeding the suspect input video into a Fast Frame Interpolation (FLAVR) model, which interpolates every 8 frames given start and end reference frames. We show that this interpolated video is more similar (i.e. \"less surprising\") to authentic video than artificial content when compared using FID.\n\nThe resulting FID + FLAVR two-model combination is an effective framework for detecting video generation such as that from OpenAI's SoRA. This innovative application enables a root-level analysis of video content, offering a robust mechanism for distinguishing between human-generated and machine-generated videos. Specifically, by using the Inception v3 and FLAVR models, we are able to look deeper into shared training data commonalities present in generated video."},{"heading":"How we built it","content":"Rather than simply analyzing the outputs of generative models, a common approach for detecting AI content, our methodology leverages patterns and weaknesses that are inherent to the common training data necessary to make these models in the first place. Our approach builds on the Binoculars framework developed by Hans et al. (Jan 2024), which is a highly accurate method of detecting LLM-generated tokens. Their state-of-the-art LLM text detector makes use of two assumptions: simply \"looking\" at text of unknown origin is not enough to classify it as human- or machine-generated, because a generator aims to make differences undetectable. Additionally, models are more similar to each other than they are to any human , in part because they are trained on extremely similar massive datasets. The natural conclusion is that an observer model will find human text to be very perplex and surprising, while an observer model will find generated text to be exactly what it expects.\n\nWe used Fréchet Inception Distance between the unknown video and interpolated generated video as a metric to determine if video is generated or real. FID uses the Inception score, which calculates how well the top-performing classifier Inception v3 classifies an image as one of 1,000 objects. After calculating the Inception score for every frame in the unknown video and the interpolated video, FID calculates the Fréchet distance between these Gaussian distributions, which is a high-dimensional measure of similarity between two curves. FID has been previously shown to correlate extremely well with human recognition of images as well as increase as expected with visual degradation of images.\n\nWe also used the open-source model FLAVR (Flow-Agnostic Video Representations for Fast Frame Interpolation), which is capable of single shot multi-frame prediction and reasoning about non-linear motion trajectories. With fine-tuning, this effectively served as our generator model, which created the comparison video necessary to the final FID metric.\n\nWith a FID-threshold-distance of 52.87, the true negative rate (Real videos correctly identified as real) was found to be 78.5%, and the false positive rate (Real videos incorrectly identified as fake) was found to be 21.4%. This computes to an accuracy of 91.67%."},{"heading":"Challenges we ran into","content":"One significant challenge was developing a framework for translating the Binoculars metric (Hans et al.), designed for detecting tokens generated by large-language models, into a practical score for judging AI-generated video content. Ultimately, we settled on our current framework of utilizing an observer and generator model to get an FID-based score; this method allows us to effectively determine the quality of movement between consecutive video frames through leveraging the distance between image feature vectors to classify suspect images."},{"heading":"Accomplishments that we're proud of","content":"We're extremely proud of our final product: BinoSoRAs is a framework that is not only effective, but also highly adaptive to the difficult challenge of detecting AI-generated videos. This type of content will only continue to proliferate the internet as text-to-video models such as OpenAI's SoRA get released to the public: in a time when anyone can fake videos effectively with minimal effort, these kinds of detection solutions and tools are more important than ever, especially in an election year .\n\nBinoSoRAs represents a significant advancement in video authenticity analysis, combining the strengths of FLAVR's flow-free frame interpolation with the analytical precision of FID. By adapting the Binoculars framework's methodology to the visual domain, it sets a new standard for detecting machine-generated content, offering valuable insights for content verification and digital forensics. The system's efficiency, scalability, and effectiveness underscore its potential to address the evolving challenges of digital content authentication in an increasingly automated world."},{"heading":"What we learned","content":"This was the first-ever hackathon for all of us, and we all learned many valuable lessons about generative AI models and detection metrics such as Binoculars and Fréchet Inception Distance. Some team members also got new exposure to data mining and analysis (through data-handling libraries like NumPy, PyTorch, and Tensorflow), in addition to general knowledge about processing video data via OpenCV.\n\nArguably more importantly, we got to experience what it's like working in a team and iterating quickly on new research ideas. The process of vectoring and understanding how to de-risk our most uncertain research questions was invaluable, and we are proud of our teamwork and determination that ultimately culminated in a successful project."},{"heading":"What's next for BinoSoRAs","content":"BinoSoRAs is an exciting framework that has obvious and immediate real-world applications, in addition to more potential research avenues to explore. The aim is to create a highly-accurate model that can eventually be integrated into web applications and news articles to give immediate and accurate warnings/feedback of AI-generated content. This can mitigate the risk of misinformation in a time where anyone with basic computer skills can spread malicious content, and our hope is that we can build on this idea to prove our belief that despite its misuse, AI is a fundamental force for good."},{"heading":"Built With","content":"github intel numpy opencv pytorch scikit-learn tensorflow"},{"heading":"Try it out","content":"github.com docs.google.com"}]},{"project_title":"Kanga: Streamlining Research Papers into Presentations","project_url":"https://devpost.com/software/kanga-slides-made-easy","tagline":"We are making research more accessible by automatically generating understandable, concise, and aesthetically clean presentation slides from a research paper with just one click.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/771/235/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Canva: Best New Canva App Submission (4x Wireless Headphones + Canva Sweatshirts)"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"canva","url":null},{"name":"llm","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sdk","url":null},{"name":"togetherai","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/theAnshulGupta/kanga"}],"description_sections":[{"heading":"Inspiration","content":"There’s two fundamental issues we want to address:\n\na) Research is not generally accessible to the public. Most people don’t want to spend hours trying to decipher jargon and figure out why a paper is important.\n\nb) Presenting research and making it accessible is a very tedious task. Authors have to compile their research into a presentation, figure out what key points to highlight, and what to present to make it understandable to those without industry knowledge. Plus – no one wants an ugly presentation, so significant time is spent on design too.\n\nPut together, this means that researchers spend a lot of time building presentations (our empirical survey found researchers spent ~5.8 hours on average building presentations) if they want to be able to present them to the general public.\n\nYet, research is incredibly valuable in driving forward innovation, ensuring people understand what is happening in society, and helping inspire and educate students currently in school who will become further scientists and leaders. It's not enough for research to simply live in the bubble of academia -- the wider public (which are all impacted by research findings) need to be engaged, and there needs to be a easier way of doing that."},{"heading":"What it does","content":"The current ways that people create slides:\n\nSlidesgo / SlidesCarnival: Tools like Slidesgo and SlidesCarnival only provide templates, rather than content creation. It is very time consuming to add content and design a presentation. Tome.ai / Gamma: These existing AI-powered slide generation tools do not have dynamic content creation and placement. These tools produced slides that look fragmented and do not flow as a full presentation.\n\nAnd neither are suitable for research presentations.\n\nOur system takes an URL to the PDF of the research paper, and retreives the research paper in order to summarize and compiled a structured presentation of the key background, contributions, and results of a given study.\n\nFrom there, we organize the data and assign it into logically-ordered slides broken down by purpose and dynamically render visual elements and text onto a Canva presentation. All of this is completed in under 60 seconds, where as most people don’t even have a title slide created during that time!"},{"heading":"How we built it","content":"We built this application onto of the Canva SDK in Typescript, using Bun as our runtime to speed up the backend. To process the PDF, we built an API endpoint using Together AI’s hosted Mistral 8x7B model and Python to provide cleaned text.\n\nFrom there, we stitched together multiple layers of LLM agents in order to analyze the research paper, extract the relevant information, convert the relevant information into slides, and organize the slides and content into Canva objects that are properly spaced and positioned.\n\nWe also utilized Canva’s images library in order to add and implement dynamic styling of elements based on a user selected theme."},{"heading":"Challenges we ran into","content":"Learning to use the Canva SDK was quite challenging, especially since it was recently released so the documentation describing various functionality and nuances were not as extensive.\n\nIt was also very challenging and time consuming finding the most optimal LLM pipeline setup that ensured that any content we had was:\n\nRelevant, and concise Accurate and minimzed hallucinations.\n\nFinally, figuring out how to best position the content on a slide in a way that can be generalized to all research papers was a major challenge. We attempted to build out a system to generate more advanced / complex elements and layouts on the fly using LLMs, but were not able to complete this in time."},{"heading":"Accomplishments that we're proud of","content":"We’re very proud of the pipeline we built — it's quite robust, and we’re able to parse essentially any research paper from any journal across any domain. This is no small feat, considering that there are libraries built specifically to parse certain journal articles from specific publications.\n\nWe’re also quite proud of how coherently we were able to get content – it’s output in a very understandable manner that is simple and accurate.\n\nFinally, we’re very proud of the interface we built. A big part of our focus was around simplicity – unlike traditional design tools like Figma or Adobe XD, we aren’t catering our tool towards designers, but rather than researchers who frequently have minimal if any design experience. Therefore, we focused on minimize the complexity and failure cases that researcher would potentially face in the process, to make our tool as easy to use as possible.\n\nWe're excited to be working on a big problem that has to potential to change the way research is being communicated if our solution works -- the \"no-code\" design market is ~60B USD (Canva's TAM is ~40B). Universities alone spent ~20B USD on software tools. We estimate that the demand for good design tools at research institutions is at least ~3B USD -- given that there's no other tool on the market doing this, we'd be positioned for significant market capture, and that's before we expand beyond just research."},{"heading":"What we learned","content":"We learned a lot about working with Canva’s SDK, which we found quite impressive. We believe there’s a lot of potential for AI to be integrated more tightly into the Canva ecosystem through it's SDK to supercharge how people design presentations and event posters.\n\nFurthermore, given that accuracy is very important in research contexts, we learned about techniques to minimize LLM hallucination — reducing the temperature parameter of models, and breaking tasks into smaller subtasks that can be processed iteratively.\n\nFinally, through the process, we learned how important it was to effectively communicate and delegate tasks – at first, we had multiple team members working on similar sub-projects because while we were all clear on the components we needed to build to make this project work, there was a bit of miscommunication as to the best way to divide up the necessary components to develop, given a lot of components have significant interdependency."},{"heading":"What's next for Kanga","content":"We plan on applying for the Canva Innovation Fund to further develop this project. We’d also like to refine the various stages of this process. While in 36 hours we managed to build out an arguably-impressive prototype, there’s a lot of room for improvement, especially in terms of designing more presentation-ready papers.\n\nSome features we’d lile to implement include:\n\nExtracting relevant diagrams and images from papers and positioning them within slides Generating icons and diagrams that present various contributions and concepts discussed within the paper using Dalle / SDXL Building more variety in the positioning of elements and layouts within a slide show Providing for fine grained control over the final result of the presentation Expand beyond just presentations into posters and other graphics\n\nGiven ample time, we’d also like to fine-tune our own LLM models that are catered towards distilling research information, to ensure that we are capturing the most optimal contributions and key points with maximum accuracy.\n\nWe want to become the default app for Canva users and become part of their daily workflow, and ideally be integrated fully into Canva as a “design-assist” agent and get acquired by Canva in 18 months.\n\nWe’d use a variety of per-user pricing plans to generate revenue, primarily catering towards researchers and educational institutions, where we’d be able to close high-value contracts and push our product to a large amount of relevant users (stuents, professors, researchers) very quickiy."},{"heading":"Built With","content":"bun canva llm node.js openai python sdk togetherai typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"care.ai","project_url":"https://devpost.com/software/medbot-u3hzt8","tagline":"For go-to assistant for everything in primary healthcare","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Use of Monster Generative AI APIs (4x XBox Series S [1st] & 1 million Monster API credits [2nd] & $400 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"fastapi","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"intel","url":null},{"name":"intel-cloud","url":null},{"name":"java","url":"https://devpost.com/software/built-with/java"},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"monsterapi","url":null},{"name":"postman","url":"https://devpost.com/software/built-with/postman"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/joyendra/TreeHacks2024"}],"description_sections":[{"heading":"Inspiration","content":"Care.ai was inspired by our self-conducted study involving 60 families and 23 smart devices, focusing on elderly healthcare. Over three months, despite various technologies, families preferred the simplicity of voice-activated assistants like Alexa. This preference led us to develop an intuitive, user-friendly AI healthcare chatbot tailored to everyday needs."},{"heading":"What it does","content":"Care.ai, an AI healthcare chatbot, leverages custom-trained Large Language Models (LLMs) and visual recognition technology hosted on the Intel Cloud for robust processing power. These models, refined and accessible via Hugging Face, underwent further fine-tuning through MonsterAPI, enhancing their accuracy and responsiveness to medical queries. The web application, powered by the Reflex library, provides a seamless and intuitive front-end experience, making it easy for users to interact with and benefit from the chatbot's capabilities. Care.ai supports real-time data analytics and critical care necessary for humans."},{"heading":"How we built it","content":"We built our AI healthcare chatbot by training LLMs and visual recognition systems on the Intel Cloud, then hosting and fine-tuning these models on Hugging Face with MonsterAPI. The chatbot's user-friendly web interface was developed using the Reflex library, creating a seamless user interaction platform.\n\nFor data collection,\n\nWe researched datasets and performed literature review We used the pre-training data for developing and fine-tuning our LLM and visual models We collect live data readings using sensors to test against our trained models\n\nWe categorized our project into three parts:\n\nInteractive Language Models: We developed deep learning models on Intel Developer Cloud and fine-tuned our Hugging Face hosted models using MonsterAPI. We further used Reflex Library to be the face of Care.ai and create a seamless platform. Embedded Sensor Networks: Developed our IoT sensors to track the real-time data and test our LLVMs on the captured data readings. Compliance and Security Components: Intel Developer Cloud to extract emotions and de-identify patient's voice to be HIPAA"},{"heading":"Challenges we ran into","content":"Integrating new technologies posed significant challenges, including optimizing model performance on the Intel Cloud, ensuring seamless model fine-tuning via MonsterAPI and achieving intuitive user interaction through the Reflex library. Balancing technical complexity with user-friendliness and maintaining data privacy and security were among the key hurdles we navigated."},{"heading":"Accomplishments that we're proud of","content":"We're proud of creating a user-centric AI healthcare chatbot that combines advanced LLMs and visual recognition hosted on the cutting-edge Intel Cloud. Successfully fine-tuning these models on Hugging Face and integrating them with a Reflex-powered interface showcases our technical achievement. Our commitment to privacy, security, and intuitive design has set a new standard in accessible home healthcare solutions."},{"heading":"What we learned","content":"We learned the importance of integrating advanced AI with user-friendly interfaces for healthcare. Balancing technical innovation with accessibility, the intricacies of cloud hosting, model fine-tuning, and ensuring data privacy were key lessons in developing an effective, secure, and intuitive AI healthcare chatbot."},{"heading":"What's next for care.ai","content":"Next, Care.ai is expanding its disease recognition capabilities, enhancing user interaction with natural language processing improvements, and exploring partnerships for broader deployment in healthcare systems to revolutionize home healthcare access and efficiency."},{"heading":"Built With","content":"amazon-web-services c++ fastapi firebase intel intel-cloud java machine-learning monsterapi postman python reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Inclusiv.ai","project_url":"https://devpost.com/software/inclusiv-ai","tagline":"Inki is a simplified system to navigate the web.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/776/414/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Use of Monster Generative AI APIs (4x XBox Series S [1st] & 1 million Monster API credits [2nd] & $400 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"chatgpt","url":null},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llms","url":null},{"name":"natural-language-processing","url":"https://devpost.com/software/built-with/natural-language-processing"},{"name":"openai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/VolVox99/inklusiv"}],"description_sections":[{"heading":"Inspiration","content":"In a world where technology is a cornerstone of daily life, it's crucial that digital access is equitable. However, not everyone experiences technology in the same way. We took this challenge to heart and crafted Inclusiv.ai to revolutionize accessibility, ensuring that individuals with disabilities can navigate the web with ease and confidence. Our main priority was reducing the hassle of different extensions and toggles. Inclusiv.ai has one button and one assistant—Inki."},{"heading":"What it does","content":"Inclusiv provides a simple way for those with disabilities to navigate the web. We focused on a hassle-free, intuitive approach with only one button. Simply begin a conversation with your assistant, \"Inki,\" by clicking on the microphone button and explain whatever issues have been limiting your experience on the web. With various modes such as colorblind, screen enhancer, screen explainer and summarizer, and an ADHD/Dyslexia mode.\n\nInki is designed to be a dynamic tool, adapting to a variety of needs through its multiple modes: Colorblind Mode: Tailors the web page colors, ensuring that colorblind users can differentiate between colors that are typically hard to distinguish. Screen Enhancer: Amplifies and clarifies website content for those with visual impairments, allowing for easier reading and interaction. Screen Explainer and Summarizer: A mode that not only explains the elements on the screen but also provides concise summaries for quick comprehension, beneficial for users with cognitive disabilities. ADHD/Dyslexia Mode: Alters the web page layout and typography to minimize distractions and optimize for readability, assisting users with attention deficits or dyslexia. Behind these user-facing features, Inclusiv utilizes a combination of large language models and a specific focus on Monster API. This approach has enabled us to create features that are both technically sophisticated and varied, ensuring a broad range of user needs are met."},{"heading":"How we built it","content":"Inclusiv was meticulously crafted by a synergistic team of two backend developers and one frontend designer, all united by the vision of making web navigation universally accessible. The project was born from a user-centric approach, focusing on the unique challenges faced by individuals with disabilities. Our team's design philosophy hinged on simplicity, leading to the creation of a one-button interface that calls upon Inki, an AI assistant, to activate different accessibility modes including colorblind, screen enhancer, screen explainer, and summarizer, and an ADHD/Dyslexia mode. By leveraging large language models for natural language processing and integrating Monster API's robust algorithms, Inclusiv transcends conventional assistive technologies. This blend of intuitive design and advanced AI capabilities was continuously refined through iterative user testing and feedback, ensuring that the product genuinely resonates with the needs of its users. Inclusiv’s launch is not an end, but a beginning to an ongoing journey of innovation and improvement, with a commitment to evolving and expanding its features to foster an inclusive web experience for all."},{"heading":"Challenges we ran into","content":"Where to start? Front-end might be harder than the back end! We spent hours fiddling with the optimal UI setup, trying to figure out how best to simplify the experience for a user of any skill level. This is not as simple as it looks, and we spent two hours trying to add a power button. Additionally, we ended up having to pivot our AI model numerous times and switched our approach throughout the project."},{"heading":"Accomplishments that we're proud of","content":"We ended up changing our AI agent multiple times throughout the project to deliver the optimal project. We're really satisfied with our ability to adapt and push ourselves out of our comfort zones throughout the 36 hours. We developed numerous, technically difficult and varied features that we believe will truly aid those with any form of disability."},{"heading":"What we learned","content":"We learned tons about building a project from scratch, implementing LLMs, and creating an intuitive and appealing front-end experience."},{"heading":"What's next for Inclusiv.ai","content":"We want to launch on the Chrome web store!"},{"heading":"Built With","content":"chatgpt css html javascript llms natural-language-processing openai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"mentis.ai","project_url":"https://devpost.com/software/ai-thena","tagline":"AI-powered tutor designed to be accessible, interactive and audiovisual.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/774/725/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Use of Monster Generative AI APIs (4x XBox Series S [1st] & 1 million Monster API credits [2nd] & $400 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"fastapi","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"monsterapi","url":null},{"name":"nextjs","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/jtcheng26/treehacks24"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration behind Mentis emerged from a realization of the vast potential that a personalized learning AI platform holds in transforming education. We envisioned an AI-driven mentor capable of adapting to individual learning styles and needs, making education more accessible, engaging, and effective for everyone. The idea was to create an AI that could dynamically update its teaching content based on live user questions, ensuring that every learner could find a path that suits them best, regardless of their background or level of knowledge.\n\nWe wanted to build something that encompasses the intersection of: accessibility, interactivity and audiovisual."},{"heading":"What it does","content":"Mentis is an AI-powered educational platform that offers personalized learning experiences across a wide range of topics. It is able to generate and teach animated lesson plans with both visuals and audio, as it generates checkpoint questions for the user and listens to the questions of its users and dynamically adjusts the remainder of the teaching content and teaching methods to suit their individual learning preferences. Whether it's mathematics, science, or economics, Mentis provides tailored guidance, ensuring that users not only receive answers to their questions but also a deep understanding of the subject matter."},{"heading":"How we built it","content":"At its core, a fast API backend powers the intelligent processing and dynamic delivery of educational content, ensuring rapid response to user queries. This backend is complemented by our use of advanced Large Language Models (LLMs), which have been fine-tuned to understand a diverse range of educational topics and specialize in code generation for the best animation, enhancing the platform's ability to deliver tailored learning experiences.\n\nWe curated a custom dataset in order to leverage LLMs to the fullest and reduce errors in both script and code generation. Using our curated datasets, we were able to fine-tune models using MonsterAPI tailoring our LLMs and improve accuracy.. We implemented several API calls to ensure a smooth and dynamic operation of our platform, for general organization of the lesson plan, script generation, audio generation with ElevenLabs, and code generation for the manim library we utilize to create the animations on our front end in Bun and Next.js.\n\nFine tuned open source model\n\nCurated custom dataset"},{"heading":"Challenges we ran into","content":"Throughout the development of Mentis, we encountered significant challenges, particularly in setting up environments and installing various dependencies. These hurdles consumed a considerable amount of our time, persisting until the final stages of development.\n\nEvery stage of our application had issues we had to address: generating dynamic sections for our video scripts, ensuring that the code is able to execute the animation, integrating the text-to-speech component to generate audio for our educational content all introduced layers of complexity, requiring precise tuning and a lot of playing with to set up.\n\nThe number of API calls needed to fetch, update, and manage content dynamically, coupled with ensuring the seamless interaction between the user and our application, demanded a meticulous approach. We found ourselves in a constant battle to maintain efficiency and reliability, as we tried to keep our latency low for practicality and interactivity of our product."},{"heading":"Accomplishments that we're proud of","content":"Despite the setbacks, we are incredibly proud of:\n\nTechnical Overcomes: Overcoming significant technical hurdles, learning from them and enhancing our problem-solving capabilities. Versatile System: Enabling our platform to cover a broad range of topics, making learning accessible to everyone. Adaptive Learning: Developing a system that can truly adapt to each user's unique learning style and needs. User-Friendly UI: Creating a user-friendly design and experience keeping our application as accessible as possible. API Management: Successfully managing numerous API calls, we smoothed the backend operation as much as possible for a seamless user experience. Fine tuned/Tailored Models: Going through the full process of data exploration & cleaning, model selection, and configuring the fine-tuned model."},{"heading":"What we learned","content":"Throughout the backend our biggest challenge and learning point was the setup, coordination and training of multiple AI agents and APIs.\n\nFor all of us, this was our first time fine-tuning a LLM and there were many things we learned through this process such as dataset selection, model selection, fine-tuning configuration. We gained an appreciation for all the great work that was being done by the many researchers. With careful tuning and prompting, we were able to greatly increase the efficiency and accuracy of the models.\n\nWe also learned a lot about coordinating multi-agent systems and how to efficiently have them run concurrently and together. We tested many architectures and ended up settling for one that would optimize first for accuracy then for speed. To accomplish this, we set up an asynchronous query system where multiple “frames” can be generated at once and allow us not to be blocked by cloud computation time."},{"heading":"What's next for mentis.ai","content":"Looking ahead, Mentis.ai has exciting plans for improvement and expansion:\n\nReducing Latency: We're committed to enhancing efficiency, aiming to minimize latency further and optimize performance across the platform.\n\nInnovative Features: Given more time, we plan to integrate cutting-edge features, like using HeyGen API to create natural videos of personalized AI tutors, combining custom images, videos, and audio for a richer learning experience.\n\nClassroom Integration: We're exploring opportunities to bring Mentis into classroom settings, testing its effectiveness in a real-world educational environment and tailoring its capabilities to support teachers and students alike."},{"heading":"Built With","content":"bun fastapi javascript llm monsterapi nextjs python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"NoWaste.ai","project_url":"https://devpost.com/software/nowaste-ai","tagline":"Providing a practical and scalable solution to every-day food waste.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/773/039/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Postman: Best Use of an API with Postman (4x Flipper0 + Postman Merch)"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"tailwind","url":null},{"name":"togetherai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/RussellBustamante/NoWasteAI"},{"label":"www.canva.com","url":"https://www.canva.com/design/DAF9JEwxwt4/CpUcKgNiGDjDeKiWDRBMqg/view"}],"description_sections":[{"heading":"Our Inspiration","content":"Have you ever looked inside the fridge, saw a particular piece of food, and wondered if it was still edible? Maybe you felt it, or looked it up online, or even gave it the old sniff test! Yet, at the end of the day, you still threw it away.\n\nDid you know that US citizens waste an average of 200lbs of food per year, per person? Much of that waste can be traced back to food casually tossed out of the fridge without a second thought. Additionally, 80% of Americans rely on the printed ‘best by’ dates to determine if their food is still fresh, when in reality it is almost never a true indicator of expiration. We aim to address these problems and more."},{"heading":"What do we do?","content":"NoWaste.ai is a two-pronged approach to the same issue, allowing us to extend our support to users all over the world. At its core, NoWaste allows users to snap photos of an edible product, describe its context, and harness the power of generative AI to receive a gauge on the quality of their food. An example could be the following: \"Pizza left on a warm counter for 6 hours, then refrigerated for 8\" + [Picture of the pizza], which would return the AI's score of the food's safety.\n\nIn developed countries, everyone has a smartphone, which is why we provide a mobile application to make this process as convenient as possible. In order to create a product that users will be more likely to consistently use, the app has been designed to be extremely streamlined, rapidly responsive, and very lightweight.\n\nHowever, in other parts of the world, this luxury is not so common. Hence, we have researched extensively to design and provide a cheap and scalable solution for communities around the world via Raspberry Pi Cameras. At just $100 per assembly and $200 per deployment, these individual servers have potential to be dispersed all across the world, saving thousands of lives, millions of pounds of waste, and billions of dollars."},{"heading":"How we built it","content":"Our hardware stack is hosted completely on the Raspberry Pi in a React/Flask environment. We utilize cloud AI models, such as LLAMA 70B on Together.ai and GPT4 Vision, to offload computation and make our solution as cheap and scalable as possible. Our software stack was built using Swift and communicates with similar APIs.\n\nWe began by brainstorming the potential services and technologies we could use to create lightweight applications as quickly as possible. Once we settled on a general idea, we split off and began implementing our own responsibilities: while some of us prototyped the frontend, others were experimenting with AI models or doing market research. This delegation of responsibility allowed us to work in parallel and design a comprehensive solution for a problem as large (yet seemingly so clear) as this.\n\nOf course, our initial ideas were far from what we eventually settled on."},{"heading":"Challenges we ran into","content":"Finalizing and completing our stack was one of our greatest challenges. Our frontend technologies changed the most over the course of the weekend as we experimented with Django, Reflex, React, and Flask, not to mention the different APIs and LLM hosts that we researched. Additionally, we were ambitious in wanting to only use open-source solutions to further drive home the idea of world-wide collaboration for sustainability and the greater good, but we failed to identify a solution for our Vision model. We attempted to train LLAVA using Intel cloud machines, but our lack of time made it difficult as beginners. Our team also faced hardware issues, from broken cameras to faulty particle sensors. However, we were successful in remedying each of these issues in their own unique ways, and we are happy to present a product within the timeframe we had."},{"heading":"Accomplishments that we're proud of","content":"We are incredibly proud of what we were able to accomplish in such a short amount of time. We were passionate about both the underlying practicalities of our application as well as the core implementation. We created not only a webapp hosted on a Raspberry Pi, equipped with odor sensors and a camera, but a mobile app prototype and a mini business plan as well. We were able to target multiple audiences and clear areas where this issue prevails, and we have proposed solutions that suit all of them."},{"heading":"What's next for NoWaste.ai","content":"The technology that we propose is infinitely extensible. Beyond this weekend at TreeHacks, there is room for fine-tuning or training more models to produce more accurate results on rotting and spoiled food. Dedicated chips and board designs can bring the cost of production and extension down, making it even easier to provide solutions across the world. Augmented Reality is becoming more prevalent every day, and there is a clear spot for NoWaste to streamline our technology to work seamlessly with humans. The possibilities are endless, and we hope you can join and support us on our journey!"},{"heading":"Built With","content":"flask javascript openai python raspberry-pi react swift tailwind togetherai"},{"heading":"Try it out","content":"github.com www.canva.com"}]},{"project_title":"SageWell","project_url":"https://devpost.com/software/sagewell","tagline":"An AI-powered Medical Assistant for the Elderly","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/768/800/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Best Use of Monster Generative AI APIs (4x XBox Series S [1st] & 1 million Monster API credits [2nd] & $400 Cash [3rd])"}],"team_members":[],"built_with":[{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"monsterapi","url":null},{"name":"react.js","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/PrabhleenKaurLamba/SageWell"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration for creating this AI project stemmed from the desire to leverage current technology to enhance the lives of our community members, particularly the elderly. Recognizing the difficulties faced by aging individuals, including our own grandparents, to embrace technological advancements, and their concerns about health conditions, we sought a way to integrate technology and healthcare services seamlessly into their lives. Thus, SageWell was conceived—an AI companion designed to provide personalized support to seniors by navigating medical information alongside their healthcare providers. At SageWell, we believe in accessibility of AI for all age groups."},{"heading":"What it does","content":"SageWell enables users to seek medical information through natural language interactions, allowing elderly individuals to pose questions verbally and receive spoken responses, mimicking human conversation."},{"heading":"How we built it","content":"SageWell leverages the capabilities of two Monster APIs: OpenAI-Whisper Large-v2 for speech-to-text conversion and Meta's llama-2-7b-chat-hf for refining our reinforcement learning model. Our model was trained on MedQuAD, a comprehensive dataset containing over 47,000 medical question-answer pairs, Drugs, Side Effects and Medical Condition dataset, and Drugs Related to Medical Conditions dataset. The frontend of the web app was built using React, a JavaScript library, and the service we integrated into our app to store all the audio files was Firebase."},{"heading":"Challenges we ran into","content":"In the process of developing SageWell, we encountered several challenges. Since there were many integrations in our application, from speech-to-text transcription and MonsterAPIs for fine-tuned LLMs, to storage service providers, we faced difficulties trying to link all the individual pieces together. We learned several new tools and technologies and also spent time fine-tuning models to provide medical information. Additionally, we encountered setbacks when we were finishing up our project as we were facing CORS errors when making API calls from the browser - we were able to work around this by adding a proxy, which served as a bridge between our client and server."},{"heading":"Accomplishments that we're proud of","content":"We empowered our primary user demographic, the elderly, to engage with SageWell through voice interactions instead of having to type text. We addressed the challenges that many seniors encounter when typing on digital devices, thereby increasing the accessibility of the process of seeking medical information and ensuring inclusivity in AI for all age groups, especially the elderly.\n\nOur reinforcement learning model has demonstrated effectiveness, evidenced by the decrease in training loss after each iteration and to 0.756 at the end. This indicates that the model fits well on the training data.\n\nWe merged three datasets: the Medical Question Answering data set, the Drugs, Side Effects and Medical Condition data set, and the Drugs Related to Medical Conditions data set. By training our model for SageWell on these datasets obtained from NIH websites and drugs.com, we allowed SageWell to provide medical information to users based on reliable, comprehensive databases."},{"heading":"What we learned","content":"Through this project, we gained valuable insights into the startup ecosystem and developer space. We expanded our skill set by refining our model, working with new APIs and storage providers, and creating a solution that addresses the specific challenges faced by our target audience."},{"heading":"What's next for SageWell","content":"The journey of SageWell is just beginning. Moving forward, we aim to expand its capabilities to assist in additional areas crucial for the well-being of the elderly, such as medication reminders and guidance on accessing support for domestic chores. Furthermore, we envision integrating features that facilitate connections between seniors and younger generations, including their grandchildren and other youth in their communities. By fostering these intergenerational connections, SageWell will expand its targeting market size by not only keeping the elderly engaged with their loved ones but also ensuring they remain connected to the evolving world around them. Through SageWell, we look forward to continuing to push for accessibility of AI for all age groups."},{"heading":"Built With","content":"firebase javascript llm monsterapi react.js"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Questi - Roblox","project_url":"https://devpost.com/software/knowledgequest-roblox","tagline":"Projected at $52B by 2030, gamified education is booming. Through Roblox Studio, we're bridging global education gaps, fostering collaboration, and supporting learners' well-being through our project","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/768/589/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Roblox: Visionary Award (4x Roblox Blankets + Backpacks + $100 Gift Card)"}],"team_members":[],"built_with":[{"name":"lua","url":"https://devpost.com/software/built-with/lua"},{"name":"luau","url":null},{"name":"opentrivia.db","url":null},{"name":"reaper","url":null},{"name":"roblox","url":null}],"external_links":[{"label":"www.roblox.com","url":"https://www.roblox.com/games/16409878855/Questi-The-Ultimate-Education-Experience"}],"description_sections":[{"heading":"What it does","content":"\"Questi,\" is an educational gameplay experience designed to promote collaborative learning and teamwork. Users can join with their friends, classmates, or team members to tackle course-related questions within a set timeframe. By leveraging their collective knowledge and collaborating effectively, players work together to answer all the questions and complete the game before time runs out. \"Questi\" transforms traditional learning into an immersive and engaging multiplayer experience, where participants not only deepen their understanding of course material but also develop essential teamwork and problem-solving skills."},{"heading":"How we built it","content":"Our team utilized Roblox Studio to create our virtual world. Through precise scripting in Luau, we engineered dynamic gameplay mechanics, seamlessly integrating an API to fetch course material questions. With an eye for design and optimization, we fine-tuned every aspect of the project to ensure optimal performance and user experience. Effective communication and collaboration were critical as we navigated technical challenges and brought our collective vision to life, culminating in a groundbreaking educational platform that blends gaming and learning seamlessly."},{"heading":"Challenges we ran into","content":"Designing the UI/UX for smooth progression proved tricky Connecting the script to track group progress was complex. We encountered difficulties with client-to-server communication Most of us had to learn Luau syntax and Roblox Studio from scratch Sometimes the API did not respond correctly"},{"heading":"Next Steps","content":"Integrating with an API that allows you to customize questions (users can make their own custom quizzes) Add more maps/levels and types of challenges/game modes to keep the users engaged Reward users for completing quests with a leaderboard and currency that can be used to purchase in-game items"},{"heading":"Built With","content":"lua luau opentrivia.db reaper roblox"},{"heading":"Try it out","content":"www.roblox.com"}]},{"project_title":"CodeRepair","project_url":"https://devpost.com/software/coderepair","tagline":"Tool to automatically identify and fix bugs in codebases using GPT-4 to use debugger tools and automatically erase bugs. Scales to codebase of any size.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"YCombinator: YC / Yoneda Labs Prize (Office Hours with YC Group Partner)"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/jasondu7297/coderepair.ai"}],"description_sections":[{"heading":"Inspiration","content":"Many aspects of the programming workflow have been automated by the introduction of LLMs—especially with regard to code synthesis with products such as Github Copilot, AlphaCode and more. Other companies such as CodeGen and Sweep AI have begun tackling more niche subtasks, such as automatically solving tickets, conducting unit testing, catching security vulnerabilities, and more. However the complexity of coding tasks that can be solved with language models such as GPT4 is still very limited.\n\nSpecifically, LLMs are used to scan and analyze codebases at a surface level to suggest code changes, identify errors, and fill code gaps. On the other hand, when humans debug code, they leverage tools such as GDB to inspect the state of a program’s internal variables and logic to garner a more complete picture of the root cause of the bug. We put these same developer tools in the hands of GPT4 to augment its current capacity for debugging code in hopes that it will be more capable in addressing runtime errors, algorithmic errors, and more."},{"heading":"What it does","content":"CodeRepair identifies and fixes errors that cause runtime failures. Anything from segmentation faults and logic errors, to unit test failures and integration test failures… We allow GPT4 to conduct a far deeper analysis of the code’s behavior by giving it control over the GDB debugging tool. You think that’s it? We were also able to provide monitored access to shell commands so GPT can dynamically explore the codebase, circumventing its normal context length limitations.\n\nAs far as we know, this is the FIRST application of LLMs where developer tools like GDB were provided to further beef up the robust model."},{"heading":"How we built it","content":"We envisioned an agentic workflow where GPT4 can determine its own decisions while requesting for additional knowledge in the form of shell command and GDB command outputs from our program. More specifically, CodeRepair is based on Python and the OpenAI API, using the GPT4 Turbo model."},{"heading":"Challenges we ran into","content":"Designing the backend architecture of the GPT agent proved to be a much larger challenge than any of us expected. We spent a significant portion of our time testing a solution that eventually appeared unnecessarily complicated and highly undoable in the time constraints.\n\nOne of the largest challenges we ran into was scalability. This problem was twofold: How do we deal with really large codebases that would exceed GPT context length? Come watch our live demo to see this unfold! How do we deal with really large agent traces? We didn’t want the length of the [(action, output), (action, output), …] chain produced by a single agent to grow too large due to performs concerns. This was a large consideration for us while developing and motivated our original approach which was to create multiple subagents, all of which were instantiated and orchestrated by a single main agent. This approach proved to overcomplicate the task, as getting the orchestrating agent to perform well was very difficult."},{"heading":"Accomplishments that we're proud of","content":"Watching GPT effectively use gdb and intelligently step through the code base was super cool and something that we are proud of. We are not aware of any other software that gives a language model that capability so it was definitely great to watch our program doing this."},{"heading":"What we learned","content":"We learned the importance of providing GPT with deliberate instructions. One of the most difficult things was getting GPT to produce the output in a correct structure so that it could be reliably used by later parts of our program. One of the things we found is that in order to do this we really had to spoon feed to GPT exactly what it was and wasn’t allowed do."},{"heading":"What's next for CodeRepair","content":"There is a lot in store for CodeRepair that we are super excited about. One particular thing we are really eager to work on next is to generalize our program to work for all languages rather than being bottlenecked by the debuggers available to us.\n\nhttps://github.com/jasondu7297/coderepair.ai"},{"heading":"Built With","content":"python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"LEED Bud","project_url":"https://devpost.com/software/leed-bud","tagline":"Get around the burdensome process of points-based LEED certification and discover where you currently stand based on Large Language Models contextualized with LEED codes and your building specs.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/775/760/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Stanford Ecopreneurship: Best Solves the User’s Pain Point ($1k Cash)"}],"team_members":[],"built_with":[{"name":"nextjs","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"stackai","url":null},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/garvcodes/leed/commit/843f90ee6668363d91e8c5db94ead643ef3bdfed"}],"description_sections":[{"heading":"Inspiration","content":"One of our team members is a community manager for a real estate development group that often has trouble obtaining certifications in their attempts to develop eco-friendly buildings. The trouble that they go through, leaves them demotivated because of the cost and effort, leading them to avoid the process altogether, choosing to instead develop buildings that are not good for the environment.\n\nIf there was some way that they could more easily see what tier of LEED certification they could fall into and furthermore, what they need to do to get to the NEXT tier, they would be more motivated to do so, benefiting both their building practices as well as the Earth."},{"heading":"What it does","content":"Our product is a model that takes in building specifications and is trained on LEED codes. We take your building specifications and then answer any questions you may have on your building as well as put it into bronze, silver, gold, or platinum tiering!"},{"heading":"How we built it","content":"The project structure is NextJS, React, Tailwind and for the ai component we used a custom openai api contextualized using past building specs and their certification level. We also used stack ai for testing and feature analysis."},{"heading":"Challenges we ran into","content":"The most difficult part of our project was figuring out how to make the model understand what buildings fall into different tiers."},{"heading":"Accomplishments that we're proud of","content":"GETTING THIS DONE ON TIME!!"},{"heading":"What we learned","content":"This is our first full stack project using ai."},{"heading":"What's next for LEED Bud","content":"We're going to bring this to builders across Berkeley for them to use! Starting of course at the company of our team member!"},{"heading":"Built With","content":"nextjs openai python react stackai tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Oasis","project_url":"https://devpost.com/software/oasis-8iv1lu","tagline":"AI agents help benchmark, assess, and enhance corporate sustainability practices","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/769/851/datas/medium.jpeg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Endex: Senior Citizen Prize ($1k Cash)"}],"team_members":[],"built_with":[{"name":"figma","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"iris","url":null},{"name":"langchain","url":null},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/anishk23733/oasis-scripts"},{"label":"github.com","url":"https://github.com/anishk23733/oasis-backend"},{"label":"github.com","url":"https://github.com/ShaishavShah04/oasis"},{"label":"www.figma.com","url":"https://www.figma.com/file/RGLohV0wO3MG8fKqMxMVgY/Stanford-Treehacks"}],"description_sections":[{"heading":"Inspiration","content":"As the world grapples with challenges like climate change, resource depletion, and social inequality, it has become imperative for organizations to not only understand their environmental, social, and governance (ESG) impacts but also to benchmark and improve upon them. However, one of the most significant hurdles in this endeavor is the complexity and inaccessibility of sustainability data, which is often buried in lengthy official reports and varied formats, making it challenging for stakeholders to extract actionable insights.\n\nRecognizing the potential of AI to transform this landscape, we envision Oasis as a solution to democratize access to sustainability data, enabling more informed decision-making and fostering a culture of continuous improvement toward global sustainability goals. By conversing with AI agents, companies are able to collaborate in real-time to gain deeper insights and work towards solutions."},{"heading":"What it does","content":"Oasis is a groundbreaking platform that leverages AI agents to streamline the parsing, indexing, and analysis of sustainability data from official government and corporate ESG reports. It provides an interface for companies to assess their records and converse with an AI agent that has access to their sustainability data. The agent helps them benchmark their practices against practices of similar companies and narrow down ways that they can improve through conversation.\n\nCompanies can effortlessly benchmark their current sustainability practices, assess their current standings, and receive tailored suggestions for enhancing their sustainability efforts. Whether it's identifying areas for improvement, tracking progress over time, or comparing practices against industry standards, Oasis offers a comprehensive suite of features to empower organizations in their sustainability journey."},{"heading":"How we built it","content":"Oasis uses a sophisticated blend of the following:\n\nLLM (LLaMA 2) parsing to parse data from complex reports. We fine-tuned an instance of meta-llama/Llama-2-7b-chat-hf on the HuggingFace dataset Government Report Summarization using MonsterAPI. We use this model to parse data points from ESG PDF text, since these documents are in a non-standard format, into a JSON format. LLMs are incredibly powerful at extracting key information and summarization, which is why we see such a strong use case here. Open-source text embedding model (SentenceTransformers) to index data including metrics and data points within a vector database. LLM-parsed data points contain key descriptors. We use an embedding model to index these descriptors in semantic space, allowing us to compare similar metrics across companies. Two key points may not have the same descriptions, but are semantically similar, which is why indexing with embeddings is beneficial. We use the SentenceTransformer model msmarco-bert-base-dot-v5 for text embeddings. We also use the InterSystems IRIS Data Platform to store embedding vectors, on top of the LangChain framework. This is useful for finding similar metrics across different companies and also for RAG, as discussed next. Retrieval augmented generation (RAG) to incorporate relevant metrics and data points into conversation To enable users to converse with the agent and inspect and make decisions based on real data, we use RAG integrated with our IRIS vector database, running on the LangChain framework. We have a frontend UI for interacting with our agent in real time. Embedding similarity to semantically align data points for benchmarking across companies Our frontend UI also presents key metrics for benchmarking a user’s company. It uses embedding similarity to find company metrics and relevant metrics from other companies."},{"heading":"Challenges we ran into","content":"One of the most challenging parts of the project was prompting the LLM and running numerous experiments until the LLM output matched what was expected. Since LLMs are non deterministic in nature and we required outputs in a consistent JSON form (for parsed results), we needed to prompt the LLM and reinforce the constraints multiple times. This was a valuable lesson that helped us learn how to leverage LLMs in intricate ways for niche applications."},{"heading":"Accomplishments that we're proud of","content":"We are incredibly proud of developing a platform that not only addresses a critical global challenge but does so with a level of sophistication and accessibility that sets a new standard in the field. Successfully training AI models to navigate the complexities of ESG reports marks a significant technical achievement. The ability to turn dense reports into clear, actionable insights represents a leap forward in sustainability practice."},{"heading":"What we learned","content":"Throughout the process of building Oasis, we learned the importance of interdisciplinary collaboration in tackling complex problems. Combining AI and sustainability expertise was crucial in understanding both the technical and domain-specific challenges. We also gained insights into the practical applications of AI in real-world scenarios, particularly in how NLP and machine learning can be leveraged to extract and analyze data from unstructured sources. The iterative process of testing and feedback was invaluable, teaching us that user experience is as important as the underlying technology in creating impactful solutions."},{"heading":"What's next for Oasis","content":"The journey for Oasis is just beginning. Our next steps involve expanding the corpus of sustainability reports to cover a broader range of industries and geographies, enhancing the platform's global applicability. We are also exploring the integration of predictive analytics to offer forward-looking insights, enabling users to not just assess their current practices but also to anticipate future trends and challenges. Collaborating with sustainability experts and organizations will remain a priority, as their insights will help refine our models and ensure that Oasis continues to meet the evolving needs of its users. Ultimately, we aim to make Oasis a cornerstone in the global effort towards more sustainable practices, driving change through data-driven insights and recommendations."},{"heading":"Built With","content":"figma flask iris langchain mongodb next.js python"},{"heading":"Try it out","content":"github.com github.com github.com www.figma.com"}]},{"project_title":"CarbonInsight: Guiding Companies to Net-zero ","project_url":"https://devpost.com/software/carbon-price-curves","tagline":"Using supply and demand projections, our software advises companies on the tradeoff between decarbonizing their existing business practices and offsetting emissions through carbon credit purchasing.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/771/644/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Stanford Ecopreneurship: Best Incorporation of Broader Stakeholders ($1k Cash)"}],"team_members":[],"built_with":[{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/lfun1/carbon-price-curves/"}],"description_sections":[{"heading":"Inspiration","content":"As the imperative to reduce carbon emissions intensifies, businesses are increasingly adopting innovative approaches to shrink their carbon footprint. One such approach, carbon offsetting, has grown into a $5.5 billion industry , garnering support from both fossil fuel companies and environmental advocates alike. Rather than curbing emissions directly at their source, carbon offsets mitigate carbon pollution by either preventing potential emissions or directly extracting carbon dioxide from the atmosphere, often employing technologies such as carbon capture and storage (CCS) or direct air capture (DAC). Presently, numerous critical sectors—such as the aviation industry—face challenges in achieving decarbonization due to financial constraints, material limitations, and a lack of innovative solutions. As such, the combination of decarbonization strategies and carbon offsetting is often the most economical; by reducing easily avoidable emissions and offsetting the rest, companies can achieve net-zero emissions without the financial pressures of complete emissions reductions.\n\nHowever, there are three existing barriers to achieving this diversified net-zero strategy. First, reducing emissions is often a lengthy process, often taking years to decarbonize key components of a company’s business model fully. Second, reducing low-cost emissions is dependent on the equilibrium price of carbon offset—a price that often needs to be calculated far in advance to successfully execute decarbonization. Third, the financial tradeoff between the price of offsets and emissions reductions is blurry, with no clear brightline between offset and reduced emissions."},{"heading":"What does our project do?","content":"Our product solves these issues by providing key insights about decarbonization strategy through forward pricing curves for carbon offsets. By calculating the expected demand and supply of carbon offsets for a given time in the future, the market equilibrium—the going price of carbon offsets at that time—can be estimated and given to the user in the present. Our results are displayed on a website dashboard.\n\nOur project contains two central components: the market price of carbon credits over time, based on calculated demand and supply curves, and a carbon-zero pathway based on these curves (based on user inputs and other external data). The demand and supply curves are generated for each year until the user’s target year, this allows them to see how the market changes year-by-year, allowing them to optimize their offset buying distribution over a certain period. The carbon-zero pathway allows\n\nAfter inputting information about a company’s carbon footprint, emissions targets, and the marginal cost of abatement, financial metrics about the best carbon-zero path are computed. The output of this function relies on market equilibrium outputs only available after computing the forward price curves. The output gives the user valuable information about the percentage breakdown between decarbonized emission reductions and credit offsets, and the financial projections of both strategies.\n\nThis tool, importantly, allows companies to see how the market price of carbon credits changes over time based on user-given inputs such as the type of carbon removal technology, the year of their net-zero goal, and the scope of emissions they are considering. This tool emphasizes the importance of decarbonization by reorienting the goal of maximizing profits towards environmental advocacy. This is key in assisting companies plan their emissions-reduction strategy in the most economical way—two priorities that have often been at odds with each other."},{"heading":"How we built our project","content":"We built all of our projection models in Python and used the Reflex framework to display our outputs in a user dashboard. Each of the three system functions—the demand curve, supply curve, and decarbonization-offset tradeoff scheme—are integrated into the user dashboard, with various inputs related to the computation of the base futures curve model and tradeoff model being imputed as fields. Each of the three system functions are described below:\n\nThe demand curve uses the emissions profiles of the top 2,000 largest companies in the world to project their future carbon emissions based on their emissions reduction goals. Using generated GICS sector data for industry-specific emissions—combined with company revenue—we project yearly emissions—including scope 1, 2, and 3 emissions—depending on carbon reduction goals. Using an exponential Pareto distribution—tuned to represent companies buying offsets closer to their consumption date—we generalize the year-to-year demand for carbon permits on a per-company basis. This data returns the expected demand for carbon credits per year for a theoretical global market. Using this data, combined with decarbonization prices generalized from half-normal, sector-level decarbonization costs, our model produces the expected carbon demand for a given period for each company. As each company faces a different cost of decarbonizing their business, the marginal benefit of each company is unique. We fit a demand curve to these projected quantity and price demands from individual companies using polynomial regressions from this data. This process is done for each year in the model.\n\nThe supply curve is interpolated from the technology pathway of the carbon credit and price range. Based on our research, most companies pay anywhere from $50/ton CO2 to $700/ton CO2 for a range of carbon credits. This data is combined with the cost of implementing the selected pathway for a single ton of CO2 and the ranking of the industries based on their relative CO2 emissions. A coefficient is calculated on these values that inform the supply curve on the scale of 50$/tCO2 to 700$/tCO2. We create a supply curve for each year until the user-specified target year with the values being adjusted for the following years accordingly.\n\nThe decarbonization-offset tradeoff graph is produced by mapping another exponential Pareto distribution to the marginal cost of reducing carbon emissions. The resulting intersection of the market price of carbon offsets for a given year with the CDF returns the percentage of total emissions for which it is more economical to reduce the production of emissions over buying offsets, and vice versa. The total cost associated with purchasing carbon offsets, alongside the quantity, can be calculated for a given company’s carbon footprint."},{"heading":"Challenges we ran into","content":"The biggest challenge we ran into was the lack of data for building the supply curves. One way in which we handled this was by taking data corresponding to the maximum and minimum prices, as well as the range of carbon credits companies were buying in bulk. We used these to generate a pseudo-supply curve that would allow us to reasonably estimate the curve as if we had the data. We expect this problem to be solved as more data becomes available."},{"heading":"What we learned and accomplished","content":"We learned a lot about time-series analysis, data analysis, and synthesizing, as well as how to use Reflex to implement our website. We made a working model and website on a project that aimed to encapsulate the entire carbon credits market for the next few decades."},{"heading":"What's next for CarbonInsights","content":"Short Term: The biggest thing we would like to change is to make the website dynamically update. Although the models we wrote are perfectly suited to update dynamically , the Reflex framework we are using has made that difficult.\n\nLong Term: Improve the statistical models to create the projection for the supply-demand curves and use ARIMA, ETS, and ML models like Random Forests to validate and provide a more detailed analysis of the trends."},{"heading":"Built With","content":"reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Med2Meals","project_url":"https://devpost.com/software/nourish-connect","tagline":"Replace medicine with alternative natural food 🥘","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/772/030/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Crossmint: Best Identity Credential Application Hack (2x rabbit r1 + $1k credit [1st] & 4x Ledger Nano S Plus + $400 credit [2nd])"}],"team_members":[],"built_with":[{"name":"bun","url":null},{"name":"crossmint","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"fetch.ai","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next.js","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"postman","url":"https://devpost.com/software/built-with/postman"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"together.ai","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ShaneYokota72/Med2Meals"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Med2Meals sprang from the universal truth that food is more than just sustenance; it's medicine, comfort, and a catalyst for connection. In today's fast-paced world, we've seen an increasing reliance on pharmaceutical solutions to health issues, often overlooking the holistic benefits of natural remedies and the healing power of human connection. This realization was compounded by the global pandemic, which highlighted the detrimental effects of isolation on mental and physical health. Med2Meals was born out of a desire to revive the ancient wisdom that food can heal and to harness the digital age's potential to bring people together over the healing power of meals. We envisioned a platform that not only encourages a natural approach to healing through diet but also fosters a sense of community and support among individuals facing health challenges."},{"heading":"What it does","content":"Med2Meals connects individuals seeking natural dietary remedies for their health conditions with local chefs and home cooks who prepare and deliver home-cooked, healing meals. Users can input their specific health concerns or the type of medication they're aiming to supplement or avoid. The platform then suggests a variety of home-cooked recipes, each tailored to address those health issues with natural ingredients known for their healing properties.\n\nBeyond just providing recipes, Med2Meals offers a service where users can request these meals to be cooked and delivered by someone in their community. This feature aims to provide comfort through nourishing food while also opening the door to new friendships and a support network. The platform caters to a range of dietary preferences and health needs, ensuring that each user receives personalized care and nutrition.\n\nIn essence, Med2Meals is more than a meal delivery service; it's a community-building tool that leverages the nurturing power of food to heal bodies and connect souls."},{"heading":"How we built it","content":"Frontend Development : We utilized Next.js for the frontend to leverage its server-side rendering capabilities, ensuring a fast, and responsive user interface. Backend Infrastructure : Our backend is powered by Node.js and Express.js, forming a robust and scalable foundation. Integration of AI Technologies : We partnered with Together.ai to fine-tune and deploy Large Language Models (LLM) and Diffusion models tailored to our specific needs. These models are crucial for generating personalized meal recommendations and understanding user queries in natural language. AI Agents for Data Exchange : The seamless execution of LLM and diffusion models, along with data exchange between them, is facilitated by AI agents using fetch.ai. This innovative approach allows for real-time, intelligent processing and a highly personalized user experience. Blockchain Technology for Transactions : We employed Crossmint for blockchain ledger transactions between the user and chef. This ensures that every cooking opportunity a chef receives is cryptographically signed, providing a transparent and secure method to verify a chef's credibility through the NFTs they have minted. Database Management : MongoDB serves as our database management system, offering a flexible, scalable solution for storing and managing our data. API Documentation : The entire API documentation was meticulously maintained in a Postman Workspace. Development Tools : We adopted Bun as our package manager and JavaScript runtime. Bun's high performance and efficiency in package management and execution of JavaScript code significantly enhanced our development workflow, allowing us to build and deploy features rapidly."},{"heading":"Challenges we ran into","content":"Customized Recipe Generation : We encountered difficulties in crafting the ideal few-shot prompts for Large Language Models (LLMs) to generate customized recipes, which was crucial for tailoring dietary solutions. Building AI Agents with Fetch.ai : The challenge of navigating Fetch.ai's occasionally misleading documentation was significant. However, the assistance from the Fetch.ai team was instrumental in overcoming these hurdles. Idea Pivot : Initially, we faced a setback with our original concept, which seemed too cliché after our initial pitch to judges. This led us to pivot to a more unique and impactful problem statement, which ultimately defined our project's direction."},{"heading":"Accomplishments that we're proud of","content":"Successful Pivot : The decision to pivot our idea proved to be valuable. Moving away from a generic concept to tackle a unique problem statement has positioned us distinctively in the space. Diverse Technological Learning : Each team member embraced the challenge of learning new technologies, from AI agents and prompt engineering for LLMs to crypto signing. This diversity in learning has been one of our project's most enriching experiences."},{"heading":"What we learned","content":"AI Agents : The project deepened our understanding of AI agents, enhancing our ability to deploy intelligent solutions. Fine-tuning LLMs : We gained valuable insights into the process of fine-tuning Large Language Models to meet specific project needs. Crypto Signing : The importance and application of crypto signing were key learnings, opening new avenues for secure data handling. Teamwork : The project underscored the indispensable value of teamwork in overcoming challenges and achieving collective goals."},{"heading":"What's next for Med2Meals","content":"Speed Optimization : Currently, the process of fetching custom recipes using LLMs is slower than desired. Our immediate focus will be on improving the speed of this feature to enhance user experience. User Interface Improvements : We plan to refine the user interface to make it more intuitive and user-friendly, ensuring that our platform is accessible to everyone, regardless of their tech-savviness. Integration with Health Platforms : We are looking into integrating Med2Meals with existing health platforms and medical databases, to provide users with a seamless experience that bridges the gap between medical advice and dietary solutions."},{"heading":"Built With","content":"bun crossmint express.js fetch.ai javascript mongodb next.js node.js postman python together.ai"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Carbon Cut","project_url":"https://devpost.com/software/carbon-cut-3d5k2g","tagline":"Your one-stop shop for climate action and impact.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/773/792/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Cotopaxi: Most Innovative Sustainability Hack (4x Allpha 35L Backpack)"}],"team_members":[],"built_with":[{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"fast-api","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"google-maps","url":"https://devpost.com/software/built-with/google-maps"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"nextjs","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"render","url":null}],"external_links":[{"label":"treehacks-ui.onrender.com","url":"https://treehacks-ui.onrender.com"},{"label":"github.com","url":"https://github.com/Charu-Gupta9/TreeHacks-UI.git"},{"label":"github.com","url":"https://github.com/AdrianLandaverde/TreeHacks-Sustainability"},{"label":"docs.google.com","url":"https://docs.google.com/document/d/1MMkqM2OEAcbFvcQA_BEyp6pos4O13CJTXfWo1gbKGVU/edit?usp=sharing"}],"description_sections":[{"heading":"🌟 Inspiration","content":"Studies show that many people know about climate change and sustainable behaviors, but not as many people adjust their actions accordingly. This is largely due to inconvenience and the perception of no environmental impact. People know that climate change is bad, but they don’t know where to start. We want to bridge the gap between knowledge and action, making it easy for people to foster sustainable behaviors in their everyday lives."},{"heading":"💡 What it does","content":"Our website, Carbon Cut, assesses a person’s carbon footprint and offers personalized recommendations on how to make the most impact with the least effort. We want to emphasize that individuals are NOT the problem, but they CAN be part of the solution . By taking the effort of research out of their hands and quantifying their impact, Carbon Cut answers users’ two most prevalent pain points. We also have established the groundwork to implement credits, rewarding users for their sustainable choices. They will be able to use these credits for discounts on partner brands, such as (ideally!) Cotopaxi, Patagonia, Reformation, and other sustainable brands. These environmental features are:\n\nItem checker: You can upload a photo from the tag that contains the details of any product (mostly clothes) and it will assign a grade to your item based on the carbon impact it has. Sustainable Restaurants : You can input any location you want, and it will display you a map with the most sustainable restaurants near you in a map, with their name and location Transportation Tracker : Just select the origin and destination of your travel and it will display you the most sustainable solution to arrive at that place\n\nFurthermore, in order to create more consciousness about the environment, we have a chatbot in which you can \"talk\" to some of the priority places of the planet, such as the Amazon Rainforest, Northern Great Plains, the Coral Triangle, among others. You can ask whatever you want to these places as if they were a person"},{"heading":"🛠️ How we built it","content":"In the creation of Carbon Cut, we began with research and brainstorming. Literature showed that high prices, perception of no environmental impact, greenwashing, inconvenience, and social image were among the biggest barriers to sustainable action. Thus, we established features to address many of these challenges and sought out feedback from other hackers (Doc link posted with all feedback and iterations). The resounding consensus of the 8 hackers we surveyed was that inconvenience and perception of no environmental impact were the two most prevalent barriers to action. With that reassurance in mind, we tailored our website to emphasize impact and optimize convenience.\n\nFrom a technical perspective, the front end was made using HTML, CSS, and Next.Js. The backend was made by creating an API using FastAPI.\n\nThe Item Checker was made using Computer Vision (Optical Character Recognition) and a custom few-shot LLM built on gpt-3 The Restaurants feature was made using Google Places API The Routes feature was made using Google Routes API The Planet LLM is a Retrieval Augmented Generative Model trained with data from WWF, using a Vector Database from Chroma with dozens of documents"},{"heading":"🚧 Challenges we ran into","content":"All's well that ends well. With that being said, no hackathon is complete without its challenges. We had 2 main challenges. Firstly, we had some issues when deploying our Chroma Vector Database, due to different dependencies and it being too heavy to be deployed on Render, so we used Google App Engine for the RAG model. Secondly, we also had some issues when deploying the image-to-text model, since we used Pytesseract and the model needed to install a file on our computer, which couldn't be done in the deployed instance, hence we had to change the way we identified the text in any image."},{"heading":"🏆 Accomplishments that we're proud of","content":"We are proud of our team. Coming from universities in Pennsylvania, Florida, and Mexico, we formed our team on Slack and met at the TreeHacks orientation. From that moment on, we maintained open communication, kept a strong work ethic, and shared many laughs.\n\nWe are also proud of the progress we have made on Carbon Cut in 36 hours and the potential it holds to shape the lives of many people. One hacker, who preferred to remain anonymous, said of Carbon Cut, \"When you see carbon footprint used in marketing it's always very condemning. It’s nice to see something focused on action.\" Another noted, in our first round of user pain point research, that inconvenience was one of their major barriers and that they would use a website such as Carbon Cut, \"especially if it showed me how much impact it would have.\" Thanks to our multi-disciplinary team and the thoughtful responses of our fellow hackers, Carbon Cut now features a dashboard that quantifies impact in a variety of ways, from percent change to trees planted to Olympic-sized swimming pools of water saved. Through the use of social math, we are proud that Carbon Cut can be used, understood, and enjoyed by a wide range of users."},{"heading":"📚 What we learned","content":"This project was an exercise in all of our skills, including our ability to learn! We tackled Carbon Cut with an army of old and new skills, and even some that we learned at the various TreeHacks workshops. From Next.js to RAG Models to generative UI (shoutout to Guillermo Rauch from Vercel!), this weekend was nothing short of an exercise in experiential learning."},{"heading":"🔮 What's next for Carbon Cut","content":"Next up for Carbon Cut is increased precision carbon tracking, partnerships, and credits. We hope to continually improve our carbon emissions tracking and keep personalized recommendations modern and up-to-date with current technologies. Additionally, we plan to establish credits to reward users for choosing sustainable alternatives. These credits will be redeemable for discounts towards partnered sustainable brands, such as Cotopaxi and Patagonia. Lastly, as always, Carbon Cut is here to help you . If you have any requests or ideas on how to improve Carbon Cut, please don't hesitate to reach out!"},{"heading":"Built With","content":"chroma css fast-api git github google-cloud google-maps html javascript nextjs openai python react render"},{"heading":"Try it out","content":"treehacks-ui.onrender.com github.com github.com docs.google.com"}]},{"project_title":"Test Project","project_url":"https://devpost.com/software/test-project-skqc27","tagline":"Test Project","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Workshop Raffle Prizes ($10k in Prizes)"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[],"description_sections":[{"heading":"What's next for Test Project","content":"test"},{"heading":"Built With","content":"python"}]},{"project_title":"MindMint: Empowering Education with AI & NFTs","project_url":"https://devpost.com/software/learnboost-empowering-education-with-ai-nfts","tagline":"NFTs, Verifiable Credentials, AI Generated Feedback, Spaced Repetition all in an Educational App: Improves Learning Experience for Children with incentives, Allows Teachers to Manage Classes with ease","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/002/774/503/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2024","hackathon_url":"https://treehacks-2024.devpost.com/","prize_name":"Crossmint: Best Identity Credential Application Hack (2x rabbit r1 + $1k credit [1st] & 4x Ledger Nano S Plus + $400 credit [2nd])"}],"team_members":[],"built_with":[{"name":"crossmint","url":null},{"name":"expo.io","url":"https://devpost.com/software/built-with/expo-io"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"react.js","url":null},{"name":"together.ai","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/shray3420/treehacks2024"}],"description_sections":[{"heading":"Inspiration","content":"Inspired by the learning incentives offered by Duolingo, and an idea from a real customer (Shray's 9 year old cousin), we wanted to elevate the learning experience by integrating modern technologies , incentivizing students to learn better and teaching them about different school subjects, AI, and NFTs simultaneously."},{"heading":"What it does","content":"It is an educational app, offering two views, Student and Teacher. On Student view, compete with others in your class through a leaderboard by solving questions correctly and earning points. If you get questions wrong, you have the chance to get feedback from Together.ai's Mistral model. Use your points to redeem cool NFT characters and show them off to your peers/classmates in your profile collection!\n\nFor Teachers, manage students and classes and see how each student is doing."},{"heading":"How we built it","content":"Built using TypeScript, React Native and Expo, it is a quickly deployable mobile app. We also used Together.ai for our AI generated hints and feedback, and CrossMint for verifiable credentials and managing transactions with Stable Diffusion generated NFTs"},{"heading":"Challenges we ran into","content":"We had some trouble deciding which AI models to use, but settled on Together.ai's API calls for its ease of use and flexibility. Initially, we wanted to do AI generated questions but understandably, these had some errors so we decided to use AI to provide hints and feedback when a student gets a question wrong. Using CrossMint and creating our stable diffusion NFT marketplace was also challenging, but we are proud of how we successfully incorporated it and allowed each student to manage their wallets and collections in a fun and engaging way."},{"heading":"Accomplishments that we're proud of","content":"Using Together.ai and CrossMint for the first time, and implementing numerous features, such as a robust AI helper to help with any missed questions, and allowing users to buy and collect NFTs directly on the app."},{"heading":"What we learned","content":"Learned a lot about NFTs, stable diffusion, how to efficiently prompt AIs, and how to incorporate all of this into an Expo React Native app.\n\nAlso met a lot of cool people and sponsors at this event and loved our time at TreeHacks!"},{"heading":"What's next for MindMint: Empowering Education with AI & NFTs","content":"Our priority is to incorporate a spaced repetition-styled learning algorithm, similar to what Anki does, to tailor the learning curves of various students and help them understand difficult and challenging concepts efficiently.\n\nIn the future, we would want to have more subjects and grade levels, and allow the teachers to input questions for the student to solve. Another interesting idea we had was to create a mini real-time interactive game for students to play among themselves, so they can encourage each other to play between themselves."},{"heading":"Built With","content":"crossmint expo.io firebase react-native react.js together.ai typescript"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-17T18:19:37.154901Z"}}