{"version":"v1","hackathon_url":"https://treehacks-2020.devpost.com","generated_at":"2026-02-17T18:20:10.326494Z","result":{"hackathon":{"name":"TreeHacks 2020","url":"https://treehacks-2020.devpost.com","gallery_url":"https://treehacks-2020.devpost.com/project-gallery","scanned_pages":9,"scanned_projects":197,"winner_count":33},"winners":[{"project_title":"PocketPT","project_url":"https://devpost.com/software/pocketpt","tagline":"Help physical therapy patients continue their exercise program at home by using deep learning to detect if their posture and form are correct","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/937/688/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Medical Access Grand Prize"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[NVIDIA] Best Use of Jetson Hack"}],"team_members":[],"built_with":[{"name":"adobexd","url":null},{"name":"computer","url":null},{"name":"deeplearning","url":null},{"name":"jetson-nano","url":"https://devpost.com/software/built-with/jetson-nano"},{"name":"nvidia","url":"https://devpost.com/software/built-with/nvidia"},{"name":"photoshop","url":"https://devpost.com/software/built-with/photoshop"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"webcam","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"For physical therapy patients, doing your home exercise program is a crucial part of therapy and recovery. These exercises improve the body and allow patients to remain pain-free without having to pay for costly repeat visits. However, doing these exercises incorrectly can hinder progress and put you back in the doctor’s office."},{"heading":"What it does","content":"PocketPT uses deep learning technologies to detect and correct patient's form in a broad range of Physical Therapy exercises."},{"heading":"How we built it","content":"We used the NVIDIA Jetson-Nano computer and a Logitech webcam to build a deep learning model. We trained the model on over 100 images in order to detect the accuracy of Physical Therapy postures."},{"heading":"Challenges we ran into","content":"Since our group was using new technology, we struggled at first with setting up the hardware and figuring out how to train the deep learning model."},{"heading":"Accomplishments that we're proud of","content":"We are proud that we created a working deep learning model despite no prior experience with hardware hacking or machine learning."},{"heading":"What we learned","content":"We learned the principles of deep learning, hardware, and IoT. We learned how to use the NVIDIA Jetson Nano computer for use in various disciplines."},{"heading":"What's next for PocketPT","content":"In the future, we want to expand to include more Physical Therapy postures. We also want to implement our product for use on Apple Watch and FitBit, which would allow a more seamless workout experience for users."},{"heading":"Built With","content":"adobexd computer deeplearning jetson-nano nvidia photoshop python webcam"}]},{"project_title":"Lucid Drums","project_url":"https://devpost.com/software/lucid-drums","tagline":"Interactive VR Game to Improve Coordination through Rhythm for Childhood Development and Patient Recovery","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/937/546/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Most Creative"}],"team_members":[],"built_with":[{"name":"deep-learning","url":null},{"name":"figma","url":null},{"name":"lots-of-hard-coding!","url":null},{"name":"madmom-music-processing-package","url":null},{"name":"oculus-quest-vr","url":null},{"name":"recurrent-neural-network","url":null},{"name":"unity","url":"https://devpost.com/software/built-with/unity"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Music is not all about melody, rhythm is a core part of music: “it’s the rhythm that provides the driving force to keep our ears tuned in and satisfy our deep instinctive appreciation of music more than anything else [1].”\n\nOur hack, Lucid Drums, was motivated by our dedication to promoting music education and early childhood development. Notable learning scientists and psychologists, such as Lev Vygotsky, believe that children learn by experiencing their environments and making observations from the world around them. Carl Orff, one of the major figures in childhood music education, promotes the idea that music education should combine with movement, playing and improvisation [2]. We believe that with VR technology, we can create a fantastic virtual reality environment that enables children to explore the world of music rhythmically and freely; through fantasy, we create a unique learning environment.\n\nWhile our VR drum game was initially motivated by our passion for childhood education, we discovered drumming can also be a positive activity to help medical patients. Rhythm has been shown to help adults who are in recovery from motor impairments and drug addiction. The American Music Therapy Association (AMTA) conducted research that has shown drums to be particularly effective in helping stroke patients communicate; patterns help patients regain movement and speech ability [3]. Drumming also enhances recovery for patients overcoming drug addiction. Findings published The American Journal of Public Health highlight the benefits of percussion which include producing pleasurable experiences and enhancing awareness which decreases sensations of isolation [4]."},{"heading":"What it does","content":"Lucid Drums is an Oculus Quest Game in which we generate a fantastic virtual reality environment that enables children to explore music freely and rhythmically. It helps children learn different kinds of rhythms by playing drums to music in a VR world. We use machine learning to generate rhythms to songs, leverage hand-tracking to capture real-time movement, and apply game UX and learning principles to engage users.\n\nApart from music education, the Lucid Drums can help patients with recovery. Patients - ranging from those immobilized in hospital settings, stroke victims at home, or drug recovery patients - will have an engaging experience in the Lucid Drums which can alleviate pain and enhance recovery."},{"heading":"How we built it","content":"Quest + Unity for VR gaming experience Deep learning (RNN from madmom[5]) for rhythm auto-generation User/learner friendly design according to UX/learning design principles"},{"heading":"Challenges we ran into","content":"1. VR game modeling\n\nThe most challenging part in VR game modeling is how to track the user's hand gesture and determine whether the user is hitting on the correct beat or not without using a controller. In order to detect the user's motion quickly and precisely, we spawned a sphere in each finger, then detected each single sphere if they hit the drum. We implemented an invisible sphere over the drum to realize the beat detection. Every time the sphere on a user's finger collides with the sphere over the drum, it is be considered as a correct hit. We adjust accuracy of a hit depending on difficulty selected; in learning mode there is a larger accuracy for hits whereas in competitive mode the window of a correct hit is much smaller.\n\nThe second challenge was how to make the beat-balls - which signified accuracy hitting a drum- move according to the beats generated by RNN. We made the realtime connection between Unity and the music: 1 unit in Unity equaled 1 second in music. We then moved the balls with the music base in rhythmic time.\n\nThe other challenge was that we originally designed the game with 1 virtual drum, but we were later inspired to implement 2 drums. After rounds of user testing and research, we realized multiple drums would be better to keep both users hands occupied; with both hands playing, they are more immerse in the game. Therefore, we implemented two drums. We have a drum master that reads the music beat data and distributes it the two drums according to left and right signals. Meanwhile, both of the ball lines (for left and right hand movement) move forward consistently with music.\n\n2. Rhythm auto-detection As humans, we can catch certain rhythms easily, but an artificial system is challenged to do so. Beat tracking may sound like a straightforward concept, but it’s actually an unsolved problem until now. We learned audio can be complex and noisy that this can befuddle an algorithm and lead it to produce false positives while detecting beats [6].\n\nWe used the Recurrent Neural Network to identify beats, but it was not giving us the desired output and predicted a lot of false positives as well. Therefore, we used neighboring methods to pick up the prediction that matched with the true beats in a song. Improved detection gave us the best/appropriate beats that nicely incorporate with each song.\n\n3. UX Design As part of UX game design, it was challenging to create assets that were both appealing, visually consistent, and intuitive for players. Since our game targets both beginners and advanced players we had to balance having user-friendly designs for a virtual reality environment with also ensuring functionality. To guide users, we developed UI cards that allow players to select different songs and the playing difficulty and, when in-game, to easily pause, restart, and go back to the main menu. The aesthetic instructional cards integrate in the VR interface to give intuitive guidance for the game.\n\nAnother challenge for UX design was that Lucid Drum targets two different user groups: children learners and adult patients. These two groups have distinguished expectations for the game. For children, the goal is to optimize their rhythm learning experience thus yielding a better learning outcome. For adult patients, the goal is to optimize their gaming experience to entertain and help develop skills towards recovery. To make the game adapt to both children learners and adult patients, we designed two different modes: learning mode with easier rhythms to motivate the learning and competitive mode with harder beats that enable better gaming experience for patients."},{"heading":"Accomplishments we’re proud of","content":"We were able to build a VR application with aesthetic features using Unity, Oculus Quest, Recurrent Neural Network and Audacity! We created a virtual environment for children as well as patients that is accessible, affordable and entertaining. As specified previously, music is an art form that satisfies everyone in one way or the other, so building this application was a creative endeavor.\n\nThrough our hack we successfully:\n\nBuilt the first VR beat game for a social good purpose, implementing a fantasy virtual reality environment with billboarding. Achieved hand-tracking that doesn’t require a controller. It is more authentic and yields an optimal user/learning experience! The auto-beat-generation algorithm enables the game to generate the playlist automatically. Literally you can play with any song you want! Specially mentioned: we bring the excellent drum beats from the Stanford Cardinal Calypso and the traditional Scottish Pipes and Drums from Carnegie Mellon into our games (team spirit!). We targeted different user groups through learning and competitive modes Essentially, we created a user-friendly game and we achieved that!"},{"heading":"Ethical Considerations","content":"While our game has implications for social good through targetting education and recovery, lots of young kids and also adult patients haven’t experienced VR games before, and there is an ethical risk that users would overindulge in the VR world which would negatively impact their connection with the real world. This can be disastrous for children’s socialization development and patients’ mental health. To prevent this from happening, we considered an anti-indulgent system that automatically reminds users to take a rest every 30 minutes. We believe this anti-indulgent system is a crucial component in VR game design and should be included in every VR game as ethical consideration for user mental health.\n\nReferences:\n\nhttps://www.musical-u.com/learn/topic/ear-training/rhythm/ https://en.wikipedia.org/wiki/Carl_Orff#Musical_works https://www.everydayhealth.com/stroke/music-therapy-for-stroke-recovery.aspx https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1447805/ https://github.com/CPJKU/madmom https://www.analyticsvidhya.com/blog/2018/02/audio-beat-tracking-for-music-information-retrieval/"},{"heading":"Built With","content":"deep-learning figma lots-of-hard-coding! madmom-music-processing-package oculus-quest-vr recurrent-neural-network unity"}]},{"project_title":"SafeCup","project_url":"https://devpost.com/software/safecup","tagline":"A cup to prevent drug-facilitated assault by notifying the owner when the content of their drink has changed.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/937/994/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Most Impactful Hack"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Best Hardware Hack"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"pushed","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/ghjgghj/SafeCup"}],"description_sections":[{"heading":"Inspiration","content":"On a night in January 2018, at least 7 students reported symptoms of being drugged after attending a fraternity party at Stanford link . Although we are only halfway into this academic year, Stanford has already issued seven campus-wide reports about possible aggravated assault/drugging. This is not just a problem within Stanford, drug-facilitated sexual assault (DFSA) is a serious problem among teens and college students nationwide. Our project is deeply motivated by this saddening situation that people around us at Stanford, and the uneasiness caused by the possibility of experiencing such crimes. This project delivers SafeCup, a sensor-embedded smart cup that warns owners if their drink has been tampered with."},{"heading":"What it does","content":"SafeCup is embedded with a simple yet highly sensitive electrical conductivity (EC) sensor which detects concentration of total dissolved solids (TDS). Using an auto-ranging resistance measurement system, designed to measure the conductivity of various liquids, the cup takes several measurements within a certain timeframe and warns the owner by pushing a notification to their phone if it senses a drastic change in the concentration of TDS. This change signifies a change in the content of the drink, which can be caused by the addition of chemicals such as drugs."},{"heading":"How we built it","content":"We used a high surface area electrodes, a set of resistors to build the EC sensor and an Arduino microprocessor to collect the data. The microprocessor then sends the data to a computer, which analyzes the measurements and performs the computation, which then notifies the owner through \"pushed\", an API that sends push notifications to Android or IOS devices."},{"heading":"Challenges we ran into","content":"The main challenge is getting a stable and accurate EC reading from the home-made sensor. EC is depended on the surface area and the distance between the electrodes, thus we had to designed an electrode where the distance does between the electrod does not vary due to movements. Liquids can have a large range of conductivity, from 0.005 mS/cm to 5000 mS/cm. In order to measure the conductivity at the lower range, we increased the surface area of our electrodes significantly, around 80 cm^2, while typical commercial TDS sensors are less than 0.5 cm^2. In order to measure such a large range of values, we had to design a dynamic auto-ranging system with a range of reference resistors.\n\nAnother challenge is that we are unable to make our cup look more beautiful, or normal/party-like... This is mainly because of the size of the Arduino UNO microprocessor, which is hard to disguise under the size of a normal party solo cup. This is why after several failed cup designs, we decided to make the cup simple and transparent, and focus on demonstrating the technology instead of the aesthetics."},{"heading":"Accomplishments that we're proud of","content":"We're most proud of the simplicity of the device. The device is made from commonly found items. This also means the device can be very cheap to manufacture. Typical commercial TDS measuring pen can be found for as low as $5 and this device is even simpler than a typical TDS sensor. We are also proud of the auto-ranging resistance measurement. Our cup is able to automatically calibrate to the new drink being poured in, to adjust to its level of resistance (note that different drinks have different chemical compositions and therefore has different resistance). This allows us to make our cup accommodate a wide range of different drinks. We are also proud of finding a simple solution to notify users - developing an app would have take away too much time that we could otherwise put into furthering the cup's hardware design, given a small team of just two first-time hackers."},{"heading":"What we learned","content":"We learned a lot about Arduino development, circuits, and refreshed our knowledge of Ohm's law."},{"heading":"What's next for SafeCup","content":"The prototype we've delivered for this project is definitely not a finished product that is ready to be used. We have not performed any test on whether liquids from the cup are actually consumable since the liquids had been in touch with non-food-grade metal and may undergo electrochemical transformation due to the applied potential on the liquid. Our next step would be to ensure consumer safety. TDS sensor also might not be sensitive enough alone for liquids with already high amount of TDS. Adding other simple complementary sensors can greatly increase the sensitive of the device. Other simple sensors may include dielectric constant sensor, turbidity sensor, simple UV-Vis light absorption sensor, or even making simple electrochemical mesurements. Other sensors such as water level sensor can even be used to keep track of amount of drink you have had throughout the night. We would also use a smaller footprint microprocessor, which can greatly compact the device. In addition, we would like to incorporate wireless features that would eliminate the need to wire to a computer."},{"heading":"Ethical Implications For \"Most Ethically Engaged Hack\"","content":"We believe that our project could mean a lot to young people facing the risk of DFSA. These people, statistically, mostly consist of college students and teenagers who surround us all the time, and are especially vulnerable to such type of crimes. We have come a long way to show that the idea of using simple TDS sensor for illegal drugging works. With future improvements in its beauty and safety, we believe it could be a viable product that improves safety of many people around us in colleges and parties."},{"heading":"Built With","content":"arduino pushed python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Neutral","project_url":"https://devpost.com/software/neutral","tagline":"Offset your carbon footprint of every online purchase you make.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/461/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Moonshot Prize"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Neo] Perseverance Prize"}],"team_members":[],"built_with":[{"name":"bootstrap","url":"https://devpost.com/software/built-with/bootstrap"},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"php","url":"https://devpost.com/software/built-with/php"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/marissajliu/Neutral_Chrome"}],"description_sections":[{"heading":"Inspiration","content":"Our team's mission is to build tools that help everyone act against climate change.\n\nWhile it's common knowledge that climate change is an urgent problem that we need to address immediately, we sought a solution that puts the problem at the forefront - starting with the habits that almost everyone participates in, online shopping. We designed Neutral to make it easy to figure out how much greenhouse gas you're emitting with every purchase you make. It's difficult to put the impact of every online purchase into perspective. With Neutral, we hope to put these insights at the front-of-mind for the user so they can make more informed, considerate decisions about their purchases and the ultimate impact they have on the Earth.\n\nWhile it's difficult in a first world country to live 100% carbon neutral, we built Neutral to allow users to easily calculate their carbon footprint, and take immediate action to offset it by donating to our line-up of trusted NGOs dedicated to our mission of tackling the climate change crisis."},{"heading":"What it does","content":"Neutral is a web application that empowers users to understand their carbon footprint, learn how to reduce their emissions, and ultimately, offset the emissions produced by each and every one of their purchases made online.\n\nUsing the ISO 14064-3 carbon reporting standard, Neutral comprehensively reports the direct and indirect carbon emissions of a user's online purchases, based on the product type and price. Neutral's user-friendly data dashboard visualizes the historical carbon emission data, delivering the insights in metrics digestible and relevant to the average user. The integration and effectiveness of the Neutral dashboard is designed to allow users to easily understand their carbon footprint and drive immediate action to offset their footprint."},{"heading":"How We built it","content":"We built a Chrome Extension and data dashboard web application. We set up a MySQL database containing Carbon Emission data on various product and category types. We utilized Google Cloud's NLP API to categorize purchase items based on their description features and calculate the total carbon dioxide emitted directly and indirectly from the purchase."},{"heading":"Challenges I ran into","content":"Our main challenge was determining how to calculate the carbon emissions of a given good sold online. With no APIs readily available on carbon emission data, one of the few options we had was to scrape an unsupported site for carbon emission metrics.\n\nWe also ran into difficulties with designing the layout of the data visualization dashboard. The ultimate mission of our project was to build a dashboard that allowed users to easily understand their carbon footprint. And so, the data we summarized and visualized was designed with the goal of making the typical carbon footprint metrics more digestible and relevant to the average user."},{"heading":"What's next for Neutral","content":"We implemented some simplifications in order to be able to deliver a working project in time. Moving forward, we'd like to support Neutral on all e-commerce sites and product types."},{"heading":"Built With","content":"bootstrap css google-cloud html javascript php react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Crowd Insights","project_url":"https://devpost.com/software/crowd-insights","tagline":"Crowd Insights enables businesses and event organizers to access more consumer data through advanced computer vision algorithms used to monitor and find patterns in human behavior in an environment.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/773/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Geospatial Grand Prize"}],"team_members":[],"built_with":[{"name":"cv2","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"For a manager of a small business -- be it a store, restaurant, gym, or even a movie theater -- improving the customer experience and understand what's going on is tremendously important. Having access to analytics of when people are entering the building, what areas they're spending time at, and what crowds and lines are forming can provide managers with incredibly useful insights -- from identifying parts of the building layout that are poorly designed and causing congestion, to figuring out that certain table setups or shop items are particularly engaging, to having a better idea of what's going on in their business and being able to make data-driven decisions about how to improve."},{"heading":"What it does","content":"Given a live video feed from an overhead camera, Crowd Insights’ AI algorithms detect human heads within the video and use this positional data to identify lines and clusters of people and create heatmaps. The small business owner can then examine this data to learn about human traffic flow within their store over a specified period of time.\n\nThere are a variety of use cases for this data: congestion tracking, popular hotspots in store, long lines, etc. By analyzing these trends over time, small business owners can make informed decisions on how to improve their business to optimize the physical interaction of customers with the store. For example, if they notice that lots of people tend to group up around a certain product, then they can know to place that product near the back of the store to prevent crowding around the store entrance.\n\nOther use cases for this technology could include event management. Event organizers such as the TreeHacks team can use this technology to monitor the congestion within each room and help disperse people from highly crowded rooms to open spaces for work. They can monitor lines, ie for food or networking, and figure out novel ways to deal with long lines and heavy foot traffic."},{"heading":"How we built it","content":"We built the theory and data science toolkits, machine learning model, frontend, and backend separately. For the machine learning, we used the Pytorch FCHD fully convolutional head detector, running on a Google Cloud VM. Afterwards, we passed the list of heads to the graph theory library that we built, which constructed the Minimum Spanning Tree through the graph, removed edges that were too long, and performed elliptical fits to determine whether a group of points was a line or a cluster. We also aggregated human location data over time to create a heatmap of the environment to see which places are interacted with the most. Firebase is used to communicate between the head detector and the computer (like a Raspberry Pi), which sends webcam feed data. Finally we have a web server using ReactJS that displays the results."},{"heading":"Challenges we ran into","content":"One main issue was finding a vision model that could provide dense data for human position in a camera frame. Most models tend to do decent at closer distances but as we try to monitor areas that are >15 feet away from a camera, the precision becomes an issue. Due to the fact that we needed this sort of density in our data, we had to work through testing many model architectures and fusion techniques to yield the best results.\n\nWe also had a lot of trouble rendering the line/cluster data from Firebase in a real-time graph on the website. This was tough because no member had extensive experience with realtime updating and with push/pull requests between Firebase and the web app. To solve this, we worked together to break the problem down into two parts—that of collecting and parsing data from Firebase, and that of displaying the data in a dynamic graph.\n\nLastly, this was our first time incorporating a big chunk of frontend programming in our application. Our experience in JavaScript, HTML, and Firebase was limited. Thus, it took us a long time to implement the syntax of the languages from scratch. However, this also made this project really impactful as it provided us with an exceptional learning opportunity."},{"heading":"Accomplishments that we’re proud of","content":"We implemented simple but effective algorithms for recognizing clusters of crowds and lines. We used minimum spanning trees and fitting ellipses to identify clusters, then took clusters with particularly elongated ellipses and fit them with best fit lines. We developed a decision tree that applied knowledge from all branches of computer science - from theory to machine learning and software engineering - together in a product that became more than the sum of its parts. The final web product took tens of hours to complete, and we’re confident that we were able to get it right."},{"heading":"What we learned","content":"A lot of new frontend learning and creating algorithms ReactJS, ChartJS, CanvasJS, Plotly, firebase ML Head and Body Detection Algorithms Kruskal’s Minimum Spanning Tree, Automatic K-Means Clustering, Depth-First Search, Firebase - Realtime graphs, how to upload data from Jetson to Firebase to web app\n\nEven though the project was divided into a frontend and backend portion, all members were able to understand the implementation on both sides. Throughout the implementation, we worked as a unified team, especially when we ran into roadblocks. The core takeaway from this project is our improved understanding of realtime databases, machine learning models, and frontend program structure."},{"heading":"What's next for Crowd Insights AI","content":"One big next step would be applying mapping techniques to create a 3D map of the shop, then localize detected crowds in that 3D map. It would allow the business owner to analyze exactly which shelves or tables are becoming crowded. Furthermore, performing spatial transforms on the angled camera footage would allow us to track 3D from a 2D space.\n\nWe'd also want to apply optical flow and motion tracking to see how people are moving through the space and what slows them down."},{"heading":"Built With","content":"cv2 firebase flask google-cloud javascript python pytorch react"}]},{"project_title":"EduVoicer","project_url":"https://devpost.com/software/eduvoicer","tagline":"EduVoicer uses Deepfake/AI technology to provide personalized dictation of educational reading materials for the dyslexic/visually impaired.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/939/782/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Voice Assistance Grand Prize"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[VoiceFlow] Most User-Friendly Voice Assistance Hack"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"numpy","url":"https://devpost.com/software/built-with/numpy"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"python-package-index","url":"https://devpost.com/software/built-with/python-package-index"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"tensorflow","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/rodrigo-castellon/text2anime"},{"label":"soundcloud.com","url":"https://soundcloud.com/user-946166607"}],"description_sections":[{"heading":"Inspiration","content":"Inspired by a team member's desire to study through his courses by listening to his textbook readings recited by his favorite anime characters, functionality that does not exist on any app on the market, we realized that there was an opportunity to build a similar app that would bring about even deeper social impact. Dyslexics, the visually impaired, and those who simply enjoy learning by having their favorite characters read to them (e.g. children, fans of TV series, etc.) would benefit from a highly personalized app."},{"heading":"What it does","content":"Our web app, EduVoicer, allows a user to upload a segment of their favorite template voice audio (only needs to be a few seconds long) and a PDF of a textbook and uses existing Deepfake technology to synthesize the dictation from the textbook using the users' favorite voice. The Deepfake tech relies on a multi-network model trained using transfer learning on hours of voice data. The encoder first generates a fixed embedding of a given voice sample of only a few seconds, which characterizes the unique features of the voice. Then, this embedding is used in conjunction with a seq2seq synthesis network that generates a mel spectrogram based on the text (obtained via optical character recognition from the PDF). Finally, this mel spectrogram is converted into the time-domain via the Wave-RNN vocoder (see this paper for more technical details). Then, the user automatically downloads the .WAV file of his/her favorite voice reading the PDF contents!"},{"heading":"How we built it","content":"We combined a number of different APIs and technologies to build this app. For leveraging scalable machine learning and intelligence compute, we heavily relied on the Google Cloud APIs -- including the Google Cloud PDF-to-text API, Google Cloud Compute Engine VMs, and Google Cloud Storage; for the deep learning techniques, we mainly relied on existing Deepfake code written for Python and Tensorflow (see Github repo here , which is a fork). For web server functionality, we relied on Python's Flask module, the Python standard library, HTML, and CSS. In the end, we pieced together the web server with Google Cloud Platform (GCP) via the GCP API, utilizing Google Cloud Storage buckets to store and manage the data the app would be manipulating."},{"heading":"Challenges we ran into","content":"Some of the greatest difficulties were encountered in the superficially simplest implementations. For example, the front-end initially seemed trivial (what's more to it than a page with two upload buttons?), but many of the intricacies associated with communicating with Google Cloud meant that we had to spend multiple hours creating even a landing page with just drag-and-drop and upload functionality. On the backend, 10 excruciating hours were spent attempting (successfully) to integrate existing Deepfake/Voice-cloning code with the Google Cloud Platform. Many mistakes were made, and in the process, there was much learning."},{"heading":"Accomplishments that we're proud of","content":"We're immensely proud of piecing all of these disparate components together quickly and managing to arrive at a functioning build. What started out as merely an idea manifested itself into usable app within hours."},{"heading":"What we learned","content":"We learned today that sometimes the seemingly simplest things (dealing with python/CUDA versions for hours) can be the greatest barriers to building something that could be socially impactful. We also realized the value of well-developed, well-documented APIs (e.g. Google Cloud Platform) for programmers who want to create great products."},{"heading":"What's next for EduVoicer","content":"EduVoicer still has a long way to go before it could gain users. Our first next step is to implementing functionality, possibly with some image segmentation techniques, to decide what parts of the PDF should be scanned; this way, tables and charts could be intelligently discarded (or, even better, referenced throughout the audio dictation). The app is also not robust enough to handle large multi-page PDF files; the preliminary app was designed as a minimum viable product, only including enough to process a single-page PDF. Thus, we plan on ways of both increasing efficiency (time-wise) and scaling the app by splitting up PDFs into fragments, processing them in parallel, and returning the output to the user after collating individual text-to-speech outputs. In the same vein, the voice cloning algorithm was restricted by length of input text, so this is an area we seek to scale and parallelize in the future. Finally, we are thinking of using some caching mechanisms server-side to reduce waiting time for the output audio file."},{"heading":"Built With","content":"css flask google-cloud html5 numpy python python-package-index pytorch tensorflow"},{"heading":"Try it out","content":"github.com soundcloud.com"}]},{"project_title":"Medley","project_url":"https://devpost.com/software/medley-4lyth6","tagline":"Medley helps manage chronic pain conditions and fights medication misuse by providing compassionate care, administering doses of medication, and keeping patients connected with their doctor.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/938/119/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"IoT Grand Prize"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Johnson Controls] JCI Innovation Prize"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"azure-iot-suite","url":"https://devpost.com/software/built-with/azure-iot-suite"},{"name":"houndify","url":null},{"name":"microsoft","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"raspiaudio","url":null},{"name":"rfid","url":null},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"medley.pantsforbirds.com","url":"http://medley.pantsforbirds.com/"}],"description_sections":[{"heading":"Inspiration","content":"Both chronic pain disorders and opioid misuse are on the rise, and the two are even more related than you might think -- over 60% of people who misused prescription opioids did so for the purpose of pain relief. Despite the adoption of PDMPs (Prescription Drug Monitoring Programs) in 49 states, the US still faces a growing public health crisis -- opioid misuse was responsible for more deaths than cars and guns combined in the last year -- and lacks the high-resolution data needed to implement new solutions.\n\nWhile we were initially motivated to build Medley as an effort to address this problem, we quickly encountered another (and more personal) motivation. As one of our members has a chronic pain condition (albeit not one that requires opioids), we quickly realized that there is also a need for a medication and symptom tracking device on the patient side -- oftentimes giving patients access to their own health data and medication frequency data can enable them to better guide their own care."},{"heading":"What it does","content":"Medley interacts with users on the basis of a personal RFID card, just like your TreeHacks badge. To talk to Medley, the user presses its button and will then be prompted to scan their ID card. Medley is then able to answer a number of requests, such as to dispense the user’s medication or contact their care provider. If the user has exceeded their recommended dosage for the current period, Medley will suggest a number of other treatment options added by the care provider or the patient themselves (for instance, using a TENS unit to alleviate migraine pain) and ask the patient to record their pain symptoms and intensity."},{"heading":"How we built it","content":"This project required a combination of mechanical design, manufacturing, electronics, on-board programming, and integration with cloud services/our user website. Medley is built on a Raspberry Pi, with the raspiaudio mic and speaker system, and integrates an RFID card reader and motor drive system which makes use of Hall sensors to accurately actuate the device. On the software side, Medley uses Python to make calls to the Houndify API for audio and text, then makes calls to our Microsoft Azure SQL server. Our website uses the data to generate patient and doctor dashboards."},{"heading":"Challenges we ran into","content":"Medley was an extremely technically challenging project, and one of the biggest challenges our team faced was the lack of documentation associated with entering uncharted territory. Some of our integrations had to be twisted a bit out of shape to fit together, and many tragic hours spent just trying to figure out the correct audio stream encoding. Of course, it wouldn’t be a hackathon project without overscoping and then panic as the deadline draws nearer, but because our project uses mechanical design, electronics, on-board code, and a cloud database/website, narrowing our scope was a challenge in itself."},{"heading":"Accomplishments that we're proud of","content":"Getting the whole thing into a workable state by the deadline was a major accomplishment -- the first moment we finally integrated everything together was a massive relief."},{"heading":"What we learned","content":"Among many things: The complexity and difficulty of implementing mechanical systems How to adjust mechatronics design parameters Usage of Azure SQL and WordPress for dynamic user pages Use of the Houndify API and custom commands Raspberry Pi audio streams"},{"heading":"What's next for Medley","content":"One feature we would have liked more time to implement is better database reporting and analytics. We envision Medley’s database as a patient- and doctor-usable extension of the existing state PDMPs, and would be able to leverage patterns in the data to flag abnormal behavior. Currently, a care provider might be overwhelmed by the amount of data potentially available, but adding a model to detect trends and unusual events would assist with this problem."},{"heading":"Built With","content":"azure azure-iot-suite houndify microsoft python raspberry-pi raspiaudio rfid sql"},{"heading":"Try it out","content":"medley.pantsforbirds.com"}]},{"project_title":"Reading Glasses","project_url":"https://devpost.com/software/reading-glasses-lhnib1","tagline":"These reading glasses read to you! Millions of people have trouble with reading due to illiteracy, dyslexia, vision problems and young age, and these glasses allow them to enjoy reading.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/938/875/datas/medium.JPG","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Education Grand Prize"}],"team_members":[],"built_with":[{"name":"3dprinting","url":"https://devpost.com/software/built-with/3dprinting"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"hot-glue","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"real-magnets","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/kolchinski/TreeHacks20"}],"description_sections":[{"heading":"Inspiration","content":"Alex K's girlfriend Allie is a writer and loves to read, but has had trouble with reading for the last few years because of an eye tracking disorder. She now tends towards listening to audiobooks when possible, but misses the experience of reading a physical book.\n\nMillions of other people also struggle with reading, whether for medical reasons or because of dyslexia (15-43 million Americans) or not knowing how to read. They face significant limitations in life, both for reading books and things like street signs, but existing phone apps that read text out loud are cumbersome to use, and existing \"reading glasses\" are thousands of dollars!\n\nThankfully, modern technology makes developing \"reading glasses\" much cheaper and easier, thanks to advances in AI for the software side and 3D printing for rapid prototyping. We set out to prove through this hackathon that glasses that open the world of written text to those who have trouble entering it themselves can be cheap and accessible."},{"heading":"What it does","content":"Our device attaches magnetically to a pair of glasses to allow users to wear it comfortably while reading, whether that's on a couch, at a desk or elsewhere. The software tracks what they are seeing and when written words appear in front of it, chooses the clearest frame and transcribes the text and then reads it out loud."},{"heading":"How we built it","content":"Software (Alex K) - On the software side, we first needed to get image-to-text (OCR or optical character recognition) and text-to-speech (TTS) working. After trying a couple of libraries for each, we found Google's Cloud Vision API to have the best performance for OCR and their Google Cloud Text-to-Speech to also be the top pick for TTS.\n\nThe TTS performance was perfect for our purposes out of the box, but bizarrely, the OCR API seemed to predict characters with an excellent level of accuracy individually, but poor accuracy overall due to seemingly not including any knowledge of the English language in the process. (E.g. errors like \"Intreduction\" etc.) So the next step was implementing a simple unigram language model to filter down the Google library's predictions to the most likely words.\n\nStringing everything together was done in Python with a combination of Google API calls and various libraries including OpenCV for camera/image work, pydub for audio and PIL and matplotlib for image manipulation.\n\nHardware (Alex G) : We tore apart an unsuspecting Logitech webcam, and had to do some minor surgery to focus the lens at an arms-length reading distance. We CAD-ed a custom housing for the camera with mounts for magnets to easily attach to the legs of glasses. This was 3D printed on a Form 2 printer, and a set of magnets glued in to the slots, with a corresponding set on some NerdNation glasses."},{"heading":"Challenges we ran into","content":"The Google Cloud Vision API was very easy to use for individual images, but making synchronous batched calls proved to be challenging! Finding the best video frame to use for the OCR software was also not easy and writing that code took up a good fraction of the total time.\n\nPerhaps most annoyingly, the Logitech webcam did not focus well at any distance! When we cracked it open we were able to carefully remove bits of glue holding the lens to the seller’s configuration, and dial it to the right distance for holding a book at arm’s length.\n\nWe also couldn’t find magnets until the last minute and made a guess on the magnet mount hole sizes and had an exciting Dremel session to fit them which resulted in the part cracking and being beautifully epoxied back together."},{"heading":"Acknowledgements","content":"The Alexes would like to thank our girlfriends, Allie and Min Joo, for their patience and understanding while we went off to be each other's Valentine's at this hackathon."},{"heading":"Built With","content":"3dprinting google-cloud hot-glue opencv python real-magnets"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"The LevelSet Suite: EquiBox, EquiTalk, and EquiTrack","project_url":"https://devpost.com/software/equitalk","tagline":"Today's companies are waking up to the importance of a company culture of inclusion. By using AI, IoT, and voice technologies, our products detect and bring attention to aggressive workplace behaviors","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/938/647/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"Best Beginner Hack"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Citadel] Best Decision Making under Uncertainty Hack"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"google-web-speech-api","url":"https://devpost.com/software/built-with/google-web-speech-api"},{"name":"heroku","url":"https://devpost.com/software/built-with/heroku"},{"name":"houndify","url":null},{"name":"iot","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"ngrok","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"soundhound","url":null},{"name":"twilio","url":"https://devpost.com/software/built-with/twilio"},{"name":"voicekit","url":null},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"},{"name":"zeit","url":null}],"external_links":[{"label":"treehacks.sungodavi.now.sh","url":"https://treehacks.sungodavi.now.sh/"},{"label":"github.com","url":"https://github.com/sungodavi/treehacks2020"}],"description_sections":[{"heading":"Inspiration","content":"While our team might have come from different corners of the country, with various experience in industry, and a fiery desire to debate whether tabs or spaces are superior, we all faced similar discomforts in our jobs: insensitivity.\n\nOur time in college has shown us that despite the fact people's diverse backgrounds, everyone can achieve greatness. Nevertheless, workplace calls and water-cooler conversations are plagued with \"microaggressions.\" A microaggression is a subtle indignity or offensive comment that a person communicates to a group. These subtle, yet hurtful comments lead to marginalization in the workplace, which, as studies have shown, can lead to anxiety and depression. Our team's mission was to tackle the unspoken fight on diversity and inclusion in the workplace.\n\nOur inspiration came from this idea of impartial moderation: why is the marginalized employee's responsibility to take the burden of calling someone out? Pointing out these microaggressions can lead to the reinforcement of stereotypes, and thus, create lose-lose situations. We believe that if we can shift the responsibility, we can help create a more inclusive work environment, give equal footing for interviewees, and tackle marginalization in the workplace from the water-cooler up."},{"heading":"What it does","content":"EquiBox:\n\nEquiBox is an IoT conference room companion, a speaker, and microphone that comes alive when meetings take place. It monitors different meeting members' sentiment levels by transcribing sound and running AI to detect for insults or non-inclusive behavior. If an insult is detected, EquiBox comes alive with a beep and a warning about micro-aggressions to impartially moderate an inclusive meeting environment. EquiBox sends live data to EquiTrack for further analysis.\n\nEquiTalk:\n\nEquiTalk is our custom integration with Twilio (a voice platform used for conference calls) to listen to multi-person phone calls to monitor language, transcribe the live conversation, and flag certain phrases that might be insulting. EquiTalk sends live data to EquiTrack for analysis.\n\nEquiTrack:\n\nEquiTrack is an enterprise analytics platform designed to allow HR departments to leverage the data created by EquiTalk and EquiBox to improve the overall work culture. EquiTrack provides real-time analysis of ongoing conference calls. The administrator can see not only the amount of micro-aggression that occur throughout the meeting but also the direct sentence that triggered the alert. The audio recordings of the conference calls are recorded as well, so administrators can playback the call to resolve discrepancies."},{"heading":"How we built it","content":"The LevelSet backend consisted of several independent services. EquiTalk uses a Twilio integration to send call data and metadata to our audio server. Similarly, EquiBox uses Google's VoiceKit, along with Houndify's Speech to Text API, to parse the raw audio format. From there, the transcription of the meeting goes to our micro-aggression classifier (hosted on Google Cloud), which combines a BERT Transformer with an SVC to achieve 90% accuracy on our micro-aggression test set. The classified data then travels to the EquiTalk backend (hosted on Microsoft Azure), which stores the conversation and classification data to populate the dashboard."},{"heading":"Challenges we ran into","content":"One of the biggest challenges that we ran into was creating the training set for the micro classifier. While there were plenty of data sets that including aggressive behavior in general, their examples lacked the subtlety that our model needed to learn. Our solution to this was to crowdsource and augment the set of the microaggressions. We sent a survey out to Stanford students on campus and compiled an extensive list of microaggressions, which allowed our classifier to achieve the accuracy that it did."},{"heading":"Accomplishments that we're proud of","content":"We're very proud of the accuracy we were able to achieve with our classifier. By using the BERT transformer, our model was able to classify micro-aggressions using only the handful of examples that we collected. While most DNN models required thousands of samples to achieve high accuracy, our micro-aggression dataset consisted of less than 100 possible micro-aggressions.\n\nAdditionally, we're proud of our ability to integrate all of the platforms and systems that were required to support the LevelSet suite. Coordinating multiple deployments and connecting several different APIs was definitely a challenge, and we're proud of the outcome."},{"heading":"What we learned","content":"By definition, micro-aggressions are almost intangible social nuances picked up by humans. With minimal training data, it is tough to refine our model for classifying these micro-aggressions. Audio processing at scale can lead to several complications. Each of the services that use audio had different format specifications, and due to the decentralized nature of our backend infrastructure, merely sending the data over from service to service required additional effort as well. Ultimately, we settled on trying to handle the audio as upstream as we possibly could, thus eliminating the complication from the rest of the pipeline. The integration of several independent systems can lead to unexpected bugs. Because of the dependencies, it was hard to unit test the services ahead of time. Since the only way to make sure that everything was working was with an end-to-end test, a lot of bugs didn't arise until the very end of the hackathon."},{"heading":"What's next for LevelSuite","content":"We will continue to refine our micro-classifier to use tone classification as an input. Additionally, we will integrate the EquiTalk platform into more offline channels like Slack and email. With a longer horizon, we aim to improve equality in the workplace in all stages of employment, from the interview to the exit interview. We want to expand from conference calls to all workplace communication, and we want to create new strategies to inform and disincentivize exclusive behavior. We wish to LevelSet to level the playing the field in the workplace, and we believe that these next steps will help us achieve that."},{"heading":"Built With","content":"azure google-cloud google-web-speech-api heroku houndify iot javascript mongodb ngrok node.js python pytorch soundhound twilio voicekit websockets zeit"},{"heading":"Try it out","content":"treehacks.sungodavi.now.sh github.com"}]},{"project_title":"MusicBlox: Tangible Programming Education in Mixed Reality","project_url":"https://devpost.com/software/musicblox","tagline":"Learn programming in mixed reality and natural classroom settings, and develop computational thinking via knowledge of everyday, noncomputer world objects.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/941/377/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"AR/VR Grand Prize"}],"team_members":[],"built_with":[{"name":"ar","url":null},{"name":"augmented-reality","url":"https://devpost.com/software/built-with/augmented-reality"},{"name":"computervision","url":null},{"name":"hardware","url":"https://devpost.com/software/built-with/hardware"},{"name":"image-recognition","url":null},{"name":"magicleap","url":null},{"name":"mixed-reality","url":null},{"name":"mixedreality","url":null},{"name":"mr","url":null},{"name":"unity","url":"https://devpost.com/software/built-with/unity"}],"external_links":[{"label":"github.com","url":"https://github.com/megatran/TreeHacks2020"}],"description_sections":[{"heading":"Motivation","content":"Coding skills are in high demand and will soon become a necessary skill for nearly all industries. Jobs in STEM have grown by 79 percent since 1990, and are expected to grow an additional 13 percent by 2027, according to a 2018 Pew Research Center survey. This provides strong motivation for educators to find a way to engage students early in building their coding knowledge.\n\nMixed Reality may very well be the answer. A study conducted at Georgia Tech found that students who used mobile augmented reality platforms to learn coding performed better on assessments than their counterparts. Furthermore, research at Tufts University shows that tangible programming encourages high-level computational thinking. Two of our team members are instructors for an introductory programming class at the Colorado School of Mines. One team member is an interaction designer at the California College of the Art and is new to programming. Our fourth team member is a first-year computer science at the University of Maryland. Learning from each other's experiences, we aim to create the first mixed reality platform for tangible programming, which is also grounded in the reality-based interaction framework. This framework has two main principles:\n\n1) First, interaction takes place in the real world , so students no longer program behind large computer monitors where they have easy access to distractions such as games, IM, and the Web.\n\n2) Second, interaction behaves more like the real world. That is, tangible languages take advantage of students’ knowledge of the everyday, non-computer world to express and enforce language syntax.\n\nUsing these two concepts, we bring you MusicBlox!"},{"heading":"What is is","content":"MusicBlox combines mixed reality with introductory programming lessons to create a tangible programming experience . In comparison to other products on the market, like the LEGO Mindstorm, our tangible programming education platform cuts cost in the classroom (no need to buy expensive hardware!), increases reliability (virtual objects will never get tear and wear), and allows greater freedom in the design of the tangible programming blocks (teachers can print out new card/tiles and map them to new programming concepts).\n\nThis platform is currently usable on the Magic Leap AR headset, but will soon be expanded to more readily available platforms like phones and tablets.\n\nOur platform is built using the research performed by Google’s Project Bloks and operates under a similar principle of gamifying programming and using tangible programming lessons.\n\nThe platform consists of a baseboard where students must place tiles. Each of these tiles is associated with a concrete world item. For our first version, we focused on music. Thus, the tiles include a song note, a guitar, a piano, and a record. These tiles can be combined in various ways to teach programming concepts. Students must order the tiles correctly on the baseboard in order to win the various levels on the platform. For example, on level 1, a student must correctly place a music note, a piano, and a sound in order to reinforce the concept of a method. That is, an input (song note) is fed into a method (the piano) to produce an output (sound).\n\nThus, this platform not only provides a tangible way of thinking (students are able to interact with the tiles while visualizing augmented objects), but also makes use of everyday, non-computer world objects to express and enforce computational thinking."},{"heading":"How we built it","content":"Our initial version is deployed on the Magic Leap AR headset. There are four components to the project, which we split equally among our team members.\n\nThe first is image recognition, which Natalie worked predominantly on. This required using the Magic Leap API to locate and track various image targets (the baseboard, the tiles) and rendering augmented objects on those tracked targets.\n\nThe second component, which Nhan worked on, involved extended reality interaction. This involved both Magic Leap and Unity to determine how to interact with buttons and user interfaces in the Magic leap headset.\n\nThe third component, which Casey spearheaded, focused on integration and scene development within Unity. As the user flows through the program, there are different game scenes they encounter, which Casey designed and implemented. Furthermore, Casey ensured the seamless integration of all these scenes for a flawless user experience.\n\nThe fourth component, led by Ryan, involved project design, research, and user experience. Ryan tackled user interaction layouts to determine the best workflow for children to learn programming, concept development, and packaging of the platform."},{"heading":"Challenges we ran into","content":"We faced many challenges with the nuances of the Magic Leap platform, but we are extremely grateful to the Magic Leap mentors for providing their time and expertise over the duration of the hackathon!"},{"heading":"Accomplishments that We're Proud of","content":"We are very proud of the user experience within our product. This feels like a platform that we could already begin testing with children and getting user feedback. With our design expert Ryan, we were able to package the platform to be clean, fresh, and easy to interact with."},{"heading":"What We learned","content":"Two of our team members were very unfamiliar with the Magic Leap platform, so we were able to learn a lot about mixed reality platforms that we previously did not. By implementing MusicBlox, we learned about image recognition and object manipulation within Magic Leap. Moreover, with our scene integration, we all learned more about the Unity platform and game development."},{"heading":"What’s next for MusicBlox: Tangible Programming Education in Mixed Reality","content":"This platform is currently only usable on the Magic Leap AR device. Our next big step would be to expand to more readily available platforms like phones and tablets. This would allow for more product integration within classrooms.\n\nFurthermore, we only have one version which depends on music concepts and teaches methods and loops. We would like to expand our versions to include other everyday objects as a basis for learning abstract programming concepts."},{"heading":"Built With","content":"ar augmented-reality computervision hardware image-recognition magicleap mixed-reality mixedreality mr unity"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"aidn","project_url":"https://devpost.com/software/aidn","tagline":"aidn is the medical virtual assistant of the future. Powered by modern AI technologies, aidn is a tool for securely aggregating and sharing medical data, all combined into a single voice assistant.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/153/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Google Cloud] Best Use of Google Cloud"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[FujiFilm] Fujifilm's NeverStop Medical Innovation Prize"}],"team_members":[],"built_with":[{"name":"alexa","url":"https://devpost.com/software/built-with/alexa"},{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"bootstrap","url":"https://devpost.com/software/built-with/bootstrap"},{"name":"cloud","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"gcp","url":null},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"platform","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"twilio","url":"https://devpost.com/software/built-with/twilio"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"aidn was born out of a need for better voice agents and data digitization models in the healthcare industry. We always tried to put ourselves in our users' shoes and tried to mitigate barriers to user-accessible healthcare as early as a first symptom case, to as late as when the user is already in an ambulance. the Aid Network is designed to be by your side as an asset to help you achieve wellness faster and better.\n\nWe thought that the inertia behind using medical records to get information was too high, because the medical record transfer process is antiquated and slow with lots of human effort required. We saw a need for a centralized, digital source of healthcare data securely powered by Face ID to provide healthcare providers and first responders with the information that they need to maximize quality of care instantaneously."},{"heading":"What it does","content":"aidn has two major tracks of use: long-term, including secure maintenance of medical records and interactive educational Alexa skills, and short-term, which encompasses the aidn Medical Emergency feature. When users sign up, they enter their chosen credentials, an emergency contact, and are able to take or upload a user photo. When a user has signed up and is logged in, they are able to enter their medical history, as well as a customizable emergency message to display in the event of an emergency. Through the power of Azure Face, facial recognition is used on the user photo, so that in an emergency situation any individual with aidn just has to snap a photo of the injured person’s face to 1) send information to their emergency contact and 2) get access to vital medical information that the person is comfortable with sharing. Integration with Amazon Alexa allows users to register their medical information using a voice assistant approach, increasing accessibility, and adds the feature of health challenges that the user can request."},{"heading":"How I built it","content":"One of the first things we did as a team was to sit down and create a task list on Trello for the entire weekend. This let us write down our thoughts and vision for aidn down long before any code had been written, and put us all on the same page for the next 36 hours.\n\n### High Level Design\n\nOn creation of accounts in the system, we train our machine learning model on Azure, hashing the incoming face and making sure to save only the hash, and not the actual facial data of the user.\n\nAll data is stored on Firestore to allow easy access to our web based chatbot, built on DialogFlow. All pictures are stored on Google Cloud Storage, and services for the chatbot are spawned and run on demand on Cloud Run.\n\nOur Alexa experience allows a user to complete a health checkup through an Alexa anywhere in the world - all data is, of course, synced back to us so we can crunch the numbers, create a diagnosis and share it with our network of doctors, should you need help from them in the future!"},{"heading":"Challenges I ran into","content":"One of the hardest parts of this project was figuring out the Azure Cognitive Services for facial recognition of users through a picture. This was very hard to figure out and took Edward and Subhankar almost 5 straight hours of early morning debugging to even get an MVP running.\n\nIn the end, we were able to persevere through and create a facial recognition service that detects people in a matter of seconds and solves a huge problem - we’re psyched!"},{"heading":"Accomplishments that I'm proud of","content":"At many points the team was uncertain in the future of this project because of the sheer magnitude of everything we did - some of us had used AWS before, but outside of that we started off of scratch learning the “big 3” cloud providers, and made them seamlessly work with each other.\n\nOur Machine learning models are also very impressive - we stress tested them a lot and are very proud of how we were able to distribute the train time across users to ensure latency."},{"heading":"What I learned","content":"Caroline had used React previously, but had never interacted with any of the APIs or other technologies implemented in this project. She was able to learn a lot about the way these technologies all get integrated and implementing APIs end-to-end.\n\nEdward had previously worked with other cloud platform providers, but this was his first time learning Azure. He very much enjoyed using all 3 major cloud form provider technologies in this project.\n\nPanda had no idea about anything in the cloud provider department, but learned Azure for deployment, machine learning and continuous delivery."},{"heading":"What's next for aidn","content":"This is just the start of aidn’s possibilities. In the future, we can implement a network of verified healthcare professionals within the app, so that in the event of an emergency, instantaneous medical information with the features of a video call are only a tap away. We hope to expand aidn’s impact and bring accessible healthcare to all."},{"heading":"Built With","content":"alexa amazon-web-services azure bootstrap cloud express.js firebase gcp google javascript node.js platform python react twilio"}]},{"project_title":"Waitless - Stanford TreeHacks","project_url":"https://devpost.com/software/test-jrqbek","tagline":"Optimizing the hospital waiting experience by pre-processing patient details with the help of voice assistance.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/942/495/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Genentech] Most Beneficial to Patients"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[SoundHound] Best Use of Houndify Voice AI API"}],"team_members":[],"built_with":[{"name":"assistant","url":null},{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"figma","url":null},{"name":"houndify","url":null},{"name":"microsoft","url":null},{"name":"natural-language-processing","url":"https://devpost.com/software/built-with/natural-language-processing"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"voice","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/PandawanFr/WREco"}],"description_sections":[{"heading":"Inspiration","content":"No one likes waiting around too much, especially when we feel we need immediate attention. 95% of people in hospital waiting rooms tend to get frustrated over waiting times and uncertainty. And this problem affects around 60 million people every year, just in the US. We would like to alleviate this problem and offer alternative services to relieve the stress and frustration that people experience."},{"heading":"What it does","content":"We let people upload their medical history and list of symptoms before they reach the waiting rooms of hospitals. They can do this through the voice assistant feature, where in a conversation style they tell their symptoms, relating details and circumstances. They also have the option of just writing these in a standard form, if it's easier for them. Based on the symptoms and circumstances the patient receives a category label of 'mild', 'moderate' or 'critical' and is added to the virtual queue. This way the hospitals can take care of their patients more efficiently by having a fair ranking system (incl. time of arrival as well) that determines the queue and patients have a higher satisfaction level as well, because they see a transparent process without the usual uncertainty and they feel attended to. This way they can be told an estimate range of waiting time, which frees them from stress and they are also shown a progress bar to see if a doctor has reviewed his case already, insurance was contacted or any status changed. Patients are also provided with tips and educational content regarding their symptoms and pains, battling this way the abundant stream of misinformation and incorrectness that comes from the media and unreliable sources. Hospital experiences shouldn't be all negative, let's try try to change that!"},{"heading":"How we built it","content":"We are running a Microsoft Azure server and developed the interface in React. We used the Houndify API for the voice assistance and the Azure Text Analytics API for processing. The designs were built in Figma."},{"heading":"Challenges we ran into","content":"Brainstorming took longer than we anticipated and had to keep our cool and not stress, but in the end we agreed on an idea that has enormous potential and it was worth it to chew on it longer. We have had a little experience with voice assistance in the past but have never user Houndify, so we spent a bit of time figuring out how to piece everything together. We were thinking of implementing multiple user input languages so that less fluent English speakers could use the app as well."},{"heading":"Accomplishments that we're proud of","content":"Treehacks had many interesting side events, so we're happy that we were able to piece everything together by the end. We believe that the project tackles a real and large-scale societal problem and we enjoyed creating something in the domain."},{"heading":"What we learned","content":"We learned a lot during the weekend about text and voice analytics and about the US healthcare system in general. Some of us flew in all the way from Sweden, for some of us this was the first hackathon attended so working together with new people with different experiences definitely proved to be exciting and valuable."},{"heading":"Built With","content":"assistant azure express.js figma houndify microsoft natural-language-processing node.js react voice"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Promise","project_url":"https://devpost.com/software/promise-sf2c3b","tagline":"A web application to help both charity donors and recipients verify supplied goods end up in the right hands","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/940/011/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Oracle] Make the World a Better Place Hack"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Oracle] Make the World a Better Place Hack"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"google-maps","url":"https://devpost.com/software/built-with/google-maps"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"oracle","url":"https://devpost.com/software/built-with/oracle"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sketch","url":"https://devpost.com/software/built-with/sketch"}],"external_links":[{"label":"github.com","url":"https://github.com/MKagesawa/treehack20"}],"description_sections":[{"heading":"Inspiration","content":"With the recent Corona Virus outbreak, we noticed a major issue in charitable donations of equipment/supplies ending up in the wrong hands or are lost in transit. How can donors know their support is really reaching those in need? At the same time, those in need would benefit from a way of customizing what they require, almost like a purchasing experience.\n\nWith these two needs in mind, we created Promise. A charity donation platform to ensure the right aid is provided to the right place."},{"heading":"What it does","content":"Promise has two components. First, a donation request view to submitting aid requests and confirm aids was received. Second, a donor world map view of where donation requests are coming from.\n\nThe request view allows aid workers, doctors, and responders to specificity the quantity/type of aid required (for our demo we've chosen quantity of syringes, medicine, and face masks as examples) after verifying their identity by taking a picture with their government IDs. We verify identities through Microsoft Azure's face verification service. Once a request is submitted, it and all previous requests will be visible in the donor world view.\n\nThe donor world view provides a Google Map overlay for potential donors to look at all current donation requests as pins. Donors can select these pins, see their details and make the choice to make a donation. Upon donating, donors are presented with a QR code that would be applied to the aid's packaging.\n\nOnce the aid has been received by the requesting individual(s), they would scan the QR code, confirm it has been received or notify the donor there's been an issue with the item/loss of items. The comments of the recipient are visible to the donor on the same pin."},{"heading":"How we built it","content":"Frontend: React Backend: Flask, Node DB: MySQL, Firebase Realtime DB Hosting: Firebase, Oracle Cloud Storage: Firebase API: Google Maps, Azure Face Detection, Azure Face Verification Design: Figma, Sketch"},{"heading":"Challenges we ran into","content":"Some of the APIs we used had outdated documentation. Finding a good of ensuring information flow (the correct request was referred to each time) for both the donor and recipient."},{"heading":"Accomplishments that we're proud of","content":"We utilized a good number of new technologies and created a solid project in the end which we believe has great potential for good.\n\nWe've built a platform that is design-led, and that we believe works well in practice, for both end-users and the overall experience."},{"heading":"What we learned","content":"Utilizing React states in a way that benefits a multi-page web app Building facial recognition authentication with MS Azure"},{"heading":"What's next for Promise","content":"Improve detail of information provided by a recipient on QR scan. Give donors a statistical view of how much aid is being received so both donors and recipients can better action going forward.\n\nAdd location-based package tracking similar to Amazon/Arrival by Shopify for transparency"},{"heading":"Built With","content":"azure firebase google google-cloud google-maps node.js oracle react sketch"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ReadAR","project_url":"https://devpost.com/software/readar-twh41m","tagline":"An interactive and seamless way to perceive content for students and individuals with learning disabilities","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/942/507/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Microsoft] Azure Champ Prize - Hack for Good"}],"team_members":[],"built_with":[{"name":"arkit","url":null},{"name":"augmented-reality","url":"https://devpost.com/software/built-with/augmented-reality"},{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"natural-language-processing","url":"https://devpost.com/software/built-with/natural-language-processing"},{"name":"oracle","url":"https://devpost.com/software/built-with/oracle"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"social-good","url":null},{"name":"spacy","url":null},{"name":"torch","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/jackyzha0/treehacks2020-backend"},{"label":"github.com","url":"https://github.com/SaiG18/TreeHacks2020"}],"description_sections":[{"heading":"Inspiration","content":"1 in 5 children in the United States have learning disabilities and attention disorders like dyslexia, ADHD, Dyscalculia, Dysgraphia, Auditory Processing Disorder (APD) and Language Processing Disorder (LPD). Individuals with Dyslexia face significant challenges in educational settings, team presentations and meetings where a lot of the presentation is solely text, numbers and/or graphs. ( https://ldaamerica.org/types-of-learning-disabilities/ )\n\nSome of the specific challenges faced by individuals lying on the spectrum of learning disabilities face are:\n\nDifficulty reading normal text (Dyslexia) context blindness (figuring out senses of different words in different scenarios) and Trouble focusing on dense, solely textual material without visualizations.\n\nThis is where readAR comes in. Through an array of services built into AR, our project aims at enhancing the learning and perceptory experience for individuals with these conditions."},{"heading":"What it does","content":"readAR's main purpose is to make it easier for these individuals to learn, whether that be about what \"work\" actually refers to in a sentence, or understanding dense physics slides. We do this through using AR to re-render the world in a more dyslexic friendly font (like Dyslexie), and giving the option to users to add machine-learning inferred context to key phrases in the sentence. This enables us to pull visualizations and images that engage the user and enable him/her to better understand the concept. We also do SpeechtoText to transcribe the entire lecture/ meeting (especially helps individuals with auditory processing disabilities) for future reference. This also enables us to do intelligent quiz generation to assess the user's weak points in his understanding of a specific topic, and then give him customized resource recommendations. (Pulled using Bing's Custom Search API)"},{"heading":"How we built it","content":"We built the AR components and mobile app using Swift and ARKit. We also managed to to re-implement a pretty neat paper dealing with word disambiguation (Wiedemann, G., Remus, S., Chawla, A., Biemann, C. 2019) and serve it on the cloud (this is what allows us to provide context-specific definitions!). We utilized Azure's neat APIs, which allowed us to provide near-realtime OCR, intelligent search for resources and image suggestions, To top it all off, (almost) everything was Dockerized and deployed through Oracle Cloud Container Service to allow for some pretty nice scaleability."},{"heading":"Challenges we ran into","content":"We all went into the hackathon knowing that this idea would be crazy -- a moonshot even. And that's what made it so challenging and fun. Some of the most significant challenges we faced were,\n\nFinding a good way to render all the additional information without being overwhelming for the user Training and serving our own word disambiguation model that was this accurate (there were almost zero resources about this) Figuring out how to use AR for the first time"},{"heading":"Accomplishments that we're proud of","content":"One problem that we noticed in existing systems was how poorly they handled WSD (Word-sense disambiguation). We recognized this as an important issue for words in specific fields that have domain-specific meanings. E.g. \"work\" in the formal physics sense vs the general meaning. This is evident in the examples below:\n\nWe tackled this by creating and fine-tuning our own BERT model to be able to accurately disambiguate word meaning, through which we were able to reach 77% accuracy (5% off from the state of the art) in around 1.5 hours on our laptops."},{"heading":"What we learned","content":"We learned to use and integrate a pretty large array of different services- ranging from our own ML models for WSD and Question Generation, Azure Cognitive Services, Oracle for deployment, with Swift for our AR app. It was quite the journey putting all these jigsaw pieces together."},{"heading":"What's next for readAR","content":"We believe we've stumbled across an idea that definitely has significant potential to be taken forward and make an tangible impact. We hope to continue refining the project and hopefully also have it beta-tested by actual users with these disabilities."},{"heading":"Built With","content":"arkit augmented-reality azure ios machine-learning natural-language-processing oracle python social-good spacy torch"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"Prevent","project_url":"https://devpost.com/software/prevent","tagline":"Empowering and Ensuring Young People's Access to Preventive Care Services","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/938/903/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[MapMyIndia] Best Use of MapMyIndia API"}],"team_members":[],"built_with":[{"name":"figma","url":null},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"}],"external_links":[{"label":"www.figma.com","url":"https://www.figma.com/proto/t8XgR6IeMIQZ2PMQUo230E/Healthcare?node-id=43%3A10&scaling=scale-down"}],"description_sections":[{"heading":"Inspiration","content":"In a 2017 survey by Advocates for Youth, only 26 percent of 18 to 29 year olds knew that insurance plans must cover preventive care with no copay or other costs. Latino young people were the least knowledgeable. Usually the transition to young adulthood is accompanied by higher rates of mortality, greater engagement in health-damaging behaviors, and an increase in chronic conditions. Because these health problems are largely preventable, primary care visits can present a key opportunity for improving the health of young adults through preventive screening and intervention, with evidence supporting the efficacy of clinical preventive services. We challenged ourselves with the question \"How can design create an experience that makes young people be continually motivated and engaged with their primary care providers?\" that led us to build a human-centered solution, making it easy for individuals to identify and access the preventative healthcare services they need."},{"heading":"What it does","content":"Our app recommends preventative healthcare services specific to the individual, reminds them in a timely manner, and finds nearby healthcare facilities that offer the service and accept the individual's insurance."},{"heading":"What we learned","content":"Starting out with the understanding of people's contexts, we soon realized to shift from reactive user experience to create proactive and value-based care in order to empower young people's willingness to use the resources our app provides. We wanted to create the conditions for purposeful data gathering (users' health records) to drive new insights and incorporate it with a cycle of actions to benefit users."},{"heading":"How we built it","content":"Swift iOS app with SwiftUI, JSON parsing, location services, Figma"},{"heading":"Built With","content":"figma json swift"},{"heading":"Try it out","content":"www.figma.com"}]},{"project_title":"Sybilline","project_url":"https://devpost.com/software/sybilline","tagline":"Navigating the Urban Labyrinth: Let the visually impaired \"see\" the room around them with AR technology.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/941/067/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Facebook] Best AR/VR Hack"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"c#","url":"https://devpost.com/software/built-with/c--2"},{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"magic-leap","url":"https://devpost.com/software/built-with/magic-leap"},{"name":"unity","url":"https://devpost.com/software/built-with/unity"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"We came to this Treehacks wanting to contribute our efforts towards health care. After checking out and working with some of the fascinating technologies we had available to us, we realized that the Magic Leap AR device would be perfect for developing a hack to help the visually impaired."},{"heading":"What it does","content":"Sibylline uses Magic Leap AR to create a 3D model of the nearby world. It calculates the user distance from nearby objects and provides haptic feedback through a specifically designed headband. As a person gets closer to an object, the buzzing intensity increases in the direction of the object. Maneuverability options also include helping somebody walk in a straight line, by signaling deviations in their path."},{"heading":"How we built it","content":"Magic Leap creates a 3D triangle mesh of the nearby world. We used the Unity video game engine to interface with the model. Raycasts are sent in 6 different directions relative to the way the user is facing and calculate the distance towards the nearest object. These Raycasts correspond to 6 actuators that are attached to a headband and connected via an Arduino. These actuators buzz with higher intensity as the user gets closer to a nearby object."},{"heading":"Challenges we ran into","content":"For our initial prototype, the haptic buzzers would either be completely off or completely on. While this did allow the user to detect when an obstacle was vaguely near them in a certain direction, they had no way of knowing how far away it was. To solve this, we adjusted the actuators to modulate their intensity.\n\nAdditionally, raycasts were initially bound to the orientation of the head, meaning the user wouldn't detect obstacles in front of them if they were slouched or looking down. We had to take this into consideration when modifying our raycast vectors."},{"heading":"Accomplishments that we're proud of","content":"We're proud of the system we've built. It uses a complicated stream of data which must be carefully routed through several applications, and the final result is an intensely interesting product to use. We've been able to build off of this system to craft a few interesting and useful quality of life features, and there's still plenty of room for more.\n\nAdditionally, we're proud of the extraordinary amount of potential our idea still has. We've accomplished more than just building a hack with a single use case, we've built an entirely new system that can be iterated upon and refined to improve the base functionality and add new capabilities."},{"heading":"What we learned","content":"We jumped into a lot of new technologies and new skillsets while making this hack. Some of our team members used Arduino microcontrollers for the first time, while one of us learned how to solder. We all had to work hard to figure out how to interface with the Magic Leap, and we learned more about how meshing works in the Unity editor as well.\n\nLastly, though we cannot hope to fully understand the experience of vision impairment or blindness, we've cultivated a bit more empathy for some of the challenges such individuals face."},{"heading":"What's next for Sybilline","content":"With industry support, we could significantly expand functionality of Sybilline to apply a number of other vision related tasks. For example, with AI computer vision, Sybilline could tell the user what are objects in front of them. We would be able to create a chunk-based loading system for multiple \"zones\" throughout the world, so the device isn't limited to a certain area. We would also want to prioritize the meshing for faster-moving objects, like people in a hallway or cars in an intersection. With more advanced hardware, we could explore other sensory modalities as our primary method of feedback, like using directional pressure rather than buzzing. In a fully focused, specifically designed final product, we would like to have more camera angles to get more meshing data with, and an additional suite of sensors to cover other immediate concerns for the user."},{"heading":"Built With","content":"arduino c# c++ magic-leap unity"}]},{"project_title":"Dronations","project_url":"https://devpost.com/software/dronations-enabling-instant-refunds-for-purchase-returns","tagline":"Enabling Instant Refunds for Purchase Returns & Increasing Access to Financial Services for People with Low Mobility","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/735/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Checkbook] Best Hack Using a Payments API"}],"team_members":[],"built_with":[{"name":"airmon","url":null},{"name":"bash","url":"https://devpost.com/software/built-with/bash"},{"name":"checkbook","url":null},{"name":"drones","url":null},{"name":"houndify","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"kali","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"snaptain","url":null},{"name":"terminal","url":"https://devpost.com/software/built-with/terminal"}],"external_links":[{"label":"github.com","url":"https://github.com/akhand42/Dronations"}],"description_sections":[{"heading":"Inspiration","content":"Since the days of yore, we have heard how an army of drones is going to bridge the last-mile gap of accessibility in the supply chain of various products for many people in the world. A good example of this can already be seen with Zipline in Rwanda and Ghana, which is already on its way to success with this model. While the quest to bridging the last-mile gap is well on it's way to getting completed, there are significant challenges in the overall supply chain that have had no evolution. As technology gets more and more advanced, people get more disillusioned by the possible use cases that are not being explored.\n\nThe inspiration for this hack very much stems from the fact that the Treehacks Hardware Lab provided us with a drone that did not come with an SDK, or any developer support. And therefore, I decided to build a whole new approach to voice recognition and instant payments that not only tries to address this growing disillusionment with technology, but also provides a use case for drones, a technology that is no longer \"unimaginable\" or \"out of this world\" that has not yet been explored: refunds and accessbility ."},{"heading":"What it does","content":"With Dronations, you can ask a voice-activated drone to:\n\nTAKEOFF TOUCHDOWN TAKE A PICTURE (using gesture) TRANSFER MONEY TO PERSON IN FRAME Now, you can receive instant refunds for a product or an item you have returned after buying. And people with limited mobility can send payments reliably to people they trust using just simple voice commands delivered to a drone."},{"heading":"How I built it","content":"Since the Snaptain drone provided to me did not come with an SDK, I was left with no choice but to find a way to hack into it. Fortunately, the flimsy drone did not have that much going on in terms of protection from adversarial attacks. After fiddling with the drone for a few hours, it was clear that the best way of trying to break into the drone in order to control it with a computer or connect it to a server would be through the unsecured WiFi it uses to connect to an app on my phone.\n\nI spent another few hours reverse engineering the drone protocol by sniffing packets on the channel and frequency used by this drone to get commands from my phone. For this purpose, I used Airmon-NG on a computer running a distribution of Kali-Linux (thankfully provided by my senior project lab at Stanford). Having figured out how to send basic takeoff, touchdown and image commands to the drone by bypassing its WiFi gave me an immense of control of the drone and now it was time to integrate voice commands.\n\nOut of all the voice recognition softwares on offer, Houndify came through as my top choice. This was because of the intuitive design of adding domains to add more functionality on their app and the fact that the mentors were super helpful while answering questions to help us debug. I build three custom commands for takeoff, touchdown and sending money.\n\nFinally, I integrated Checkbook's API for sending money. The API didn't quite work as expected and it took a while for me to finagle it into making it do what I wanted. However, eventually, everything came together and the result is an end-to-end solution for instant payments using voice-activated drones."},{"heading":"Challenges I ran into","content":"Wifi reliability issues on the cheap hardware of a Snaptain drone Checkbook API's refusal to accept my POST requests Sleep Deprivation? The endless bad humor of the team sitting next to me"},{"heading":"Accomplishments that I'm proud of","content":"Literally hacking into a drone with little to no prior experience hacking an embedded system Sniffing packets over the air to reverse engineer drone protocol Going through tens of videos showcasing obscure details of packet sniffing Making a voice-activated drone Integrating with Checkbook's API Making new friends in the process of this hackathon"},{"heading":"What I learned","content":"How to sniff packets on an unsecure HTTP connection How to write server level Node.js commands that communicate directly with the terminal How to spoof my MAC address How to spoof my identity to gain unauthorized access into a device How to build an app with Houndify How to integrate voice commands to trigger Checkbook's API If it exists, there is an AR/VR project of it"},{"heading":"What's next for Dronations: Enabling Instant Refunds for Purchase Returns","content":"Integrate more features from Checkbook's API to provide an end-to-end payment solution Understand more edge use cases for drones - I have barely scratched the surface Do user research with people with low accessibility to understand if this can become a viable alternative to send payments reliably to people they trust Getting in touch with Snaptain and letting them know how easy it is to hack their drones"},{"heading":"Built With","content":"airmon bash checkbook drones houndify javascript kali python snaptain terminal"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Inspo | An exploration tool for kids","project_url":"https://devpost.com/software/inspo-an-exploration-tool-for-kids","tagline":"\"What do you want to be when you grow up?\" This powerful AR tool helps kids explore their surroundings & learn of different hobbies and professions — all while having fun. Be something new daily!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/938/652/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Google Cloud] Best Use of Google Cloud"}],"team_members":[],"built_with":[{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"invision","url":null},{"name":"lensstudio","url":null},{"name":"snap","url":null},{"name":"snapchat","url":"https://devpost.com/software/built-with/snapchat"},{"name":"spectacles","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"xcode","url":"https://devpost.com/software/built-with/xcode"}],"external_links":[{"label":"github.com","url":"https://github.com/angelapwen/inspo"}],"description_sections":[{"heading":"Inspiration","content":"Often as children, we were asked, \"What do you want to be when you grow up?\" Every time we changed our answers, we were sent to different classes to help us embrace our interests. But our answers were restricted and traditional: doctors, engineers or ballerinas. We want to expand every child's scope: to make it more inclusive and diverse, and help them realize the vast opportunities that exist in this beautiful world in a fun way.\n\nLet's get them ready for the future where they follow their passion — let's hear them say designers, environmentalists, coders etc."},{"heading":"What it does","content":"The mobile application uses Augmented Reality technology to help children explore another world from a mobile phone or tablet and understand the importance of their surroundings. It opens up directly into the camera, where it asks the user to point at an object. The app detects the object and showcases various career paths that are related to the object. A child may then pick one and accomplish three simple tasks relevant to that career, which then unlocks a fun immersion into the chosen path's natural environment and the opportunity to \"snap\" a selfie as the professional using AR filters. The child can save their selfies and first-person immersion experience videos in their personal in-app gallery for future viewing, exploration, and even sharing."},{"heading":"How I built it","content":"Our team of three first approached the opportunity space. We held a brainstorming exercise to pinpoint the exact area where we can help, and then stepped into wireframing. We explored the best medium to play around with for the immersive, AR experience and decided upon Spectacles by Snap & Lens Studio, while exploring Xcode and iOS in parallel. For object detection, we used Google's MLKit Showcase App with Material Design to make use of Google's Object Detection and Tracking API. For the immersion, we used Snap's Spectacles to film real-world experiences that can be overlaid upon any setting, as well as Snap's Lens Studio to create a custom selfie filter to cap off the experience.\n\nWe brought in code together with design to bring the app alive with its colorful approach to appeal to kids."},{"heading":"Challenges I ran into","content":"We ran into the problem of truly understanding the perspectives of a younger age group and how our product would successfully be educational, accessible, and entertaining. We reflected upon our own experiences as children and teachers, and spoke to several parents before coming up with the final idea.\n\nWhen we were exploring various AR/VR/MR technologies, we realized that many of the current tools available don't yet have the engaging user interfaces that we had been hoping for. Therefore we decided to work with Snap's Lens Studio, as the experience in-app on Snapchat is very exciting and accessible to our target age range.\n\nOn the technical side, Xcode and Apple have many peculiarities that we encountered over the course of Saturday. Additionally, we had not taken into consideration the restrictions and dependencies that Apple imposes upon iOS apps."},{"heading":"Accomplishments that I'm proud of","content":"We're proud that we did all of this in such a short span of time. Teamwork, rapid problem solving and being there for each other made for a final product that we are all proud to demo.\n\nWe're also proud that we took advantage of speaking to several sponsors from Snap and Google, and mentors from Google and Apple (and Stanford) throughout the course of the hackathon. We enjoyed technically collaborating with and meeting new people from all around the industry."},{"heading":"What I learned","content":"We learnt how to collaborate and bring design and code together, and how both go hand in hand. The engineers on the team learned a great amount about the product ideation and design thinking, and it was interesting for all of us to see our diverse perspectives coalesce into an idea we were all excited about."},{"heading":"What's next for Inspo | An exploration tool for kids","content":"On the technical side, we have many ideas for taking our project from a hackathon demo to the release version. This includes:\n\nStronger integration with Snapchat and Google Cloud More mappings from objects to different career pathways Using ML models to provide recommendations for careers similar to the ones children have liked in the past An Android version\n\nOn the product side, we would like to expand to include:\n\nA small shopping list for parents to buy affordable, real-world projects related to careers A \"Career of the Week\" highlight Support for a network/community of children and potentially even professional mentors"},{"heading":"Built With","content":"firebase google-cloud invision lensstudio snap snapchat spectacles swift xcode"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Soteria","project_url":"https://devpost.com/software/soteria-58hg6m","tagline":"A privacy-protecting telemedicine platform for refugees","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/938/168/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Stanford McCoy Family Center for Ethics in Society] Most Ethically Engaged Hack"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"alwaysAI - Computer Vision for Medical Access"}],"team_members":[],"built_with":[{"name":"3dprinting","url":"https://devpost.com/software/built-with/3dprinting"},{"name":"c","url":"https://devpost.com/software/built-with/c"},{"name":"caffe","url":null},{"name":"caffeine","url":"https://devpost.com/software/built-with/caffeine"},{"name":"camera","url":null},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"sensor","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Daniel-Wu/Soteria"}],"description_sections":[{"heading":"We are Soteria, an patient-centric telehealth platform for refugees","content":"We consider ethical technology development an important responsibility for all engineers.\n\nRead on to see our ethical analysis!"},{"heading":"Inspiration","content":"One of our hackers spent over 1500 hours working with refugees fleeing Syria, Afghanistan, and South Sudan. She found that many of these refugees had plenty of health issues, but also plenty of (justified) fear of authority figures, preventing them from seeking out medical care. These refugees will delay treatment because they are scared of having their identities exposed.\n\nGlobal refugee crises have critical implications to a country’s economic, political and social development. During their migration, the refugees are often in dire need of medical care, but realize that it is too costly and too risky to seek such professional help. They are scared that if they go to a doctor in a country they pass by before they enter their destination country, they will never be granted legal asylum status there. They also lack the paperwork that reflects their medical history and provides them with medical care. As a result, in European countries like Germany and Belgium where healthcare is free for all citizens, refugees suffer from a lack of safe and low cost medical care that creates social inequality.\n\nSoteria aims to strike a delicate balance -- between providing caregivers important information about each patient, crucial for accurate diagnoses, while maintaining patient privacy such that individuals can seek medical help confidently without fear of retribution."},{"heading":"What it does","content":"As migratory pressure becomes increasing strenuous on overburdened healthcare systems, legislative, administrative, and financial barriers drastically reduce individual access to medical care. Issues range from lack of reliable information on the illness to cultural barriers, lack of knowledge of available services, and failure of administrative coordination.\n\nNamed after the Greek goddess of salvation, Soteria is a telemedicine platform that connects refugees with confidential, low cost, high quality medical care. Our intent is to make healthcare accessible for populations in transit in all kinds of scenarios--from routine health check ups of day to day life in a new country to the organized chaos of a newly established refugee camp.\n\nWe envision our product applied in two primary verticals: emergency medicine, and daily usage. In large scale public health emergencies, we see our product having the potential to minimize risk and exposure for healthcare workers in the diagnosis and triage of large populations, in both minimizing workload through our automated classifiers and minimizing exposure to harmful pathogens. In day to day usage, we see our low-cost telemedicine solution as an alternative to a doctor’s office, allowing vulnerable populations to access the care they need without the financial and legislative barriers originally in place.\n\nWith that in mind, the Soteria device is designed to be light weight, compact, and mobile to allow for usage in dynamic, rapidly changing environments closely tied with the mobile nature of refugee populations. Government and non-governmental organizations (e.g. MSF) are capable of providing rapid and highly specialized care without the expenses of ground deployment of specialists or the risk of contracting rare infectious diseases, while still allowing doctors to perform their diagnoses effectively given the remote collection of basic classifiers.\n\nSoteria’s core platform consists of a camera and a heart rate sensor, attached to a raspberry pi. When a patient uses the device, Soteria is designed to automatically recognize the age, gender, and basic classifiers of the patient to expedite the triage process, particularly for individuals that generally lack detailed healthcare records due to the nature of their status. Automating the collection of basic identifiers allows us to categorize and triage patients on a far greater scale, in the vein of expanding low cost affordable healthcare for marginalized, vulnerable patients without access to healthcare."},{"heading":"How we built it","content":"We had three major components to our hack: The hardware, backend/ML, and the frontend.\n\nHardware\n\nWe used a Tragoods Heart Rate Monitor, wrote low-level c++ code to interface with it on a Arduino Uno, and sent the heart rate information on to a Raspberry Pi over serial. We also used a Raspberry Pi camera to collect video of each patient.\n\nWe 3D printed, over the course of 9 hours, a custom stand and diagnosis apparatus for holding the Raspberry Pi, power source, sensors, camera, and Intel Movidius NCS, together in the correct position.\n\nMachine Learning/Backend\n\nWe use three different neural networks in our hack - A face detector using SSD on a ResNet-10 backbone, an age detector using SSD on a MobileNetV2 backbone, and a gender detector using SSD on a MobileNetV2 backbone.\n\nThe age and gender recognition models were trained on the Adience Gender and Age Classification Benchmark, while the face detector was trained on the MSCOCO dataset. The face detector’s predictions were used to select faces for the gender and age detector to predict on. We then blur out faces by applying a Gaussian filter over all regions of the image containing a face.\n\nThe models were deployed to a Raspberry Pi 4 with a Intel Neural Compute Stick 2, and ran in real time (10fps) .\n\nFrontend\n\nWe built a Flask webpage to demo our telehealth platform, using HTML and CSS with some jQuery. The page consists of three parts, the intro, mission, and demo of the product. The frontend is mostly static with the exception of the live video feed, which runs each frame through the AlwaysAI backend and dynamically updates the results."},{"heading":"Challenges we ran into","content":"Integrating low-level sensor libraries with python code across multiple edge devices and the development computer. Stitching together code from python, c, javascript, and using dozens of packages from all over. Integration of heart rate sensor/arduino with raspberry pi. 3D printing/Autocad structural design. Getting sleep."},{"heading":"Accomplishments that we're proud of","content":"Getting 3 computer vision models to play well together on a raspberry pi for real-time inference. Understanding how to connect a raspberry pi camera and an arduino heart rate sensor into one product. Getting sleep."},{"heading":"What we learned","content":"How to stream video robustly with dynamic flask webpages. How to build peer-to-peer communication How to integrate across the whole stack, from hardware to machine learning. Rapid prototyping."},{"heading":"What's next for Soteria","content":"Firstly, understanding the interplay between too much or too little information would immediately inform our ability to fulfill our mission of helping improve the lives of refugees. While it might seem interesting to load our platform with all kinds of sensors and monitors, it becomes impossible to retain our mission of low-cost accessible care. On the flipside, a bare-bone device with a minimal amount of sensors and data might compromise the ability of physicians to provide accurate, effective medical advice due to lack of information. As two sides of the same coin, these considerations are invariably intertwined: do you provide an abundance of information at risk of decreasing accessibility? Or do you prioritize accessibility at the risk of compromising your core service/value? These are questions we hope to better understand.\n\nIt would be worthwhile to investigate the intrinsic value of our basic classifiers to doctors, and understand some of the drawbacks to telemedicine that we can work to address. While we can imagine that the physical limitations means doctors might not be able to listen to your breathing through a stethoscope, it’s important to understand if there exists the ability to mirror that information/conclusion using another sensor or develop a similar understanding through other metrics. Understanding how doctors generally diagnose patient conditions, particularly in a telemedicine setting, would allow us to develop our product in a patient/doctor-centric manner and optimize our sensor array to maximize the amount of useful information while retaining financial accessibility.\n\nA potential application we considered was the reduction of social stigmas in certain areas through the use of telemedicine; more specifically, it means that refugees might have access to doctors that spoke their language and understood their culture through our platform, conditions that may not exist in their region. Understanding how we could potentially tailor our platform for certain regions, cultural differences, ethnicities, etc. may be useful in optimizing for regions where political instability produces a significantly larger proportion of refugees, or targeting certain areas where refugee health outcomes are particularly poor.\n\nBuilding out a robust backend and productionizing our telehealth platform are the natural next steps towards building a useful product."},{"heading":"Ethics","content":"We believe that access to healthcare is a basic human right, and that it is ethically wrong for us as a society to not act against the problem of the lack of medical care among refugees. And this is not a small problem. There were an estimated 70 million refugees worldwide in 2018, who are afflicted with a bevy of chronic and acute health conditions, exacerbated by poor living conditions and lack of access to medical care.\n\nWhen approaching this problem, we had the advantage of having a team member who had spent a lot of time working with refugees, who understood how they oriented themselves towards authority figures and medical care. Privacy was their largest concern - thus, privacy was our largest focus.\n\nContinuing on with our patient-centered design process, we then turned to the ethical rule set by Edmund Pellegrino (Sulmasy, 2014). He argues that there exist three main components to the doctor-patient relationship: A patient who is sick and needs help A doctor who feels responsible for helping the patient Medical action/application of medical science\n\nWe believe that patients should voluntarily refer to doctors, which can only be done when he/she trusts that doctor. In order to eliminate barriers and to facilitate this development of trust, we hence blur the patient’s image in order to eliminate stereotypes within and amongst physicians and thereby believe we can even out the quality of care.\n\nAt the same time, we recognize the need for a patient’s respect for physician feedback and vice versa and acknowledge that the use of telemedicine makes this more difficult. Nonetheless, we believe that this low-cost, highly effective live video streaming surface could bring a large amount of benefit, and that as long as we prioritize the patient first, both in their health and in their freedom, we will be doing patients a favor rather than harming them.\n\nHowever, it’s important to be thoughtful of some of the ethical pitfalls of our technology. There exist many ethical issues to further explore:\n\nFirst, we must ensure that patient information security and confidentiality are of utmost importance, in regards to both immigration status and health status. Otherwise, any benefit we provide them could be more than offset by the damage we do to their identity. Thus, we must make sure we keep patient data 100% confidential when receiving, storing, and transferring data. Secondly, we must consider that while this will increase access to healthcare for some, there will exist segments of the population that will continue to be deprived of even telemedicine services, due to either knowledge or financial gaps. This is true not just on an individual level but also on a country level, where high cost often deters them from investing in such technologies. Typically, it means that Western nations end up providing care for unstable regions in Africa, the Middle East, Asia, etc. We must recognize the inherent biases this presents--in healthcare providers being almost exclusively from Europe or North America--whether it’s through unconscious biases with which we view and treat refugee populations, or potential personal biases (eg. patronizing views for Syrian refugees) that physicians impose upon vulnerable populations throughout their treatment and care. Third, there is well known research (Bulamwini, 2018) that shows facial recognition with AI can be highly biased, and perform worse on minorities because the underlying training dataset might be skewed towards residents of first-world nations. We think this is a particularly large area of concern, as refugee populations often have a disproportionate number of ethnic minorities. Long-term strategies to address this bias include retraining our AI models on more diverse and representative datasets. Ethical considerations must include a robust understanding of the issue from the public health perspective. Refugees, knowing their identities are protected, might contact doctors with a highly contagious disease, like coronavirus. It then becomes impossible for doctors to, firstly, make a conscious decision of whether or not to report this individual for suspicious symptoms given the competing elements of ‘the greater good’ and their Hippocratic Oath. Even if the doctor should choose to prioritize public health, the built in privacy features of our product may mean that they then lack the means to effectively report the suspected patient due to lack of specific identifiers, and a built-in tracking option for public health emergencies would thereby undermine the patient-centric vision of our product. In future development, it becomes crucial for us to understand and delineate between acceptable and unacceptable situations; while patient privacy remains at the center of our focus, when and if authorities should be allowed to unblur or identify the patient they are speaking to. In addition, we have to understand how our service might be integrated or made available in various social and political contexts. In certain states where censorship is particularly prevalent, complex political environments introduce an additional element for concern. Perhaps our platform could be utilized by certain politically-motivated groups for the spread of information (or disinformation). For example, political groups in rural India, with access to these refugees who could one day become formal citizens with voting powers, might be influenced in a tremendous capacity by the information they receive from an authoritative medical figure given a lack of other sources of media and information. In certain countries such as China, it may be impossible for MSF to provide telemedicine services to refugees due to the inability of the government to regulate the information and topics of conversation that emerge on air; state monitoring of our services would then compromise our promise of privacy and anonymity. It’s incredibly important, in the development of our platform, to understand the complex and multifaceted social/political environments (noting the significant correlation between social/political instability and refugee populations) in which our product might be applied, and perhaps tailor different adaptations or versions of our product for specific scenarios.\n\nA good tangible next step in addition to the ideas mentioned above would be to design a proper patient consent form that both supports patients’ ethical rights and removes any concern about confidentiality of the data. Our goal is to provide a safe space for refugees to seek medical access and thus should be as transparent as possible. We ardently hope that Soteria is the first step towards a healthier future for displaced peoples around the globe."},{"heading":"Bibliography","content":"Buolamwini, Joy, and Timnit Gebru. \"Gender shades: Intersectional accuracy disparities in commercial gender classification.\" Conference on fairness, accountability and transparency. 2018\n\nSulmasy, Daniel P. \"Edmund Pellegrino's philosophy and ethics of medicine: an overview.\" Kennedy Institute of Ethics Journal 24.2 (2014): 105-112"},{"heading":"Built With","content":"3dprinting c caffe caffeine camera css html javascript python pytorch raspberry-pi sensor"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"FireNet: Wildfire Risk Forecaster for Social Equality","project_url":"https://devpost.com/software/wildfire-risk-treehacks","tagline":"A deep learning system saving you from falling into the trap of wildfire risk and social inequality","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/942/589/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Google Cloud] Best Use of Google Cloud"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"earth-engine","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"keras","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"kkraoj.users.earthengine.app","url":"https://kkraoj.users.earthengine.app/view/wildfire-danger"},{"label":"github.com","url":"https://github.com/kkraoj/wildfire-risk-treehacks"}],"description_sections":[{"heading":"Why forecast wildfire risk?","content":"Climate change is exacerbating wildfire likelihood and severity in the western USA. For instance, in the state of California, USA, in the last century, 12 of the largest wildfires, 13 of the most destructive wildfires, and 7 of the deadliest wildfires occurred during the last decade. Wildfires emit massive amounts of harmful particulate matter posing a direct threat to humans. The severity of wildfire's effects combined with the probability of them worsening in the future due to climate change prompts an urgent need to understand, estimate, and forecast wildfire risk. Our FireNet, a deep-learning-powered system, helps improve the forecasting of wildfire risk by the following \"PR. S^3\" solutions:\n\nP--Prevent smoke inhalation : Wildfire smoke is known to cause serious respiratory disorders. Wildfire exposure ratings can help better forecast the severity and probability of wildfire smoke. R--Reallocate disaster relief resources : By knowing when and where fires are likely to occur, fire managers can better allocate their resources to be well-prepared when wildfires do occur. S--Save lives : High resolution maps of wildfire risk, if prepared periodically, can help evacuate people in advance of an occurrence of wildfire. S--Sustainability : Wildfires not only cause damages to human society, but also decrease species diversity and increase greenhouse gas emissions. S--Social Equality: People from different backgrounds, such as wealth, urban proximity, race, and ethnicity, are unequally exposed to wildfire risk. Poor communities and black, Hispanic or Native American experience greater vulnerability to wildfires compared with other communities. We are dedicated to eliminate the \"unnatural\" humanity crisis and consequences of natural disasters, such as wildfires."},{"heading":"What does our tool do?","content":"FireNet is a rapid risk forecasting tool to characterize the three fundamental components of risk--hazard, exposure, and vulnerability--by combining high resolution remote sensing imagery with deep learning. The system integrates microwave remote sensing (Sentinel-1) and optical remote sensing (Landsat-8) information over 3 months to produce accurate estimates of fuel conditions (cross-validated r-squared = 0.63) exceeding previous methods by a wide margin (r-squared = 0.3). Moreover, by linking the Long Short Term Memory (LSTM) outputs for fuel conditions with data on human settlements and population density, FireNet indicates the aggregate risk imposed by wildfires on humans. FireNet is hosted as a Google Earth Engine App."},{"heading":"Wait, but what is wildfire risk exactly?","content":"Wildfire risk depends on 3 fundamental quantities - hazard, exposure, and vulnerability. It can be assessed by combining the extent of its exposure (e.g., how likely is a fire), severity of the hazard (e.g., how big a fire can occur) and the magnitude of its effects (e.g., how much property could be destroyed). Assessing wildfire risk presents several challenges due to uncertainty in fuel flammability and ignition potential. Due to complex physiological mechanisms, estimating fuel flammability and ignition potential has not been possible on landscape scales. Current wildfire risk estimation methods are simply insufficient to accurately and efficiently estimate wildfire risk because of the lack of direct flammability-associated inputs."},{"heading":"Is FireNet better than other risk estimation methods out there?","content":"Absolutely! FireNet is the first wildfire risk forecasting system that operates at 250 m spatial resolution. The best wildfire danger models like the National Fire Danger Rating System operate at approximate 25 Kms. This was possible because of the system's unique use of microwave as well as optical remote sensing. Moreover, existing systems have lesser accuracy in predicting fuel flammability (r-squared = 0.3) than our system (r-squared = 0.63)."},{"heading":"This sounds like yet another deep learning fad without any explainability?","content":"FireNet is fully transparent and explainable. We anticipated the need for explainability for FireNet and thus did NOT use deep learning to estimate fire risk. The deep learning model (LSTM) merely estimates fuel flammability (how wet or dry the forests are) using supervised learning on remote sensing. The flammability estimates are then combined with fuel availability (observed) and urban proximity (census data) to produce a transparent estimate."},{"heading":"I am sold! Where can I checkout FireNet?","content":"Here"},{"heading":"What did we learn from this project?","content":"Training a deep learning model is child's play compared to the amount of work required in data cleaning. Data cleaning is child's play compared to the amount of time needed to design an app with UX in mind"},{"heading":"Built With","content":"earth-engine google-cloud javascript keras python"},{"heading":"Try it out","content":"kkraoj.users.earthengine.app github.com"}]},{"project_title":"ClassiFly","project_url":"https://devpost.com/software/classifly-1gbpif","tagline":"Epidemiological Drone Imaging","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/942/157/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Samsung] Best Bixby Hack"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"}],"external_links":[{"label":"github.com","url":"https://github.com/sergiogcharles/ClassiFly?fbclid=IwAR1uwX1qmWyC6da2ZXrGIMpPBfPa8IY-sHcJocXH0vKWxe8OCUL4a_oAGsc"}],"description_sections":[{"heading":"Inspiration","content":"Epidemiology is critical in figuring out how to stop the spread of diseases before it’s too late"},{"heading":"What it does","content":"ClassiFly uses image data to classify individuals with known disease symptoms. For demonstration purposes, we selected Yellow Fever and Methicillin-resistant Staphylococcus aureus, and Eelephantiasis."},{"heading":"How I built it","content":"The app was developed in Swift, and the classification model was trained using a split data classifier method, which leveraged Apple's native CreateMLUI framework to build an image classifier model with 89% accuracy."},{"heading":"Challenges We ran into","content":"We initially planned on building an autonomous drone for tracking that could be used to identifying certain key epidemiological characteristics in a medically unsafe and infected region. That is, this would effectively increase accessibility to remote areas that are susceptible to infection. However, there was no clear way to interface with the drone via an API so we decided to simply build a classification app that would allow you to use a drone image taken in a contaminated area and derive certain key epidemiological insights."},{"heading":"Accomplishments that I'm proud of","content":"I am proud that we were able to successfully work together efficiently to build an image classification app."},{"heading":"What I learned","content":"We learned how we navigate managing development projects as a team, as well as how to leverage really powerful computer vision capabilities with CoreML."},{"heading":"What's next for ClassiFly","content":"What if we could have an army of medical detectives in the sky, able to reach the most remote populations? Briefly: Navigate to remote areas, collect image data of populus, use Machine Learning to classify afflictions based on visible symptoms. This paints a better picture of the disease landscape much faster than any human observation."},{"heading":"Built With","content":"python swift"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Eloquent — Learn Out Loud","project_url":"https://devpost.com/software/eloquent-learn-out-loud","tagline":"Learn to Speak, and Speak to Learn","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/695/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[SoundHound] Best Use of Houndify Voice AI API"}],"team_members":[],"built_with":[{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"kanna","url":null},{"name":"reductio","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"}],"external_links":[{"label":"github.com","url":"https://github.com/cliffpanos/Eloquent-iOS"}],"description_sections":[{"heading":"What it does","content":"Eloquent has two primary functions, both influenced by a connection between speaking and learning\n\nThe first is a public speaking coach, to help people practice their speeches. Users can import a speech or opt to ad-lib — the app will then listen to the user speak. When they finish, the app will present a variety of feedback: whether or not the user talked to fast, how many filler words they used, the informality of their language, etc. The user can take this feedback and continue to practice their speech, eventually perfecting it.\n\nThe second is a study tool, inspired by a philosophy that teaching promotes learning. Users can import Quizlet flashcard sets — the app then uses those flashcards to prompt the user, asking them to explain a topic or idea from the set. The app listens to the user's response, and determines whether or not the answer was satisfactory. If it was, the user can move on to the next question; but if it wasn't, the app will ask clarifying questions, leading the user towards a more complete answer."},{"heading":"How we built it","content":"The main technologies we used were Swift and Houndify. Swift, of course, was used to build our iOS app and code its logic. We used Houndify to transcribe the user's speech into text. We also took advantage of Houndify's \"client matches\" feature to improve accuracy when listening for keywords.\n\nMuch of our NLP analysis was custom-built in Swift, without a library. One feature that we used a library for, though, was keyword extraction. For this, we used a library called Reductio, which implements the TextRank algorithm in Swift. Actually, we used a fork of Reductio, since we had to make some small changes to the build-tools version of the library to make it compatible with our app.\n\nFinally, we used a lightweight HTML Parsing and Searching library called Kanna to web-scrape Quizlet data."},{"heading":"Challenges we ran into","content":"I (Charlie) found it quite difficult to work on an iOS app, since I do not have a Mac. Coding in Swift without a Mac proved to be a challenge, since many powerful Swift libraries and tools are exclusive to Apple systems. This issue was partially alleviated by the decision to do most of the NLP analysis from the ground up, without an NLP library — in some cases though, coding without the ability to debug on my own machine was unavoidable.\n\nWe also had some difficulties with the Houndify API, but the #houndify Slack channel proved very useful. We ended up having to use some custom animations instead of Houndify's built-in one, but in the end, we solved all functionality issues."},{"heading":"Built With","content":"ios kanna reductio swift"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Super Smart Board","project_url":"https://devpost.com/software/tba-nxy0z4","tagline":"Imagine if your teacher's smartboard understood what's drawn on it","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/941/366/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[OVAL Lab] Best Almond Hack"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Samsung] Best Bixby Hack"}],"team_members":[],"built_with":[{"name":"almond","url":null},{"name":"bixby","url":"https://devpost.com/software/built-with/bixby"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"We've noticed that many educators draw common structures on boards, just to erase them and redraw them in common ways to portray something. Imagine your CS teacher drawing an array to show you how bubble sort works, and erasing elements for every swap. This learning experience can be optimized with AI."},{"heading":"What It Does","content":"Our software recognizes digits drawn and digitizes the information. If you draw a list of numbers, it'll recognize it as an array and let you visualize bubble sort automatically. If you draw a pair of axes, it'll recognize this and let you write an equation that it will automatically graph.\n\nThe voice assisted list operator allows one to execute the most commonly used list operation, \"append\" through voice alone. A typical use case would be a professor free to roam around the classroom and incorporate a more intimate learning experience, since edits need no longer be made by hand."},{"heading":"How We Built It","content":"The digits are recognized using a neural network trained on the MNIST hand written digits data set. Our code scans the canvas to find digits written in one continuous stroke, puts bounding boxes on them and cuts them out, shrinks them to run through the neural network, and outputs the digit and location info to the results canvas.\n\nFor the voice driven list operator, the backend server's written in Node.js/Express.js. It accepts voice commands through Bixby and sends them to Almond, which stores and updates the list in a remote server, and also in the web user interface."},{"heading":"Challenges We Ran Into","content":"The canvas was difficult to work with using JavaScript It is unbelievably hard to test voice-driven applications amidst a room full of noisy hackers haha"},{"heading":"Accomplishments that We're Proud Of","content":"Our software can accurately recognize digits and digitize the info!"},{"heading":"What We Learned","content":"Almond's, like, really cool Speech recognition has a long way to go, but is also quite impressive in its current form."},{"heading":"What's Next for Super Smart Board","content":"Recognizing trees and visualizing search algorithms Recognizing structures commonly found in humanities classes and implementing operations for them Leveraging Almond's unique capabilities to facilitate operations like inserting at a specific index and expanding uses to data structures besides lists More robust error handling, in case the voice command is misinterpreted (as it often is) Generating code to represent the changes made alongside the visual data structure representation"},{"heading":"Built With","content":"almond bixby express.js javascript node.js"}]},{"project_title":"GAiN a Friend","project_url":"https://devpost.com/software/gain-a-friend","tagline":"A GAN-generated virtual friend that allows older folks to battle psychosocial issues","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/341/datas/medium.gif","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[SoundHound] Best Use of Houndify Voice AI API"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"conversational-ai","url":null},{"name":"deep-learning","url":null},{"name":"generative-adversial-networks","url":null},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"houndify","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"scikit-video","url":null},{"name":"speech-to-text","url":null},{"name":"torchvision","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Senior suicides have tripled in the last decade due to the large baby boomer population retiring alone. Loneliness is the biggest problem seniors face that contributes to their mental health. According to an AARP survey, a third of adults over the age of 65 are lonely and highly prone to suicidal thoughts. With this in mind, we aspired to create a solution to this problem by allowing senior citizens to gain a friend!"},{"heading":"What It Does","content":"GAiN a Friend is a speech-driven video chat service powered by Generative Adversarial Networks (GAN). This is done by selecting a human face and voice that resonates with the user most. After the persona is selected, the user can begin interacting with the GAN-generated person 24/7."},{"heading":"How We Built It","content":"GAiN a Friend is built using Generative AI, particularly Generative Adversarial Networks. In order to run these large models, we needed to use Deep Learning VMs from Google Cloud to run these models on high compute engines. For conversational AI and speech conversation, Houndify APIs were used."},{"heading":"Ethical Considerations","content":"Mass media has been increasingly focused on a future with AI, with movies such as Her and Ex Machina to TV shows such as Black Mirror. As our team was brainstorming this idea of a chatbot to improve communication and curb feelings of isolation, we kept considering the ethical implications of our work and focused on three main conditions.\n\nPrivacy and Data Ownership: As seen from the first ELIZA chatbot in the 1960s, humans are inclined to trust bots (and not believe that it's a program!) and tell it sensitive information. It is imperative that a user's valuable information stays with the user and does not fall into other hands. With this in mind, our team decided to store the program and information locally and not share the data collected. We are also not collecting any videos of the user to ensure that their autonomy is respected. Transparency: In order to build trust between the user and company, we must be upfront about our chatbot. To ensure there is no confusion between the bot and a human, we have added a disclaimer to our site letting the user know their conversation is with a bot. Moreover, our code is open source so anyone that is curious to understand the \"behind the scenes\" of our chatbot is welcome to take a look! User Safety and Protection: There have been many cases of hackers influencing digital personal assistants to go rogue or turn into wiretaps. To ensure this doesn’t happen, our “friend” uses the Houndify small-talk API to make sure the user can have broad conversations. Furthermore, with minorities and LGBTQ+ elders feeling significantly more lonely than others, our chatbot will not make any assumptions about the user and can be easily accessible by all populations."},{"heading":"Challenges We Ran Into","content":"Learned how to use GANs for the first time Using Python 2.7 packages for Python 3.5 was... INSANELY CHALLENGING Dealing with deprecated packages Caching errors on Jupyter Notebook that have cryptic errors Using JavaScript in a Flask app for audio recording Learned how to use ethics into consideration in our design solution Working with team you've never worked with before"},{"heading":"Accomplishments That We Are Proud Of","content":"We are proud of being an ALL WOMEN team :)"},{"heading":"What's Next for GAiN a Friend","content":"In the future, this concept can easily be expanded to the service sector for service providers, such as psychologist, physicians, and family members. GAiN a Friend wants to incorporate additional API. Focus on a more accessible design for the App development to help elderly folks interact with the app better."},{"heading":"Built With","content":"conversational-ai deep-learning generative-adversial-networks google google-cloud houndify python scikit-video speech-to-text torchvision"}]},{"project_title":"health.ai","project_url":"https://devpost.com/software/healthier-nfck7s","tagline":"ML optimizing workflow so physicians have more time to see you.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/939/514/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"nltk","url":"https://devpost.com/software/built-with/nltk"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"voice","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/k8iechen/Treehacks2020"}],"description_sections":[{"heading":"Inspiration","content":"An Annals of Internal Medicine study* showed that of 430 physician office hours observed, only 27% was spent directly with patients whereas 49% of their time was spent on Electric Health Records (EHR). This is not including the additional 1-2 hours spent outside of office hours each day, dedicated primarily to EHR tasks.\n\nWe built a comprehensive app that uses ML to reduce the time spent on EHR tasks so that physicians may have more time for seeing patients."},{"heading":"What it does & How we built it","content":"Task 1: Ordering Tests\n\nPhysicians have to order tests for patients based on their medical records. Some are regular (e.g. annual) tests, while others are specific to certain symptoms. It takes a physician a long time to meticulously read through the entire patient medical record and even then, some less common tests are occasionally forgotten.\n\nOur solution consists of a Machine Learning (ML) algorithm that uses both Natural Language Processing (ANN) and classification (kNN) to scan through patient medical records for doctor notes (text) and test results (numerical). NLP extracts the key words from the physician's notes and uses a pre-trained model to identify which conditions need to be tested for. Likewise, classification uses the medical profiles (previous test results) of other patients to rank the need for certain tests.\n\nTask 2: Adding notes\n\nPhysicians often make jot notes on paper, then later sit down by a desktop to type up paragraphs to add to the digital patient record file. This meant that more time was spent on re-familiarizing with the case, not to mention paper notes can be more easily displaced.\n\nWe implemented an iOS feature that can automatically translate speech to text , so that physicians can take elaborate notes on the go, and only light revision is necessary later on."},{"heading":"Ethical Considerations","content":"How much should we rely on ML/AI ? The technology has advanced very quickly and predictions are not more accurate than ever, but where medicine is concerned, can we ever leave human lives in the hands of AI? Throughout the process of building this app, we've been very clear on the fact that this is meant to be a helper, not a replacement for physicians. While we hope to maximize the efficiency of the hospital workflow so more people can be helped, we believe that it would be unethical to leave the health of people in the hands of only AI. Thus we conclude that while AI has proven helpful in its ability to reduce repetitive work, it would be unwise to eliminate the human factor completely.\n\nThese considerations are reflected in our app design, as physicians must approve of suggestive tests for them to be ordered. We have designed it such that the physician must click on each prompt to order the suggested test, thus reinforcing review of AI's suggestions."},{"heading":"Challenges I ran into","content":"compatibility issues between Python scripts and the JavaScript backend finding publicized medical record datasets"},{"heading":"Accomplishments that I'm proud of","content":"being able to evaluate using both paragraphs and numerical test results learning react on the go!"},{"heading":"What I learned","content":"how NLP-ANN works how to connect between different languages & platforms"},{"heading":"What's next for health.ai","content":"develop ML to be more robust, maintaining accuracy for a greater variety of medical conditions."},{"heading":"Sources","content":"*Published: Ann Intern Med. 2016;165(11):753-760, DOI: 10.7326/M16-0961, Published at www.annals.org on 6 September 2016, © 2016 American College of Physicians"},{"heading":"Built With","content":"express.js firebase ios machine-learning nltk node.js python react voice"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MedReader","project_url":"https://devpost.com/software/medreader","tagline":"User-friendly, assistive technology to read medical and prescription information aloud to patients","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/753/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"css3","url":"https://devpost.com/software/built-with/css3"},{"name":"django","url":"https://devpost.com/software/built-with/django"},{"name":"google-cloud-image-api","url":null},{"name":"google-cloud-speech-api","url":null},{"name":"google-cloud-vision-api","url":null},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/Joyliu290/MedReader.git"}],"description_sections":[{"heading":"Inspiration","content":"To help patients, the elderly population and those with vision impairments and/or reading difficulties better understand and remember their medical and prescription information from both electronic and handwritten documentation."},{"heading":"What it does","content":"Our project is a user-friendly, assistive digital tool based on image-to-text and text-to-speech technology to read aloud prescription and medical record information. Users can take a photo or upload a picture of their report/prescription instructions, and the text is converted to an audio recording and played back."},{"heading":"How I built it","content":"We integrated machine learning models from Google Cloud Vision API and Cloud Text-to-Speech API to convert images to text data and text data to speech data, respectively. Django was used to build out the back-end logic and integrating the back-end with the front-end."},{"heading":"Challenges I ran into","content":"Two main challenges we faced in this project were working with Django to build out the front-end and building a web-based platform that works on the mobile interface."},{"heading":"Accomplishments that I'm proud of","content":"We are proud that we learned how to combine multiple Google Cloud machine learning APIs to do image-to-text and text-to-speech extraction. Also, as a group, we didn't have much prior experience in front-end development, so it was a fun learning experience to integrate our final model into a web-based platform."},{"heading":"What I learned","content":"We refined our skills in front-end development."},{"heading":"What's next for MedReader","content":"Future work would focus on refining detection of specific medical and prescription data related to dosage and usage and exclude all other information on reports or prescriptions. To preserve patient privacy, we will also incorporate functionality to detect and anonymize all personal patient information in a given image at the time of upload. Moreover, we plan to sync the extracted audio recording with the phone calendar to set both audio and text medication reminders for users and add support for multiple languages."},{"heading":"Built With","content":"css3 django google-cloud-image-api google-cloud-speech-api google-cloud-vision-api html5 python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Talk to the Hand","project_url":"https://devpost.com/software/talk-to-the-hand-deal1z","tagline":"Making conversations more inclusive.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[],"external_links":[{"label":"github.com","url":"https://github.com/SanskritiL/talk_to_my_hand"}],"description_sections":[{"heading":"Inspiration","content":"In today's world, technology has made it possible for people from various backgrounds and cultures to interact and understand each other through various cross-cultural and cross-linguistic platforms. Spoken language is a much smaller barrier than it was a few decades ago. But there still remains a community amongst us with whom a majority of us can't communicate face-to-face due to our lack of knowledge of their mode of communication. What makes their language different from any other is that their speech isn't spoken, it is shown.\n\nThis is particularly pronounced in the education domain for students and educators in this domain can feel isolated in mixed learning environments and this project hopes that through it, they are able to better communicate and integrate with the world around them."},{"heading":"What it does","content":"Our contribution is Talk To The Hand — a web application that helps hearing impaired people share their message with the world, even if they don't have the physical or financial access to an interpreter. Sign language speakers open the application and sign into their computer’s camera. Talk To The Hand uses computer vision and machine learning to interpret their message, transmit the content to their audience via voice assistant, and create a written transcript for visual confirmation of the translation. After the user is done speaking, Talk To The Hand gives the opportunity to share the written transcript of their talk through email, text message, or link.\n\nWe imagine that this tool will be especially helpful and powerful in public speaking settings — not unlike presenting at a Hackathon! Talk To The Hand dramatically increases the ability of deaf and hard of hearing people to speak to a broad audience."},{"heading":"How we built it","content":"We have two components to the application, the first being the machine learning model that recognizes hand gestures and predicts the corresponding meaning and second being the web application that provides the user with an intuitive interface to perform the task of interpreting the signs and speaking them out with multiple language support for speech.\n\nWe built the model by training deep neural nets on a Kaggle dataset - Sign Language MNIST for hand gesture recognition ( https://www.kaggle.com/datamunge/sign-language-mnist ). Once we set up the inference mechanism to get the prediction from the model for the hand gesture given as an image, we used the prediction and converted it to speech in English using the Houndify text-to-speech API. We then set up the web application through which the user can interact using images of hand gestures and the interpretation of the gesture is both displayed as text and spoken out in their language of choice."},{"heading":"Challenges we ran into","content":"One of the biggest hurdles we faced as a team was the development of our hosting platform. Despite the lack of experience in the domain, we wanted to make our application more accessible and intuitive for our users. After exploring some of the more basic web development technologies such as HTML, CSS and Javascript, we shifted to nuanced web/mobile app development to make our application implementable in various domains with ease. We faced obstacles during the transfer of data from the frontend to the backend and vice versa for our images and speech responses from API calls. In the process, we managed to set up a web based application."},{"heading":"Accomplishments that we're proud of","content":"First and foremost, we are proud of having thought of and built the first iteration of an application that will allow for people dependent on sign language to cross any barriers to communication that may come their way. We are hopeful about the impact it will have on this community and are looking forward to carrying to the next phase. We are thrilled about developing a model that can predict the letter corresponding to the Sign Language and integrating it with Text-to-Speech API and deploying a functional web application even though our team is inexperienced with web development. Overall, we relish the experience for having pushed ourselves beyond what we thought was possible and working on something that we believe will change the world."},{"heading":"What we learned","content":"One of the biggest takeaways for our team as a whole is going through the entire development life cycle fo a product starting from ideation to building the minimum viable product. We were exposed to more applications of Computer Vision through this project."},{"heading":"What's next for Talk to the Hand","content":"Today’s version of Talk To The Hand is a very minimal representation created in 36 hours in order to show proof of concept. Next steps would include in-depth sign education and refined experience based on user testing and feedback. We believe Talk To The Hand could make a powerful impact in public speaking and presentation settings for the deaf and hard of hearing, especially in countries and communities where physical and financial access to interpreters proves difficult. Imagine a neighborhood activist sharing an impassioned speech before a protest, a middle school class president giving his inaugural address, or a young hacker pitching to her panel of judges."},{"heading":"Try it out","content":"github.com"}]},{"project_title":"HackerSat","project_url":"https://devpost.com/software/hackersat","tagline":"HackerSat demonstrates laser communication between PCB-sized satellites, which could open the doors for swarms of small satellites doing advanced research near-earth and in deep space.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/940/980/datas/medium.JPG","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"c","url":"https://devpost.com/software/built-with/c"},{"name":"msp430","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"In the past decade, the popularization of CubeSats and rideshare programs has led to the democratization of space in which small companies, academic researchers, and even student organizations have gotten the opportunity to go to space.\n\nAs these opportunities become cheaper and more numerous, a new possibility of satellite system becomes very interesting. Namely, the opportunity to create a swarm of CubeSats that could do more advanced research than a single satellite at a fraction of the cost of a flagship satellite from a large company. For example, a single cubesat can't have a very high resolution camera, but a swarm of CubeSats could combine the results of each of their cameras to get a result similar to that possible from a multi million dollar satellite.\n\nFurthermore, the StarLink constellation launched by SpaceX uses laser communication to maintain the relative positions of their hundreds of satellites. Inspired by this as well as by the work on chipsats by the rex lab at Stanford we chose to create a demonstration of laser communication using microcontrollers to act as our satellites."},{"heading":"What it does","content":"HackerSat demonstrates laser communication between two independent satellites, allowing the transmission of text over long distances."},{"heading":"How we built it","content":"This project was done using an Arduino Uno, a TI MSP430, a laser diode, a photoresistor, as well as various other electrical components. The software was written in C for the microcontrollers, allowing the Arduino to encode information and send it through the laser to the MSP430, which was able to then decode and display the information."},{"heading":"Challenges we ran into","content":"Getting both HackerSats to synchronize in their data sending/receiving. Even if both devices' clock speeds are the same, the sampling times may be different, causing very noisy results. We fixed this by starting and ending with a specific flag and modifying the result to not include that sequence.\n\nWe also faced the challenge of dynamically changing lighting conditions, which caused the system to falsely detect when the laser was turned on. To fix this we began by taking baseline measurements of the ambient lighting and comparing incoming data against that, which was robust enough to remove essentially all false measurements."},{"heading":"Accomplishments that we're proud of","content":"We were very excited to have a working demo in such a short period of time. There were many things to learn before we were able to even begin engineering including learning how to use the electronics, learning to program the microcontrollers, which use a modified version of C, and learning how communications systems work to adapt those ideas to our own project."},{"heading":"What's next for HackerSat","content":"We hope to continue to develop this project using more advanced techniques like sending error correcting codes, implementing communications between more than two satellites at a time, and maybe even demonstrating it in space!"},{"heading":"Built With","content":"arduino c msp430"}]},{"project_title":"Glucose Guardian","project_url":"https://devpost.com/software/glucose-guardian-i0x8bg","tagline":"Reimagining Patient Education and Treatment Delivery through Gamification","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/940/468/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"swift","url":"https://devpost.com/software/built-with/swift"}],"external_links":[{"label":"drive.google.com","url":"https://drive.google.com/file/d/1igUjgRS7tdqvpnZRVfaKcPkg8np9h3D6/view?usp=sharing"}],"description_sections":[{"heading":"Reimagining Patient Education and Treatment Delivery through Gamification","content":"Imagine walking into a doctors office to find out you’ve been diagnosed with a chronic illness. All of a sudden, you have a slew of diverse healthcare appointments, ongoing medication or lifestyle adjustments, lots of education about the condition and more. While in the clinic/hospital, you can at least ask the doctor questions and try to make sense of your condition & management plan. But once you leave to go home, you’re left largely on your own .\n\nWe found that there is a significant disconnect between physicians and patients after patients are discharged and diagnosed with a particular condition. Physicians will hand patients a piece of paper with suggested items to follow as part of a \"treatment plan\". But after this diagnosis meeting, it is hard for the physicians to keep up-to-date with their patients on the progress of the plan. The result? Not surprisingly, patients quickly fall off and don’t adhere to their treatment plans, costing the healthcare system upwards of $300 billion as they get readmitted due to worsening conditions that may have been prevented.\n\nBut it doesn’t have to be that way…\n\nWe're building an engaging end-to-end experience for patients managing chronic conditions, starting with one of the most prevalent ones - diabetes. More than 100 million U.S. adults are now living with diabetes or prediabetes"},{"heading":"How does Glucose Guardian Work?","content":"Glucose Guardian is a scalable way to gamify education for chronic conditions using an existing clinical technique called “teachback” (see here- link ). We plan to partner with clinics and organizations, scrape their existing websites/documents where they house all their information about the chronic condition, and instantly convert that into short (up to 2 min) voice modules.\n\nGlucose Guardian users can complete these short, guided, voice-based modules that teach and validate their understanding of their medical condition. Participation and correctness earn points which go towards real-life rewards for which we plan to partner with rewards organizations/corporate programs.\n\nGlucose Guardian users can also go to the app to enter their progress on various aspects of their personalized treatment plan. Their activity on this part of the app is also incentive-driven.\n\nThis is inspired by current non-health solutions our team has had experience with using very low barrier audio-driven games that have been proven to drive user engagement through the roof."},{"heading":"How we built it","content":"We've simplified how we can use gamification to transform patient education & treatment adherence by making it more digestible and fun. We ran through some design thinking sessions to work out how we could create a solution that wouldn’t simply look great but could be implemented clinically and be HIPAA compliant.\n\nWe then built Glucose Guardian as a native iOS application using Swift. Behind the scenes, we use Python toolkits to perform some of our text matching for patient education modules, and we utilize AWS for infrastructure needs."},{"heading":"Challenges we ran into","content":"It was difficult to navigate the pre-existing market of patient adherence apps and create a solution that was unique and adaptable to clinical workflow. To tackle this, we dedicated ample time to step through user journeys - patients, physicians and allied health professionals. Through this strategy, we identified education as our focus because it is critical to treatment adherence and a patient-centric solution."},{"heading":"We're proud of this","content":"We've built something that has the potential to fulfill a large unmet need in the healthcare space, and we're excited to see how the app is received by beta testers, healthcare partners, and corporate wellness organizations."},{"heading":"Learning Points","content":"Glucose Guardian has given our cross-disciplined team the chance to learn more about the intersection of software + healthcare. Through developing speech-to-text features, designing UIs, scraping data, and walking through patient journeys, we've maximized our time to learn as much as possible in order to deliver the biggest impact."},{"heading":"Looking Ahead","content":"As per the namesake, so far we've implemented one use case (diabetes) but are planning to expand to many other diseases. We'd also like to continue building other flows beyond patient education. This includes components such as the gamified digital treatment plan which can utilize existing data from wearables and wellness apps to provide a consolidated view on the patient's post-discharge health. Beyond that, we also see potential for our platform to serve as a treasure trove of data for clinical research and medical training. We're excited to keep building and keep creating more impact."},{"heading":"Built With","content":"amazon-web-services ios python swift"},{"heading":"Try it out","content":"drive.google.com"}]},{"project_title":"ML Algorithm Visualizer","project_url":"https://devpost.com/software/ml-algorithm-visualizer","tagline":"Show don't tell how machines learn!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/937/852/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[DRW] Best Data Visualization"},{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[InfoSys] Most Energy Efficient/Sustainable Energy Hack"}],"team_members":[],"built_with":[{"name":"dash","url":"https://devpost.com/software/built-with/dash"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"plotly","url":"https://devpost.com/software/built-with/plotly"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sklearn","url":null}],"external_links":[{"label":"ml-visualizer.herokuapp.com","url":"https://ml-visualizer.herokuapp.com/"},{"label":"github.com","url":"https://github.com/sagnibak/ml-visualizer/"}],"description_sections":[{"heading":"Inspiration","content":"Sagnik is a student-instructor for the UC Berkeley Intro to ML class. Some students complained that they had no intuitive idea what happened when they changed model parameters, and that they'd like to see how the model's output changed interactively with change in parameters."},{"heading":"What it does","content":"Visualizes the decision boundary of various machine learning algorithms (SVM, decision trees, boosted trees, multilayer perceptrons, etc.).\n\nThis helps democratize access to machine learning education . There are many people who are getting started with machine learning on their own, and they rely completely on free resources available online. This visualizer will be a great tool for such people to get an in-depth understanding of decision boundaries, overfitting, generalization, etc.\n\nSince this is a free, open-source tool available online, instructors across the world can use it in their classrooms to teach, explain, and demo machine learning algorithms, and then students can go home and use it to further their understanding."},{"heading":"How we built it","content":"We started with the code of Dash's SVM visualizer and heavily modified it to add support for other machine learning algorithms."},{"heading":"Challenges we ran into","content":"Refining the UI -- positioning the logo correctly Python issues with libraries"},{"heading":"Accomplishments that we're proud of","content":"Sagnik: Finishing a project at a hackathon!!! Completing the parameters for other ML algorithms."},{"heading":"What I learned","content":"Sagnik: Making a web app and deploying it to Heroku Colin: Changing around frontend attributes using python Komila: Finishing designs using Adobe Illustrator"},{"heading":"What's next for ML Algorithm Visualizer","content":"Sagnik will use this as a teaching tool in class, and keep adding to the web app as needed to maximize students' learning. The following are planned for the very near future:\n\nadding more options for the existing algorithms ability to import custom data add animated APIs add the ability for students to interactively click on their graph data points add descriptions for each ML algorithm so students can get exposed to new ones add a separated frontend component so that visuals can become more customized"},{"heading":"Built With","content":"dash flask plotly python sklearn"},{"heading":"Try it out","content":"ml-visualizer.herokuapp.com github.com"}]},{"project_title":"SaferSolo - Virtual Voice Directed Copilot","project_url":"https://devpost.com/software/safersolo-virtual-voice-directed-copilot","tagline":"SaferSolo gives every solo pilot a virtual copilot. Copilot never complains, is ready to fly when you are, helps you manage the cockpit, keeps you company and keeps you safer in the sky.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/941/144/datas/medium.jpg","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Infosys] Sponsored Prize for Education"}],"team_members":[],"built_with":[{"name":"houndify","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"speech-recognition","url":null},{"name":"texttospeech","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"At 2am on the first day of the hackathon, my team determined our project was not viable and withdrew from the hackathon. I decided to continue on solo. I thought, \"hey, this isn't the first time I've 'flown solo,' and actually, how can I make this experience better?\""},{"heading":"What it does","content":"SaferSolo is a voice assistant system designed specifically for solo pilot operations. SaferSolo can run checklists and remind you if you have forgotten a procedure (many accidents are caused by this). SaferSolo can also remind you what air traffic controllers assigned you to avoid accidental pilot deviations (also a big accident causer), keep track of other traffic around you to increase situation awareness, and assist in emergencies (like a real, reliable copilot!)."},{"heading":"How I built it","content":"I use a Raspberry Pi as a lightweight, mobile, computing platform that can integrate with any aircraft communications system. The script is written in Python and takes advantage of Houndify for it's excellent, intelligent speech recognition system. A simple, non-invasive, hardware interface along with both visual and aural feedback makes SaferSolo easy and intuitive to use."},{"heading":"Challenges I ran into","content":"Hotword detection was very difficult to implement as an offline network for recognizing a custom hotword would take a long time to develop within a 36 hour period. After spending several hours testing a wide variety of implementations, I decided to implement hotword detection in a future version. Additionally, allowing for natural phraseology while extracting the particular aviation contexts was quite challenging. Detecting certain keywords while not getting distracted by other filler words required extensive testing. Aviation itself is very regulated in terms of required phraseology, but a real copilot should be spoken to like a normal human. This makes interaction with SaferSolo easier and more intuitive."},{"heading":"Accomplishments that I'm proud of","content":"Checklist mechanics will prevent a pilot from skipping checklist steps or entire checklists. Copilot remembers what phase of flight you are in and will even prompt you of the next checklist to run.\n\nCopilot will listen to air traffic control commands, extract the important information, and remind you what your assigned headings, navigation, or altitude is to prevent pilot deviations from ATC assignments.\n\nYou can speak to copilot quickly, fluidly, and naturally. Through the Houndify system, intelligent speech detection is possible with the copoilot allowing for a better and intuitive user experience."},{"heading":"What I learned","content":"Learned how to program python on Raspberry pi (have never done so until Saturday morning) Learned how to implement speech recognition systems Learned theory behind natural language processing"},{"heading":"What's next for SaferSolo - Virtual Voice Directed Copilot","content":"Hotword detection, better independent sound card for cockpit integration, faster speech recognition, greater range of features for copilot, more natural speaking voice, avionics integration."},{"heading":"Built With","content":"houndify python raspberry-pi speech-recognition texttospeech"}]},{"project_title":"seedHacks","project_url":"https://devpost.com/software/treehacks_2020","tagline":"Webcam-enabled reforestation software with a splash of ML magic!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/000/939/757/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"alwaysAI - Computer Vision in IoT"}],"team_members":[],"built_with":[{"name":"azure","url":"https://devpost.com/software/built-with/azure"},{"name":"canvas","url":"https://devpost.com/software/built-with/canvas"},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"pygame","url":"https://devpost.com/software/built-with/pygame"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/RomanMatthew/treehacks_2020"}],"description_sections":[{"heading":"Description","content":"Our planet's in dire straits.\n\nOver the past several decades, as residential, commercial, and industrial demands have skyrocketed across several industries around the globe, deforestation has become a major problem facing humanity. Though we depend on trees and forests for so much, they seem to be one of our fastest depleting natural resources. As our primary provider for oxygen and one of our biggest global carbon sinks, this is very dangerous news.\n\nseedHacks is a tool to help save the world.\n\nThrough the use of cloud-based image classification, live video feed, geospatial optimization, and robotic flight optimization, we've made seedHacks to facilitate reforestation at a rate and efficiency that people might not be able to offer. Using our tech, a drone collecting simple birds-eye images of a forest can compute the optimal positions for seeds to be planted, aiming to approach a desired forest density collected from the user. Once it has this map, planting them's just a robotics problem! Easy, right?"},{"heading":"How did we build?","content":"We broke the project up into three main parts: video feed and image collection, image tagging and seed location optimization, and user interface/display.\n\nTo tackle image/video, we landed on using pygame to set up a constant feed and collect images from that feed upon user request.\n\nWe then send those captured images to a Microsoft could computing server, where we trained an object detection model using Azure's custom-vision platform which returns a tagged image with locations of the trees in the overhead image.\n\nFinally, we send to an optimization algorithm that utilizes all the free space possible as well as some distance constraints to fill the available space with as many trees as possible. All this was wrapped up in an elegant and easy-to-interpret UI that allows us to work together with the expertise of users to make the best end-result possible!"},{"heading":"Technical Notes","content":"Azure custom vision was used to train an object detection model that could label tree and trees. We used about 33 images found online to train this machine learning model, resulting in a precision of 82.5% We used the custom vision API to send our aerial view images of forests to the prediction endpoint which returned predictions consisting of confidence level, label, and a bounding box. We then parsed the output of the object detection by creating a 2D numpy array in Python representing the original image. We filled indices of the array with 1’s where pixels were labeled as “tree” or “trees” with at least a 50% confidence. At the same time, we extracted the max width and height of the canopy of the trees to automate the process for users. The users are allowed to input a buffer, as a percentage, which increases the bounding box for tree growth based on the current density/present species; this is especially important if the roots of the tree need space to grow or the tree species is competitive. After the 2D array was filled with pre-existing trees, we iterated through the array to find places where new trees could be planted such that there was enough space for the tree to mature to its full canopy size. We labeled these indices with 2 to differentiate between existing trees and potential new trees."},{"heading":"What did we learn?","content":"First off, that selecting and training good object detector can be complicated and mysterious, but definitely worth putting some time into. Though our initial models had promise, we needed to optimize for overhead forest views, which is not something that's used to train too many.\n\nSecond, that keeping it simple is sometimes better for realizing ideas well. We were very excited to get our hands on a Jetson Nano and trick it out with AlwaysAI's amazing technologies, but we realized some time in that because we didn't actually end up using the hardware and software to the fullest of their abilities, they might not be the best approach to our particular problems.\n\nSo, we simplified! Finally, that the applicability of cutting-edge environmental robotics carries a lot of promise going forward. With not too much time, we managed to develop a somewhat sophisticated system that could potentially have a huge impact - and we hope to be able to contribute more to the field in the future!"},{"heading":"What's next for seedHacks?","content":"Next steps for our project would include:\n\nFurther optimization on seed location (more technical approach using botantical/silvological expertise, etc) Training object detector better and better to pick out individual and clusters of trees from an overhead view More training on burnt trees and forests Robotic pathfinding systems to automatically execute paths through a forest space Actuators on drones to make seed planting possible Generalizing to aquatic and other ecosystems"},{"heading":"Built With","content":"azure canvas css html5 javascript node.js pygame python react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Questions?","project_url":"https://devpost.com/software/questions","tagline":"Questions? aims to streamline the process of asking questions during lectures by allowing attendees to use their smartphone as a microphone, ensuring everyone gets a chance to share their ideas.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/000/941/881/datas/medium.png","prizes":[{"hackathon_name":"TreeHacks 2020","hackathon_url":"https://treehacks-2020.devpost.com/","prize_name":"[Infosys] Sponsored Prize for Education"}],"team_members":[],"built_with":[{"name":"angular.js","url":"https://devpost.com/software/built-with/angular-js"},{"name":"audio","url":null},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"websocket","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Stanford-Treehacks-2020"},{"label":"www.figma.com","url":"https://www.figma.com/file/KcQrXfHphx0vhUQ7saO8Bx/Questions-Student-Mobile-View?node-id=0%3A1"},{"label":"www.figma.com","url":"https://www.figma.com/file/BmUb6zxGAyJWGEDgbC2hxc/Questions-Professor-Web-View?node-id=0%3A1"}],"description_sections":[{"heading":"Inspiration","content":"Almost all undergraduate students, especially at large universities like the University of California Berkeley, will take a class that has a huge lecture format, with several hundred students listening to a single professor speak. At Berkeley, students (including three of us) took CS61A, the introductory computer science class, alongside over 2000 other students. Besides forcing some students to watch the class on webcasts, the sheer size of classes like these impaired the ability of the lecturer to take questions from students, with both audience and lecturer frequently unable to hear the question and notably the question not registering on webcasts at all. This led us to seek out a solution to this problem that would enable everyone to be heard in a practical manner."},{"heading":"What does it do?","content":"Questions? solves this problem using something that we all have with us at all times: our phones. By using a peer to peer connection with the lecturer’s laptop, a student can speak into their smartphone’s microphone and have that audio directly transmitted to the audio system of the lecture hall. This eliminates the need for any precarious transfer of a physical microphone or the chance that a question will be unheard. Besides usage in lecture halls, this could also be implemented in online education or live broadcasts to allow participants to directly engage with the speaker instead of feeling disconnected through a traditional chatbox."},{"heading":"How we built it","content":"We started with a fail-fast strategy to determine the feasibility of our idea. We did some experiments and were then confident that it should work. We split our working streams and worked on the design and backend implementation at the same time. In the end, we had some time to make it shiny when the whole team worked together on the frontend."},{"heading":"Challenges we ran into","content":"We tried the WebRTC protocol but ran into some problems with the implementation and the available frameworks and the documentation. We then shifted to WebSockets and tried to make it work on mobile devices, which is easier said than done. Furthermore, we had some issues with web security and therefore used an AWS EC2 instance with Nginx and let's encrypt TLS/SSL certificates."},{"heading":"Accomplishments that we're (very) proud of","content":"With most of us being very new to the Hackathon scene, we are proud to have developed a platform that enables collaborative learning in which we made sure whatever someone has to say, everyone can hear it. With Questions? It is not just a conversation between a student and a professor in a lecture; it can be a discussion between the whole class. Questions? enables users’ voices to be heard."},{"heading":"What we learned","content":"WebRTC looks easy but is not working … at least in our case. Today everything has to be encrypted … also in dev mode. Treehacks 2020 was fun."},{"heading":"What's next for Questions?","content":"In the future, we could integrate polls and iClicker features and also extend functionality for presenters and attendees at conferences, showcases, and similar events. _ Questions? _ could also be applied even broader to any situation normally requiring a microphone—any situation where people need to hear someone’s voice."},{"heading":"Built With","content":"angular.js audio css node.js websocket"},{"heading":"Try it out","content":"github.com www.figma.com www.figma.com"}]}],"generated_at":"2026-02-17T18:20:10.326494Z"}}