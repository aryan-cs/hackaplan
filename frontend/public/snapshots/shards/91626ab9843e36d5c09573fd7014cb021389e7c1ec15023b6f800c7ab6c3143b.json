{"version":"v1","hackathon_url":"https://hackthenorth2025.devpost.com","generated_at":"2026-02-17T18:29:41.106293Z","result":{"hackathon":{"name":"Hack the North 2025","url":"https://hackthenorth2025.devpost.com","gallery_url":"https://hackthenorth2025.devpost.com/project-gallery","scanned_pages":11,"scanned_projects":261,"winner_count":54},"winners":[{"project_title":"DUM-E","project_url":"https://devpost.com/software/dum-e-kgx6at","tagline":"DUM-E is an NLP-powered robotic arm that uses CV and fast inference to perform actions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/738/994/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"cad","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"}],"external_links":[{"label":"github.com","url":"https://github.com/pekachoo/htn-DUM-E"}],"description_sections":[{"heading":"Inspiration","content":"Inspired by Iron Man‚Äôs DUM-E , we set out to build a robotic arm that could act as a versatile assistant on the workspace. Most robotic arms are limited to pre-programmed routines, but we wanted something intelligent, an arm that could understand natural language commands, reason about its environment, and execute complex tasks on demand. Our goal was to combine NLP, computer vision, and real-time robotics control into a single, responsive system. Initially, we thought of using VLAs. However, they are limited in their context and are also an active area of research. Thus, we designed our own pipeline which combines fast inference with reverse-IK and homography to achieve similar results that can scaled much better."},{"heading":"What it does","content":"DUM-E can interpret both spoken and typed commands to manipulate objects with precision. Some of the things it can do include:\n\nSorting parts on a cluttered table. Holding tools while the user works. Executing multi-step tasks based on natural language instructions. Waving, showing sentiment, and other actions.\n\nBy integrating NLP with computer vision, DUM-E doesn‚Äôt just react, it understands the scene and decides the best way to interact with objects. Its design is highly generalizable, meaning it can handle new tasks without requiring custom programming."},{"heading":"How we built it","content":"The arm itself has 3 standard servos and 3 microservos , controlling yaw, three levels of pitch, roll, and the claw. The pipeline works as follows:\n\nObject Detection: A webcam captures the workspace, and OpenCV draws bounding boxes using a segment model onto a birds-eye warped frame through homography. LLM Analysis: Pre-processed images are sent to a multimodal LLM (via Groq) for fast inference. The LLM analyzes object positions and decides which actions the arm should take. It can even access APIs or online data to enhance its reasoning. Command Execution: The LLM sends the instructions to a Flask server on the Raspberry Pi, which communicates with the Arduino Uno. Servo movements are calculated using inverse kinematics to ensure smooth and precise actions. 5DOF IK Arm: The Arm consists of 5 degrees of freedom (1 yaw, 3 pitch, 1 roll + a claw), where we use inverse kinematics to map our arm onto a 3D Cartesian plane for movement. Combining this with homography for accurate 2D Cartesian alignment allows us to open up more potential actions DUM-E can do. NLP stack: The LLama Maverik 109B multimodal was used through Groq so that we can pass in homography and CV detections with cords directly into the model. The Whisper Large V3 was used for speech-to-text capabilities.\n\nThe system uses a feedback loop: real-time arm positions are sent back to the LLM, ensuring consistent and accurate manipulation."},{"heading":"Challenges we ran into","content":"Building DUM-E was far from straightforward. Some of the biggest hurdles included:\n\nTranslating natural language into precise movements across multiple degrees of freedom. Ensuring robust object detection under variable lighting and cluttered environments. Integrating multiple software and hardware layers (LLM, Raspberry Pi, Arduino, CV, voice interface) while keeping latency low. Fine-tuning inverse kinematics to make movements smooth and accurate. Adding conversions and bounds to zero our servos to work with the IK effectively and intuitively. Working with the circuitry as each of the large servos could take up to 2A and small servos 1A, meaning 9A of current running in total. Managing wiring with a 5DOF arm with essentially full rotation capabilities.\n\nEach challenge required iterative testing and creative problem-solving, but overcoming them made the system much stronger."},{"heading":"Accomplishments that we're proud of","content":"Successfully combining NLP, computer vision, and robotics hardware into a seamless system. Achieving low-latency, smooth arm movements across multiple degrees of freedom. Creating a generalizable platform capable of executing diverse commands without extra programming. Implementing a feedback-based control loop that improves accuracy and reliability.\n\nIt‚Äôs incredibly rewarding to see DUM-E respond in real-time to voice commands and perform tasks with precision."},{"heading":"What we learned","content":"Through this project, we gained experience bridging advanced NLP with physical robotics. We learned about modular software design, real-time object detection, homography-based localization and vision, and inverse kinematics. Perhaps most importantly, we discovered the value of feedback loops for ensuring consistent and precise execution in multi-layered systems.\n\nWe also learned that the breadboard can surprisingly handle a lot of current relatively well, and that sometimes adding four extra power supplies isn't too bad of an idea!"},{"heading":"What's next for DUM-E","content":"Looking forward, we plan to:\n\nAdd adaptive grasping for irregular and fragile objects. Expand NLP understanding to handle multi-step, context-aware instructions. Implement autonomous task planning for DUM-E to sequence actions independently. Optimize the pipeline for faster inference and lower latency. Explore additional sensors like force or tactile feedback. Develop a portable, standalone version for broader deployment. Use a depth camera for more accurate 3D object processing.\n\nWith these improvements, DUM-E will become an even more intelligent, versatile assistant for any workspace."},{"heading":"Built With","content":"arduino c++ cad groq opencv python raspberry-pi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SpeakEasy","project_url":"https://devpost.com/software/speakeasy-vdkc9n","tagline":"AI-powered interview practice tool that helps job seekers master behavioral questions through video-based practice. It delivers real-time feedback on what you say & how you say it, so you can improve","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/742/454/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/Dante-Capobianco/speakeasy-HTN-25"}],"description_sections":[{"heading":"Built With","content":"express.js javascript node.js postgresql python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ChessMate","project_url":"https://devpost.com/software/chessmate-nwygvq","tagline":"ChessMate is a smart chess companion: play on a web app or real board with motorized pieces, get instant coaching, and voice controls for accessibility, making chess inclusive and interactive.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/741/850/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"ai","url":null},{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"nextjs","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/o-bm/ChessMate"}],"description_sections":[{"heading":"üåç Inspiration","content":"All of us come from immigrant families and wanted to relive the experiences of playing chess with family abroad. ChessMate brings the joy of real chess to people separated by distance. Where one person plays virtually, and the pieces move on the board in real-time. We also wanted to make chess more inclusive and accessible, especially for those who may struggle with mobility or vision."},{"heading":"‚ôüÔ∏è What it does","content":"ChessMate is a smart chess companion and assistant that helps you improve at chess, whether playing alone or virtually with others It blends physical and digital gameplay by moving pieces on a real chessboard through a robotic gantry system. Features accessibility options such as voice commands to move pieces and AI guidance for coaching and practice."},{"heading":"üõ†Ô∏è How we built it","content":"ChessMate is powered by a 2D, dual-axis gantry that perceives, reasons, and executes chess moves on a physical chessboard. The gantry runs on two NEMA 17 stepper motors controlled by DRV8825 drivers. The Raspberry Pi 5 serves as the brain: Uses a computer vision (CV) model with a webcam to detect piece movements. Runs a chess engine to reason and decide the next move. Executes a pathfinding algorithm to determine the best way to move pieces without collisions. Instructions are sent to an Arduino, which translates them into motor movements. Chess pieces are magnetized and moved using an electromagnet attached to the gantry. On the software side, the web app integrates AI agents, coaching features."},{"heading":"‚ö° Challenges we ran into","content":"The two biggest challenges we can into were building the 2nd axis of the gantry, and the Computer Vision system to detecting chess pieces. In addition, calibrating the pathfinding algorithm for smooth, collision-free piece movements was tricky and time-consuming. Integrating multiple hardware and software components (Pi, Arduino, motors, CV, chess engine) into a seamless system. Our two most significant issues took up to majority of the hackathon period to solve, and only worked as we worked simultaneously and collaborated"},{"heading":"üèÜ Accomplishments that we're proud of","content":"The finished product looks good and the 2d gantry works flawlessly. Successfully merged AI agents with both the physical board and web app, providing live coaching and guidance. Built a system that is both functional and user-friendly, despite the short hackathon timeframe."},{"heading":"üìö What we learned","content":"Hardware hacks require significant planning, testing, and take more time than anticipated. Importance of team collaboration and parallel problem-solving to meet deadlines."},{"heading":"üöÄ What's next for ChessMate","content":"Adding a built-in camera system so that remote players can appear via webcam, allowing the physical player to see and talk to them directly during gameplay. Exploring self-learning AI that adapts to a player‚Äôs style and provides tailored coaching."},{"heading":"Built With","content":"agents ai arduino groq nextjs python raspberry-pi react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ROSS - Remote Operated Semantic Sketching","project_url":"https://devpost.com/software/ross-42pnvi","tagline":"We're making art accessible to the blind by bridging visual ideas with touch, temperature and sound!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/373/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"blood","url":null},{"name":"cohere","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"huggingface","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sweat","url":null},{"name":"tears","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/FowlFarmer/HTN2025"}],"description_sections":[{"heading":"Inspiration","content":"Visual art is such a large component of human culture. If you visit any art gallery, you'll notice that many of the exhibits are paintings on 2D surfaces. What if you're blind and looking to experience the art as well? What if you could take a picture of a painting and feel the shapes and curves being drawn on your hands?\n\nOur inspiration for this project was based in accessibility, to make the idea of art accessible to the visually impaired by adding the elements of touch, sound, and temperature.\n\nThe inspiration for the name specifically is Bob Ross, who believed that art should be accessible and joyful for everyone, regardless of skill or background."},{"heading":"What it does","content":"ROSS takes in any sort of visual art - paintings, sketches, photography - and transforms it into complete touch and sound experiences without losing its creative information.\n\nThe image is converted into line representations tagged as either warm or cool in color. A two axis robot draws on your outstretched palms using two brushes. One draws with alcohol, the other draws with a heated resistor. These hot and cold brushes convey the color and spacial representations of the art. Narration is then applied for each component drawn, giving detailed descriptions of what is being shown, conveying the semantic information within the art."},{"heading":"How we built it","content":"On the hardware side, we utilized 2 Nema Stepper Motors (to move the rails), servo motors (to move the brushes up and down), many 3D printed parts, Arduino to control the different motors, a paint brush dipped in hand sanitizer, and a second paintbrush with a 10 ohm resistor (which we used to generate the heat for the second tip). A web camera was also utilized to provide a way for users to capture the painting image by voice.\n\nTake an input image and feed it into Meta SAM (Segment Anything model) in order to isolate meaningful elements in the painting (e.g. different objects like a tree, person, or building) For each mask, generate a simplified stroke outline. Convert detected strokes into intelligent graph structures that understand how lines connect, where they start and end, and the optimal way to traverse them, similar to how a human artist plans their drawing sequence Transform pixel-based stroke data into smooth, mathematical curves using algorithms like Ramer-Douglas-Peucker simplification. Convert digital coordinates into real-world millimeter measurements with 1mm spacing accuracy, ensuring the robotic reproduction maintains proper proportions and detail resolution on physical canvas. We categorize parts of the painting as warm or cold depending on how similar the colour is to red or blue. Colours closer to red are labeled as red, and colours closer to blue are labeled as blue. Warm colours paint using a warm paint brush warmed by a resistor, and a cold paint brush has hand sanitizer on it. This way one can distinguish between colours. (This is not working completely, so for demo we are using no colours and just one brush) Take all the numerical outputs and feed it to an Arduino which controls the hardware. While painting, we would also play some music which is relevant to the sentiment of the painting, as well as a Bob Ross voiceover (from this fine-tuned HuggingFace model: https://huggingface.co/drewThomasson/Xtts-FineTune-Bob-Ross )"},{"heading":"Challenges we ran into","content":"Many issues with making sure the hardware could move the brushes in the right direction We tried to find many free speech to voice APIs to mimic Bob Ross's voice, but we could not find a super good one."},{"heading":"Accomplishments that we're proud of","content":"Built a complete end-to-end system that connects computer vision, stroke optimization, and hardware control into one seamless demo. Translated complex research models into a functional prototype that can be used in real-world accessibility contexts. Designed and assembled a multi-motor hardware system with custom 3D-printed parts, proving out our concept under tight time constraints. Created a unique multi-sensory art experience by combining touch, temperature, and sound. Overcame numerous technical and integration challenges to deliver a working prototype by the end of the hackathon."},{"heading":"What we learned","content":"We learned that things that seemed simple can be a lot harder than they seem. We had initially thought that we could use an LLM to do the picture simpliciation, the component labeling, and basically most of the work. However, we figured out that the results were inconsistent, leading us to go with a more mathematical approach."},{"heading":"What's next for ROSS - Remote Operated Semantic Sketching","content":"Two brush method with warm and cold colours working. We had wanted to do this, but we were unable to get the functionality working in time/ We would do the wawrm and cold colours in a more professional manner, instead of using a resister for warmth and hand sanitizer for cold. A portable camera wearable to make it easier to feed images A larger scale arm setup that could possibly draw on the backs of people Custom music generation to make music unique to each painting"},{"heading":"Built With","content":"arduino blood cohere gemini huggingface python sweat tears"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Puppeteer","project_url":"https://devpost.com/software/puppeteer-7429qv","tagline":"Bring your code to life. Orchestrate your code in the runtime.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/741/678/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Warp: Best Developer Tool by Warp"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"ai","url":null},{"name":"cohere","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"martian","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"}],"external_links":[{"label":"github.com","url":"https://github.com/BrianLi23/puppeteer/tree/main"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1ho9I1U3gEizO2jnna1JPgtkiIb5TXyGPUGmWPmZIA7I/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"As a frontend engineer, if you are blocked by the backend, then your progress comes to a halt. With our tool, you can continue without interruption because it simulates traffic and data."},{"heading":"What it does","content":"Puppeteer looks deeper into the runtime by probing into variables. It can change data on the fly. For example, in a frontend application, the database can act like it exists through our puppeteering approach."},{"heading":"How we built it","content":"We started by building a terminal user interface to process user input. The LLM then decides what to probe by looking at key figures. Once probes are attached, the AI runtime is active. It can either send changed items as triggers back to the terminal UI and notify the user in natural language, or it can forward them to the Martian LLM router. The router applies rule based decisions to direct traffic to different LLMs. We also made it multimodal with both text and image generation."},{"heading":"Challenges we ran into","content":"Connecting different LLMs and shaping the environment so that the models return the results we need."},{"heading":"Accomplishments that we are proud of","content":"We built a working product that we truly care about because we have all experienced being blocked and wishing for a simulated backend or orchestrated traffic to test applications before shipping to users at scale. Another accomplishment is pushing the boundaries of the Martian LLM by having it work with image generation models, smarter text models, and lighter text models to save costs as the tool iterates and supports the user."},{"heading":"What we learned","content":"We learned how to work with cutting edge technologies like Martian LLM and Cohere."},{"heading":"What's next for Puppeteer","content":"Next, we want to make Puppeteer faster and more capable on its own."},{"heading":"Built With","content":"agents ai cohere gemini martian python react sqlite"},{"heading":"Try it out","content":"github.com docs.google.com"}]},{"project_title":"Maatchaa ","project_url":"https://devpost.com/software/maatchaa","tagline":"Match your content to the right sponsors with Maatchaa","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/756/057/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"blacksheep","url":null},{"name":"cohere","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"nano-banana","url":null},{"name":"nextjs","url":null},{"name":"pinecone","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"shopify","url":"https://devpost.com/software/built-with/shopify"},{"name":"supabase","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/StockerMC/Maatchaa"},{"label":"maatchaa.vercel.app","url":"https://maatchaa.vercel.app/"}],"description_sections":[{"heading":"Inspiration","content":"We love short-form content. Short-form content creators love sponsors. Sponsors love to advertise their products to new groups. The solution? Maatchaa."},{"heading":"What it does","content":"Maatchaa automatically matches Shopify products with trending YouTube Shorts. Businesses can browse potential creator partnerships, tinder style. Once creators confirm deals, Maatchaa adds the sponsored content for them, automatically generating neat product sets for sponsors."},{"heading":"How we built it","content":"Backend: Python, Shopify API, YouTube API, vector database for embeddings (text + image).\n\nFrontend: Next.js, scrollable ‚ÄúTinder-style‚Äù dashboard for businesses to browse reels and creators.\n\nAI & Embeddings:\n\nMulti-modal embeddings for videos (Cohere) using Pinecone\n\nGemini for automatic product selection and video analysis\n\nNano Banana (newly released) for AI-generated visuals - product sets, thumbnails, and social-ready imagery that match content style and brand identity\n\nAutomation:\n\nVideo analysis ‚Üí convert video to text + thumbnail\n\nQuery vector DB ‚Üí return top matching products\n\nOnboard business ‚Üí push Shopify products to DB ‚Üí generate product set\n\nCreator confirmation ‚Üí auto-update video description via YouTube Studio API"},{"heading":"Challenges we ran into","content":"Automation of brand deal links while respecting creator control.\n\nMatching algorithm balancing engagement, niche, and brand safety.\n\nUI design for a scrollable, aesthetic dashboard with multiple product categories."},{"heading":"Accomplishments that we're proud of","content":"Functional prototype connecting Shopify products to YouTube Shorts using AI.\n\nReal-time matching algorithm that categorizes content by viral performance and engagement.\n\nAI-generated visual product sets that look professional and compelling.\n\nDashboard that allows both businesses and creators to confirm partnerships seamlessly."},{"heading":"What we learned","content":"How to integrate multiple APIs (Shopify + YouTube + Gemini/Cohere).\n\nHow to use vector databases for semantic product matching.\n\nUsing new image-gen tech like Nano Banana."},{"heading":"What's next for Maatchaa","content":"Expand to other platforms: Instagram, TikTok, and other platforms.\n\nImprove matching AI to include stylistic and aesthetic considerations.\n\nIntegrate automated reporting and performance analytics for creators and businesses.\n\nMonetization options for businesses and creators."},{"heading":"Built With","content":"blacksheep cohere gemini nano-banana nextjs pinecone python shopify supabase"},{"heading":"Try it out","content":"github.com maatchaa.vercel.app"}]},{"project_title":"Coach Bob! ","project_url":"https://devpost.com/software/coach-bob","tagline":"Get warmed up at the gym with this interactive AR experience!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/724/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/ultratrikx/oak_camera_project"}],"description_sections":[{"heading":"Inspiration","content":"One of our teammates had to sit out of Taekwondo Nationals due to a strain injury. The cause? A rushed 5-minute warm-up that just wasn‚Äôt enough. It was a harsh reminder: skimping on warm-ups (Even if it's boring) is a common mistake, but it leads to real consequences...injury, lost training time, and missed opportunities. It's also a great way for people to get active and stay healthy!"},{"heading":"What it does","content":"We set out to turn that mundane yet critical step into something athletes actually look forward to: an interactive AR warm-up game. By combining guided drills, live feedback, and playful challenges, our system makes proper warm-ups engaging, adaptive, and fun‚Äîso athletes are ready to perform and stay safe.\n\nOnce you put on our glasses and wear our helmet, Coach Bob will shout playful insults to encourage you to get into shape and warm up! He'll also highlight with coloured nodes on the punching bag from the AR glasses whether to do a kick, knee, hook, jab, or straight to the head or body to add variance to the routine.\n\nLastly, performance data such as speed, force, and time are all relayed onto a dashboard which can be viewed after warming up. (Keep in mind that frame rate is much smoother when wearing glasses and not streaming the live feed!)"},{"heading":"How we built it","content":"Used XReal1 glasses to project targets onto Bob with open CV Used OAK 1 AF Camera to get first person footage Used Gemini and Eleven Labs to give Coach Bob a voice Used a MPU-6050 and Bob's wobbles after impact to calculate force of hits and to register them Pulled everything together :)"},{"heading":"Challenges we ran into","content":"We ran into a few big challenges during the build. Since we started later than most teams, we didn‚Äôt have specialized sensors to log hit force, so we had to get creative with how we handled impact registration using only the materials we had on hand. On top of that, the AR experience initially suffered from lag and jitter, which meant we had to spend extra time refining performance and smoothing out visuals to keep it engaging. Finally, because the setup would be actively struck and moved around, we had to reinforce and secure all the hardware to make sure it stayed intact during real use."},{"heading":"Accomplishments that we're proud of","content":"We‚Äôre proud that, despite starting later than other teams, we delivered a working AR warm-up game that athletes actually enjoyed using!"},{"heading":"What we learned","content":"NEVER GIVE UP! PLEASE WARM UP BEFORE TRAINING!\n\nIn the future we want to build a robust teaching feature which can hopefully help people sharpen their martial arts techniques! (Also a multiplayer mode / leaderboard)."},{"heading":"Built With","content":"c++ python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Sauron","project_url":"https://devpost.com/software/sauron-zvo2je","tagline":"What if you could build a time machine to find who hurt you and your loved ones? We want to reduce the amount of cold cases by making police work 10x more efficient.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/738/996/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"apis","url":null},{"name":"cohere","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"geocode","url":null},{"name":"gps","url":"https://devpost.com/software/built-with/gps"},{"name":"nextjs","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"reverse-engineering","url":null},{"name":"supabase","url":null},{"name":"wps","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/alangrewco/sauron"}],"description_sections":[{"heading":"Inspiration","content":"The clearance rate for violent crime is only about 50% in the US and Canada. Non-violent crime? Forget about it. Time and time again, ordinary citizens are told law enforcement doesn't have the resources to help them and their families. Could you imagine going through a tragedy without any closure?\n\nWe set out on a mission to solve this problem, by making police investigative work 10x more efficient."},{"heading":"What it does","content":"Sauron is a surveillance and investigation platform that functions like a temporal-spatial time machine, building a vast database of device movements over time.\n\nIts core features include:\n\nTrajectory Visualization: An interactive Mapbox interface displays the historical paths of thousands of devices, allowing an investigator to see where any device has been over any period of time. Crime Track: Intelligent path estimation using location data - crime committed at a specific location? See who was there, and where they went! AI-Powered Investigation: A natural language chatbot, powered by Cohere, allows users to ask complex questions like, \"Who was near the bank at 43.47¬∞N, -80.54¬∞W between 2:00 PM and 2:15 PM? Where did they go?\""},{"heading":"How we built it","content":"Sauron is a full-stack application built with a modern, robust technology stack.\n\nThe tech stack\n\nBackend : Python, Flask, PostgreSQL with PostGIS, Psycopg2 Frontend : Next.js 15 (with Turbopack), React 19, TypeScript, Mapbox GL JS, Tailwind CSS, shadcn/ui AI & Data : Cohere for the AI chatbot Infrastructure & DevOps : Supabase for the managed PostgreSQL database"},{"heading":"Challenges we ran into","content":"Making AI Actionable: Simply having a chatbot isn't enough. We needed it to interact with our specific dataset. The challenge was implementing Cohere's Tool Use feature effectively. This involved writing a precise tool definition, creating a Python function that could be called by the model, and ensuring the data passed between the LLM and our database was correctly formatted. Frontend Complexity: Building a highly interactive, map-centric application is non-trivial. We had to manage complex state between the map viewport, the search components, and the data filtering panels, ensuring a smooth and responsive user experience in a resizable layout (resizable-layout.tsx)."},{"heading":"Accomplishments that we're proud of","content":"Executing Real-World Device Tracking: Our proudest accomplishment is the successful execution of the live tracking phase. We built a system that could actively monitor and record the real-world movements of hundreds of devices simultaneously, generating a unique and powerful dataset for investigation. Creating a True AI Analyst: We successfully integrated a Cohere-powered LLM that goes beyond simple chat. By using Tool Use, our chatbot acts as an intelligent data analyst, allowing non-technical users to perform complex spatio-temporal queries using plain English."},{"heading":"What we learned","content":"This project was a deep dive into the practical challenges of large-scale data engineering. Generating hundreds of thousands of location pings was just the first step; managing, querying, and visualizing that data proved to be a significant undertaking. We learned that with a massive spatio-temporal dataset, naive queries are unacceptably slow, making database performance paramount. This underscored the importance of proper database indexing, specifically with PostGIS's GIST indexes, and writing efficient SQL to query by both location and time simultaneously. We also encountered significant bottlenecks in transferring and rendering the data on the frontend. Sending hundreds of thousands of points to a web browser is not feasible, which taught us to design our backend API to be intelligent, performing the heavy lifting of filtering and aggregation on the server side. This ensures that the frontend only has to render data relevant to the user's view, preventing the map from freezing. Finally, we learned about the complexity of maintaining data integrity at scale. Ingesting data at high frequency requires atomic database transactions to prevent partial writes and robust error handling to deal with API failures without corrupting the dataset. Managing this volume of data taught us to think defensively about every step of the data pipeline."},{"heading":"What's next for Sauron","content":"Moving forward, our primary focus is on enhancing the platform's accuracy and reliability. To increase the richness of our data, our next step is to integrate additional data sources, exploring implementations similar to Geospy to fuse location data from multiple providers and create a more comprehensive picture of device movements. We also plan to engage directly with potential customers in law enforcement and private investigative sectors to ensure our development aligns with their real-world needs and workflows. To make the platform more bulletproof against future changes to the undocumented APIs we rely on, we will build a dedicated iOS emulator. This controlled environment will allow us to safely analyze the API's behavior in-depth, ensuring our data ingestion process remains stable and reliable for the long term."},{"heading":"Built With","content":"apis cohere flask geocode gps nextjs react reverse-engineering supabase wps"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Tango","project_url":"https://devpost.com/software/tango-q37d4z","tagline":"Siri if it actually worked on your Mac.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/738/975/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Graphite: Engineering Dream Team"}],"team_members":[],"built_with":[{"name":"agent","url":null},{"name":"ai","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"voice","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/demanr/tango"}],"description_sections":[{"heading":"Inspiration","content":"How many times have you copy pasted something just to put it through an LLM and regurgitate it's answer back where you were? Tango started out as a bridge for clipboard copy and pasting, but grew into so much more.\n\nWe want a world where you get all the benefits of LLMs without interrupting your workflow. From searching the web to rephrasing and reformatting, or meme generation. Just speak your request and Tango's got your back."},{"heading":"What it does","content":"Tango is a lively agent that monitors your clipboard and acts on voice commands to increase your productivity. It's like Siri if it actually worked.\n\nYour friend sent you a picture you know would make the perfect meme? Take a screenshot and get Tango to add the caption for you. Perfectly timed humour, every time.\n\nWant a bit of guidance for the new tech project you're starting? Wondering \"What is the Pandas equivalent in Rust,\" or \"how can I implement a live camera in Python?\". No need to pause and prompt. Tango will keep you coding.\n\nWant a concrete list of whats on in Goose Games? CMD-A (or Ctrl for those linux fiends out there) and get Tango to make it a CSV. No hassle.\n\nTrying to avoid using \"engineered\", \"improved\", or the dreaded \"spearheaded\" a fifth time on your resume? Simply copy any word, ask Tango for a synonym, and paste. No more needless and distracting tab switching, thesaurus searching, or chatGPT prompting.\n\nTango is here to enhance how you use your computer. Remove redundant switching, and keep yourself in focus. As the saying goes, it takes two to Tango."},{"heading":"How we built it","content":"We used Python to build a MacOS-native application with deep integration into the operation system's clipboard and notifications API. We run low-energy Voice Activity Detection (VAD) and hotword/wakeword detection to trigger Tango on user request and route requests through fast LLM providers to manage your clipboard.\n\nWe used Graphite to collaborate on this project and work asynchronously."},{"heading":"Challenges we ran into","content":"Voice Activity Detection was hard but we found WebRTC has solid open-source implementations Managing complexity with speed - slow responses are never an option Steering the LLM towards desired output"},{"heading":"Accomplishments that we're proud of","content":"The breadth of tasks that Tango can do How much it feels like what we always wished Siri would be It's fast The tweaking for detecting the wake word and sentence end to ensure speed The fact we want to use it ourselves"},{"heading":"What we learned","content":"Guardrails add exponential latency LLM agents can be really fun when done right"},{"heading":"What's next for Tango","content":"Faster memory, faster searching, and MCP support"},{"heading":"Built With","content":"agent ai gemini groq openai python voice"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Stu3dio","project_url":"https://devpost.com/software/vibe-director","tagline":"Make a film by talking to 3D workers in the studio","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/741/160/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"fastify","url":null},{"name":"nextjs","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"supabase","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Jeff15321/Stu3dio-HackTheNorth2025"}],"description_sections":[{"heading":"What it does","content":"Make an AI movie by talking to others in the 3D studio.\n\nFirst, chat with an Director to brainstorm ideas, build characters, and outline scenes.\n\nSecondly, collaborate with an Scriptwriter to flesh out dialogue, actions, and narrative details.\n\nNext, meet the Character Artist, where you can tweak style, clothes (ANYTHING YOU WANT) by simply scribbling on the character‚Äôs image for instant AI edits.\n\nLastly, view a timeline of all the scenes and their frames, you could edit the scene images directly using scribbling tools and getting instant responses from our image gen AI.\n\nFinally ... generate the video ...\n\nVoiLa! Your very own AI movie is built!"},{"heading":"How we built it","content":"Projects & Nodes ‚Äì Each film is a project with a tree of nodes (logline, characters, objects, scenes, final film). Editing a parent marks descendants as stale so they regenerate with fresh context. Only required context is passed when generating each node.\n\nGeneration Jobs ‚Äì Every AI action (LLM plot, character sheet, image, video, final stitch) is a generation, queued as a job in Redis. Workers pick jobs, call providers (LLM, image, video), and store results and artifacts.\n\nArtifacts ‚Äì JSON descriptors, images, frame stills, scene clips, and the stitched film are all stored as downloadable artifacts in blob storage."},{"heading":"Built With","content":"fastify nextjs react supabase"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TradeOff","project_url":"https://devpost.com/software/tradeoff-c24mwv","tagline":"Stock market simulation game where students race to beat the market.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/496/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Amazon: AWS Campus Champions - Best Use of DynamoDB Streams"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"cerebras","url":null},{"name":"dynamodb","url":null},{"name":"english","url":null},{"name":"graphite","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/redmac135/htn-2025"}],"description_sections":[{"heading":"üí≠ Inspiration","content":"Financial literacy remains a critical gap for many people, especially young adults entering the investment world. Traditional learning methods often feel abstract and disconnected from real market dynamics.\n\nWe were inspired to create an engaging, risk-free environment where users could develop genuine investment skills through hands-on experience. The idea came from recognizing that the best way to learn investing isn‚Äôt through textbooks, but through practice, without the fear of losing real money.\n\nWe wanted to bridge this gap by combining gamification with real-world market simulation, making financial education both accessible and exciting while showcasing the powerful tools and opportunities available through RBC‚Äôs investment platform."},{"heading":"üéì What it Does","content":"TradeOff is an interactive stock market simulation game specifically designed to empower students to achieve their life goals through smart investing.\n\nPlayers begin by setting personalized financial milestones, whether it‚Äôs saving for their first car, funding that dream graduation trip, or building an emergency fund and the game creates customized investment scenarios to help them reach these targets.\n\nThe platform features a realistic trading environment where students can practice going long or short on various holdings, guided by a dynamic news feed that influences stock prices in real-time. This mimics actual market conditions and teaches players how external events impact investment decisions, building crucial financial literacy skills without any real-world risk.\n\nWhat sets TradeOff apart is its seamless integration with RBC InvestEase‚Äôs philosophy of making investing accessible and confidence-building. As students play and learn, they discover how early and consistent investing can accelerate their path to achieving real-life goals. The game demonstrates InvestEase‚Äôs user-friendly approach to portfolio management, automated investing features, and goal-based investment strategies."},{"heading":"‚öôÔ∏è How We Built It","content":"We built our user interface using React.js with Tailwind CSS, creating an intuitive and responsive design with candles and the news functionality as well.\n\nRBC API Integration: We integrated RBC‚Äôs Portfolio Simulation & Market Sandbox API to provide authentic portfolio return calculations and realistic market behaviour.\n\nOur backend combines Python and JavaScript to handle complex financial calculations, including sophisticated order book management and interpolation logic for accurate portfolio returns. This dual-language approach allowed us to leverage Python‚Äôs financial computation strengths while maintaining JavaScript‚Äôs web integration capabilities.\n\nDatabase: We implemented Amazon DynamoDB as our primary database solution, efficiently managing our order book data, game state persistence, and user portfolio tracking. The NoSQL structure perfectly accommodates the dynamic nature of trading data and user interactions. Real-time updates: We utilized DynamoDB Streams to enable instantaneous stock market data updates, ensuring that price movements and market events are reflected immediately across all user sessions, creating an authentic trading experience. Hardware Deployment: We deployed our application on QNX hardware, showcasing the platform‚Äôs capabilities in running modern web applications and demonstrating the versatility of embedded systems in financial applications. Development Workflow: We accelerated our development process using Graphite‚Äôs AI-powered code review platform, which helped us maintain code quality while enabling rapid iteration and feature deployment through streamlined pull request management. News Feed: As for the news feed, we used Cerebras Inference API calls to generate fake stock market news that seemed authentic to add authenticity and depth for the project. Having low latency was crucial for this point."},{"heading":"üöß Challenges We Ran Into","content":"Implementing the core order book logic proved more intricate than anticipated. We had to balance realism with educational value, ensuring the simulation was complex enough to teach real concepts while remaining accessible to beginners.\n\nMarket Simulation Authenticity: Achieving realistic stock market behaviour through randomization was particularly challenging. We needed to create price movements that reflected genuine market dynamics, incorporating volatility clustering, momentum effects, and news-driven price actions, while maintaining educational clarity.\n\nThe interpolation logic for portfolio returns required careful calibration to ensure mathematical accuracy."},{"heading":"üí™Accomplishments that We're Proud Of","content":"Built a full-stack financial simulation in under 36 hours. all while balancing workshops, interviews, activities, and attending every food event. Integrated RBC‚Äôs Portfolio Simulation & Market Sandbox API, delivering realistic portfolio returns and market behaviour. Created dynamic, AI-powered news feeds with Cerebras API, simulating authentic market events that influence gameplay. Implemented real-time market updates with DynamoDB Streams, ensuring all players see instant market movements. Deployed on QNX hardware, demonstrating the adaptability of our app across embedded systems. Collaborated and team bonded as four Western students, iterating quickly, solving challenges, building and losing sleep together."},{"heading":"üìñWhat We Learned","content":"We deepened our understanding of Amazon Web Services, particularly DynamoDB‚Äôs capabilities and best practices.\n\nWe also learned to leverage DynamoDB Streams for real-time applications opened our eyes to the power of event-driven architectures in modern web development.\n\nAPI Integration Expertise: Working with RBC‚Äôs financial APIs taught us valuable lessons about handling external data sources, managing authentication, and building robust error handling. These skills are directly applicable to professional software development. Collaborative Development: Using Graphite‚Äôs platform enhanced our understanding of modern development workflows, code review processes, and the importance of maintaining clean, reviewable code even under tight deadlines. We learned how AI-powered tools can accelerate development without sacrificing quality."},{"heading":"üîÆ What's Next for TradeOff","content":"We envision TradeOff evolving into a comprehensive financial education ecosystem with exciting new features and capabilities:\n\nEnhanced Asset Universe: Expanding beyond traditional stocks to include bonds, ETFs, mutual funds, cryptocurrencies, commodities, and sector-specific investments. This would give students exposure to diverse asset classes and modern portfolio theory in action. Advanced Trading Features: Implementing options trading, margin accounts, dividend reinvestment programs, and international markets to provide comprehensive investment education covering both basic and sophisticated strategies. Industry-Specific Scenarios: Creating specialized simulation modules for different sectors (technology, healthcare, energy, ESG investing) with industry-specific news feeds and market dynamics, helping students understand sector rotation and thematic investing.- Multiplayer Tournament System: Introducing multiplayer modes where students can challenge each other, join investment clubs, and compete in friendly tournaments and long-term portfolio challenges with real scholarship incentives."},{"heading":"Built With","content":"api cerebras dynamodb english graphite javascript python react tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Fragments","project_url":"https://devpost.com/software/fragments-gmleoj","tagline":"Clip, save, and organize the exact moments that spark your ideas.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/390/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Amazon: AWS Campus Champions - Best Use of DynamoDB Streams"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Cohere: Best Use of Cohere API"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"cohere","url":null},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"figma","url":null},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"nextjs","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/charity-g/fragment-HTN2025"}],"description_sections":[{"heading":"Inspiration","content":"Great work is connected.\n\nAnything you ever make is produced by drawing on everything you've ever consumed.\n\nBeing a better creator, whether in video production, frontend development, or freestyle dance, begins with being a better consumer.\n\nSo how can we be better consumers?\n\nBy leaving precise fragments of the best work we find across the internet for our future selves -- all in one place.\n\nTruth is, in most creative domains, a video is worth a million words.\n\nThat's why we built Fragments, your second brain for video that helps you clip, save, and organize the exact moments that spark your ideas."},{"heading":"Core Features","content":"1. Cross-Platform Clipping System\n\nChrome Extension Integration : Hover-to-record button appears on all video elements across the web (instagram, tiktok, youtube, netflix etc.)\n\nSmart Recording UI : Dark overlay with timer, stop/delete controls, and a metadata form that enables you to leave specific tags and notes for every clip (fragment).\n\n2. AI-Powered Video Analysis & Tagging\n\nFrame Extraction : Frames are extracted every 2 seconds for fragments using OpenCV\n\nCohere Vision AI Integration : Based on extracted frames, Cohere's vision AI model generates video summaries and intelligent tags\n\n3. Multi-Format Video Processing Pipeline\n\nFormat Optimization : The original WebM video files are converted using FFmpeg to GIFs & MP4s, stored in S3, and then stored with the video metadata in DynamoDB.\n\n4. Interactive Video Gallery\n\nGIF Preview Gallery : Quick browsing of all fragments with various filters\n\nMetadata Display : Upon drill-down, tags, notes, source URLs, and AI-generated descriptions are shown next to each fragment\n\nPrivacy Controls : Each fragment has a public/private visibility setting\n\n5. Real-Time Second Brain Search\n\nOpenSearch Integration : All data in DynamoDB is indexed to enable global fuzzy search across titles, descriptions, tags, notes, and AI-generated content to enable quick finding of any fragment\n\nReal-Time Sync : DynamoDB streams automatically update search index\n\n6. Content Discovery\n\nCommunity Curation : Find popular collections and fragments from the community\n\nSocial Network : Follow creators you like to see their latest fragments in a feed"},{"heading":"How we built it","content":"Tech Stack\n\nFrontend : Next.js 15, React 19, TypeScript, Tailwind CSS, Chrome Extension API, MediaRecorder API, MutationObserver API\n\nBackend : Python, FastAPI, DynamoDB, Boto3 (AWS SDK), OpenCV, FFmpeg\n\nInfrastructure : AWS (S3, Lambda, DynamoDB, OpenSearch, CloudWatch, MediaConvert), Vercel, Auth0\n\nAI : Cohere AI (c4ai-aya-vision-32b), Computer Vision (OpenCV)"},{"heading":"Challenges we ran into","content":"Screen recording videos on different sites Debugging auth0 Setting up AWS permissions (across team and aws services) Latency and load times Using Cohere to analyze clips for instructions"},{"heading":"Accomplishments that we're proud of","content":"Finishing the MVP and having a working demo Chrome extension has a smooth UI and actually works across platforms! Landing page visual hook OpenSearch indexing + DynamoDB Streams integration AWS Infrastructure stack"},{"heading":"What we learned","content":"Technical Learnings\n\nBy default, all aws services have no permissions. Service-to-service delegation requires even more permissions! Lambda default timeout is 3s, which is way too short for most cases OpenSearch automatically creates inverse indices for each field in a document (and thus DynamoDB table if indexing on one), which helps make search really quick!\n\nPersonal Learnings\n\nA great idea requires going deep in a field. This is best done when at least one person on the team already has a lot of context in that field!"},{"heading":"What's next for Fragments","content":"Most importantly, we'll be working on this in our free time to get a beta version out to early users!\n\nHere are a few features we'd like to add:\n\nadd quick video editing tools to crop and trim clips before saving introduce an algorithmic feed of recommendations with two sections: a \"for you\" page, and a following page add collaborative Collections, letting friends and teams build shared libraries of inspiration together convert WebM files to WebP on top of GIF + add lazy loading for quicker load times"},{"heading":"Built With","content":"amazon-web-services cohere css figma html javascript nextjs opencv python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"S-KBD67","project_url":"https://devpost.com/software/s-kbd67","tagline":"Why click a mouse when you can pull a trigger? We hacked a Nerf gun into an FPS controller; shooting, reloading, moving, aiming translates to game input, letting you play your favorite shooters IRL.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/523/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"esp32","url":null},{"name":"esp8266","url":null},{"name":"hw504","url":null},{"name":"mpu6050","url":null},{"name":"nerfgun","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/Amelia1110/Valorant-Nerf-Gun"}],"description_sections":[{"heading":"Inspiration","content":"\"What if you could play an FPS game IRL\" was the question that inspired us. Our goal was to enhance the mechanics of an FPS (first person shooter) game into real life using a Nerf blaster. The gameplay turns into something significantly more realistic and immersive."},{"heading":"What it does","content":"The system lets players control an FPS game using only a Nerf gun (and a bunch of other cool things). Inputs and motions are translated into the game: aiming, turning, pressing our feature buttons and joystick to move, reload and switch weapons. Instead of using a mouse, you can just shoot a (toy) gun, creating a smooth and satisfying connection between the physical and digital experience playing,"},{"heading":"How we built it","content":"We divided into hardware and backend teams.\n\nHardware: Outfitted Nerf blasters with ESP8266 microcontrollers, gyroscopes, joysticks, and buttons to capture movement and input. Backend: Developed a Python application that receives sensor data over Wi-Fi using fast UDP packets, decodes it, and maps it to mouse/keyboard events. This pipeline allowed quick prototyping and tuning for low latency."},{"heading":"Challenges we ran into","content":"Managing packet timing and size so the controls felt responsive Initial hardware limitations with the ESP8266, requiring a switch to ESP32 in some cases Ensuring stability and durability of the hardware during rapid movements Calibrating sensors for smooth and natural input Working around games with anticheat"},{"heading":"Accomplishments that we're proud of","content":"We‚Äôre proud of successfully integrating microcontrollers with PC software, achieving low-latency wireless transmission of high-frequency sensor data, and translating raw motion and analog readings into intuitive, responsive game controls."},{"heading":"What we learned","content":"Through this project, we learned how to integrate hardware and software, optimize UDP communication for low latency, and calibrate sensors to create smooth, natural gameplay."},{"heading":"What's next for S-KBD67","content":"Next, we plan to expand hardware support for more blaster types, refine motion tracking for greater accuracy, and explore multiplayer and VR/AR integration to push the immersion even further."},{"heading":"Built With","content":"c++ esp32 esp8266 hw504 mpu6050 nerfgun python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Sematic","project_url":"https://devpost.com/software/diaframe","tagline":"AI Powered Voice-Based Systems Design Tool","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/779/155/datas/medium.gif","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Amazon: AWS Campus Champions - Best Use of DynamoDB Streams"}],"team_members":[],"built_with":[{"name":"amazon-dynamodb","url":"https://devpost.com/software/built-with/amazon-dynamodb"},{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"amplify","url":"https://devpost.com/software/built-with/amplify"},{"name":"auth0","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"reactflow","url":null},{"name":"windsurf","url":null}],"external_links":[{"label":"sematic.ca","url":"https://sematic.ca"},{"label":"github.com","url":"https://github.com/koralkulacoglu/sematic"}],"description_sections":[{"heading":"Inspiration ‚ú®","content":"Manually drawing diagrams during a brainstorm or technical discussion is a flow-killer. The moment someone has to stop talking to click, drag, and type, the creative momentum is lost. We wanted to build a tool that could visualize ideas as naturally as speaking, turning conversations directly into collaborative designs."},{"heading":"What it does üó£Ô∏è‚û°Ô∏èüìä","content":"Sematic is a spatially-intelligent, collaborative whiteboard that transforms voice commands, text prompts, and even image uploads into perfectly structured diagrams in real-time. You can simply talk to it‚Äîsaying \"add a user database connected to the auth service\" or \"delete all the nodes on the right\"‚Äîand Sematic executes the command.\n\nIt doesn't just blindly add elements; it understands the entire diagram's state, including the 2D coordinates and connections of existing nodes. This spatial reasoning allows it to make intelligent layout decisions, ensuring the generated diagram is clean, organized, and contextually aware."},{"heading":"How we built it üõ†Ô∏è","content":"We built Sematic on AWS Amplify Gen 2, with AWS Lambda functions orchestrating calls to Google's Gemini 2.0 Flash API.\n\nHere's the pipeline:\n\nMultimodal Input: The React frontend captures voice commands as WebM audio, which is uploaded directly to Gemini's File Manager.\n\nContext is Key: We send Gemini more than just the audio; we provide a complete snapshot of the current diagram, including the precise coordinates, sizes, and connections of every element.\n\nAI Processing: Gemini uses its advanced multimodal capabilities to transcribe the audio and process it alongside the spatial data of the diagram.\n\nStructured Commands: The API returns a structured JSON payload containing exact commands for the frontend (e.g., createNode, connectNodes, deleteById) with precise positioning data.\n\nReal-time Execution: Our React app parses this JSON and executes the commands, updating the diagram for all collaborators instantly."},{"heading":"Challenges we ran into üßó","content":"Making the AI's spatial reasoning robust was a major challenge. We had to design a custom, simplified representation of the diagram's geometry that Gemini could process effectively to make intelligent layout decisions. Additionally, ensuring the voice-to-command pipeline was fast enough for a real-time collaborative experience required significant optimization of our Lambda functions and data flow."},{"heading":"Accomplishments that we're proud of üèÜ","content":"Implementing a true, end-to-end multimodal pipeline using Gemini 2.0 Flash to process voice, text, and spatial geometry simultaneously.\n\nAchieving genuine spatial awareness in an AI agent, allowing it to modify diagrams contextually rather than just adding elements.\n\nBuilding a resilient serverless backend on AWS Amplify Gen 2 that handles real-time data streaming and API calls seamlessly.\n\nGetting Gemini to reliably generate structured JSON commands that our React frontend could execute flawlessly."},{"heading":"What we learned üß†","content":"We gained deep insights into the power of multimodal AI, especially how to leverage spatial data as a critical form of context for the model. We learned to design complex, event-driven systems on a modern serverless stack and, most importantly, how to build an AI tool that feels like an intuitive collaborator rather than just a static piece of software."},{"heading":"What's next for Sematic üöÄ","content":"Vector DB Integration: We plan to use a vector database to provide the AI with context from an entire organization's past projects, enabling it to suggest relevant structures and components automatically.\n\nPlatform Integrations: Native apps for platforms like Zoom and Google Meet to capture conversations where they happen.\n\nEnhanced Audio Capture: Support for capturing both system audio and microphone input for a more complete conversational context."},{"heading":"Built With","content":"amazon-dynamodb amazon-web-services amplify auth0 gemini reactflow windsurf"},{"heading":"Try it out","content":"sematic.ca github.com"}]},{"project_title":"Lattice","project_url":"https://devpost.com/software/lattice-flck7q","tagline":"Holographic Communication.Anytime, Anywhere.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/738/988/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Hack the North 2025: Finalists"}],"team_members":[],"built_with":[{"name":"c#","url":"https://devpost.com/software/built-with/c--2"},{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"kinect-windows-sdk","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"}],"external_links":[{"label":"github.com","url":"https://github.com/KenC2006/Lattice"}],"description_sections":[{"heading":"üí≠Inspirationüí≠","content":"Every day, technology around us improves. We‚Äôre now in an era where we can move away from flat 2D screens and create immersive 3D experiences. VR/AR already exist, but they mostly rely on artificial creations (environment for VR, objects for AR). The next logical step in this direction is to bring the real world into the digital space.\n\nImagine being able to scan anything, like people and objects, in real time and share it live. Technology has as many use cases as you can think of - remote collaboration, medical training, live entertainment, etc. We wanted to make this possible, and with as little expensive tech as possible."},{"heading":"ü§îWhat it doesü§î","content":"Lattice is a holographic video capturing system for live 3D reconstruction.\n\nIt uses multiple (three is best ) Xbox Kinect v2 for real time capture. Data from all sensors is merged into a single, 3D point cloud, creating a full reconstruction, at more than 20 FPS.\n\nConnected with the Microsoft Hololens, anybody can view this reconstruction. Essentially, Lattice enables holographic communication, similar to the holoprojector in star wars."},{"heading":"üî®How we built itüî®","content":"Hardware: three Microsoft Kinect v2 Sensors, three laptops (one per kinect as per the SDK) Architecture: Client-Server design - each computer connected to the kinect runs the client app, and they are all connected a central server that synchronizes the data, and merges them to produce the output hologram. Languages: C++, C# Libraries/stack technologies: Kinect for Windows v2 SDK, OpenCV,"},{"heading":"üí™Challenges we ran intoüí™","content":"Hardware limitations: The Kinect v2 SDK only supports connecting one Kinect per PC, meaning we had to use multiple laptops just to run the app, even for testing. Sensor interference: The Kinects use LiDAR for distance sensing, and if positioned weirdly, th elasers can interfere with each other, making the detection inaccurate Discontinuation of the Hololens made it hard to find good documentation for using it. It's also incompatible with most new technology since. For example, it only supports Wi-fi 4, which was (and is) a huge bottleneck for the performance of this project, and required us to use a phone as a router. This required us to also connect the computers to the same slower network."},{"heading":"üèÜAccomplishments that we are proud ofüèÜ","content":"The most important thing to us is that we created a functional prototype that actually closely aligned with our initial vision. Some notable accomplishments are that we:\n\nMade a working project using the Microsoft Hololens, a product they discontinued, with limited developer documentation Achieved record FPS of 23 while running a real-time 3D reconstruction hologram Designed a working noise filter for the kinect sensors"},{"heading":"üéìWhat we learnedüéì","content":"How to calibrate multiple sensors - we did lots of research for this, ended up using a visual market and used ICP (Iterative Closest Point algorithm) for refinement. Everything there is to learn about 3D graphics - calibration, error correction, point cloud rendering, etc. Big Picture: Sometimes, it doesn't take brand-new huge-scale inventions to bring us to the next step in tech. With just a few days of focused thinking and coding, we combined existing technology, some of it even old, to bring the future to the present."},{"heading":"üîúWhat's next for Latticeüîú","content":"This project has so much potential and we've only covered a small portion of it; think of this project as a proof of concept rather than a final product. We will:\n\nImplement auto-calibration without a marker sheet Improve the noise-filtering and output a mesh instead of a point cloud to improve visualization resolution Integrate the use of newer sensors in lieu of the outdated Xbox Kinect sensors Develop the application for use with AR headsets that are not discontinued (lol), like the Magic Leap, or Rayban Metas."},{"heading":"Built With","content":"c# c++ kinect-windows-sdk opencv"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"RepoStory","project_url":"https://devpost.com/software/placeholder-it3zq7","tagline":"Learn about the process of how any GitHub project came to be! Get a timeline of the project construction process, learn about the technologies used, and gain insights to the development process.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/742/484/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Cerebras: Best Use of Cerebras"}],"team_members":[],"built_with":[{"name":"cerebras","url":null},{"name":"fastapi","url":null},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"nextjs","url":null},{"name":"openai-agent","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/SubwayMan/HTN2025"}],"description_sections":[{"heading":"Inspiration","content":"Have you ever seen a cool project someone's made, and you really want to know how it works and how it was built, but you can't get over the hump of going through a billion files and folders? Learning from others is one of the best ways to improve and learn new things, but we believe that this sort of learning is growing more and more inaccessible; with so many new technologies and tools coming out all the time, and projects often being unreadable, we believe that there is a disproportionate lack of educational tools compared to how well new technologies such as AI agents are suited to the task."},{"heading":"What it does","content":"Our project takes in any public Github repository and separates the history into \"milestones\", creating a readable project timeline that allows you to put yourself in the developer's shoes and understand the iterative design process. Using AI agents, we intelligently process only the necessary context from git commits and objects in order to lazily retrieve expensive file diffs. The whole stack is heavily optimized for performance in order to create a seamless user experience."},{"heading":"How we built it","content":"We interface with Git using python's subprocess module. We use FastAPI to serve our backend and our frontend is a simple react/nextJS layer. For our AI agent, we use openai-agent with models powered by Cerebras for blazing-fast performance. Cerebras was especially important; it was significantly faster than every other model, which let us provide a seamless timeline streaming interface. To provide a comprehensive full-project summary, we used Cohere.\n\nIn our gallery, you can see RepoStory telling you exactly how we built it!"},{"heading":"Challenges we ran into","content":"The project can be thought of as having 4 main steps:\n\nFetching: this is where Github objects are programmatically queried and loaded in memory/storage. Milestone detection: this is a challenging tasks that asks us what the best way to define a \"Milestone\" is; Milestones are defined by start and end commits, and have to encode data that will allow an AI agent to process the milestone, creating a key data availability problem. Processing: in this step, we take raw milestones and use the power of NLP and AI agents to create data for our front facing application. The AI agent is trained to lazily collect data about the commits in the milestone in steps of increasing complexity, then finally use that data to choose files to examine (e.g. it's more likely to query the diff for mymodule.py, than node_modules, for example). Frontend: Create a smooth UX for the user of the application. Super simple: just paste in a project, and watch the timeline as it smoothly generates. Required streaming data about the agent's tool uses to keep the user in the loop.\n\nLet's talk about the main problems:\n\nPerformance\n\nPerformance is our #1 priority, from the top to bottom of our entire stack. When we initialize a git repository, we don't fetch any blobs; we only fetch git tree objects. Many git operations rely on finding diffs between blobs; when we do these, we have to make sure those operations are executed as little as possible.\n\nHere's one large architecture problem that we're really proud of: For our system, we want to select commit hashes that represent the edges of \"milestones\". The naive strategy is to select based on groups of N commits. However, this is not a good idea as the contents of commits are very varied and there are other factors such as merge commits. People also tend to commit at different frequencies.\n\nFor our initial naive approach, we simply selected every single merge commit, as these merges indicate the end of some unit of work done. However, this is also very inconsistent; people who use a rebase strategy do not produce merge commits, and this would simply not work at all on completely linear histories.\n\nFor our final approach, we devised a super cool solution. We created heuristics that could roughly measure \"work done\" between two commits, and output it as a scalar; then starting from the first commit, we want to find the next commit that exceeds the threshold value. Assuming that our measurement of \"work done\" is increasing, we can binary search for this commit!\n\nGit provides a \"bisect\" utility that is usually used for finding the last \"good\" commit before some bug was introduced. You mark a bad commit and a good commit, and the bisect tool will iterate through some commits, asking you to mark them as bad/good, effectively binary searching for the last bad commit. You can provide this tool with a script that can check whether the commit is bad or good automatically. Thus, we can use this to perform our procedure.\n\nGit bisect docs\n\nWhen starting from commit A, we can mark A as a \"bad\" commit, and the latest commit as a \"good\" commit. We also have this script:\n\n../../scoregen.py <ref1> <ref2> --limit <float>\n\nscoregen.py will exit with code 1 if the \"work\" between the two references is greater than the limit. Thus, we can pass it in to git bisect, and it will eventually land on our desired commit!\n\nOur project was full of super interesting problems with really interesting solutions."},{"heading":"Accomplishments that we're proud of","content":"Working git subprocess interface; the amount of work we put into ensuring we had a nice, modular git interaction system, as well as hyper-optimizing the performance Working agentic AI subsystem able to dynamically load file diffs to acquire its own context Working milestone selection framework and strategy ## What we learned Git internals! Git is such a complex and amazing piece of software under the hood, but most people only interact with it on a surface level, so it's really cool to learn more about how it works. Sleep is overrated ## What's next for RepoStory We wanted to implement a diff viewer for the files, along with the option to explain specific lines. These would make great additions to our UX."},{"heading":"Built With","content":"cerebras fastapi git nextjs openai-agent python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Network Threat Explorer ","project_url":"https://devpost.com/software/network-threat-explorer","tagline":"Insightful Network Traffic Visualization and Analysis","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/739/128/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"CSE: Network Traffic Exploration"}],"team_members":[],"built_with":[{"name":"arkime","url":null},{"name":"cohere","url":null},{"name":"duckdb","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"malcolm","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/JeremyFriesenGitHub/nte"}],"description_sections":[{"heading":"Inspiration","content":"As students with no prior experience in data analysis or cybersecurity, we were eager to broaden our skill set and deepen our understanding of these fields. We saw an opportunity with this challenge not only to learn but also to support other aspiring professionals like ourselves in developing these skills for the industry."},{"heading":"What it does","content":"Our project is made to help analyze threats and anomalies within network traffic. We used the legacy MACCDC dataset as the premise to build our tools from."},{"heading":"How we built it","content":"We used flask, duckdb and python to create a simple app to help analyze the network traffic data from the dataset. We also setup a Malcolm suite (Dashboards, Arkime, CyberChef, Netbox) locally for manual visualization and manipulation on the dataset."},{"heading":"Challenges we ran into","content":"Ram space (16GB is not enough) File size of dataset Analyzing and visualizing the network traffic at scale Sleep + time management"},{"heading":"Accomplishments that we're proud of","content":"We're proud of integrating an AI API (cohere) to help assist with visualizing and analyzing network traffic, as well as integrating Malcolm into the project as well."},{"heading":"What we learned","content":"We learned network traffic analysis techniques, docker and docker compose, back-end processes and how to work with/manipulate large enterprise scale network traffic data."},{"heading":"What's next for the project","content":"The next thing for this project is to generate/automate reports for network traffic anomalies, alerts and CVE's, to help improve productivity for SOC analysts. Additionally, also updating the Suricata rules is a must do on the list as well."},{"heading":"Built With","content":"arkime cohere duckdb flask malcolm python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Solshare","project_url":"https://devpost.com/software/solshare-cmxous","tagline":"Splitting bills with friends is messy‚Äîreceipts get lost, people forget to pay, and tracking balances is a headache. SolShare fixes this: scan, assign, and settle instantly on Solana.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/743/427/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Cohere: Best Use of Cohere API"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Solana: Best Consumer Payment Experience with Solana Pay"}],"team_members":[],"built_with":[{"name":"cohere","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"gcp","url":null},{"name":"solana","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"swiftui","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/orgs/HTN-2025/repositories"}],"description_sections":[{"heading":"Inspiration","content":"At the University of Waterloo, ‚ÄúCostco‚Äù has become our second home. Since we‚Äôre broke college students, we only had one membership, so everyone piled into a single trip to stock up. The problem came later. Splitting a giant receipt fairly was always a headache.\n\nAt the same time, we‚Äôve been promised seamless integration between third-party apps and banks for years, but that vision still hasn‚Äôt arrived. That pushed us to explore blockchain as a way to enable trustless, instant payments.\n\nSolShare grew out of both frustrations: the real student struggle of splitting receipts and the broader gap in payment technology. By combining receipt scanning, a smooth UI/UX, and blockchain settlement, we created a focused solution that makes cost-sharing transparent, secure, and effortless."},{"heading":"What it does","content":"SolShare is a cost-sharing mobile app designed to make splitting bills with friends easy, transparent, and secure, using blockchain technology. Here‚Äôs what it does:\n\nScan receipts : Users take a photo of a receipt, and the app extracts items and prices automatically. Assign items to friends : Each item can be assigned to a specific person, so everyone pays their fair share. Calculate totals and balances : The app automatically tallies what each person owes, avoiding messy calculations or forgotten IOUs. Settle payments instantly via blockchain (Solana): Payments are trustless and immediate, without relying on traditional bank integration. Real-time group tracking: Everyone in a group can see updates instantly."},{"heading":"How we built it","content":"iOS / SwiftUI ‚Äì Chosen for a smooth, native interface and portability; mobile app allows quick receipt scanning and on-the-go payments. Firebase ‚Äì Enables real-time data synchronization across users, so balances and updates are instantly visible. Solana blockchain ‚Äì Provides secure, trustless, and instant payments between users without relying on traditional banking integration. Cohere API ‚Äì Handles receipt scanning and item parsing using a self-criticism learning process, reducing errors and automating item assignment."},{"heading":"Challenges we ran into","content":"During development, we faced several challenges. SwiftUI introduced compiler issues and source control difficulties, which slowed progress at times. Coordinating work across multiple components, backend, UI, blockchain, and LLM integration, required careful planning and communication. On top of that, most of the team was new to blockchain, so learning Solana and implementing trustless payments added an extra layer of complexity. Despite these hurdles, we collaborated closely to overcome them and keep the project moving forward."},{"heading":"Accomplishments that we're proud of","content":"We are proud of the Cohere API integration, as the parsing process was highly reliable, thanks to our system prompts, reliable JSON and self-correction architecture using the command-a vision and reasoning models.\n\nWe're also proud of our integrations of the blockchain and databases. When planning the idea out, it felt very complex and we were concerned about the time constraints that were presented, but ultimately, we were able to handle most of our objectives in time."},{"heading":"What we learned","content":"We learned a lot about full-stack development, prompt engineering and blockchain. But above all, one of the most important things we learned was perseverance. A lot of the database integration happened within the last two hours, and at times we felt as though this was unachievable, however with grit and determination, we were able to get all of our functionality across."},{"heading":"What's next for Solshare","content":"We want to enhance individual profiles on the app by adding features like profile pictures and allowing users to join multiple groups."},{"heading":"Built With","content":"cohere firebase gcp solana swift swiftui typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Deepsint","project_url":"https://devpost.com/software/deepsint","tagline":"An OSINT tool specializes in generating an accurate profile card from a username by leveraging artificial intelligence.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/742/413/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Cohere: Best Use of Cohere API"}],"team_members":[],"built_with":[{"name":"cohere","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"streamlit","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Sai-sys-cmd/Deepsint"}],"description_sections":[{"heading":"üåü Inspiration","content":"We all hear the warnings of data breaches and the dangers of the internet . We've also heard that someone can reconstruct everything about you using OSINT tools‚Äîwithout ever meeting you.\n\nBut in practice, existing OSINT tools are powerful yet slow, complex, and fragmented across 30 different tabs . We wanted something simpler: ‚û°Ô∏è A single-click experience. ‚û°Ô∏è One username ‚Üí one trusted profile card."},{"heading":"‚öôÔ∏è What It Does","content":"Input ‚Üí a single username. Crawl ‚Üí Blackbird scans the open web for likely matches. Collect ‚Üí a web scraper gathers relevant, publicly available data from each hit. Reason ‚Üí Cohere embeddings correlate personas, detect behavioral patterns, and connect fuzzy signals. Synthesize ‚Üí outputs an explainable profile card : Clean facts Source links Timestamps Per-claim confidence scores Guardrails ‚Üí public data only, PII redaction, audit trail, and sensitive inferences disabled by default."},{"heading":"üõ†Ô∏è How We Built It","content":"Pipeline architecture:\n\nDiscovery ‚Üí Blackbird for username enumeration and candidate gathering. Extraction ‚Üí site-aware scraping of bios, handles, links, timestamps. Normalization ‚Üí unify fields, dedupe items, standardize time/text. Correlation ‚Üí reasoning models score cross-platform matches using: Handle similarity Cross-linked bios Writing-style cues Semantic similarity Evidence Grading ‚Üí assign confidence based on independent signals + recency. Profile Card ‚Üí concise summary with sources, timestamps, and caveats."},{"heading":"üöß Challenges We Ran Into","content":"Entity resolution is hard ‚Üí avoiding false positives requires careful scoring & explicit caveats. Noisy & incomplete data ‚Üí profiles change, vanish, or contradict each other. Anti-automation & rate limits ‚Üí building a polite, robust collector without brittle hacks. UX for trust ‚Üí making confidence, evidence, and caveats visible without overwhelming users ."},{"heading":"üèÜ Accomplishments","content":"Built a usable ‚Äúusername ‚Üí trusted profile card‚Äù in minutes, not hours . Evidence-first design ‚Üí every claim is traceable, timestamped, and scored. Cross-platform correlation beyond exact string matches: Semantic similarity Image reuse detection A clean, analyst-friendly UI ‚Üí facts first, exploration second."},{"heading":"üìö What We Learned","content":"In OSINT, speed is nothing without explainability . Confidence scores + links build trust ‚Äîand catch mistakes early. Most value comes from normalization & correlation , not just bigger models. Ethical defaults are essential for adoption and long-term viability."},{"heading":"üöÄ What‚Äôs Next for Deepsint","content":"Name-based discovery ‚Üí privacy-respecting search by name to widen correlation. Image-based discovery ‚Üí profile picture correlation layered with AI reasoning & embeddings."},{"heading":"Built With","content":"cohere python sqlite streamlit"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"YeetThePacket","project_url":"https://devpost.com/software/nocapnethack","tagline":"From packets to plain‚ÄëEnglish, actionable security narratives.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/556/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"CSE: Network Traffic Exploration"}],"team_members":[],"built_with":[{"name":"bash","url":"https://devpost.com/software/built-with/bash"},{"name":"cohere","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"fastapi","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"streamlit","url":null},{"name":"uvicorn","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ashokcpg/YeetThePacket"}],"description_sections":[{"heading":"Inspiration","content":"We‚Äôve watched analysts spend hours sifting through packet captures, then struggle to communicate findings to non-technical stakeholders. We wanted to bridge that gap: turn raw network traffic into clear, defensible stories that responders and decision-makers can act on. Using the MACCDC 2012 dataset as a proving ground, we set out to show that AI can explain incidents without guesswork."},{"heading":"What it does","content":"Detects events like port scans, brute-force attempts, beaconing/C2, suspicious connections, and data exfiltration from PCAPs. Generates evidence‚Äëbacked narratives with a one‚Äëline summary, technical analysis, executive summary, severity, MITRE ATT&CK mapping, remediation steps, and a confidence score. Provides an interactive UI: timeline exploration, filters, search, and a network graph of host relationships. Exports incident reports (PDF) for briefings and documentation."},{"heading":"How we built it","content":"Ingestion and ETL: tshark/pyshark to extract flows and features from PCAPs; pandas for normalization. Detection: lightweight heuristic rules (e.g., high unique destination ports, periodic beacons, failed-auth bursts). AI narratives: a pluggable LLM client (Cohere, Gemini, OpenAI) with evidence‚Äëgrounded prompts and structured outputs. Backend: FastAPI for upload, processing, and narrative endpoints; background tasks for longer jobs. Frontend: Streamlit for rapid iteration with timeline, detail panes, and graph visualizations. Packaging: Docker and docker‚Äëcompose for one‚Äëcommand setup; JSONL storage (optional SQLite planned) for portability."},{"heading":"Challenges we ran into","content":"Grounding LLM outputs to evidence to avoid overclaims. Keeping performance reasonable on larger PCAPs (batching/streaming). Consolidating noisy signals into coherent incidents with sensible severity. Maintaining UI responsiveness with thousands of flows and events. Handling provider rate limits, timeouts, and behavioral differences across LLMs."},{"heading":"What we learned","content":"Evidence‚Äëfirst prompts dramatically improve reliability and clarity. Simple, transparent detection rules are highly effective for a first pass. Clean UX (timelines and summaries) makes complex investigations approachable. Strong schemas for events and narratives unlock reporting and search features. Investing early in containerization and scripts pays off during demos."},{"heading":"Accomplishments that we're proud of","content":"End‚Äëto‚Äëend pipeline from PCAP ingestion to evidence‚Äëbacked AI narratives, running reliably on real data (MACCDC 2012). Multi‚Äëprovider LLM integration (Cohere, Gemini, OpenAI) with a structured, grounded prompt design to minimize overclaims. Interactive UI with a timeline, filters, search, detailed event views, and a network graph for host relationships. Exportable PDF incident reports that combine executive summaries with technical evidence and remediation steps. One‚Äëcommand Docker deployment and documented FastAPI endpoints with live docs, enabling quick setup and demos. Heuristic detection for port scans, brute force, beaconing/C2, suspicious connections, and data exfiltration. Background processing and streaming/batching patterns that keep the UI responsive on larger PCAPs. Clean event and narrative schemas (JSONL) that make reporting, search, and downstream integrations straightforward. A complete demo script and smoke tests to validate the flow and reduce demo risk."},{"heading":"What we learned","content":"Evidence‚Äëfirst prompts and structured outputs dramatically improve reliability and trust in AI narratives. Transparent heuristic rules are highly effective for a first‚Äëpass detection layer and easy to reason about. Strong schemas are leverage: once the data model is right, features like reporting and search come naturally. Performance tuning (batching, streaming, and incremental processing) matters as PCAP size grows. UX clarity‚Äîtimelines, severity, and concise summaries‚Äîmakes complex investigations accessible to all stakeholders."},{"heading":"What's next for YeetThePacket","content":"Real‚Äëtime analysis: streaming ingestion, incremental detection, and live narrative updates. Data layer: PostgreSQL/SQLite for indexing, queries, caching, and long‚Äëterm storage. Scalability: Redis task queues, horizontal workers, and Kubernetes deployment manifests. Detection depth: DNS tunneling, DGA, TLS fingerprinting/JA3, lateral movement, and exfil heuristics by protocol. ML‚Äëassisted anomaly detection to complement heuristics with explainable features."},{"heading":"Built With","content":"bash cohere docker fastapi python streamlit uvicorn"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Pew Pew","project_url":"https://devpost.com/software/pew-pew-9mi8pb","tagline":"Pew Pew is a FastAPI analyzer that turns the legacy CCDC intrusion dataset into a forensic-ready narrative by auto-mapping hosts, aggressive attackers, targeted victims, and an attack timeline.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/744/610/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"CSE: Network Traffic Exploration"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"fastapi","url":null},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"uvicorn","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/stony-su/network-traffic-ai"}],"description_sections":[{"heading":"Features","content":"Parses full Suricata fast.log once at startup Builds force-directed network graph (hosts + alert signature nodes) Simple z-score based anomaly detection on per-source alert volume Pure FastAPI + D3.js frontend"},{"heading":"Built With","content":"css fastapi html5 javascript python uvicorn"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SOTA Computer use agent challenge","project_url":"https://devpost.com/software/sota-computer-use-agent-challenge","tagline":"Competing to build a SOTA computer use agent (Cua + Hud + Ollama)","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Cua: Best State-of-the-Art Computer-Use Agent"}],"team_members":[],"built_with":[{"name":"cua","url":null},{"name":"hud","url":null},{"name":"ollama","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Ram-Raghav-S/cua/tree/ram"}],"description_sections":[{"heading":"Inspiration","content":"Test the limits of current computer-use agents on realistic desktop tasks."},{"heading":"What it does","content":"Executes high-level natural language instructions (e.g. ‚Äúdownload dataset, unzip, open in Excel, make pivot table‚Äù) by perceiving the screen and controlling mouse/keyboard."},{"heading":"How we built it","content":"Screen capture + OCR for visual grounding Vision-language model for reasoning Action planner for granular interactions Safety/retry logic for robustness"},{"heading":"Challenges we ran into","content":"UI variability across themes/states Long-horizon task planning Latency vs. accuracy in perception"},{"heading":"Accomplishments that we're proud of","content":"Agent completed multi-step workflows end-to-end Modular architecture for adding new skills Automatic recovery from common failures"},{"heading":"What we learned","content":"Robust grounding is the bottleneck, not just model quality Simple guardrails/retries greatly boost success Human-like adaptability > perfect execution"},{"heading":"What's next for SOTA Computer Use Agent Challenge","content":"Benchmark on standardized real-world tasks Extend to hybrid environments (desktop + web + APIs) Release starter framework for community use"},{"heading":"Built With","content":"cua hud ollama"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Optimate","project_url":"https://devpost.com/software/optimate","tagline":"Smarter underwriting through AI-powered dashboards, deep insights, and reinforcement learning.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/755/441/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Federato: Best RiskOps Solution for Underwriters"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Y Combinator: Unicorn Prize"}],"team_members":[],"built_with":[{"name":"amazon-dynamodb","url":"https://devpost.com/software/built-with/amazon-dynamodb"},{"name":"auth0","url":null},{"name":"aws-dynamodb","url":null},{"name":"cohere","url":null},{"name":"cohereai","url":null},{"name":"d3.js","url":"https://devpost.com/software/built-with/d3-js"},{"name":"federato-api","url":null},{"name":"next","url":null},{"name":"next.js","url":null},{"name":"oauth","url":"https://devpost.com/software/built-with/oauth"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"recharts","url":null},{"name":"reinforcement-learning","url":null},{"name":"retrieval-augmented-generation-(rag)","url":null},{"name":"shacdn","url":null},{"name":"shadcn/ui","url":null},{"name":"tailwind","url":null},{"name":"tailwindcss","url":null},{"name":"vercel","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/jruttan1/Optimate"},{"label":"optimate-two.vercel.app","url":"https://optimate-two.vercel.app/auth"}],"description_sections":[{"heading":"Inspiration","content":"We were inspired by Federato‚Äôs challenge statement: ‚ÄúReimagine the RiskOps landing page experience. The goal is to build a dashboard experience that, upon user login, intelligently surfaces a curated set of submissions that are most aligned with the carrier's appetite; enabling faster and more effective underwriting decisions.‚Äù\n\nThis sparked the idea for Optimate , our AI-powered underwriting dashboard. Underwriters often face the challenge of sifting through massive amounts of policy data to make high-stakes decisions under pressure. We wanted to reimagine this workflow by combining:\n\nAI and reinforcement learning to prioritize in-appetite submissions. Interactive dashboards for clarity and transparency. Data-driven insights grounded in underwriting guidelines and risk metrics. Live tracking and plots to monitor portfolio performance in real time. Heatmap visualizations to instantly show policies across states, colored by appetite and risk scores.\n\nOur inspiration came from observing how underwriters juggle complex, and sometimes conflicting, factors; such as Total Insured Value (TIV), loss ratios, building age, and state regulations. By building Optimate, we aimed to show how modern AI and intuitive visual design can turn this complexity into actionable intelligence, enabling smarter, faster, and more confident decisions."},{"heading":"What it does","content":"Optimate is an AI-powered underwriting dashboard designed to make risk assessment smarter, faster, and more transparent. It transforms raw policy data into actionable insights and visualizations that help underwriters focus on what matters most.\n\nWith Optimate, underwriters can:\n\nSee top matches : Instantly view the top 10 submissions that best align with appetite guidelines and the underwriter‚Äôs selected policy. Understand the ‚Äúwhy‚Äù : Access a detailed breakdown of AI-driven justifications and rules that explain why each submission was prioritized. Take action with confidence : Approve or decline submissions directly from the dashboard, supported by clear reasoning and data. Train the AI : Provide feedback and satisfaction scores that feed into reinforcement learning, making the system smarter over time. Visualize risks : Explore an interactive heatmap of U.S. states showing areas of high and low risk appetite, alongside dynamic plots and live tracking of portfolio performance. Chat with AI : Engage in two-way conversations with the AI at the individual policy level, enabling underwriters to ask questions, receive clarifications, and co-decide on risk assessment with contextual awareness.\n\nTogether, these features make Optimate a one-stop platform for turning underwriting complexity into clarity."},{"heading":"How we built it","content":"We built Optimate as a full-stack application combining a modern web framework, AI services, and scalable cloud infrastructure to deliver real-time insights to underwriters.\n\nFrontend & Deployment Built using Next.js (App Router) and deployed on Vercel for fast, serverless performance and seamless CI/CD. Used TailwindCSS and shadcn/ui to design a clean, consistent, and responsive interface with minimal overhead, enabling rapid iteration of charts, heatmaps, and dashboards. Integrated Auth0 for secure authentication and role-based login, ensuring underwriters have personalized access to their own submissions and preferences. Data Layer & Processing Pulled the Federato API dataset (policies, premiums, TIV, loss ratios, etc.) and ingested it through a Python pipeline , where we enriched the raw data with: Risk scores and appetite scores based on underwriting guidelines. Live tracking metrics for policies over time. Aggregated account-level analytics weighted by premium and loss ratio. Stored enriched data in a format optimized for visualization and AI query workloads. AI & Insights Engine Used Cohere‚Äôs LLMs with a Retrieval-Augmented Generation (RAG) pipeline: Broke policy data and underwriting guidelines into embeddings and indexed them. Enabled the AI to justify its decisions (‚Äúwhy this policy is in-appetite‚Äù) by pulling grounded references from guidelines and historical data. Implemented reinforcement learning from underwriter feedback , where every approve/decline and satisfaction score helps refine appetite scoring logic over time. Added a context-aware AI chatbot that works at the individual policy level , enabling underwriters to have two-way conversations with the AI about specific submissions. Storage & State Management All application state, including user preferences, saved searches, favorite policies, chat history, and cached embeddings metadata , is persisted in DynamoDB , giving us low-latency reads and writes at scale. This persistence enables underwriters to pick up where they left off, track past decisions, and build a living history of their interactions with the AI. Visualizations & Analytics Leveraged Recharts and custom D3 visualizations to build: Interactive plots of policy performance metrics. Heatmaps of U.S. states colored by appetite/risk scores. Dynamic leaderboards of top in-appetite policies. Designed the dashboard to surface both macro insights (portfolio-level risk distribution) and micro insights (policy-by-policy explanations).\n\nIn short, Optimate integrates a modern web stack with AI-driven insights, reinforcement learning, and powerful visualizations; all aimed at making underwriting decisions smarter and more transparent."},{"heading":"Challenges we ran into","content":"Domain Understanding Insurance and underwriting were completely new domains for our team. At first, we struggled to understand concepts like Total Insurable Value (TIV) , loss ratios , and appetite guidelines . It took deliberate effort to translate Federato‚Äôs problem statement and sample guidelines into clear, actionable rules for our AI model and dashboard. Once we built that context, we could design features that truly aligned with underwriters‚Äô workflows.\n\nCollaboration & Version Control Working as a team of four developers in a time-boxed hackathon meant rapid iteration and frequent code merges. Naturally, merge conflicts cropped up in our GitHub repo. We learned to coordinate by:\n\nEstablishing a practice of announcing merges into main , Pulling changes immediately after merges, and Splitting work into smaller, well-scoped branches to reduce overlap. This improved our velocity and reduced lost time on conflicts.\n\nAI Model Development We wanted our AI to do more than simple filtering. Designing the reinforcement learning loop was challenging, especially under hackathon constraints. We had to:\n\nDefine reward functions based on underwriter satisfaction feedback, Balance rule-based scoring (from appetite guidelines) with LLM-powered reasoning , and Build a lightweight but extendable training loop that could adapt over time. This gave us a crash course in connecting theory (RL foundations) with real-world application .\n\nCloud Infrastructure We chose AWS DynamoDB for storing user data, embeddings, and chat histories because of its scalability and low latency. But setting it up securely was non-trivial. Challenges included:\n\nConfiguring IAM roles and policies to avoid over-permissive access, Ensuring smooth integration with our Next.js app, and Handling rate limits and schema design under the pressure of real-time queries.\n\nVisualization & Real-Time Data Designing meaningful heatmaps, plots, and live tracking views presented both technical and design hurdles. We had to map appetite scores and risk metrics into visuals that underwriters could understand at a glance. Optimizing rendering performance for large datasets while keeping the UI responsive was a balancing act.\n\nDespite these hurdles, each challenge became an opportunity: we learned new domains, tightened our team workflow, pushed our AI design skills, and deployed a secure, scalable cloud app; all within the hackathon timeframe."},{"heading":"Accomplishments that we're proud of","content":"Timely Delivery We successfully built and delivered a working product within the hackathon timeframe, ensuring it stayed aligned with Federato‚Äôs problem statement while going beyond the basics with live tracking, heatmaps, and an AI chatbot.\n\nAdopting New Technologies Our team dove into new tools and frameworks; CohereAI, Auth0, AWS DynamoDB, Next.js (App Router), and shadcn/ui ; and integrated them into a production-ready application in just a few days. This rapid learning curve not only expanded our technical toolkit but also boosted our confidence in quickly mastering unfamiliar technologies.\n\nAI Model Exploration We researched and implemented the fundamentals of reinforcement learning , applying it to real-world underwriting data. While lightweight, our model demonstrated how underwriter feedback (approve/decline, satisfaction scores) could guide iterative improvement of appetite scoring.\n\nVisual Storytelling We built intuitive plots, live tracking views, and heatmaps that made abstract metrics like risk score and appetite score immediately understandable. Seeing complex insurance data come alive visually was one of our proudest achievements."},{"heading":"What we learned","content":"Improved Team Communication Working under time pressure taught us the importance of concise, transparent communication. We learned to coordinate merges, delegate responsibilities clearly, and make fast collective decisions.\n\nTechnical Growth We deepened our knowledge of:\n\nBuilding RAG pipelines with Cohere, Designing secure and scalable systems with AWS DynamoDB , Implementing reinforcement learning concepts in a practical setting, and Deploying production-ready UIs with Next.js + TailwindCSS + shadcn/ui .\n\nIndustry Insight We gained valuable exposure to the insurance and underwriting industry . By working with appetite guidelines, TIV, loss ratios, and premiums, we developed a clearer picture of how underwriters balance competing factors and why decision-support tools like Optimate can make a real impact.\n\nHackathon Mindset Perhaps most importantly, we learned how to take a vague, high-level challenge and turn it into a polished, end-to-end solution that blends AI, cloud, and UX in a way that solves real-world problems."},{"heading":"What's next for Optimate","content":"We see Optimate not just as a hackathon project, but as the foundation of a scalable underwriting intelligence platform. Our next steps include:\n\nDeeper AI Integration : Expand beyond RAG and reinforcement learning by exploring fine-tuned LLMs that can model more nuanced underwriting strategies and simulate ‚Äúwhat-if‚Äù scenarios for submissions. Real-Time Data Feeds : Integrate external data sources such as weather risks, property valuation APIs, or catastrophe modeling to make appetite scores more dynamic and responsive to changing conditions. Explainability at Scale : Enhance the AI explanations with richer visual cues (confidence intervals, score breakdowns, and sensitivity analysis) so underwriters can trust not just the results, but the reasoning. Collaborative Workflows : Add features for teams of underwriters; such as shared views, comments, and case histories; to bring transparency and accountability to group decisions. Mobile & Cross-Platform Support : Extend Optimate to mobile and tablet experiences so underwriters can access dashboards and chat with the AI on the go. Production-Grade Infrastructure : Move from DynamoDB prototypes to a more robust multi-region cloud setup with monitoring, audit trails, and enterprise-grade security, making it deployable in real-world underwriting teams.\n\nBy continuing to evolve Optimate, we aim to transform it into a trusted partner for underwriters; combining the precision of AI, the clarity of visualization, and the adaptability of human feedback."},{"heading":"Conclusion","content":"With Optimate , we set out to reimagine underwriting by blending AI, data visualization, and human feedback into one seamless platform. What started as a hackathon challenge grew into a vision for how underwriters can make faster, smarter, and more confident decisions; supported by explainable AI, reinforcement learning, and intuitive dashboards.\n\nOur journey taught us not only new technologies and industry insights, but also the power of collaboration under pressure. Most importantly, we demonstrated that even in a complex and highly regulated domain like insurance, innovation can come from rethinking the experience at the intersection of AI, design, and trust ."},{"heading":"Built With","content":"amazon-dynamodb auth0 aws-dynamodb cohere cohereai d3.js federato-api next next.js oauth python recharts reinforcement-learning retrieval-augmented-generation-(rag) shacdn shadcn/ui tailwind tailwindcss vercel"},{"heading":"Try it out","content":"github.com optimate-two.vercel.app"}]},{"project_title":"BeyondSight ","project_url":"https://devpost.com/software/beyondsight-589mh7","tagline":"BeyondSight gives DeafBlind people a voice, a touch, and a path forward. We have created a wearable system designed to empower individuals by using a vest, a gauntlet, combined with AI communication.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/170/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Genesys: Empathy in Action with Genesys Cloud APIs"}],"team_members":[],"built_with":[{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"elevenlabs","url":null},{"name":"genesys","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"slam","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/WhyILived/HTN"}],"description_sections":[{"heading":"‚ú® What it does","content":"BeyondSight is a full wearable ecosystem designed to empower DeafBlind individuals with independence, communication, and safety.\n\nüëï Navigation & Object Detection\n\nUsing an Xbox depth camera , the vest's four haptic motors guide the wearer through physical spaces. The motors on the left and right indicate turning directions, while the top and bottom motors guide vertical adjustments. With the gauntlet's Braille keypad , the user can request specific objects or destinations‚Äîwhether that's a door, a chair, or even a water bottle. Once selected, the motors provide directional cues to help the user orient and reach the target.\n\n‚úã \"Vibe Hand\" & \"Spidey Senses\"\n\nWe designed unique interaction modes:\n\nVibe Hand uses rapid haptic pulses to deliver instructions for playful or cooperative actions, like playing catch. Spidey Senses alerts the user when fast-approaching objects are detected, with all motors buzzing simultaneously to deliver an urgent warning .\n\nüó£Ô∏è Conversational System\n\nThe gauntlet also acts as a two-way communication device :\n\nUser input via Braille is processed through the Gemini API , expanded into natural sentences, and spoken aloud with ElevenLabs through a portable speaker. Incoming speech from others is captured with a microphone, transcribed via OpenAI's Whisper , then translated into Braille through six haptic motors on the gauntlet‚Äîallowing the wearer to \"feel\" the conversation in real time .\n\nüíª Frontend & Backend Support\n\nOn the software side, we built a MongoDB-backed system to store user profiles, track conversations, and log locations. Genesys APIs enhance this by providing geolocation services and conversation summarization‚Äîoffering caretakers or loved ones insights into where the user has been and what interactions took place."},{"heading":"üí° Inspiration","content":"Our inspiration comes directly from someone close to us: our friend Damian ‚Äîaffectionately nicknamed Damy ‚Äîwho has been DeafBlind since childhood. Despite countless challenges, Damian made it all the way to the University of Chicago , showing resilience and determination that inspires us every day.\n\nBut we've also witnessed firsthand the struggles he faces: navigating unfamiliar spaces, trying to communicate in real-time, and depending heavily on others for basic independence. We wanted to build something that could transform those struggles into opportunities ‚Äînot just for Damian, but for DeafBlind individuals around the world.\n\nBeyondSight is our way of showing that accessibility isn't a luxury‚Äîit's a necessity, and with technology, it can become reality."},{"heading":"üõ†Ô∏è How we built it","content":"From the moment we set foot in E7 , we knew we wanted to tackle this challenge head-on. On Friday night, we secured an Xbox 360 depth camera off Facebook Marketplace, traveling across town on the 301 ION to pick it up. Meanwhile, our teammate Sy , a computer engineering student, brought a treasure trove of haptic motors to the table.\n\nThe hardware build began with 3D-printing the gauntlet to house the Braille keypad and motors, while wiring up the vest for navigation feedback. Simultaneously, we coded the AI communication pipeline ‚Äîtying together:\n\nWhisper for speech recognition Gemini for natural language expansion ElevenLabs for speech synthesis\n\nPiece by piece, the hardware and software came together into a system where every component‚Äîfrom motors to models‚Äîworked in perfect harmony ."},{"heading":"‚ö° Challenges we ran into","content":"Our biggest hurdle came from the Xbox camera itself . Because it's nearly two decades old , support is scarce. Our teammate Noah had to wrestle with:\n\nOutdated drivers Installing Ubuntu across multiple USB drives Experimenting with Docker images\n\n...before finally getting depth sensing and SLAM technology working.\n\nEven once it ran, compatibility issues with Linux and the lack of modern documentation pushed us to the edge‚Äîbut persistence paid off . In the end, we managed to breathe new life into a piece of legacy hardware and integrate it into a modern accessibility system."},{"heading":"üèÜ Accomplishments that we're proud of","content":"‚úÖ Successfully configuring the old Xbox depth camera with Docker and Ubuntu to deliver real-time depth sensing.\n\n‚úÖ Designing and building a vest and gauntlet system that are both accessible and comfortable to wear, despite the complex wiring and motor placements.\n\n‚úÖ Optimizing OpenAI's Whisper pipeline , making it faster and more accurate in noisy environments than our initial tests.\n\n‚úÖ Creating unique haptic feedback modes like \"Vibe Hand\" and \"Spidey Senses,\" which add both safety and fun to the experience."},{"heading":"üìö What we learned","content":"\"It ain't over 'til it's over.\" - Yogi Berra\n\nThis project taught us resilience more than anything else. Many of our hardware breakthroughs‚Äîlike the camera setup and Whisper optimization‚Äîonly came together in the final hours before presentations.\n\nWe learned to:\n\nStay calm under pressure Keep experimenting when things looked impossible Lean on each other's strengths\n\nWe also gained valuable hands-on experience with haptic hardware , multimodal AI systems , and accessibility design ‚Äîskills we'll carry forward long after the hackathon."},{"heading":"üåç The Current Market and What's Next","content":"Accessibility technology is advancing rapidly , but solutions for DeafBlind individuals are still limited. Current tools often focus on one sense or one mode of interaction , leaving significant gaps. BeyondSight bridges those gaps by combining navigation , communication , and real-time safety into a single integrated system.\n\nStudies show that people with disabilities consistently express a desire for greater independence and reduced reliance on caretakers . We believe BeyondSight is a step toward that reality.\n\nüîÆ Next steps include:\n\nüéØ Replacing the bulky Xbox camera with lightweight, portable depth or LiDAR sensors.\n\nüéØ Refining the haptic patterns through user testing with the DeafBlind community.\n\nüéØ Optimizing for battery life and portability to make BeyondSight wearable for daily use.\n\nüéØ Exploring partnerships with accessibility organizations to expand impact."},{"heading":"üåü Our Vision","content":"Our vision is bold but clear: make BeyondSight affordable, reliable, and available to DeafBlind people everywhere."},{"heading":"Built With","content":"docker elevenlabs genesys python slam"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TrueCount","project_url":"https://devpost.com/software/truecount","tagline":"Voting outcomes often face backlash. That‚Äôs why trust matters. TrueCount keeps every vote secure and permanently recorded on an open ledger, ensuring every choice is counted correctly.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/180/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"ETHGlobal: Ethereum's Infinite Garden"}],"team_members":[],"built_with":[{"name":"hardhat","url":null},{"name":"rainbowkit","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"wagmi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/MartinPatr/TrueCount"}],"description_sections":[{"heading":"Inspiration","content":"One of our teammates experienced a moment in student body government where a candidate's votes were reduced by half because a moderator believed she violated a subjective rule. The issue wasn't just the penalty ‚Äî it was how easily one person's judgment could undermine trust in the entire process.\n\nTrueCount was born to fix this. By moving voting on-chain, we eliminate human bias and create a system where trust is placed in code , not moderators."},{"heading":"What It Does","content":"TrueCount: A Decentralized, Phase-Based Voting System\n\nTrueCount is a commit‚Äìreveal voting system for communities that need fairness, transparency, and verifiable results. No moderators. No tampering. Every vote is private to remove bias, later revealed and then permanently auditable on-chain.\n\nBuilt on Ethereum using a commit‚Äìreveal‚Äìfinalize flow:\n\n1. Poll Creation\n\nAnyone can create a poll with custom options and configurable commit/reveal durations. Smart contracts enforce deadlines and rules without a central authority.\n\n2. Commit Phase\n\nVoters pick an option. A random 32-byte salt is generated client-side. The frontend computes a hash: solidity keccak256(encodePacked(option, salt, voter, pollId)) The commitment is submitted on-chain ‚Äî keeping the actual vote hidden.\n\n3. Reveal Phase\n\nVoters resubmit their chosen option along with the salt. The contract verifies the commitment matches the reveal. Invalid reveals are automatically rejected.\n\n4. Finalize Phase\n\nOnce the reveal window closes, anyone can finalize the poll. The tallies are locked and publicly accessible forever."},{"heading":"Key Features:","content":"üîê Wallet Integration - Seamless connection with RainbowKit and wagmi üìä Customizable Polls - Flexible options with configurable time windows ü§ù Privacy-First - Votes remain hidden during the commit phase üîç Cryptographic Verification - Mathematical proof of vote integrity ‚úÖ Permanent Transparency - Results locked on-chain forever üé® Intuitive UX - Real-time phase indicators, countdown timers, and comprehensive error handling"},{"heading":"How We Built It","content":"Smart Contract Architecture:\n\nSolidity contracts implementing the commit-reveal scheme with automatic phase transitions Hardhat development environment for testing, compilation, and deployment Ignition deployment modules for consistent contract deployment\n\nFrontend Technology:\n\nReact + TypeScript for type-safe, component-based UI development wagmi + RainbowKit for Web3 wallet connectivity viem for efficient blockchain interactions and transaction handling TailwindCSS for responsive, modern styling\n\nDevelopment Workflow:\n\nLocal Hardhat node for rapid iteration and testing Comprehensive error handling for both on-chain and off-chain failures Client-side salt generation and secure storage using localStorage Real-time phase detection with automatic UI updates\n\nSecurity Considerations:\n\nCryptographic commitments prevent vote manipulation Salt generation ensures vote privacy until the reveal phase to counter voter bias Smart contract validation prevents invalid or duplicate reveals Time-locked phases prevent premature or late submissions"},{"heading":"Challenges","content":"The toughest challenge was balancing vote privacy with decentralization ‚Äî two contradicting ideas. On the one hand, we wanted an open and transparent system. On the other hand, exposing votes too early could create bias and influence voters mid-election. By using cryptographic salts, we achieve the perfect balance between privacy and transparency .\n\nAdditionally, setting up faucets and managing wallet keys for testing and building our product was a new experience that required some time and practice."},{"heading":"What We're Proud of","content":"We're incredibly proud that True Count isn't just a technical demo‚Äîit's a solution with genuine real-world impact. By eliminating central authorities, True Count replaces institutional trust with mathematical certainty through transparent, verifiable code.\n\nWhat excites us most is the system's universal scalability. The same cryptographic guarantees that secure a student body election work equally well for community decisions, corporate governance, or international organizations like the UN. Fair voting shouldn't depend on the scale of the election‚Äîit should be mathematically guaranteed at every level."},{"heading":"What We Learned","content":"We learned how to balance privacy with transparency, work with smart contracts, manage wallets and faucets, and design a system that scales from small groups to global communities."},{"heading":"The Future","content":"Near Roadmap:\n\nLayer 2 Deployment - Deploy to Optimism, Arbitrum, or Base for reduced gas costs and faster transactions Enhanced Security - Add multi-signature poll creation for high-stakes community votes Mobile Optimization - Responsive design improvements and mobile wallet integration testing More Decorative Options - Banners, colours, widgets to enhance user experience\n\nLong-term Vision:\n\nCross-chain Compatibility - Enable voting on one blockchain with verification on another Governance Framework Integration - Full DAO tooling integration with popular governance platforms Institutional Adoption - Partner with educational institutions, non-profits, and community organizations"},{"heading":"Built With","content":"hardhat rainbowkit react solidity typescript wagmi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ScholarMarket","project_url":"https://devpost.com/software/scholarmarket","tagline":"Polymarket but for your grades","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/387/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"ETHGlobal: Ethereum's Infinite Garden"}],"team_members":[],"built_with":[{"name":"auth0","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next.js","url":null},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"solidity","url":"https://devpost.com/software/built-with/solidity"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/joshua-z-zheng/hack-the-north-2025"}],"description_sections":[{"heading":"Inspiration","content":"Have you ever promised to yourself you would lock in only to end up procrastinating for hours? Many students feel unmotivated in the age of modern distractions, from doomscrolling to brainrot and even to digital gambling. ScholarMarket uses Ethereum smart contracts to solve this problem by introducing a motivational betting platform to encourage students to invest greater efforts into their studies. Students can bet cryptocurrency on improving their academic performance to motivate them to work harder. It also provides an opportunity for universities to release \"probabilistic scholarships\" to students to who do well. By harnessing modern distractions and transforming them into monetary incentives to perform better, ScholarMarket helps build a more productive, competitive, and (hopefully) successful learning environment for students."},{"heading":"What it does","content":"ScholarMarket is essentially Polymarket for students to bet on their grades. The web backend uses a machine learning model to predict the odds of a student achieving a particular goal based on their performance trends. It then allows the student to bet a chosen value of Ethereum and win a greater sum for reaching the goal based on these odds."},{"heading":"How we built it","content":"We trained a convolutional neural network using PyTorch to calculate the probability of a student getting a specific mark given their past performance. The model was trained on datasets of students' performance for different courses over time while factoring in a difficulty estimate for each course.\n\nOur web app was implemented using NextJS, and we integrated blockchain functionality using Hardhat and Ethereum L2 smart contracts. The smart contract deployment allows users to place small bets on their grade predictions, with the potential to earn $1 per share if they achieve a grade above a certain point. We stored corresponding data for users in MongoDB to speed up bets by caching contract addresses and similar data."},{"heading":"Challenges we ran into","content":"For the ML model, we originally used a 1D convolutional layer to process the sequence of past grades. However, this model yields to a 40% accuracy, which indicates it cannot reflect the trends well. We did researches and found that we can actually use a LSTM model for this task. This model helped us to reach a eventual accuracy of 70%+."},{"heading":"Accomplishments that we're proud of","content":"We didn't have much experience with crypto and Ethereum before this hackathon, so we're glad we learned something new while integrating it into a cool project. We're particularly proud of implementing a working smart contract system that handles real value transactions while maintaining security and solvency protections."},{"heading":"What we learned","content":"We learned how to use smart contracts and the Ethereum development ecosystem from the ground up. This included learning Hardhat for the first time, understanding how to write and deploy smart contracts with Solidity, and figuring out how to connect blockchain functionality to a traditional web app. We discovered the differences between testnets and mainnet, learned about Layer 2 networks and why they're useful for reducing costs, and got hands-on experience with the entire Web3 development workflow."},{"heading":"What's next for ScholarMarket","content":"Our current machine learning model does not take into account different types of courses and their similarities or differences. It only uses the course difficulty and order in which they were taken to predict trends. However, for a more accurate prediction, the model could recognize that similar courses such as 1st and 2nd year calculus are more likely to affect each others' performance."},{"heading":"Built With","content":"auth0 express.js mongodb next.js pytorch solidity typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"One Take","project_url":"https://devpost.com/software/one-take","tagline":"One Take generates professional product demos in minutes. Provide your GitHub or website URL, One Take navigates your product, captures the perfect shots, and adds an authentic AI voiceover.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/738/925/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Groq"}],"team_members":[],"built_with":[{"name":"chromium","url":null},{"name":"cohere","url":null},{"name":"ffmpeg","url":"https://devpost.com/software/built-with/ffmpeg"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"windsurf","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/NikhilHooda/One-Take"}],"description_sections":[{"heading":"Inspiration","content":"As developers, we've all been there, spending weeks creating demo videos for our projects, hiring expensive video production teams, or settling for low-quality screen recordings that don't do our hard work justice. We watched countless brilliant open-source projects go unnoticed because they lacked compelling demos, and saw startups struggle to create professional product showcases for investor pitches. The traditional video production process is slow, expensive, and often inaccessible to individual developers and small teams. We realized there had to be a better way to bridge the gap between amazing products and the professional demos they deserve."},{"heading":"What it does","content":"One Take transforms how companies showcase their products by generating professional product demos in minutes, not weeks. Our AI-powered platform automatically creates polished demo videos from just a GitHub repo URL or website link. Here's how it works:\n\nIntelligent Product Navigation : Our AI crawls and understands your product, automatically identifying key features and user flows Automated Screen Capture : The system navigates through your application, capturing the perfect shots and interactions Natural AI Voiceover : Generates authentic, human-like narration that explains your product's value proposition Professional Post-Production : Automatically applies transitions, pacing, and visual enhancements for a polished final product\n\nOne Take reduces demo creation from 10 days to 10 minutes and from $5,000 to $50, making professional product videos accessible to everyone."},{"heading":"How we built it","content":"One Take is built on a sophisticated AI-powered architecture designed for scalability and reliability:\n\nFrontend: React.js with TypeScript for a responsive web application\n\nBackend: Python for video processing workflows\n\nAI/ML Stack:\n\nCohere for AI models to run storyboard agent and generate voice over script Groq for natural AI voice over for the demo video\n\nVideo Processing: FFmpeg integration for video rendering and post-production effects\n\nAdditional Technologies:\n\nChromium for the web browser Windsurf for development"},{"heading":"Challenges we ran into","content":"Our biggest challenge centered on seamlessly integrating three distinct components: storyboard generation, demo video creation, and AI voice-over generation. With each team member working on a separate component independently, we initially struggled to establish effective communication between these moving parts to integrate them into one unified product. The breakthrough came when we standardized on JSON as our universal data format across all three components. This decision created a consistent interface that allowed the storyboard generator to pass structured data to the video renderer, which could then seamlessly hand off timing and content information to the voice-over system. By establishing this common language between our components, we transformed what could have been a complex integration nightmare into a streamlined, modular architecture. This approach not only solved our immediate integration challenges but also made our system more maintainable and scalable for future development."},{"heading":"Accomplishments that we're proud of","content":"We're incredibly proud of developing a polished product that leverages agentic AI to intelligently analyze web pages and automatically generate comprehensive storyboards. Our system goes beyond simple screen capture - it understands the purpose and flow of web interfaces, creating structured narratives that guide users through meaningful product demonstrations. We are also proud of our seamless incorporation of Groq Cloud's text-to-speech AI voice agent to create dynamic, contextual voiceovers. Rather than using generic text-to-speech, our system intelligently interprets the JSON storyboard data to generate natural, engaging narration that adapts to each demo's specific content . The voice agent understands the context of each scene, creating smooth transitions and explanatory commentary that feels genuinely helpful rather than robotic. What makes us most proud is how these components work together to create an autonomous demo generation pipeline, from webpage analysis to final voiced video, maintaining professional quality throughout."},{"heading":"What we learned","content":"This project provided invaluable insights across multiple domains:\n\nTechnical Skills: We deepened our expertise in advanced web automation techniques, mastered large-scale video processing workflows, and gained hands-on experience integrating multiple AI models into a cohesive system. The complexity of orchestrating browser automation, video rendering, and AI processing taught us valuable lessons about system architecture and performance optimization.\n\nProduct Development: We discovered just how critical high-quality demos are for product adoption - and more importantly, how much time development teams actually invest in creating compelling video content. This reinforced our belief that automating this process addresses a real pain point that many teams face but rarely discuss openly.\n\nAI/ML Applications: Working with agentic AI models revealed the practical challenges of prompt engineering, response consistency, and managing AI unpredictability in production environments. Implementing Groq's Voice AI agent API taught us about balancing API costs, latency considerations, and the nuances of creating natural-sounding, contextually aware voice synthesis that enhances the user experience."},{"heading":"What's next for One Take","content":"Our next major focus is expanding beyond simple product demos to tackle complex, multi-step workflows. We envision One Take automatically understanding and demonstrating intricate processes like onboarding sequences, checkout flows, admin dashboards, and cross-platform integrations. This means developing more sophisticated AI that can recognize workflow patterns and user journeys without explicit guidance.\n\nCurrently, users need to specify what type of demo they want - but we're working toward complete autonomy. Imagine simply providing a URL and having One Take intelligently analyze the webpage, identify the most valuable user flows, determine the target audience, and automatically generate multiple demo variations. The system would understand context clues like page structure, user interface patterns, and business objectives to create relevant demonstrations without any human prompting.\n\nWe're exploring AI that can automatically customize demos for different audiences - generating technical deep-dives for developers, high-level overviews for executives, and user-focused walkthroughs for end customers, all from the same source material.\n\nThe ultimate vision is a system that proactively generates updated demos whenever it detects changes to a website, ensuring marketing and sales teams always have current, professional video content without lifting a finger."},{"heading":"Built With","content":"chromium cohere ffmpeg groq javascript json python react typescript windsurf"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CourseIntelligence","project_url":"https://devpost.com/software/courseinteligence","tagline":"Making academic and professional lives easier for students around the world! Our project scrapes community data to provide students with valuable information to simplify the course selection process.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/965/datas/medium.JPG","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Databricks: Best Use of Databricks"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"databricks","url":null},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"llama","url":null},{"name":"openai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sql","url":"https://devpost.com/software/built-with/sql"}],"external_links":[{"label":"github.com","url":"https://github.com/SultanAlzoghaibi/Hack-the-North-2025/tree/main"}],"description_sections":[{"heading":"Inspiration","content":"What it does\n\nCourseIntelligence scrapes Reddit communities like r/uwaterloo and r/UCalgary for course-related discussions, combines them with official university catalog data, and aggregates the results into structured metrics: ‚Ä¢ How hard is this course? ‚Ä¢ Is it time-consuming? ‚Ä¢ Lab-based or theory-heavy? ‚Ä¢ Which professors are recommended? ‚Ä¢ Resume/industry value.\n\nThe processed data is uploaded into Databricks, where we use AI models to summarize and standardize student experiences into comparable course profiles.\n\nHow we built it ‚Ä¢ Web scraping & Reddit API (PRAW): Pulled posts and comments from multiple subreddits. ‚Ä¢ Data pipelines (Python + JSON): Stored scraped discussions in structured JSON with partial-save fail-safes. ‚Ä¢ Databricks: Used bronze ‚Üí silver ‚Üí gold layered processing to clean, aggregate, and query the data. ‚Ä¢ AI integration: Leveraged LLMs via Databricks model serving to summarize free-form Reddit text into structured course evaluation metrics. ‚Ä¢ Frontend (Dash app): Simple interface to search for a course and instantly see summarized insights.\n\nChallenges we ran into ‚Ä¢ Reddit data is noisy and unstructured, requiring heavy cleaning. ‚Ä¢ API rate limits slowed scraping significantly. ‚Ä¢ Getting Databricks files API to properly accept uploads was tricky. ‚Ä¢ Designing consistent JSON schemas so AI outputs could be queried easily.\n\nAccomplishments that we‚Äôre proud of ‚Ä¢ Built a scalable pipeline that continuously scrapes, saves partial progress, and avoids total data loss. ‚Ä¢ Summarized thousands of Reddit comments into clean, comparable insights. ‚Ä¢ Integrated Databricks model serving to directly run AI queries on Reddit data. ‚Ä¢ Proved that students can make better-informed course choices using aggregated real-world feedback.\n\nWhat we learned ‚Ä¢ How to connect real-world unstructured data into a structured analytics workflow. ‚Ä¢ The power of Databricks bronze ‚Üí silver ‚Üí gold pipelines for data cleaning and aggregation. ‚Ä¢ Best practices for fault-tolerant scraping pipelines (e.g., partial JSON saving). ‚Ä¢ How to balance Reddit‚Äôs raw opinions with AI summarization without losing nuance.\n\nWhat‚Äôs next for CourseIntelligence ‚Ä¢ Expand beyond Waterloo and Calgary to other universities worldwide. ‚Ä¢ Add sentiment analysis dashboards to capture positive/negative trends. ‚Ä¢ Build a recommendation engine that suggests courses based on a student‚Äôs career goals. ‚Ä¢ Release as a student-facing web app where anyone can search for courses and instantly get community-driven insights."},{"heading":"Built With","content":"css databricks html llama openai python sql"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"LeafPilot","project_url":"https://devpost.com/software/leafpilot","tagline":"LeafPilot is an AI-powered LaTeX editor, built for education. Ask about a concept, upload rough notes or record your lecture's audio, and you'll receive clean PDF results with embedded animations!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/759/548/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Groq"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"manim","url":null},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"shadcn","url":null},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"zustand","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/jbauerko/htn-2025"}],"description_sections":[{"heading":"Inspiration","content":"For such a widely used typesetting platform (you could format this very blurb in it!), we were surprised that there wasn‚Äôt an editor for LaTeX with built-in AI support. We saw a clear opportunity: students and researchers often spend more time debugging formatting than focusing on content. We wanted to change that, especially for students who want to create clean, professional notes, complete with interactive diagrams and even animated visualizations."},{"heading":"What it does","content":"Think of LeafPilot as Overleaf with Copilot , or Cursor for LaTeX . It allows you to:\n\nGenerate and compile LaTeX directly in your browser. Seamlessly embed Manim (3Blue1Brown-style) animations into your documents. Upload your own resources (like text notes or even an audio recording ) and have LeafPilot structure, typeset, and enhance them into a polished PDF.\n\nIn short: LeafPilot turns raw ideas into beautifully formatted, animated documents with AI as your co-pilot."},{"heading":"How we built it","content":"Frontend : Next.js + Tailwind for speed and flexibility. Backend : Python with FastAPI to handle requests efficiently. AI Integration : Groq as the inference engine powering LaTeX generation, file summarization, mp3 transcription, and embedding-based retrieval. Gemini flash as the reasoner, deciding on agent tool calls, speeding up inference times! Extras : We connected LaTeX compilation and Manim rendering into the pipeline so everything works in-browser without the typical setup headaches."},{"heading":"Challenges we ran into","content":"Getting LaTeX compilation and Manim rendering to work smoothly in a browser context. Managing asynchronous agent workflows (AI + compilation + file processing) without things breaking mid-stream. Designing a frontend experience that feels fast and intuitive despite the heavy lifting happening behind the scenes. Debugging‚Ä¶ lots of debugging."},{"heading":"Accomplishments that we're proud of","content":"Built a working prototype of an AI-powered LaTeX + Manim editor , something we couldn‚Äôt find anywhere else. Successfully integrated multimodal input (notes, documents, audio) into the pipeline. Learned to wrangle AI outputs into valid, compilable LaTeX ! Showed that it‚Äôs possible to merge the power of agentic workflows with a typesetting tool students actually want to use."},{"heading":"What we learned","content":"When working with agentic workflows , it‚Äôs critical to design with them in mind from the very beginning. Retrofitting agents into an existing pipeline is messy and inefficient. LaTeX is powerful but extremely sensitive AI outputs need guardrails, validation, and retries to ensure documents actually compile. We got hands-on experience balancing usability (smooth UI/UX) with technical depth (AI, rendering, pipelines). And most importantly: the line between an AI-powered helper and a frustrating black box is thin, design choices matter a lot."},{"heading":"What‚Äôs next for LeafPilot","content":"Expanding agentic workflow support to make the AI more autonomous and context-aware. Adding real-time collaborative editing (think ‚ÄúGoogle Docs meets Overleaf with AI‚Äù). More export options : slides, interactive HTML, and richer animation controls. Improved error handling so users spend less time chasing LaTeX bugs and more time creating. Ultimately, we want LeafPilot to become the go-to AI-powered typesetting environment for students, researchers, and creators."},{"heading":"Built With","content":"fastapi groq manim next.js python shadcn tailwind typescript zustand"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"QNXMobility ","project_url":"https://devpost.com/software/canqnx","tagline":"In a world where productivity is everything, every second counts, time spent away from the desk is time wasted. With, QNXMobility move around your workspace with ease, speed and greater efficiency.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/742/327/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"QNX: Best Use of QNX: Hardware and Software Prizes"}],"team_members":[],"built_with":[{"name":"o-drive","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"qnx","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Our team happened to have access to an a few hub motors used for bracket bots early in the competition. With our team being robotics enthusiasts, having access to these motors meant we had to build something that moves. We decided to build a swerve drive base as it allows driving in any direction and decided to put a chair on it as a swerve completely changes up how people can get from point to point without every having to get up. We wanted to implement a unique way of control which would let the user tilt in the direction they want to go to. The more they tilt, the faster they travel!"},{"heading":"What it does","content":"The control uses and IMU to get a reading of the users posture. A neutral position keeps the user in their location, but deliberate leaning starts to move the user slowly in the direction their choice. Each swerve module consists of a drive motor and a turn motor. The turn motor lets the module face in any direction, allowing for driving forward and backward, side to side and even at diagonals. The drive motor control the speed at which the user is moving."},{"heading":"How we built it","content":"The drive base was built around a custom designed swerve drive module. The swerve drive module consisted of the hub motor, connected to the turn motor using a hex shaft. The housing for the motors and shafts were 3d printed and mounted on to a piece of aluminum extrusion which served as the body of the module. We copied this design 4 times, and build an aluminum extrusion frame for the hardware to mount on to. We CNC cut some plates out of aluminum which is where the modules mounted on to the frame.\n\nThe hardware is controlled using o-drive FOC controllers for brushless materials, The core of the software was the QNX raspberry pi kit, which interfaced with an IMU which determined how the swerve would move. The pi would do some of the kinematics before sending commands to the o-drive controllers."},{"heading":"Challenges we ran into","content":"Building this complex of a mechanical system in this amount of time was definitely the biggest challenge. Though we had our design down early, printing, manufacturing, and testing took many hours. We also needed to redesign some parts to be stronger to be able to take on the large loads it would be placed under,\n\nInterfacing this machine with QNX was also a challenge as it didn't natively have libraries built in for what we wanted."},{"heading":"Accomplishments that we're proud of","content":"We are proud of the fact that we could build a working swerve. Building a swerve drive is a difficult to start with, but to build one to carry a person with limited metal machining and time is even harder."},{"heading":"What we learned","content":"We learned to use QNX RTOS without hardware projects. This technology enabled our project to have professional aspects similar to what's used in the industry."},{"heading":"Built With","content":"o-drive python qnx"}]},{"project_title":"Orbit","project_url":"https://devpost.com/software/orbit-59jths","tagline":"Real-time facial recognition that identifies connections, remembers conversations, and maps your professional network","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/743/575/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Windsurf: Best Project built with Windsurf"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Groq"}],"team_members":[],"built_with":[{"name":"cerebras","url":null},{"name":"deepface","url":null},{"name":"fastapi","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"langchain","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"whisper","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/qiuethan/Orbit"}],"description_sections":[{"heading":"Inspiration","content":"Networking and meetings often start with guesswork‚Äîwho's in the room, what do they do, how should you start the conversation? Then afterwards, much of the context is forgotten. We wanted a tool that helps before, during, and after the conversation‚Äîgiving you instant context, saving the discussion, and mapping out relationships."},{"heading":"What it does","content":"Orbit transforms networking with real-time AI:\n\nInstant face recognition pulls public profiles the moment someone enters your view Persistent identity indexing remembers everyone across sessions with full conversation history Live transcription captures every word while generating smart follow-up suggestions Dynamic relationship mapping visualizes connections between people and shared interests\n\nEvery interaction becomes structured, searchable, actionable intelligence."},{"heading":"How we built it","content":"This is technically insane for a 36-hour hackathon:\n\nCustom computer vision pipeline with OpenCV + DeepFace + FaceCheckID handling multiple faces simultaneously through parallel processing Real-time multithreading architecture achieving sub-second latency via WebSocket connections Persistent face indexing creates searchable memory‚Äîinstant recognition with full conversation history when someone reappears Live conversation intelligence using Whisper + LLMs for real-time context analysis and follow-up generation Advanced relationship mapping with Sigma.js algorithms connecting people through shared interests Polished React interface hiding massive technical complexity behind intuitive design"},{"heading":"Challenges we ran into","content":"Real-time performance while orchestrating multiple heavy AI models simultaneously Reliable face indexing across lighting conditions and multiple participants Complex data pipeline from video ‚Üí recognition ‚Üí scraping ‚Üí analysis without latency bottlenecks"},{"heading":"Accomplishments that we're proud of","content":"Revolutionary networking experience never built before‚Äîreal-time social intelligence through computer vision Technical breakthrough : Parallel face recognition, conversation indexing, and relationship mapping working seamlessly Exceptional UX : Complex AI systems hidden behind interface anyone can use instantly"},{"heading":"What we learned","content":"Real-time AI orchestration requires careful architecture‚Äîevery millisecond matters for instant user experience Computer vision complexity multiplies in real-world conditions with lighting, angles, and multiple faces"},{"heading":"What's next for Orbit","content":"Enhanced recognition accuracy under challenging conditions Privacy-first architecture with comprehensive opt-in controls Enterprise deployment for conferences and professional networking at scale"},{"heading":"Built With","content":"cerebras deepface fastapi groq langchain opencv python react whisper"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SpeakEasy","project_url":"https://devpost.com/software/speakeasy-arzjmb","tagline":"SpeakEasy gives nonverbal users a voice. Built on Snap Spectacles, it detects the world around them, offers smart prompts, and speaks responses instantly.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/315/datas/medium.jpeg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Groq"}],"team_members":[],"built_with":[{"name":"bluetooth","url":"https://devpost.com/software/built-with/bluetooth"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"lensstudio","url":null},{"name":"spectacles","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/ammiellewb/SpeakEasy-public"}],"description_sections":[{"heading":"Inspiration","content":"We were inspired by how isolating conversations can feel without a voice. Many existing assistive communication tools are bulky or require users to look away from others, making social interactions even harder. We wanted to create something that blends into daily life and helps nonverbal users feel confident and included in conversations."},{"heading":"What it does","content":"SpeakEasy is an assistive communication device built on Snapchat Spectacles to empower people who are nonverbal or speech-impaired. Using an object detection system powered by Gemini and accelerated by Groq, the glasses analyze the user‚Äôs surroundings and generate context-aware conversation prompts on an interactive AR keyboard built in Lens Studio. Users select responses through a small handheld controller, which are then spoken aloud through the glasses."},{"heading":"How we built it","content":"We integrated Gemini‚Äôs object detection model to understand the user‚Äôs surroundings and ran it through Groq for ultra-fast language generation. We then built a custom AR keyboard in Lens Studio where prompts are displayed and navigated via a handheld controller. Finally, the selected response is converted to speech and played through the glasses, completing an end-to-end assistive communication pipeline."},{"heading":"Challenges we ran into","content":"Our biggest challenge was merging all the components together into one smooth experience. We also faced difficulties building the keyboard interface in Lens Studio, as none of us had prior experience with it or other AR platforms. Optimizing for low latency while managing multiple systems (LLM, controller, and speech output) was also challenging. Huge shout-out to the Snap mentors - Alessio, Jesse, and both Stevens - for their patience, insightful suggestions, and time! We learned so much from everything they shared."},{"heading":"Accomplishments that we're proud of","content":"We‚Äôre proud to have taken on a challenging project in a completely new environment and pushed ourselves outside our comfort zones. Even though the system isn‚Äôt fully complete yet, we managed to connect multiple complex components and build the foundation for an assistive tool that could truly make a difference. This experience showed us that real-time object detection and language generation can be combined on wearable hardware - and that we‚Äôre capable of learning fast under pressure."},{"heading":"What we learned","content":"We learned a lot about building for AR and using Lens Studio, from designing custom interfaces to integrating real-time content on Snapchat Spectacles. We also learned how to combine object detection with language generation, optimize inference on low-latency hardware using Groq, and rapidly prototype hardware-software systems while keeping user experience at the centre."},{"heading":"What's next for SpeakEasy","content":"Next steps include enabling real-time object detection directly on the glasses (without interactive previews), expanding prompt options for each detected object, refining the keyboard and interface for faster navigation, attaching a louder speaker to improve conversational clarity, and conducting interviews with our intended user base of nonverbal users."},{"heading":"Built With","content":"bluetooth gemini groq lensstudio spectacles typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"QNX-RasPi-Setup-Util","project_url":"https://devpost.com/software/qnx-raspi-setup-util","tagline":"Configuring your QNX Raspberry Pi can be a long and boring process. Accelerate it with QNX-RasPi-Setup-Util, do all the essential steps through an interactive program in the simple way!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/739/763/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"QNX: Best Use of QNX: Hardware and Software Prizes"}],"team_members":[],"built_with":[{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"cmake","url":null},{"name":"ftxui","url":null},{"name":"qnx","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Qubik65536/QNX-RasPi-Setup-Util/"}],"description_sections":[{"heading":"Inspiration","content":"Manually configuring the QNX Raspberry Pi from scratch is a long and complicated process, and often require the users to have terminal and Linux/Unix related skill and experiences. Coming from a robotics development background, a lot of people working on the robot may not have such experiences, hence the need of a tool that makes the process simpler and quicker."},{"heading":"What it does","content":"QNX-RasPi-Setup-Util is a easy to use interactive program that allows the user to easily configure all the essential stuff on a QNX Raspberry Pi, without having to mess around with different configuration files, terminal file editors, and other stuff, allowing everyone to quickly configure the Raspberry Pi with QNX OS on it in the way they need."},{"heading":"How we built it","content":"The program is written in C++ with CMake as the build system. QNX Software Development Platform 8.0, QNX Toolkit, and Docker Build Environment for QNX on macOS are used to power the program development and building environment. FTXUI is used when the terminal supports UTF-8 to provide a more intuitive user interface when possible."},{"heading":"Challenges we ran into","content":"QNX requires a unique set of tools (e.g. compilers) to have the software built for it. Which took me a relatively long time to properly configure and develop an efficient workflow to build, upload, and test the program."},{"heading":"Accomplishments that we're proud of","content":"Since the environment I am using is not officially supported by QNX, I've developed some customized scripts to automate the development workflow that involves code compiling and executable uploading and testing that makes the workflow more efficient and usable."},{"heading":"What we learned","content":"This is the first time I write any user application with C++, and alongside the extra C++ knowledges I gained that are more particular to building a program for a user to interact with, I also learned a lot about configuring C/C++ toolchain while tinkering with the project."},{"heading":"What's next for QNX-RasPi-Setup-Util","content":"In the future, the terminal user interface of the program needs to be more polished. Also, looking back at what inspired this project, configuration export and snapshot features can be developed, as this program is meant to make the configuration process of the QNX Raspberry Pi easier, and these features allows the user to deploy multiple Raspberry Pi with the same configuration if needed, and if any faulty configuration changes is being done, the snapshot feature can be used to roll it back to a working state as long as the terminal is usable. A binary upload feature can also be done which could facilitate the process of uploading and/or updating programs written for QNX OS for it to be run."},{"heading":"Built With","content":"c++ cmake ftxui qnx"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"LangSketch","project_url":"https://devpost.com/software/langsketch","tagline":"LangSketch is a visual agent builder that leverages Databricks for data processing and Martian rule routing for affordable drag-and-drop agent creation, making agentic workflows accessible for all.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/672/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Martian: LLM Routing with Martian"}],"team_members":[],"built_with":[{"name":"appimage","url":null},{"name":"asteval","url":null},{"name":"chart.js","url":"https://devpost.com/software/built-with/chart-js"},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"databricks","url":null},{"name":"databricks-vector-search","url":null},{"name":"dateparser","url":null},{"name":"deimos-router","url":null},{"name":"dmg","url":null},{"name":"dotenv","url":null},{"name":"electron","url":"https://devpost.com/software/built-with/electron"},{"name":"fastapi","url":null},{"name":"font-awesome","url":"https://devpost.com/software/built-with/font-awesome"},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"google-fonts","url":"https://devpost.com/software/built-with/google-fonts"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"langchain","url":null},{"name":"langgraph","url":null},{"name":"linux","url":"https://devpost.com/software/built-with/linux"},{"name":"macos","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"nsis","url":null},{"name":"openai-gpt","url":null},{"name":"pydantic","url":null},{"name":"pypdf2","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"requests","url":"https://devpost.com/software/built-with/requests"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"uvicorn","url":null},{"name":"windows","url":"https://devpost.com/software/built-with/windows"}],"external_links":[{"label":"github.com","url":"https://github.com/rajshah6/langsketch"}],"description_sections":[{"heading":"Inspiration","content":"The world of AI agents and autonomous workflows has long been dominated by complex frameworks, such as LangChain and LangGraph, creating a steep learning curve that excludes many potential users. We envisioned a future where anyone, regardless of their technical background, could harness the power of agentic workflows through intuitive visual design tools. LangSketch was born from this vision: a no-code, visual development environment that transforms the complexity of agentic workflows into an accessible, drag-and-drop experience."},{"heading":"What it does","content":"LangSketch is a comprehensive platform that democratizes agentic AI development through a visual, no-code interface. Users can drag and drop agents and functions onto a canvas, connect them visually to create workflows, and instantly compile them into production-ready Python code. The platform leverages the Martian Deimos router for intelligent, cost-optimized model selection, automatically choosing the best AI model based on task complexity and agent capabilities. Our deep Databricks integration offers enterprise-grade data management, featuring Delta Lake storage, RAG embeddings for document retrieval, and comprehensive analytics dashboards that track performance, costs, and execution metrics in real-time. It removes the barriers of learning complex APIs while maintaining the full power and flexibility of modern AI agent systems."},{"heading":"How we built it","content":"We built LangSketch using a multi-layered architecture combining Electron for the desktop interface, Python for the agent runtime, and Databricks for enterprise analytics. The visual canvas system enables drag-and-drop workflow creation with real-time connection management. The Martian Deimos router integration offers sophisticated model selection through custom routing rules that analyze agent capabilities, task complexity, and message length to automatically select between Claude, GPT-4, and other models for an optimal cost-performance balance. Our Databricks integration leverages Delta Lake for ACID transactions and time-travel queries, stores all agent executions with comprehensive metadata, and enables RAG embeddings through vector search capabilities. The platform uses Databricks notebooks for data processing and visualization. At the same time, the dynamic agent runtime supports dynamic tool loading, comprehensive input/output validation with Pydantic schemas, and visual-to-code compilation that generates production-ready Python from visual workflows."},{"heading":"Challenges we ran into","content":"The biggest challenges included managing complex state across the visual canvas, agent runtime, and analytics systems, which were solved by building a centralized state management system. Creating intuitive visual connections between workflow components required developing a sophisticated connection system with real-time preview, collision detection, and dynamic redrawing during zoom/pan operations. Implementing intelligent model routing that truly understands agent capabilities required comprehensive agent description generation and context-aware routing rules. Finally, providing enterprise-grade data management and analytics demanded deep integration with Databricks, including Delta Lake storage, RAG embeddings pipelines, and real-time performance monitoring dashboards."},{"heading":"Accomplishments that we're proud of","content":"We're most proud of creating the first visual-to-code compilation system that generates production Python from drag-and-drop workflows, achieving up to 60% cost reduction through intelligent model routing, and building a truly no-code interface that makes complex agentic AI accessible to non-technical users. The platform successfully combines enterprise-grade capabilities (Delta Lake storage, ACID transactions, comprehensive analytics) with an intuitive visual interface. We also pioneered context-aware model selection that automatically chooses the optimal AI model based on agent capabilities and task complexity, while maintaining complete transparency in routing decisions."},{"heading":"What we learned","content":"We learned that democratizing complex technology requires not just simplifying the interface, but fundamentally rethinking the entire development workflow. The key insight was that users don't want to learn frameworks; they want to solve problems. We discovered that intelligent routing isn't just about cost optimization, but about matching the right model to the right task based on a comprehensive understanding of agent capabilities. We also learned that enterprise users need both simplicity and power, requiring us to build sophisticated backend systems while maintaining an intuitive frontend. Most importantly, we realized that visual programming can be just as powerful as traditional coding when combined with intelligent compilation and comprehensive tooling."},{"heading":"What's next for LangSketch","content":"We're expanding LangSketch to support multi-modal agents for image, audio, and video processing, enabling real-time collaborative workflow design, and building a marketplace for community-driven agent and tool sharing. A significant focus is on adding visual fine-tuning capabilities that leverage Databricks MLflow for model training and optimization, allowing users to customize models directly within the visual interface. We're also developing advanced Martian routing rules for specialized use cases, such as domain-specific model selection and custom performance optimization strategies. Additionally, we're working on predictive performance modelling and optimization features, advanced analytics with machine learning insights, and integration with more enterprise data sources. The goal is to create a complete ecosystem where users can design, deploy, monitor, fine-tune, and optimize agentic workflows entirely through visual interfaces, making AI development truly accessible to everyone while maintaining enterprise-grade capabilities and performance."},{"heading":"Built With","content":"appimage asteval chart.js css databricks databricks-vector-search dateparser deimos-router dmg dotenv electron fastapi font-awesome git google-fonts html javascript json langchain langgraph linux macos node.js nsis openai-gpt pydantic pypdf2 python requests sqlite uvicorn windows"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Help Me If You Can","project_url":"https://devpost.com/software/help-me-if-you-can","tagline":"A task & reward hub with AI-powered candidate matching and XRPL payments","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/742/610/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Ripple: Best in Ledger Award"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"ripple","url":"https://devpost.com/software/built-with/ripple"},{"name":"rlusd","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Afformativ/HackTheNorth"}],"description_sections":[{"heading":"Inspiration","content":"While working on my thesis, I sometimes needed feedback on my code in programming languages I wasn‚Äôt fully comfortable with. Having someone quickly review and validate my work would save me a lot of time and frustration. That pain point inspired me to build a platform where anyone can post a task, get expert help, and be confident in the quality of the result. At the same time, I wanted to experiment with RLUSD, a new and not yet widely adopted stablecoin on XRPL. Integrating it into real task flows felt like the perfect opportunity to explore its potential and push its adoption in practical use cases."},{"heading":"What it does","content":"Help Me If You Can is a decentralized task and reward hub with three core parts: 1. Tasks & Rewards ‚Äì Users create tasks, fund them with RLUSD or XRP, and assign solvers. Payments are held in escrow (native XRPL) or custodial holds (RLUSD) until work is approved. 2. AI Candidate Ranking ‚Äì Gemini AI ranks applicants based on their skills and solved history, giving task creators clear recommendations. 3. AI Task Review Assistance ‚Äì AI checks submissions (length, originality, relevance) and can suggest rework before final approval."},{"heading":"How we built it","content":"Frontend: React Backend: Flask + MongoDB, with JWT authentication, wallet management, and XRPL/RLUSD payment integration. Payments: Rippled / XRPL testnet escrow for XRP, and custodial logic for RLUSD. AI: Gemini API for ranking candidates and reviewing solutions;"},{"heading":"Challenges we ran into","content":"Handling XRPL escrow transactions correctly (preventing tecNO_PERMISSION and ensuring funds are released only with valid conditions). Integrating Gemini AI for ranking candidates in a way that feels useful and not arbitrary. Also i tried to implement voice AI here, but unfortunately was not managed to do this. Frontend also was kind of a problem, because it is a lot of different elements."},{"heading":"Accomplishments that we're proud of","content":"Seamless XRPL testnet integration, successfully funding wallets and paying out in both RLUSD and XRP."},{"heading":"What we learned","content":"How to use AI and implement RLUSD payments in everyday life."},{"heading":"What's next for Help Me If You Can","content":"Add voice AI, do better design, add more features from rippled"},{"heading":"Built With","content":"flask gemini mongodb python react ripple rlusd"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Token Turrets ","project_url":"https://devpost.com/software/token-turrets","tagline":"Play, Pay, Pulverize - on chain! ‚õìÔ∏è‚Äçüí•","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/291/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Ripple: Best in Ledger Award"}],"team_members":[],"built_with":[{"name":"escrow","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"ipfs","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"nft","url":null},{"name":"nfts)","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"tcp","url":"https://devpost.com/software/built-with/tcp"},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"webgl","url":"https://devpost.com/software/built-with/webgl"},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"},{"name":"xrpl","url":null},{"name":"xrpl-(rlusd","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/KlausMikhaelson/hackthenorth"},{"label":"tokenturrets.vercel.app","url":"https://tokenturrets.vercel.app"}],"description_sections":[{"heading":"üöÄ Inspiration","content":"We wanted to make online competition feel real. The game is skill, tanks, and action - but the stakes are on-chain.\n\nBy weaving XRPL features like RLUSD (a USD-pegged token), escrow, and tokenization directly into the match flow, we created a trustless tournament model : stake ‚Üí battle ‚Üí payout. No middleman, no organizer - just players competing, with XRPL guaranteeing the rewards.\n\nLive in production here! https://tokenturrets.vercel.app"},{"heading":"üí° What it does","content":"Token Turrets is a real-time multiplayer tank battle where anyone can create a room, start a tournament pot, and let the players fight it out. Each participant stakes RLUSD to enter, the pool is locked in escrow, and when the battle ends, XRPL automatically pays the winner.\n\nOn top of that, players can personalize their tanks in our skin store , where every skin purchased with RLUSD is minted as a unique NFT and stored on-chain. This turns cosmetic purchases into verifiable, tradable assets - bridging gameplay, payments, and tokenization."},{"heading":"üõ†Ô∏è How we built it","content":"Networking & gameplay : Real-time tank arena using WebSockets, TCP protocol, and Three.js for smooth multiplayer action. Tournament flow : Any player can spin up a room ‚Üí others stake RLUSD to join ‚Üí our backend secures the pot in XRPL escrow ‚Üí the winner is determined by in-game results ‚Üí escrow automatically releases payouts. Skin store : A full in-game shop where skins are purchased with RLUSD. Each skin is minted as an NFT on XRPL, giving players unique, ownable assets tied to their tanks. Trustless by design : Because both payments and assets live on-chain, players don‚Äôt have to trust an organizer. The system itself guarantees fairness and ownership."},{"heading":"‚öîÔ∏è Challenges we ran into","content":"Synchronizing real-time gameplay with on-chain financial flows without slowing things down. Designing escrow and NFT minting flows that feel seamless in-game. Balancing scope: building multiplayer networking + escrow + NFT skin store all under hackathon time constraints."},{"heading":"üèÜ Accomplishments that we‚Äôre proud of","content":"Built a real-time multiplayer tank game from scratch and fully integrated XRPL. Created a trustless tournament model with RLUSD staking and escrow. Launched an on-chain skin store where every purchase mints an NFT, giving players verifiable ownership. Showcased XRPL‚Äôs power across payments, escrow, and tokenization in one cohesive MVP."},{"heading":"üîÆ What‚Äôs next for Token Turrets","content":"Building a secondary marketplace for NFT skins, enabling trading and rarity-driven economies. Introducing seasonal rewards and governance features to make Token Turrets a community-run esport. Positioning the game as a showcase for how XRPL can power trustless, on-chain esports ecosystems."},{"heading":"Built With","content":"escrow express.js ipfs javascript mongodb nft nfts) node.js tcp three.js typescript webgl websockets xrpl xrpl-(rlusd"},{"heading":"Try it out","content":"github.com tokenturrets.vercel.app"}]},{"project_title":"Honkanomics","project_url":"https://devpost.com/software/honkanomics","tagline":"Learn investing through AI-tailored lessons, playful goose gamification, and real portfolio simulations linked to RBC InvestEase","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/084/datas/medium.jpeg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"RBC: RBC InvestEase - Investing for the Future"}],"team_members":[],"built_with":[{"name":"gemeni","url":null},{"name":"investease","url":null},{"name":"next.js","url":null},{"name":"openai","url":null},{"name":"radix","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"shadcn","url":null},{"name":"supabase","url":null},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"www.honkanomics.com","url":"https://www.honkanomics.com/"},{"label":"github.com","url":"https://github.com/svn05/v2webhonkonomics"}],"description_sections":[{"heading":"Inspiration","content":"We combined the habit mechanics of language apps with paper-trading sandboxes to help beginners build confidence."},{"heading":"Built With","content":"gemeni investease next.js openai radix react shadcn supabase tailwind typescript"},{"heading":"Try it out","content":"www.honkanomics.com github.com"}]},{"project_title":"Money Talks","project_url":"https://devpost.com/software/money-talks-d93izn","tagline":"Clean cash I want you, Clean cash I need you. Upload one credit card monthly statement and receive thousands of insights, budgets and roadmaps in a second!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/768/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"RBC: RBC InvestEase - Investing for the Future"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"investeasyapi","url":null},{"name":"martian","url":null},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"zillow","url":"https://devpost.com/software/built-with/zillow"}],"external_links":[{"label":"github.com","url":"https://github.com/KadenDaya/Money-Talks"}],"description_sections":[{"heading":"Inspiration","content":"As students ourselves, we were frustrated by the complexity and intimidation factor of traditional financial planning tools. Most budgeting apps are either too basic (just tracking expenses) or too complex (requiring financial expertise we didn't have). We wanted something that would:\n\nMake finance approachable for everyday students who are just starting their financial journey Spark curiosity about budgeting and investing without the intimidating jargon Provide real insights from actual spending data, not just generic advice Make financial planning fun through humor and engaging interactions\n\nWe noticed that while we had access to bank statements and spending data, there was no easy way to extract meaningful insights from them. Traditional tools required manual categorization and didn't provide the \"aha moments\" that would actually change spending behavior."},{"heading":"What it does","content":"Money Talks transforms raw bank statements into actionable financial insights through:\n\nüîç Smart PDF Processing\n\nAI-powered extraction using Martian API to parse messy bank statement PDFs Automatic categorization of transactions (groceries, entertainment, transportation, etc.) Data cleaning that handles OCR errors and inconsistent formatting\n\nUnlimited Analytics Dashboard\n\nInteractive visualizations with pie charts, bar charts, and trend analysis AI insights that highlight spending patterns and opportunities Category-specific analysis with merchant-level breakdowns Time-based trends showing daily, weekly, and monthly spending patterns\n\nüéØ Investment Planning & House Buying Analysis\n\nGoal-based planning with customizable prompts (house, car, general savings) Risk tolerance assessment with 5 different investment strategies Compound growth calculations showing potential returns over time Real estate integration with actual house listings in your area RBC API integration for realistic investment projections\n\nÔøΩÔøΩ Engaging & Humorous Insights\n\n\"Insult-style\" feedback that's funny but educational (e.g., \"You spent $600 on Uber. Maybe go for a walk next time, you need it.\") Contextual advice that's specific to your actual spending patterns Martian credits awareness - acknowledging that this is student money being analyzed\n\nüèóÔ∏è Hybrid Architecture\n\nSQLite database for reliable local storage and fast queries Databricks integration for advanced analytics and visualization Smart data sync between local and cloud systems Event-ready demo perfect for showcasing at hackathons and conferences"},{"heading":"How we built it","content":"Frontend Stack\n\nNext.js 15 with App Router for modern React development Shadcn/ui + Tailwind CSS for beautiful, responsive components Recharts for interactive data visualizations TypeScript for type safety and better development experience\n\nBackend Architecture\n\nFastAPI for high-performance API endpoints SQLite for reliable local data storage Databricks for advanced analytics and cloud processing Martian API for intelligent PDF parsing and data extraction and cleaning, probably the most abused API ever used (went crazy, and costed us $0.11 in credits)\n\nKey Technical Features\n\nPDF Processing Pipeline : Raw PDF ‚Üí Text extraction ‚Üí AI parsing ‚Üí Structured data Real-time Analytics and AI insights : SQL queries optimized for dashboard performance Investment Calculations : Compound interest formulas with risk-adjusted returns House Search Integration : Real estate API calls with location-based filtering\n\nDevelopment Approach\n\nModular design with separate components for dashboard, investments, and trends Error handling with graceful fallbacks for API failures Performance optimization with caching and efficient data queries User experience focus with loading states and interactive feedback"},{"heading":"Challenges we ran into","content":"PDF Processing Complexity\n\nOCR Quality Issues : Bank statements often have poor scan quality, leading to garbled text Format Variations : Every bank formats statements differently, making parsing unpredictable Data Extraction Accuracy : Extracting amounts, dates, and merchants consistently across different formats Solution : Implemented robust error handling and fallback parsing with the Martian API\n\nPerformance Optimization\n\nDatabase Query Speed : Initial dashboard queries were taking 10+ seconds Real-time Analytics : Balancing detailed insights with response time Memory Management : Large PDF files causing memory issues during processing Solution : Implemented query optimization, caching, and streaming file processing\n\nUser Experience Design\n\nMaking Finance Fun : How to present serious financial data in an engaging way Balancing Humor : Making \"insults\" funny but not discouraging Information Overload : Presenting complex analytics without overwhelming users Solution : Developed contextual tooltips, progressive disclosure, and personality-driven insights\n\nIntegration Complexity\n\nMultiple APIs : Coordinating Martian, Databricks, and RBC APIs Data Synchronization : Keeping SQLite and Databricks in sync Authentication : Handling user sessions across different services Solution : Built a hybrid architecture with smart fallbacks and optional integrations"},{"heading":"Accomplishments that we're proud of","content":"üöÄ Technical Achievements\n\nComplete end-to-end pipeline from PDF upload to actionable insights Real-time dashboard with interactive visualizations and live data Advanced analytics using Databricks SQL with window functions and statistical analysis Investment modeling with compound growth calculations and risk assessment House buying analysis with real estate integration and location-based recommendations\n\nüéØ User Experience Wins\n\nMade finance approachable for students who previously avoided budgeting Created engaging insights that actually change spending behavior Built a fun, shareable platform that users want to show their friends Developed contextual advice specific to individual spending patterns\n\n** Architecture Success**\n\nHybrid data strategy combining reliability of SQLite with power of Databricks Event-ready demo that works reliably for presentations and showcases Scalable design that can handle multiple users and large datasets Clean, maintainable code with proper separation of concerns\n\nüé™ Innovation Highlights\n\n\"Insult-based\" financial coaching that's funny but effective Martian credits integration acknowledging the student context Real estate goal setting making long-term planning tangible Shareable insights that create social engagement around financial health"},{"heading":"What we learned","content":"Technical Insights\n\nPDF processing is harder than expected - OCR quality and format variations create significant challenges Performance optimization is crucial - Users expect instant feedback, especially for financial data Error handling is essential - Financial applications need to be reliable and graceful when things go wrong API integration complexity - Coordinating multiple services requires careful architecture planning. AI as a microservice - Martian saved us so much time and money, including dealing with edge cases we would be pulling our hairs with.\n\nUser Experience Discoveries\n\nHumor makes finance approachable - Students respond much better to funny insights than serious warnings Context matters - Generic advice is less effective than insights based on actual spending data Visualization is powerful - Charts and graphs make abstract financial concepts concrete Goal setting motivates action - Connecting current spending to future goals (like buying a house) drives behavior change\n\nProduct Development Lessons\n\nStart with user pain points - Our own frustration with existing tools drove the solution Iterate based on feedback - The \"insult\" feature evolved from user testing and feedback Focus on core value - PDF processing and analytics are the foundation, everything else is enhancement Build for your audience - Student-specific features (Martian credits, house buying) resonate more than generic tools\n\nBusiness Insights\n\nThere's a gap in the market for student-focused financial tools that are both powerful and approachable Social features drive engagement - Shareable insights and funny feedback create viral potential Integration opportunities - Banks and financial institutions could benefit from better data visualization Event showcase value - The platform works well for demonstrating advanced analytics capabilities"},{"heading":"What's next for Money Talks","content":"üéØ Immediate Enhancements\n\nMobile app development - Native iOS/Android apps for easier statement uploading Enhanced AI insights - More sophisticated spending pattern recognition and predictions Social features - Friend comparisons, spending challenges, and group budgeting Expanded integrations - Support for more banks and financial institutions\n\nüìà Advanced Analytics\n\nPredictive modeling - Forecast future spending and savings potential Behavioral insights - Identify spending triggers and suggest interventions Goal tracking - Progress monitoring with milestone celebrations Risk assessment - Personal financial health scoring and recommendations\n\nüåç Platform Expansion\n\nMulti-language support - Serve international students and users Institutional partnerships - Work with universities to provide financial literacy programs API marketplace - Allow third-party developers to build on our platform White-label solutions - Offer our technology to banks and fintech companies\n\nüé™ Community Building\n\nStudent ambassador program - Recruit campus representatives to spread awareness Financial literacy content - Educational resources and tutorials Mentorship network - Connect students with financial professionals Gamification - Points, badges, and challenges to make financial health fun\n\nüöÄ Technical Roadmap\n\nMachine learning integration - Personalize insights based on user behavior Real-time processing - Instant insights as transactions occur Advanced security - Bank-level encryption and fraud detection Scalability improvements - Handle millions of users and transactions\n\nMoney Talks started as a solution to our own financial planning frustrations, but it's become a platform that could genuinely help students everywhere take control of their finances. By making budgeting fun, insights actionable, and goals tangible, we're building the financial literacy tool we wish we had when we started college."},{"heading":"Built With","content":"fastapi investeasyapi martian next.js python typescript zillow"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"RippleRelief","project_url":"https://devpost.com/software/ripplerelief","tagline":"Every donation, tracked and trusted.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/741/660/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Ripple: Best in Ledger Award"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"ripple","url":"https://devpost.com/software/built-with/ripple"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/peterson-htn25/HTN252"}],"description_sections":[{"heading":"Inspiration","content":"When disasters strike, billions in aid are pledged, but much of it never reaches the people who need it most. Funds are delayed, misused, or lost in bureaucracy.\n\nAs a team, we asked: What if every donor could see their impact, and every recipient could trust the system delivering it?\n\nThis inspired us to build RippleRelief, a transparent aid ecosystem powered by the XRP Ledger (XRPL) ."},{"heading":"What it does","content":"RippleRelief is a web app that connects donors, NGOs, recipients, and merchants in a transparent aid ecosystem:\n\nDonors contribute funds to a pooled XRPL wallet. NGOs issue digital vouchers (IOUs) directly on XRPL. Recipients receive those vouchers as QR/SMS codes. Merchants redeem vouchers instantly for stablecoins/XRP. The web app includes a donor dashboard (to track impact), NGO interface (to issue and manage vouchers), and merchant portal (to scan and redeem vouchers). Every step is recorded on XRPL, making the flow auditable and tamper-proof."},{"heading":"How we built it","content":"Frontend: RippleRelief is a React web app built with Next.js + Tailwind CSS for a responsive, mobile-first interface. We designed separate flows for donors, NGOs, merchants, and recipients, all integrated into one ecosystem. Backend: A Flask server acts as the core API, handling requests from the frontend, connecting to XRPL via the xrpl-py SDK, and managing identity + fraud logic. Blockchain Integration: We used the XRP Ledger (XRPL) for donation tracking, decentralized identifiers (DIDs), and instant settlement. Every transfer is transparent and auditable. Face ID Wallets: Recipients access aid through Face Recognition wallets. We generate encrypted facial embeddings with liveness checks and bind them to wallet keys, ensuring passwordless, fraud-resistant authentication. Payments: Integrated Stripe so donors can give in fiat (credit card, bank, etc.), while each donation is mirrored on XRPL as a transparent, traceable transaction. Database: Chose Supabase for user management, transaction logs, NGO/merchant registries, and fraud detection. Supabase gave us a Postgres backend plus built-in authentication and real-time APIs."},{"heading":"Challenges we ran into","content":"Biometric wallet access: Implementing Face ID‚Äìsecured wallets required balancing security vs. privacy. We had to learn about embeddings, liveness checks, and how to keep biometric data off-chain. System integration: Connecting donors, NGOs, recipients, and merchants meant building multiple flows (web app ‚Üî server ‚Üî XRPL ‚Üî Stripe). Ensuring they worked together under hackathon time pressure was a major challenge. Low-tech accessibility: Designing for disaster zones meant building with constraints in mind ‚Äî intermittent internet, older devices, and non-technical users. Keeping the app simple but secure was harder than expected."},{"heading":"Accomplishments that we're proud of","content":"Learnt how to build on-chain. It was all of our first times building something with a cryptocurrency! Face ID‚Äìsecured wallets: Implemented biometric login for recipients, mapping facial features into embeddings for secure, passwordless access to their aid. End-to-end XRPL integration: Built donor ‚Üí NGO ‚Üí recipient ‚Üí merchant flows entirely on the XRP - Ledger, enabling real-time, auditable aid distribution. Hybrid payments: Connected Stripe with XRPL so a fiat donation could be mirrored on-chain as a traceable transaction."},{"heading":"What we learned","content":"XRPL fundamentals: We learned how to work with the XRP Ledger ‚Äî from issuing tokens and DIDs to integrating it with traditional payment systems like Stripe ‚Äî and how it enables transparent, low-cost financial flows. Ecosystem thinking: Instead of designing a single app, we built an ecosystem connecting donors, NGOs, recipients, and merchants. That shift taught us how system design, user flows, and communication between services (web app ‚Üî server ‚Üî DB ‚Üî ledger) all tie together. Biometric identity: We explored face embeddings, liveness checks, and privacy-preserving storage methods, which gave us experience in balancing security and usability for vulnerable populations. Human-centered design: Building for low-tech users in disaster contexts taught us to simplify interfaces, reduce friction, and make design decisions that don‚Äôt require people to abandon their existing technology. Fintech + blockchain fusion: We discovered how traditional rails (Stripe) and decentralized rails (XRPL) can complement each other, letting donors transact in fiat but still watch their impact transparently on-chain. Collaboration under pressure: Coordinating system engineering, identity flows, and UI design in such a short time reinforced the importance of communication and iteration in a hackathon setting."},{"heading":"What's next for RippleRelief","content":"Pilot program with NGOs: Launch a small-scale test deployment with real donors, recipients, and merchants to validate usability and flows. Fraud prevention & compliance: Auto fraud detection using anomaly detection on XRPL transactions. Production-ready Face ID wallets: Improve biometric login with advanced liveness detection, privacy-preserving embeddings, and fallback mechanisms for low-tech users. Donor transparency dashboard: Richer reporting with impact metrics (e.g., meals delivered, aid redeemed) tied directly to transactions on XRPL. Deeper XRPL features: Incorporate NFTs as impact certificates, stablecoin flows for price stability, and escrows for conditional payouts. Low-tech accessibility: Expand SMS/USSD wallet access for disaster zones with limited smartphones or internet. Scaling the ecosystem: From pilot ‚Üí regional rollout ‚Üí global network where every aid dollar is traceable, secure, and accountable."},{"heading":"Built With","content":"flask javascript node.js python react-native ripple typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Lavoe","project_url":"https://devpost.com/software/lavoe","tagline":"Cursor for Music Production","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/743/577/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Windsurf: Best Project built with Windsurf"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Rox: Best AI Agent"}],"team_members":[],"built_with":[{"name":"ai-sdk","url":null},{"name":"claude","url":null},{"name":"cohere","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"vercel","url":null},{"name":"windsurf","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Leonardomontesqui/Lavoe"}],"description_sections":[{"heading":"Inspiration","content":"Lavoe comes from the simple desire to make music. We wanted to lower the barrier for people to access music production. AI enhances everyone‚Äôs abilities, but in the creative industry, tech hasn‚Äôt given as much attention to accessibility. Our goal is to make it easier for others to express themselves and take the first step toward facilitating music production for everyone , not just those with years of technical training."},{"heading":"What it does","content":"Lavoe provides an intuitive and AI-augmented DAW interface with features designed to streamline creativity:\n\nüéµ AI-generated beats ‚Äì Instantly create drum patterns and rhythms tailored to your track. ‚úÇÔ∏è Agentic chopping of beats ‚Äì Automatically slice, rearrange, and re-contextualize loops. üéö Agentic sorting of sounds ‚Äì Organize samples, instruments, and recordings intelligently. üéõ Agentic music engineering ‚Äì AI suggestions for mixing, EQ, and sound design. üé§ Live audio recording ‚Äì Record vocals and instruments seamlessly within the workflow."},{"heading":"How we built it","content":"We combined modern web frameworks with audio processing and ML tooling:\n\nFrontend : React + TypeScript + Next.js for a responsive DAW-style UI. AI SDK : Vercel AI SDK for seamless prompt orchestration. Backend : Python + FastAPI to handle audio processing and serve the AI pipeline. Audio Processing : Librosa for beat detection, tempo analysis, and waveform manipulation. Cohere/Anthropic/OpenAI : Cohere for language modeling, Pandas + Scikit-learn for dataset wrangling and prototyping."},{"heading":"Challenges we ran into","content":"‚è± Getting audio to play in sync with the time marker . ü§ñ Designing the agentic workflow and making agents communicate with each other. üîó Connecting the backend audio pipeline to the frontend in real-time. ‚úÇÔ∏è Deciding how to chop audio into meaningful segments for remixing. üìù Figuring out the best way to describe audio to LLMs in a way they can understand and act on."},{"heading":"Accomplishments that we're proud of","content":"üåç Taking a step toward democratizing music production with AI-driven tools. üé® Bringing a creative lens to the tech community , showing AI is not just for code or text. üèó Pushing design engineering to new heights by combining DAW workflows with agentic AI. ü•™ And importantly: we are NOT just a GPT wrapper ‚Äî we‚Äôre a sandwich , layering multiple agents, models, and workflows into something truly new."},{"heading":"What we learned","content":"üéß The complexities of audio processing beyond just waveforms. üåÄ Exploring different types of ‚Äúvibecoding‚Äù ‚Äî making the interface feel musical, not mechanical. üñ• How to implement AI agents directly into a UI , balancing usability and power. üîä Handling audio playback in a browser environment, with all its quirks and limitations."},{"heading":"What's next for Lavoe","content":"üöÄ Deployment ‚Äì making Lavoe available for real musicians to use and test. üí∏ Monetization strategies ‚Äì exploring models for subscriptions, plugins, and creator marketplaces. üå± Y Combinator application ‚Äì taking Lavoe beyond a hackathon project and into a scalable startup.\n\nLavoe is about making music production more accessible, intelligent, and collaborative. Just as Cursor changed coding, Lavoe aims to change music creation."},{"heading":"Built With","content":"ai-sdk claude cohere python react vercel windsurf"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SpectraSphere","project_url":"https://devpost.com/software/htn-zq6138","tagline":"Turning snapshots into spaces you can actually explore","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/907/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Snap: Spectacles AR Hackathon: Game On!"}],"team_members":[],"built_with":[{"name":"cohere","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"lens-studio-sdk","url":null},{"name":"postman","url":"https://devpost.com/software/built-with/postman"},{"name":"remote-service-gateway","url":null},{"name":"snap-spectacles","url":null},{"name":"snapchat","url":"https://devpost.com/software/built-with/snapchat"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/tyseer2335/SpectraSphere"}],"description_sections":[{"heading":"üåü Inspiration","content":"It had been years since some of us had last been together in person, and Hack the North gave us that chance. Old friends reconnected ü§ù, and through Raed we met Allen, who quickly became part of the group. By the end of the weekend, we were no longer just teammates; we were friends building something meaningful together.\n\nAs we caught up, we realized how much of our friendship had always been built around storytelling üìñ: sharing memories, retelling old moments, and imagining new ones. Stories are more than just words; they are how we reconnect, how we preserve identity, and how we relive what matters most.\n\nThat realization sparked SpectraSphere ‚ú®. We wanted to make stories something you can not only tell but actually step inside. With Snapchat Spectacles üëì, we set out to reimagine how people experience their own memories, ideas, and imagination. Instead of scrolling through flat images, why not live inside them?"},{"heading":"üõ†Ô∏è What it does","content":"SpectraSphere turns your prompts and images into immersive AR stories üåå that you can step inside using Snapchat Spectacles.\n\nFrom the user‚Äôs perspective, it feels simple and magical:\n\nOn our TypeScript web app üíª, you start by writing a story prompt or narrating one with your voice üéôÔ∏è. You then upload four images üì∑ that capture the essence of your story. With a single click, those inputs are sent directly to your Spectacles üì°. Inside Spectacles, your photos appear around you in a rotatable 3D environment üîÑ. You can instantly switch between creative animation styles üé® such as Ghibli , cyberpunk , comic , and more each reframing your story through a different ‚Äúspectra.‚Äù\n\nBehind the scenes, SpectraSphere works by combining multiple technologies:\n\nCohere API üß† expands and contextualizes the story prompt so the AR experience feels richer than raw text. Gemini API üåê manages image generation and styling to adapt uploaded photos to different artistic modes. Lens Studio SDK üï∂Ô∏è and Spectacles API bring the story into AR, rendering the 3D environment and enabling smooth style transitions.\n\nThe result is a pipeline that moves seamlessly from prompt to photos to immersive AR storytelling .\n\nRight now, SpectraSphere is designed for photos. Looking ahead, we envision extending this to generational video, evolving narratives, and interactive branching stories where memories come alive as moving, dynamic experiences."},{"heading":"‚öôÔ∏è How we built it","content":"We approached SpectraSphere as a full pipeline, from frontend interaction to AR rendering, with careful attention to performance on Snap Spectacles.\n\nFrontend and Backend + APIs\n\nWe built a TypeScript web app üíª as the entry point for storytelling. It handles both text and voice input, manages image uploads, and packages everything into a structured JSON format. The frontend connects directly to Cohere and Gemini through API calls, and performs validation before sending data downstream. We also designed a lightweight Node.js backend üîó to coordinate between APIs and our AR pipeline:\n\nCohere is called first to enrich prompts, producing narrative-friendly text. Gemini is then used for media handling, applying transformations and style presets to the uploaded images. The backend includes custom error handling, request batching, and asset optimization to keep responses fast enough for real-time use inside Spectacles.\n\nAR environment\n\nInside Lens Studio üé≠, we built a system for displaying static assets as immersive 3D environments. This includes:\n\nA panel management system that positions and rotates user images in space. A style-switching module that applies multiple artistic filters on demand.\n\nWe tuned these features for stable performance on Spectacles hardware, with optimizations like power-of-two texture scaling and lightweight shader effects.\n\nTesting and validation\n\nBecause hardware time was limited ‚è±Ô∏è, we built a local Node.js testing suite üß™ to simulate the pipeline. This allowed us to:\n\nValidate API integration with real Cohere and Gemini calls. Benchmark asset loading, memory use, and frame rate. Confirm Lens Studio compatibility before deployment."},{"heading":"üöß Challenges we ran into","content":"Stepping into AR for the first time üï∂Ô∏è\n\nNone of us had built for AR or VR before Hack the North. Only Allen had prior Unity experience, and that became our bridge into Lens Studio. The rest of us had to pick things up from scratch: how components work, the strict inheritance rules in TypeScript files, and how Lens Studio expects scripts to interact with 3D objects. The first hours were frustrating, filled with broken builds and cryptic errors, but gradually we adapted and started thinking in terms of AR instead of web.\n\nGemini integration üîë\n\nMaking the Gemini API key compatible with Snap Spectacles was another major hurdle. Unlike a standard web app, Spectacles access APIs through the Remote Service Gateway, which has strict rules about authentication and security. What should have been ‚Äúone line of code‚Äù turned into an entire debugging session across both the Spectacles environment.\n\nLimited hardware access ‚è≥\n\nOur time with physical Spectacles was limited, which meant most of development happened blind. To work around this, we built a local testing setup that mimicked the pipeline as closely as possible, complete with mocked API responses and asset loading checks. While this kept progress moving, it also forced us to make educated guesses about real-world performance until late in the build.\n\nHackathon constraints ‚è∞\n\nWe also hit the universal hackathon hurdles: sleep-deprived coding sessions, APIs that did not always behave as expected, and tough tradeoffs. For example, we wanted to experiment more deeply with generative video, but Gemini‚Äôs rate limits and our time window meant focusing on photo panels and text to speech instead of video generation."},{"heading":"üèÜ Accomplishments that we're proud of","content":"Bridging web and AR environments üåâ\n\nOne of our proudest achievements was successfully connecting a modern web stack with Snap Spectacles. Moving data from a TypeScript application into Lens Studio required us to reconcile two very different development paradigms. We built a consistent pipeline that packages user prompts and images in a format Spectacles could reliably interpret. This integration made it possible for Spectrasphere to feel seamless to the user, despite the complexity happening under the hood.\n\nOvercoming Gemini integration barriers üîí\n\nGetting Gemini to work within the constraints of the Spectacles environment was another milestone. Through the Remote Service Gateway, we had to solve authentication issues, handle asynchronous responses, and adapt outputs for rendering in Lens Studio. By the end of the hackathon, we had a functioning system where Gemini-generated content flowed directly into AR, a result that required both persistence and careful engineering.\n\nRapid mastery of Lens Studio ‚ö°\n\nMost of the team had never touched AR before, yet in under two days we went from confusion to competence with Lens Studio. We learned how components, inheritance rules, and TypeScript scripts fit together in a 3D space. By the final demo, we had a working AR environment that positioned panels correctly and allowed creative style switching. The speed at which we adapted to these tools is something we take pride in."},{"heading":"üìö What we learned","content":"Planning before executing üìù\n\nWe discovered quickly that jumping straight into development without mapping out the user flow created unnecessary setbacks. Early attempts to link the frontend, APIs, and Lens Studio led to broken states and confusing errors. Taking a step back to plan the pipeline, from prompt to images to AR rendering, proved essential. This reinforced how important architecture and data flow design are, even under hackathon pressure.\n\nSimplifying the stack üß©\n\nIt was tempting to keep adding new technologies to cover gaps, but we learned that simplicity is often more powerful. Instead of experimenting with extra services, we focused on making Cohere, Gemini, and Lens Studio work reliably together. Keeping the stack lean allowed us to spend more time polishing the experience and less time wrestling with unnecessary complexity.\n\nRapidly learning new tools üöÄ\n\nFor most of the team, Lens Studio was completely unfamiliar. We had to quickly adapt to its component system, TypeScript scripting rules, and the way it handles 3D environments. This showed us that even without prior AR experience, strong programming fundamentals and a willingness to learn can get you from zero to a working demo in a short time."},{"heading":"üîÆ What's next for HTN","content":"Seamless integration with Snapchat Memories üì≤\n\nOne of our first priorities is to connect SpectraSphere directly to Snapchat Memories. Instead of manually uploading images, users could select past experiences already saved in their accounts, and instantly relive them in AR. This would allow stories to be created and experienced with minimal friction, making SpectraSphere feel like a natural extension of everyday life.\n\nExpanded creative toolset üé®\n\nWe plan to broaden the style library far beyond the initial Ghibli, cyberpunk, and comic filters. Future iterations will allow users to layer effects, mix artistic aesthetics, and even train personal styles unique to their own memories. By giving users more control over how their stories look and feel, SpectraSphere becomes not only a storytelling platform but also a creative studio.\n\nInteractive and collaborative narratives ü§ù\n\nStorytelling becomes even more powerful when it is shared. We envision interactive narratives where users can make choices that branch the story in real time, or invite friends to co-create an experience together. Imagine building a collective trip diary, where each person‚Äôs photos and memories merge into a single immersive AR environment.\n\nApplications beyond personal storytelling üåç\n\nWhile SpectraSphere began with memories, the same platform can support education, travel, and entertainment. Teachers could create explorable history lessons, travellers could share experiences as immersive journals, and artists could experiment with interactive AR exhibitions. By positioning SpectraSphere as a flexible framework, we open possibilities across many domains."},{"heading":"Built With","content":"cohere gemini google-cloud json lens-studio-sdk postman remote-service-gateway snap-spectacles snapchat typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CRASH OUT!","project_url":"https://devpost.com/software/crash-out-bte6q0","tagline":"lets you crash out, whenever, wherever, whatever (on the drums).","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/741/227/datas/medium.jpg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Snap: Spectacles AR Hackathon: Game On!"}],"team_members":[],"built_with":[{"name":"adc","url":null},{"name":"ar","url":null},{"name":"audacity","url":null},{"name":"blender","url":"https://devpost.com/software/built-with/blender"},{"name":"chatgpt","url":null},{"name":"circuits","url":null},{"name":"embedded","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"lego","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"qnx","url":null},{"name":"raspberry","url":null},{"name":"redbull","url":null},{"name":"rtos","url":null},{"name":"spectacle","url":null},{"name":"sugar","url":null},{"name":"ts","url":null},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"}],"external_links":[{"label":"github.com","url":"https://github.com/dagsion/HTN25_Instrument"}],"description_sections":[{"heading":"Inspiration","content":"Coming into Hack The North, all four of us were hyped to do a hardware-based project. We couldn't resist checking out the AR glasses Snap Spectacles, the Raspberry Pi running a blazing fast RTOS from QNX, and, of course, the LEGO! After a long night of brainstorming, we had so many intriguing ideas, but we had to pick only one. Of all the things we could present in an augmented world, we chose one of the most expressive instruments to play: the drum kit!"},{"heading":"What it does","content":"CRASH OUT! brings you a full drum set wherever you are. Many people wish to learn or at least to try playing the drums, but small apartments, sleepy neighbours, and empty wallets forbid our dreams of being rock stars. With the augmented reality capabilities of the Snap Spectacles, CRASH OUT! turns your air drumming into an immersive experience with realistic audio and minimal gear. To keep it interesting, you can imagine any variety of drumsticks and they'll materialize in front of you to play with."},{"heading":"How we built it","content":"All video analysis, physics simulation, and audio/video generation is performed inside the Spectacles in front of your eyes! Borderless interactions are facilitated by physics principles, collision detection, and audio directed straight to your senses. The optional kick drum pedal uses a Raspberry Pi running the QNX real-time operating system for seamless responses. A pressure sensor helps you express your inner emotions, transmitting the exact strength of your strike to the Spectacles over a speedy web socket for instant feedback. The Snap 3D generative AI system turns voice commands into creative and unexpected models, which are loaded into the environment for interaction."},{"heading":"Challenges we ran into","content":"Setting up the 3D environment was difficult, especially in the unfamiliar Lens Studio SDK, but with help from the passionate hearts of the experts around, we deftly adapted. Moreover, despite the expansive hardware inventory, we had trouble finding the components to interface between the pressure sensor and the Spectacles. We worked our way through Bluetooth incompatibilities and confounding electronic bugs to create a swift and stable linking circuit."},{"heading":"Accomplishments that we're proud of","content":"We leveraged modern LLM development tools to accelerate the development of a driver for the MCP3008 ADC chip, which finally bridged the gap between the analog sensor and the digital RTOS. We're very proud of the real-time audio triggered by robust collision detection between drums and sticks. To achieve this, we had to transform and segment existing models into multiple meshes, then compose them back into a cohesive whole. One of our biggest and most relieving accomplishments was tuning the hand posture to mimic a natural grip on the drumsticks; a serious improvement over pinching them like darts."},{"heading":"What we learned","content":"We were surprised to find how much we could achieve in the 24 hours between formulating our idea and submitting it as a polished product. We also learned the importance of developing a healthy team culture early. We discussed our goals, priorities and values to avoid misunderstandings. Thus, we agreed to focus more on creating a fun, meaningful and impactful project than optimizing prizes."},{"heading":"What's next for CRASH OUT!","content":"In all our brainstorming, there were always some brilliant ideas left behind. CRASH OUT! is not limited to percussion instruments, but is open to further growth for other instruments. In the future, maybe people can jam together with CRASH OUT! on Snap Spectacles."},{"heading":"Built With","content":"adc ar audacity blender chatgpt circuits embedded javascript lego python qnx raspberry redbull rtos spectacle sugar ts websockets"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ARchemy","project_url":"https://devpost.com/software/archemy","tagline":"Did you ever want to learn alchemy, but you were limited by the laws of physics? Well, fret no more. With the help of AR-chemy, you can be a wizard and change the world. Kinda.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/739/570/datas/medium.jpeg","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Snap: Spectacles AR Hackathon: Game On!"}],"team_members":[],"built_with":[{"name":"ar","url":null},{"name":"cloudflare","url":"https://devpost.com/software/built-with/cloudflare"},{"name":"gpt","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"snap3d","url":null},{"name":"spectacles","url":null},{"name":"sql","url":"https://devpost.com/software/built-with/sql"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/jasonnpan/ARchemy/"}],"description_sections":[{"heading":"üí° Inspiration","content":"Our team really enjoyed playing games like Little Alchemy, and Infinite Craft, where we get to take basic building blocks and challenge the limits of our imagination. But we wanted to take this to the next level. What if instead of clicking pixels on a screen with a mouse, we move these building blocks into the real world? What if we could spawn anything from a toaster, to a dragon, to a transformer right before our eyes, all with the use of our fingertips? Well, you can achieve all of this through ARchemy."},{"heading":"‚öôÔ∏è What it does","content":"ARchemy is an augmented reality crafting game that lets players combine elemental building blocks to create new ones in real time. Using Snap Spectacles and Snap3D, players can pick up, merge, and transform objects around them into entirely new creations. Whether it‚Äôs fusing a chair with fire to make a ‚Äúthrone of flames‚Äù or combining a duck with a car to make a ‚Äúquackmobile,‚Äù the possibilities are endless. The experience blends physical gestures with immersive AR visuals so you feel like a true digital alchemist."},{"heading":"üß± How we built it","content":"We used Snap Spectacles and Lens Studio as the foundation for the AR experience. Snap3D powered our object generation pipeline, while a backend service managed caching and retrieval of previously generated 3D assets for speed and efficiency. We also connected to an external LLM (GPT) service to help interpret user prompts and guide object generation, ensuring creative and unexpected combinations."},{"heading":"‚õàÔ∏è Challenges we ran into","content":"Optimizing 3D asset generation/retrieval. An issue with the Snap3D API was its slow 3D object generation. The app generates lots of different objects and so we wanted a way to cache previously generated 3D assets and quickly load them from storage. We tried two approaches: storing objects in memory, and storing the objects in an external database and querying them with a dedicated backend server."},{"heading":"ü•á Accomplishments that we're proud of","content":"Our team has never worked with AR and so it was very rewarding to learn how it worked. Additionally, we are proud of the fact that we dove into Snap's developer ecosystem, working with the Snap3D API, remote gateway LLM services, and Lens Studio."},{"heading":"üß™ What we learned","content":"We learned how to integrate multiple technologies into one seamless experience, from AR technology to efficient backend services. We also gained experience in optimizing system performance through caching 3D objects. On the design side, we learned how important neat component design (modularity, event driven architecture) is to scaling the project."},{"heading":"üöÄ What's next for ARchemy","content":"We want to expand ARchemy into a multiplayer experience, so friends can collaborate or compete to create the most imaginative objects in shared AR worlds. We also plan to improve the object generation pipeline with better caching and fine-tuned AI models, as well as expand the ‚Äúcrafting tree‚Äù to support thousands of possible combinations. In the future, we envision ARchemy evolving into a platform for creativity, education, and entertainment, helping people explore the boundary between imagination and reality."},{"heading":"Built With","content":"ar cloudflare gpt python snap3d spectacles sql sqlite typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"socialCRM","project_url":"https://devpost.com/software/socialcrm","tagline":"Turning creators into data-driven businesses through the power of AI.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/684/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Rox: Best AI Agent"}],"team_members":[],"built_with":[{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"prisma","url":null},{"name":"puppetlabs","url":"https://devpost.com/software/built-with/puppetlabs"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"shadcn","url":null},{"name":"tailwind","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ochen1/creator-intelligence/"}],"description_sections":[{"heading":"Inspiration","content":"Every day, millions of creators pour their hearts into building content and growing their presence, whether for personal expression or business. But when it comes to understanding their audience, they‚Äôre flying blind.\n\nWhile SaaS companies rely on CRMs and data dashboards, creators are stuck with surface-level platform insights. They can see follower counts go up or down‚Äîbut not who , why , or what it means .\n\nTherefore, we asked: what if creators had a personal CRM , powered by AI, that helps them track engagement, flag churn risks, and identify revenue opportunities?\n\nThat‚Äôs the goal of our project: to build an Agentic Creator Intelligence Platform that gives creators full control over their audience data and leverages autonomous agents to generate strategic insights and recommendations."},{"heading":"How it works and Key Features","content":"Creator Intelligence is a platform that brings Agentic CRM vision to the content creation world. It is a web app that ingests user profile data and activates AI Agents to:\n\nTrack audience engagement and churn Attribute follower changes to specific campaigns Parse public profiles into intelligent segments Recommend campaign strategies Help creators gain clarity from messy structured and unstructured data"},{"heading":"How we built it","content":"We have a Next.js + TypeScript frontend to power dashboards, analytics views, and interactive components styled with Tailwind CSS and the Shad CN library. We prototyped our designs in Figma initially.\n\nOur backend uses Next.js API routes to handle business logic, with Prisma as the ORM connecting to a lightweight SQLite database that stores profiles, campaigns, tags, and interaction events. Data ingestion happens through Node.js scripts and bulk upload endpoints, while AI features are supported by a mock classifier service (Python) and custom AI endpoints that generate campaign insights via Martian API keys. We also integrated Puppeteer for scripted profile ingestion and enrichment. To keep our workflow smooth, we used pnpm for package management, ESLint for linting, and Prisma Studio for database inspection."},{"heading":"Challenges we ran into","content":"Parsing inconsistent Instagram exports: The raw data files had nested, irregular structures and missing fields, making it difficult to reliably extract and normalize user and event data for our database. Coordinating a multi-language stack: Integrating the TypeScript/Next.js frontend, Node.js/Prisma backend, and Python AI service required careful API design and debugging to keep data and state in sync. Concurrency and scaling issues: Processing thousands of profiles in parallel exposed race conditions and performance bottlenecks, so we had to optimize our queries and refactor our data flow for stability."},{"heading":"What we learned","content":"Data normalization is critical: Building robust import pipelines for real-world social data requires handling edge cases, missing fields, and evolving formats up front. Clear API contracts save time: Defining strict interfaces between frontend, backend, and AI services early on helps prevent bugs and makes debugging much easier. Scalability needs planning: Even for prototypes, designing for concurrency and efficient data flow is important when working with large datasets or real-time processing."},{"heading":"What's next for Creator Intelligence?","content":"In the future, we hope to expand beyond Instagram, integrating with other platforms to provide a unified, privacy-respecting CRM for creators everywhere. We envision Creator Intelligence as a core tool in the creator stack and strive to empower individuals to run their brand with the same data-driven precision, automation, and strategic insight as the world‚Äôs top businesses."},{"heading":"Built With","content":"javascript node.js openai prisma puppetlabs react shadcn tailwind"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Tarazoo","project_url":"https://devpost.com/software/tarazoo","tagline":"An e-commerce platform with two faces: customers checkout by camera no barcode, just the item‚Äôs look while businesses get catalog, inventory, sales, forecasting & intelligent procurement.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/740/371/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Shopify: Hack Shopping with AI"}],"team_members":[],"built_with":[{"name":"claude","url":null},{"name":"cohere","url":null},{"name":"cplex","url":null},{"name":"minlp","url":null},{"name":"next.js","url":null},{"name":"pyomo","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"pytorch","url":"https://devpost.com/software/built-with/pytorch"},{"name":"scip","url":null},{"name":"shopify","url":"https://devpost.com/software/built-with/shopify"},{"name":"superbase","url":null},{"name":"vercel","url":null}],"external_links":[{"label":"tarazoo.shop","url":"https://tarazoo.shop"},{"label":"github.com","url":"https://github.com/MeharPro/Tarazoo"},{"label":"github.com","url":"https://github.com/ishan-singh-3005/tarazoo/releases/tag/deployment"}],"description_sections":[{"heading":"Inspiration ‚ú®","content":"We wanted to make trading in e-commerce quick, natural, and effortless . Seeing experimental checkout systems in Paris and Amsterdam inspired us to go further: why shouldn‚Äôt checkout be as easy as just pointing your camera at an item, no barcode required?\n\nOn the merchant side, we aimed to remove the pain of managing catalogs, inventory, forecasting, and procurement by giving businesses a smart, automated system."},{"heading":"What It Does üõí","content":"Customer Face: Shoppers can open their camera, point at an item, and add it instantly to their cart‚Äîno barcode needed. Merchant Face: A full management suite for catalog editing, inventory overrides, sales tracking, demand forecasting, and procurement optimization powered by MINLP.\n\nTogether, Tarazoo delivers frictionless shopping for consumers and intelligent operations for merchants ."},{"heading":"How We Built It üõ†Ô∏è","content":"Frontend: Next.js App Router (TypeScript), Tailwind CSS. Backend: Local JSON data ( backend/data ) + Supabase client ( lib/supabase.ts ). Data Management: Editable catalog, inventory, and forecasts in app/merchent/* and components/dashboard/* . Optimization: MINLP solver models purchase orders, balancing MOQ, case packs, and demand requirements. UI/UX: Optimistic saves on blur/Enter, minimal visual noise, consistent Tailwind styling."},{"heading":"Challenges We Ran Into üöß","content":"Validating exactly 52 weekly demand values and 12-week forecast arrays. Keeping inline editing responsive while persisting changes reliably. Preserving legacy forecast/PO flows while introducing new editors and routes. Redirecting optimization processes seamlessly without disrupting the user flow."},{"heading":"Accomplishments That We're Proud Of üéâ","content":"Achieved everything we set out to build within the timeframe. Delivered a barcode-free checkout experience that feels futuristic. Integrated procurement optimization with real data, making merchant decisions smarter. Built a system that bridges the gap between customer convenience and merchant intelligence ."},{"heading":"What We Learned üìö","content":"Designing intuitive UIs while enforcing strict backend validation. The value of optimistic UI updates with proper error handling. How to model procurement as a Mixed-Integer Non-Linear Program with real-world constraints. Inspiration matters‚Äîseeing innovative systems abroad sparked ideas we could apply globally."},{"heading":"What's Next for Tarazoo üöÄ","content":"We want to abstract the procurement software even more , making it accessible as a standalone service that merchants can plug into any e-commerce platform. By decoupling the optimization layer, Tarazoo could evolve into a universal procurement intelligence API , powering not just our system but the entire ecosystem of online commerce."},{"heading":"Built With","content":"claude cohere cplex minlp next.js pyomo python pytorch scip shopify superbase vercel"},{"heading":"Try it out","content":"tarazoo.shop github.com github.com"}]},{"project_title":"Aircraft Studio","project_url":"https://devpost.com/software/aircraft-studio","tagline":"Design 3D aircraft using AI, and simulate piloting them in AR. On mobile browser--no app needed!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/102/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Windsurf: Best Project built with Windsurf"}],"team_members":[],"built_with":[{"name":"auth0","url":null},{"name":"flux","url":"https://devpost.com/software/built-with/flux"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"stability","url":null},{"name":"webxr","url":null},{"name":"windsurf","url":null}],"external_links":[{"label":"aircraft-studio.netlify.app","url":"https://aircraft-studio.netlify.app"},{"label":"github.com","url":"https://github.com/YehyunLee/aircraft-studio"}],"description_sections":[{"heading":"Inspiration","content":"Most 3D design tools are desktop-first and heavy. I wanted a phone-first workflow to sketch an idea, iterate quickly with AI, view it in AR, and then play with it‚Äîwithout a workstation or VR setup."},{"heading":"What it does","content":"Aircraft Studio is a mobile-focused studio for prototyping and testing aircraft:\n\nCreate/select an aircraft in the Hangar Use text prompts to guide design decisions Generate concept images fast (Fireworks Flux 1) Convert images to lightweight 3D ( .glb ) for quick previews (Spar 3D) Review and enter an AR simulation to fly, shoot, and clear waves Submit runs to a global leaderboard backed by MongoDB Share a QR code for quick mobile access\n\nCurrent prototype focuses on the home page, Hangar flow, and a responsive AR-style kinematic simulation loop."},{"heading":"How I built it","content":"Frontend: Next.js (mobile-first UI) Auth: Auth0 (optional; guest mode supported) Data: MongoDB Atlas for global leaderboard Client storage: IndexedDB for local model blobs ( aircraft-studio/models ) AI integrations: Groq for engineering prompt assistance, name and stats extraction Fireworks Flux 1 for image generation Spar 3D for image-to-3D conversion AR approach: WebXR-style camera + anchored model preview; kinematic motion loop optimized for phones"},{"heading":"Sponsor tracks","content":"Groq: Prompt-engineering endpoint powers aircraft stats and naming in src/pages/api/prompt-engineering.js . Windsurf: Built and iterated entirely in an AI‚Äënative IDE workflow to accelerate feature delivery. Worked as a solo, and 70~80% of the code came from vibe coding. MongoDB Atlas: Backs the global leaderboard with server-side queries ( src/pages/leaderboard.js , src/lib/mongodb.js ). Auth0: Optional Google login enables authenticated run submissions; guest play remains supported."},{"heading":"Technical highlights","content":"Supporting both iOS and Android via web browser without app install required Mobile-first UX: touch-first controls, small UI footprint, quick iteration Kinematic flight model: responsive motion without full aero sim Smoothed velocity targets, clamped rates, joystick-driven yaw/pitch/roll Lightweight effects: audio cues and hit markers to keep FPS high on phones Telemetry scaffolding: documented event shapes for sessions, frames, collisions, and gameplay‚Äîdesigned for batched posting and on-device sampling Enhanced prompt with Groq, text to image with Flux 1, and image to 3D with Spar 3D"},{"heading":"Challenges","content":"Balancing realism vs. responsiveness for phone AR Setting up the physical engine for the AR mode, making the jet move, rotation, attack, enemy jets, applying stats, etc. Making the pipeline ‚Äúfast enough to iterate‚Äù: prompt ‚Üí image ‚Üí .glb ‚Üí preview ‚Üí play Keeping the stack simple while leaving room for AI/3D integrations"},{"heading":"Accomplishments I‚Äôm proud of","content":"A clean, phone-first flow that gets from idea to playable prototype quickly A leaderboard that works with or without login (submission requires auth) Clear telemetry schema for future tuning and analytics A pragmatic AR loop that performs well on mid-range devices"},{"heading":"What I learned","content":"How far you can get with kinematics before needing full aero Practicalities of local caching (IndexedDB) for heavy model blobs The tradeoffs of optional auth in a game-like flow"},{"heading":"What‚Äôs next","content":"Physics: optional gravity, basic lift and drag, simple stall modelling Collisions: convex hulls or per-part hitboxes AI: tighter integration with Groq + Flux 1 for ‚Äúdesign and iterate‚Äù loops 3D: more robust Spar 3D conversion and materials pipeline Multiplayer: shared anchors for basic dogfights Deeper analytics: flight envelopes, trim curves, auto-tuning from telemetry"},{"heading":"Try it locally","content":"Requirements\n\nNode.js 18+ macOS/Chrome recommended for camera permissions\n\n1) Clone and install:\n\ngit clone https://github.com/YehyunLee/aircraft-studio.git cd aircraft-studio npm install\n\n2) Add environment variables in .env.local (see README.md for details):\n\nAUTH0_DOMAIN , AUTH0_CLIENT_ID , AUTH0_CLIENT_SECRET , AUTH0_SECRET , APP_BASE_URL MONGODB_URI , MONGODB_DB , NEXT_PUBLIC_BASE_URL , etc.\n\nYou can run as a guest without Auth0, but leaderboard submission requires login. MongoDB is needed for leaderboard to function.\n\n3) Start the dev server with HTTPS (needed for camera/AR):\n\nnpx next dev --experimental-https"},{"heading":"Built with","content":"Next.js Auth0 MongoDB IndexedDB Groq Fireworks Flux 1 Spar 3D WebXR"},{"heading":"Built With","content":"auth0 flux groq mongodb stability webxr windsurf"},{"heading":"Try it out","content":"aircraft-studio.netlify.app github.com"}]},{"project_title":"Tunnel","project_url":"https://devpost.com/software/tunnel-dqv1k4","tagline":"AI Agents for Simulated Market Research","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/742/898/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"VAPI: Best Voice AI Application"},{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"MLH: Best Use of Auth0"}],"team_members":[],"built_with":[{"name":"auth0","url":null},{"name":"cloudflare","url":"https://devpost.com/software/built-with/cloudflare"},{"name":"cohere","url":null},{"name":"dagre","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next.js","url":null},{"name":"radix","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vapi","url":null},{"name":"vercel","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/krish1905/tunnel"}],"description_sections":[{"heading":"Inspiration","content":"Every founder's nightmare: spending months building a product only to discover nobody wants it.\n\nThe statistics are brutal - 90% of startups fail, with 35% citing \"no market need\" as the primary reason. Traditional market research takes 3-6 months and costs $50,000+, making it inaccessible for most entrepreneurs. User interviews are biased, surveys have low response rates, and focus groups are expensive and geographically limited.\n\nWe asked ourselves: What if you could simulate your entire target market in seconds?\n\nWhat if, instead of guessing whether your B2B SaaS will reach $1M MRR, you could watch 200+ AI personas - each with unique demographics, psychographics, and behavioral patterns - react to your product in real-time? What if you could have actual conversations with the personas who rejected your idea to understand exactly why?\n\nThat's when Tunnel was born."},{"heading":"What it does","content":"Tunnel is an AI powered market simulation platform that lets you test product ideas against hundreds of intelligent personas in real-time.\n\nCore Features\n\n1. AI-Powered Persona Generation\n\n162 Unique Personas : Each with detailed demographics (age, location, industry), psychographics (risk tolerance, tech adoption), and personality traits (openness, conscientiousness) Dynamic Behavior Modeling : Personas react based on their unique characteristics - a risk-averse CFO responds differently than an innovative startup founder Global Distribution : Personas span 50+ cities across 6 continents, representing diverse markets and cultures\n\n2. Interactive 3D Market Visualization\n\nReal-time Globe Interface : Watch your idea spread across the world as personas react Color-Coded Sentiment : Green (interested), Yellow (neutral), Red (not interested) - instantly see market reception Drill-Down Analytics : Click any persona to see their detailed feedback and reasoning\n\n3. Focus Group Simulation\n\nNiche Targeting : AI automatically identifies the 5 most relevant personas for your specific idea using Cohere's reranking Semantic Understanding : Goes beyond keywords - understands \"AI tool for lost cars\" connects to automotive industry professionals Iterative Refinement : Collect feedback from skeptical personas and refine your idea in real-time\n\n4. Voice Conversations with AI Personas\n\nNatural Dialogue : Powered by Vapi, have actual phone-like conversations with personas Contextual Responses : Each persona speaks from their unique perspective - gender-matched voices, industry jargon, generation-specific language Deep Insights : Understand not just \"no\" but exactly why - uncover hidden objections and opportunities\n\n5. Global Deployment Testing\n\nScale Simulation : After focus group validation, deploy to all 200+ personas globally Market Penetration Analysis : See adoption patterns across different demographics and regions Viral Coefficient Calculation : Predict how your idea might spread organically\n\n6. Intelligent Session Management\n\nAuto-Save System : 2-second debounced saving ensures you never lose progress State Persistence : Every interaction, reaction, and refinement is preserved Shareable URLs : Send analysis sessions to team members or investors with one link Session History : Track how your idea evolved through multiple iterations"},{"heading":"üõ†Ô∏è How we built it","content":"Architecture Overview\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Frontend (Next.js 15) ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇThree.js ‚îÇ ‚îÇReact 19 ‚îÇ ‚îÇTypeScript‚îÇ ‚îÇTailwind ‚îÇ ‚îÇ ‚îÇ ‚îÇGlobe ‚îÇ ‚îÇComponents‚îÇ ‚îÇ ‚îÇ ‚îÇCSS ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ API Layer (Next.js API Routes) ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇSession ‚îÇ ‚îÇPersona ‚îÇ ‚îÇAnalysis ‚îÇ ‚îÇVoice ‚îÇ ‚îÇ ‚îÇ ‚îÇManagement‚îÇ ‚îÇGeneration‚îÇ ‚îÇEngine ‚îÇ ‚îÇBridge ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚ñº ‚ñº ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ External Services ‚îÇ ‚îÇ Database Layer ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ ‚îÇ ‚îÇCohere AI ‚îÇ‚îÇ ‚îÇ ‚îÇMongoDB Atlas ‚îÇ ‚îÇ ‚îÇ ‚îÇ- Text Generation ‚îÇ‚îÇ ‚îÇ ‚îÇ- Filtering ‚îÇ ‚îÇ ‚îÇ ‚îÇ- Embeddings ‚îÇ‚îÇ ‚îÇ ‚îÇ- Sessions ‚îÇ ‚îÇ ‚îÇ ‚îÇ- Reranking ‚îÇ‚îÇ ‚îÇ ‚îÇ- Projects ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ ‚îÇ- Simulations ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ ‚îÇVapi ‚îÇ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îÇ- Voice AI ‚îÇ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ ‚îÇAuth0 ‚îÇ‚îÇ ‚îÇ ‚îÇ- Authentication ‚îÇ‚îÇ ‚îÇ ‚îÇ- User Management ‚îÇ‚îÇ ‚îÇ ‚îÇ- Personas ‚îÇ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nTechnical Implementation\n\nFrontend Magic\n\nNext.js 15.5 with React 19 RC for cutting-edge performance Three.js + React Three Fiber : Custom WebGL shaders for smooth 60fps globe rendering with 200+ interactive points Framer Motion : Orchestrated animations for seamless state transitions Tailwind CSS + Radix UI : Accessible, responsive components with dark mode support\n\nAI Pipeline Architecture\n\n// Multi-stage AI processing pipeline 1. Persona Selection ‚Üí Cohere reranking for relevance scoring 2. Opinion Generation ‚Üí Parallel processing with retry logic 3. Sentiment Analysis ‚Üí Structured extraction with Zod validation 4. Insight Synthesis ‚Üí Aggregated patterns and recommendations\n\nDatabase Design\n\nMongoDB Atlas with Mongoose ODM for flexible schema evolution Optimized Indexes : Compound indexes on (userId, projectId) for fast queries Data Isolation : Tenant-based partitioning for enterprise-grade security Session Versioning : Complete audit trail of all analysis iterations\n\nReal-time Features\n\nWebSocket-like Polling : 1-second intervals for live reaction updates Optimistic UI Updates : Instant feedback with background synchronization Conflict Resolution : Last-write-wins with client-side reconciliation\n\nVoice AI Integration\n\nVapi SDK : WebRTC-based real-time voice streaming Dynamic System Prompts : Each persona gets unique instructions based on their profile Gender-Matched Voices : PlayHT voices selected based on persona demographics Context Injection : Full feedback history available during conversations\n\nPerformance Optimizations\n\nLazy Loading : Code-split by route, reducing initial bundle by 60% Image Optimization : Next.js Image component with WebP conversion Database Connection Pooling : Reused connections reduce latency by 40% Parallel API Calls : Batch processing for opinion generation (25 personas/second) Memoization : React.memo and useMemo prevent unnecessary re-renders"},{"heading":"Challenges we ran into","content":"1. AI Hallucination & Consistency\n\nProblem : Cohere would sometimes generate inconsistent persona reactions or invalid JSON. Solution : Implemented multi-layer validation:\n\nZod schemas for type safety Retry logic with exponential backoff Fallback templates for malformed responses Structured prompting with few-shot examples\n\n2. 3D Performance at Scale\n\nProblem : Rendering 200+ interactive globe points caused frame drops on mobile devices. Solution :\n\nImplemented LOD (Level of Detail) system Frustum culling for off-screen points GPU instancing for repeated geometries Progressive rendering with requestIdleCallback\n\n3. Voice AI Context Management\n\nProblem : Vapi conversations would lose context about the specific persona and their feedback. Solution :\n\nSessionStorage for persona context bridging Dynamic system prompt generation Stateful conversation management Graceful fallbacks for connection issues\n\n4. Rate Limiting & Cost Control\n\nProblem : Cohere trial keys limited to 40 requests/minute. Solution :\n\nRequest queuing with priority system Caching layer for repeated queries Batch processing for bulk operations Progressive disclosure UI patterns\n\n5. State Synchronization\n\nProblem : Complex state across globe visualization, personas, and sessions. Solution :\n\nCentralized state management with Zustand Atomic updates with optimistic UI Debounced auto-save system URL-based state restoration"},{"heading":"Accomplishments that we're proud of","content":"1. Zero to Insight in 30 Seconds\n\nFrom idea to comprehensive market analysis with 200+ persona reactions in under half a minute. What traditionally takes months now happens in real-time.\n\n2. Conversation-Ready AI Personas\n\nNot just data points - each persona can hold natural conversations about why they like or dislike your idea, complete with personality quirks and industry-specific knowledge.\n\n3. Beautiful, Intuitive UX\n\nComplex market data presented through an interactive 3D globe that's actually fun to use. No spreadsheets, no boring dashboards - just instant visual insights.\n\n4. Production-Ready Architecture\n\n99.9% uptime during development <100ms API response times Handles 200+ concurrent persona generations Zero data loss with auto-save system"},{"heading":"What we learned","content":"Technical Insights\n\nStructured AI > Clever Prompts : Robust schemas and validation beat complex prompting every time Batch Everything : Parallel processing reduced our API costs by 70% State is King : Proper state management made complex features like session restoration trivial WebGL is Powerful : Three.js can handle way more than we initially thought with proper optimization\n\nProduct Insights\n\nVisual > Textual : Users understand data 10x faster through visualization Conversation > Survey : Voice interactions revealed insights text never could Iteration > Perfection : The refinement loop was our most loved feature Speed Matters : Sub-second responses kept users engaged\n\nBusiness Insights\n\nDevelopers are Entrepreneurs : Every developer we showed this to had an idea to test B2B Needs This More : Enterprise clients desperate for faster product validation Global Markets Matter : 40% of successful ideas came from unexpected geographic insights"},{"heading":"What's next for Tunnel","content":"Immediate Roadmap (Next 3 Months)\n\nEnhanced AI Capabilities\n\nGPT-4 Integration : More nuanced persona responses Custom Persona Creation : Upload your actual customer data Competitive Analysis : Compare multiple ideas simultaneously Trend Prediction : ML models trained on successful product launches\n\nEnterprise Features\n\nTeam Workspaces : Collaborative analysis with role-based permissions API Access : Integrate Tunnel into existing workflows Custom Branding : White-label solution for agencies Compliance Tools : GDPR/CCPA compliant data handling\n\nMobile Experience\n\nNative Apps : iOS/Android apps with offline support AR Visualization : Point phone at real world to see market data overlay Voice-First Interface : Complete analysis through conversation alone\n\nLong-term Vision (Next Year)\n\nTunnel Marketplace\n\nIdea Exchange : Buy/sell validated product concepts Persona Marketplace : Share custom persona sets Template Library : Industry-specific analysis templates\n\nAI Agent Evolution\n\nPersistent Personas : Agents that remember past interactions Social Dynamics : Personas influence each other's opinions Temporal Simulation : See how reactions change over time\n\nSuccess Tracking\n\nLaunch Monitoring : Track real vs. predicted performance Feedback Loop : Improve AI accuracy with real-world data Success Stories : Showcase products launched through Tunnel\n\nUltimate Goal\n\nMake market research so fast, affordable, and accurate that no product ever launches without validation. Reduce the startup failure rate from 90% to 50% by ensuring every idea meets a real market need."},{"heading":"Built With","content":"Frontend : Next.js 15, React 19, TypeScript, Tailwind CSS, Three.js Backend : Node.js, Next.js API Routes, MongoDB, Mongoose AI/ML : Cohere AI, OpenAI (via Martian), Vapi Infrastructure : Vercel, MongoDB Atlas, Auth0, Cloudflare Tools : Framer Motion, Radix UI, Zod, Zustand"},{"heading":"Technology Deep Dive","content":"Cohere API - The Brain Behind Our Personas\n\nCohere powers the entire intelligence layer of Tunnel through multiple advanced capabilities:\n\n1. Semantic Understanding & Reranking We use Cohere's embeddings and reranking API to understand the semantic meaning behind user ideas. When someone enters \"AI tool for lost cars\", our system doesn't just match keywords - it understands this relates to automotive technology, IoT, and safety. The reranking API then scores all 200+ personas by relevance, ensuring we select the perfect focus group of 5 personas who would genuinely care about this product.\n\n2. Multi-Stage Text Generation Each persona's reaction is generated through a sophisticated Cohere-powered pipeline:\n\nContext Analysis : Understanding the product idea's nuances Persona Modeling : Generating responses that align with each persona's unique characteristics Sentiment Extraction : Structured analysis of positive/negative signals Insight Synthesis : Aggregating patterns across all reactions\n\n3. Intelligent Refinement When users collect feedback and hit \"Refine\", Cohere analyzes all negative feedback, identifies common themes, and generates an improved product pitch that addresses the concerns - all while maintaining the original vision.\n\nVapi - Bringing Personas to Life Through Voice\n\nVapi transforms our AI personas from data points into conversational partners:\n\n1. Dynamic Voice Selection Each persona gets a voice that matches their profile - we programmatically select from PlayHT's voice library based on gender, age, and personality traits. A Gen Z female developer sounds different from a Baby Boomer male CFO.\n\n2. Contextual Conversations When users click \"Call\" on a persona, Vapi receives:\n\nThe complete persona profile (demographics, psychographics, interests) The original product idea The persona's specific reaction and reasoning Historical context from the simulation\n\nThis creates incredibly realistic conversations where personas defend their positions, explain their concerns, and even suggest alternatives.\n\n3. Real-Time WebRTC Streaming Voice conversations happen with <100ms latency, making interactions feel natural. Users can interrupt, ask follow-ups, and have genuine back-and-forth discussions about their product ideas.\n\nMongoDB Atlas - Scalable Data Architecture\n\nMongoDB Atlas serves as our flexible, scalable backbone for complex data relationships:\n\n1. Document-Based Persona Storage Each persona is stored as a rich document with nested attributes - demographics, psychographics, personality scores, and interaction history. MongoDB's flexible schema lets us evolve persona structures without migrations.\n\n2. Session State Management Complete simulation states are stored as documents, including:\n\nAll persona reactions Globe visualization states Refinement history Voice conversation transcripts\n\nThis enables our powerful session restoration - users can share a URL and recipients see the exact same analysis state.\n\n3. Geospatial Queries We leverage MongoDB's geospatial indexing for the 3D globe - efficiently querying personas by geographic region and calculating geographic market penetration patterns.\n\nAuth0 - Revolutionary Agent Management\n\nThe most innovative aspect of Tunnel is how we use Auth0's Management API to create living, breathing digital personas:\n\n1. Dynamic Agent Creation We leverage Auth0's Management API to dynamically create an account for every single agent that grows and evolves over time. After submitting a prompt, we generate Auth0 accounts with rich metadata attributes:\n\nDemographics (age, location, industry) Psychographics (risk tolerance, tech adoption) Personality scores (openness, conscientiousness) Behavioral patterns that evolve based on interactions\n\n2. Multi-Tenant Isolation Each user's workspace is completely isolated using Auth0's tenant features. Your competitors can't see your simulations, and enterprise clients get dedicated environments.\n\n3. Persona Evolution As users interact with personas through voice calls or multiple simulations, we update the Auth0 user profiles. Personas remember past interactions, creating a more realistic simulation over time - they might reference previous products they've evaluated or show fatigue for similar ideas.\n\nTunnel - Because your next billion-dollar idea deserves more than a guess."},{"heading":"Built With","content":"auth0 cloudflare cohere dagre javascript mongodb next.js radix react three.js typescript vapi vercel"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Vraille","project_url":"https://devpost.com/software/vraille","tagline":"Redefining Accessibility: Real-Time Awareness for the Visually Impaired With Gemini-Powered Glasses & Dynamic Braille","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/740/643/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"VAPI: Best Voice AI Application"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"cohere","url":null},{"name":"facedetection","url":"https://devpost.com/software/built-with/facedetection"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"meta","url":"https://devpost.com/software/built-with/meta"},{"name":"multimodal","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"solidworks","url":null},{"name":"supabase","url":null},{"name":"vapi","url":null},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"},{"name":"windsurf","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/KaivalSShah/vraille-htn2025"}],"description_sections":[{"heading":"Inspiration","content":"Approximately 2.2 billion people worldwide suffer from mild to severe vision impairment, negatively impacting their physical and digital interactions. To that end, the latest advancements in smart glasses and vision technology prioritize convenience features and narrowly defined use cases over comprehensive systems purposefully designed for the visually impaired. As a result, those with visual impairments are unable to tap into the ways technology can change their lives.\n\nThink memory recall systems for those who are forced to match faint voices to their dearest loved ones, or navigation tools that provide real-time spatial awareness in complex, unfamiliar environments.\n\nThe lack of sufficient technological progress in these directions leaves the visually impaired with no choice but to resort to archaic solutions such as guide dogs and canes‚Äîtools that, while reliable, fundamentally fail to harness the potential of modern computing.\n\nBut, what if we could hack the previously antiquated physical and digital systems to bring novelty and the latest advancements to the lives of billions of visually impaired folks around the world?\n\nIntroducing Vraille (Vision + Braille) ‚Äî the AI glasses layer built for the visually impaired using Gemini-orchestrated voice automation and a dynamic braille interface."},{"heading":"What it does","content":"Vraille transforms the way visually impaired individuals interact with their surroundings by combining three core innovations:\n\nReal-time environmental understanding: using Meta glasses with live video and audio feeds, Vraille interprets the user‚Äôs environment and delivers contextual awareness powered by Gemini‚Äôs multimodal AI. Conversational interaction through Gemini tool calling: through Gemini-orchestrated voice automation, users can naturally ask questions and receive immediate, meaningful responses about their environment ‚Äî from identifying objects to recalling contextual details. Dynamic braille interface: a tactile, adaptive braille pad translates digital and environmental information into touch in real time, offering a discreet alternative to audio cues. This ensures users can navigate and learn without causing disruption in quiet or shared spaces.\n\nTogether, these features give visually impaired users an intuitive and empowering interface to both their physical and digital worlds. Vraille addresses the long-standing stagnation in accessibility tech by rethinking and adapting the latest advancements (e.g. Multimodal AI, voice agents, tool calling, among other technologies) to create solutions that truly make a lasting difference."},{"heading":"How we built it","content":"We developed the software and hardware components of Vraille by integrating several cutting-edge technologies and collaborating effectively to make it happen! The notable technologies we adopted to create Vraille include:\n\n‚Ä¢ Gemini tool calling for real-time multimodal reasoning and environment understanding. ‚Ä¢ Vapi voice agents to enable seamless, natural conversation between the user and the system. ‚Ä¢ Cohere embeddings and models to handle contextual memory recall and natural language understanding. ‚Ä¢ A custom-built dynamic braille pad powered by microcontrollers and actuators, designed to adapt text and environmental information into tactile feedback in real time. ‚Ä¢ Real-time face detection using the Face++ API\n\nSpecifically on the hardware side of the project, we designed a custom motorized piston graille pattern to dynamically change the graille text being displayed. This was an ambitious but was achieved due to the amazing resources provided throughout the hackathon."},{"heading":"Challenges we ran into","content":"On the technical side, applying relatively new technologies like Meta Rayban smart glasses, Vapi voice-control agents, and Gemini Tool calling were the biggest challenges we ran into. Not only did we have to consult company professionals during the event to tackle unprecedented issues, but also did we used multiple layers of processing in order to unleashed the full potential of an existing commercial product. For example, our biggest challenge was to jailbreak the Meta Rayban glasses to use the vision system in our own way by video calling on WhatsApp call, redirecting audio output into the AI Voice Agent in Python, and capturing video feed from the camera on the glasses. The whole process requires a total of 4 additional virtual audio I/O devices. We were constantly working with technologies in ways that not many had done before.\n\nLogistically, we faced a teaming crisis where our team of 4 was coming and leaving. Started with the idea of building something with a combination of hardware and software, we purposefully recruited more mechanically focused role. However, the original two people we reached out too left the team before the competition started. One of the other members later decided to quit because of the misalignment of goals and ideas for this project. With the lack of time and mentors in the hardware area, our progress on the hardware component was severely behind when there was almost no 3D printers available towards the end. Therefore, we built everything without 3d printer and it is completely hand-crafted."},{"heading":"Accomplishments that we're proud of","content":"Hacked for 30+ hours, turning what started out as an idea into something that could be so much more! Worked around challenges involving Vapi package dependency incompatibility and gemini tool calling, eventually turning Vraille as a truly intelligent voice-powered agent that could perform (via tool call) various actions for the user. Last but not least, we were able to integrate our unique strengths to build a hardware and software project that has the potential to leave great impact amongst the visually impaired community, which brings us great personal satisfaction."},{"heading":"What we learned","content":"‚Ä¢ Discuss goals and alignments of ideas ahead of time in the teaming experience ‚Ä¢ Don't spend too much time on ideas; ideas don‚Äôt come fully formed. Start right away and gain some insights along the way, and you might get more sparks! ‚Ä¢ The hardware component of a project can be the limiting factor for the completion of a project, especially when the team lacks expertise in this area. ‚Ä¢ Do not rely on materials, resources, and opportunities provided to you. There is no guarantee for what one can get. ‚Ä¢ Have a structured plan and firm timeline to help the team execute on an idea more efficiently."},{"heading":"What's next for Vraille","content":"‚Ä¢ Refining the dynamic braille interface to improve responsiveness and making it more seamless to use in one‚Äôs everyday life ‚Ä¢ Enhancing context given to the LLM responsible for orchestrating the multimodal tool calling workflows. ‚Ä¢ Improve video and audio quality output from the Meta Rayban glasses, find alternatives, or build our own market."},{"heading":"Built With","content":"arduino c++ cohere facedetection gemini meta multimodal python solidworks supabase vapi websockets windsurf"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Stacy","project_url":"https://devpost.com/software/stacy-g7zptj","tagline":"Stacy is a voice safety companion. Stacy logs incidents, your live location, alerts your loved ones, and dispatches to the police. Built to keep you safe, for the moments when every second matters.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/738/987/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"VAPI: Best Voice AI Application"}],"team_members":[],"built_with":[{"name":"openai","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"twilio","url":"https://devpost.com/software/built-with/twilio"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/apollo-ullah/HackTheNorth2025"}],"description_sections":[{"heading":"Built With","content":"openai swift twilio typescript vapi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"ShapeShift","project_url":"https://devpost.com/software/shapeshift-kvhjxr","tagline":"A gesture-first, AI-assisted 3D workspace where your hands craft the scene.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/739/670/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Y Combinator: Unicorn Prize"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"fal","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"martian","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"rodin","url":null},{"name":"vite","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/itsmarsss/shapeshift"}],"description_sections":[{"heading":"Inspiration","content":"We've always been fascinated by the futuristic interfaces in movies like Minority Report and Iron Man . Traditional 3D modeling software, while powerful, often has a steep learning curve and relies on clunky keyboard shortcuts and mouse clicks. We asked ourselves: what if we could sculpt 3D objects as naturally as if we were molding clay with our own hands? Our inspiration was to bridge the gap between human intuition and digital creation , building an accessible, immersive, and magical 3D modeling experience right in the browser. We wanted to create a tool that feels less like operating a machine and more like an extension of our own creativity."},{"heading":"What it does","content":"ShapeShift is an innovative, browser-based 3D modeling platform that transforms your webcam into a powerful creation tool. It allows users to interact with a 3D environment in two revolutionary ways:\n\nüëã Real-Time Gesture Control: Using your hands, you can directly manipulate objects in the 3D scene. ShapeShift tracks your hand movements in real-time, allowing you to grab, move, rotate, and scale objects with intuitive gestures, eliminating the need for a mouse and keyboard. ü§ñ AI Agent Assistant: ShapeShift features an intelligent AI assistant that understands natural language. You can simply tell the AI what you want to create or modify. For example, you can say, \"Create a red sphere and a blue cube, then place the sphere on top of the cube,\" and the AI will execute the commands. ‚ú® AI-Powered 3D Model Generation: Stuck for ideas? You can describe a concept like \"a futuristic spaceship\" or \"a stylized tree,\" and our integrated generative AI will create a detailed 3D model for you on the fly, ready to be imported and manipulated in your scene."},{"heading":"How we built it","content":"ShapeShift is built on a modern, multi-faceted tech stack designed for real-time performance and intelligence.\n\nFrontend: The entire user interface is a dynamic web application built with React , TypeScript , and Vite . For rendering the 3D environment, we used the powerful Three.js library, primarily through the declarative APIs of @react-three/fiber and @react-three/drei . Global state management is handled efficiently by Zustand . Backend (Computer Vision): The gesture-control engine is powered by a Python backend using Flask and Eventlet for high-performance WebSocket communication. We use OpenCV to capture the video feed from the user's webcam and the MediaPipe library to perform real-time hand landmark detection. This landmark data is then streamed to the frontend. Backend (AI Agent): The AI assistant is orchestrated by a Node.js server using Express . This server acts as a middleware that defines a set of tools (functions) the AI can use to manipulate the 3D scene. It integrates with LLM providers (like Martian) and the Fal AI API for generative 3D model creation from text prompts.\n\nThe components communicate seamlessly: the Python backend streams hand data to the React frontend via WebSockets, while the frontend sends user prompts to the Node.js server to trigger AI actions."},{"heading":"Challenges we ran into","content":"Building a project this ambitious came with its fair share of hurdles.\n\nOne of the biggest challenges was minimizing latency . For the gesture control to feel natural, the delay between a physical hand movement and the response in the 3D viewport had to be negligible. We spent a significant amount of time optimizing our Python backend, implementing frame-skipping logic and efficient data serialization to ensure the WebSocket connection remained snappy.\n\nAnother difficulty was designing a robust gesture recognition system . Raw hand landmark data from MediaPipe is noisy and varies between users. We had to develop normalization and rotation-invariant processing techniques to translate this raw data into consistent, reliable commands like \"pinch\" or \"grab.\"\n\nFinally, integrating the three distinct parts of our application (Python CV, Node.js AI, and React frontend) was a complex architectural task. Ensuring smooth communication and state synchronization between these services required careful planning and debugging."},{"heading":"Accomplishments that we're proud of","content":"We are incredibly proud of creating a truly multi-modal interface for 3D creation. The ability to seamlessly switch between direct, physical manipulation with your hands and high-level, abstract commands with your voice is the core of what we wanted to achieve.\n\nThe low-latency performance of the hand-tracking pipeline is a major accomplishment. It feels fluid and responsive, which is crucial for making the experience immersive rather than frustrating.\n\nFurthermore, we developed a comprehensive and powerful toolset for our AI agent . The AI can do more than just create primitives; it can perform complex boolean operations, modify materials, duplicate objects, and even generate entirely new models, giving users immense creative leverage through simple language."},{"heading":"What we learned","content":"This project was a massive learning experience. We learned a great deal about the intricacies of real-time computer vision and the importance of performance optimization at every step of the data pipeline. We also gained a deep appreciation for the complexities of human-computer interaction design , especially when creating novel interfaces that don't rely on established conventions.\n\nOn the AI front, we learned how to effectively design and implement a tool-using AI agent . Defining clear, non-overlapping functions and engineering the system prompts to get reliable, structured output from the LLM was a fascinating challenge that pushed our understanding of applied AI."},{"heading":"What's next for ShapeShift","content":"The future is bright and three-dimensional! We have many ideas for where to take ShapeShift next:\n\nExpanded Gesture Library: We plan to introduce more complex and two-handed gestures for advanced actions like scaling the entire scene, drawing custom shapes, or performing intricate sculpting operations. Multi-User Collaboration: We want to turn ShapeShift into a collaborative space where multiple users can join a session and build together in real-time, seeing each other's virtual hands. AR/VR Integration: The ultimate goal is to break free from the 2D screen entirely. We plan to adapt our interaction model for augmented and virtual reality headsets to create a fully immersive 3D modeling environment. Smarter AI: We will continue to enhance our AI's capabilities, enabling it to understand more complex, multi-step commands and maintain a better contextual awareness of the user's project goals."},{"heading":"Built With","content":"express.js fal flask martian node.js opencv python react rodin vite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Evident AI","project_url":"https://devpost.com/software/evident-ai","tagline":"Automatically transforms bodycam footage into professional evidence reports using AI.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/739/239/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"Y Combinator: Unicorn Prize"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"django","url":"https://devpost.com/software/built-with/django"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"openai","url":null},{"name":"openai-agents-sdk","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/asalsali/evidentai-main"}],"description_sections":[{"heading":"Inspiration","content":"Evidence processing is the bottleneck. Officers spend hours transcribing body-cam footage and writing reports, which is time not spent serving the community. Vendors charge thousands and take days for basic transcripts. I realized there‚Äôs a better way: automate the grind while preserving legal standards. Evident AI was born, an AI evidence-processing copilot that turns body-cam video into a professional, review-ready report in minutes, anchored with blockchain proofs."},{"heading":"What it does","content":"Evident AI orchestrates specialized AI agents across the full evidence lifecycle:\n\nFull-stack analysis: Extracts audio and transcribes speech with Whisper + error correction and speaker tagging. Visual intelligence: Key-frame analysis (GPT-4V) to surface people, vehicles, objects, and salient actions. Professional reporting: Generates a structured draft with a timeline, entities, action summaries, and narrative* with citations and confidence scores * for quick human review (no legal determinations made by AI). Blockchain integrity: XRPL anchoring + XLS-70 credentials for officer identity, producing cryptographic attestations and a tamper-evident chain of custody. Court-ready output: One-click PDF/Docx with officer checklist, digital signatures, and an audit log (hashes, model versions, timestamps).\n\nTL;DR: Body-cam in ‚Üí transcript + key frames ‚Üí cited, structured report ‚Üí signed & anchored. Minutes, not days."},{"heading":"How we built it","content":"Backend: Django 5.2.6 (REST) behind Gunicorn + Nginx AI Agents: OpenAI Agents SDK coordinating transcription, vision, and report-writer roles with structured JSON I/O Video: OpenCV + MoviePy for frame sampling and audio separation Blockchain: XRPL integration; officer XLS-70 creds; memo-based anchors with SHA-256 hashes per stage Database: PostgreSQL (prod), SQLite (dev) Deployment: Dockerized; runs on Heroku/Railway/Vercel Security: Fernet encryption for secrets; audit trails with immutable source hashes"},{"heading":"Challenges","content":"Agent orchestration: Retries, schema validation, and timeouts across multiple models/services Large files: GB-scale videos; streamed processing and memory-safe chunking Evolving standards: XLS-70 is emerging: DevNet only with limited use case examples Security & compliance: Encryption at rest/in transit, RBAC, and explicit human-in-the-loop review Production hardening: Containerization, DB tuning, back-pressure and job queues"},{"heading":"Accomplishments","content":"End-to-end demo: Upload ‚Üí transcript + key frames ‚Üí cited report ‚Üí signed PDF with XRPL anchor Human-in-the-loop UX: Confidence chips and inline citations (hover to see the exact timestamp/frame) Integrity by design: Original media hash preserved; every export logs model versions and anchors"},{"heading":"What we learned","content":"Practical agent hand-offs with the OpenAI Agents SDK (tools, structured outputs, recovery paths) XRPL anchoring patterns and XLS-70 credential flows for attestations Real-world video optimization and keeping AI pipelines responsive Building for policing means: assist, cite, and log"},{"heading":"What‚Äôs next","content":"Pilot with agencies: Run side-by-side with existing RMS workflows to measure minutes saved per report and edit-accept rates Model upgrades: Domain-tuned entity recognizers and policy-aware templates (traffic stop, use-of-force, etc.) Mobile ingest: Secure field upload from camera to case Analytics: Case stats, throughput, and time-savings dashboards RMS adapters: Push approved drafts into Axon/Mark43/Tyler/Niche via export & API connectors"},{"heading":"Built With","content":"css django javascript openai openai-agents-sdk python"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"VibeMovie","project_url":"https://devpost.com/software/promptmotion","tagline":"Agentic & Conversational video editing!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/160/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"MLH: Best Use of Gemini API"}],"team_members":[],"built_with":[{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"remotion","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vite","url":null},{"name":"zustand","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/EvanJYHe/vibe-movie"}],"description_sections":[{"heading":"üí° Inspiration","content":"The world of video editing is filled with powerful, professional-grade tools that are capable of producing cinematic masterpieces. However, this power often comes at the cost of complexity. Steep learning curves, cluttered interfaces with hundreds of buttons, and the manual nature of timeline-based editing can feel more like operating heavy machinery than engaging in a a creative act.\n\nWe were inspired by the recent breakthroughs in conversational AI and asked ourselves: What if you could create a video just by describing it?\n\nWe envisioned a tool that would remove the technical barriers and allow creators to focus purely on their vision. Instead of searching through menus to find the \"fade\" effect, you could simply ask:\n\n\"Fade in the title text.\"\n\nInstead of manually scrubbing a timeline to find a scene, you could say:\n\n\"Cut to the part where the rocket launches.\"\n\nThis desire to make video creation as intuitive and fluid as a conversation was the core inspiration for VibeMovie."},{"heading":"üé¨ What it does","content":"VibeMovie is an AI-native video editor that turns natural language into a rendered video. It works like a creative partner. A user can start with a raw video file, or even a blank canvas, and direct the entire editing process through a simple chat interface.\n\nüí¨ Chat-Based Editing\n\nUsers can ask the AI to perform a wide range of editing tasks, such as:\n\n\"Add a title that says 'My Awesome Trip'.\" \"Cut the first 5 seconds of the video.\" \"Put a smooth fade-out effect at the end.\" \"Make the text on screen red and larger.\"\n\nüìä Live Visual Timeline\n\nWhile the user chats with the AI, a dynamic timeline provides a real-time visual representation of the video's structure. Users can see the clips, tracks, and their arrangement as the AI makes changes.\n\nüëâ Direct Manipulation\n\nAlthough AI-driven, the interface still allows for direct, hands-on adjustments. Users can drag clips to change their timing, trim their edges, or split them with a double-click.\n\n‚ú® High-Quality Export\n\nOnce the user is happy with the result, they can click the \"Export\" button to have the entire composition rendered into a high-quality MP4 video, with all the AI-generated edits, effects, and assets included."},{"heading":"üõ†Ô∏è How We Built It","content":"VibeMovie is architected as a modern monorepo, with a powerful frontend engine that talks to an intelligent backend brain. The entire system is built around a single, declarative principle: video is just data.\n\nüñ•Ô∏è The Frontend: A Full-Featured Video Editor Engine\n\nWe didn't just build a UI; we engineered a complete video editing engine from the ground up, designed to run entirely in the browser.\n\nThe Core: The interface is built on React . At its heart it is a highly interactive, scalable, and draggable timeline component . This custom component uses @dnd-kit to provide users with the precise, intuitive drag-and-drop editing experience they expect from a professional tool. State Management: State is managed by Zustand . The entire video structure‚Äîevery clip, text overlay, and transition‚Äîlives as a single, predictable JSON object. This is our \"single source of truth.\" Live Preview: We use @remotion/player to provide a frame-accurate, real-time preview. It reads the JSON state directly from Zustand and renders the visual output, giving users instant feedback on every edit.\n\nüß† The Backend: The AI Director & Render Farm\n\nOur Node.js and Express server acts as the central intelligence and heavy-lifting powerhouse for the editor.\n\nThe AI Bridge: It leverages the Google Gemini API to translate a user's natural language prompts into precise actions. We engineered a sophisticated prompt that teaches the AI how to read the incoming video JSON and rewrite it to apply the requested edits. The Render Engine: On export, the backend transforms into a powerful render farm. It uses @remotion/renderer to programmatically render the final composition into a high-quality MP4 file using a headless Chrome instance.\n\nüåâ The Bridge: Our JSON-to-Video Compiler\n\nThe secret sauce is how we connect everything. The entire video is represented as a declarative JSON object. We built a custom JSON compiler that dynamically maps this data structure into a tree of React components. Remotion then takes these components and renders them, frame by frame, into the final video. This architecture is incredibly powerful: whether the user drags a clip or the AI edits the vibe, they are both just manipulating a simple JSON object."},{"heading":"üöß Challenges We Overcame","content":"1. Teaching the AI to Edit Video Correctly\n\nTranslating ambiguous human language like \"make this cooler\" into a precise JSON data structure was our biggest hurdle. Getting the AI to consistently output perfectly structured, nested JSON without a single misplaced comma was a massive prompt engineering challenge.\n\nSolution: We developed a multi-layered approach: advanced prompts with few-shot examples, a strict schema definition provided to the AI, and a robust validation layer on our backend to catch and sanitize any malformed AI output before it could break the editor.\n\n2. Building a Performant, Interactive Editing Engine\n\nA video editor needs to feel fluid and instant. Building a performant UI with a draggable, scalable timeline that renders a live preview without lagging was a significant frontend engineering feat, especially as projects became more complex.\n\nSolution: This required a deep focus on React performance, meticulous state management with Zustand to prevent unnecessary re-renders, and leveraging battle-tested libraries like @dnd-kit to handle the complex interactive elements efficiently.\n\n3. Mastering Server-Side Video Rendering\n\nRendering a video on a server requires that the server have access to all the assets. However, blob: URLs for media uploaded in the browser are completely inaccessible to our backend rendering process.\n\nSolution: We engineered an on-demand asset pipeline . Just before rendering, the frontend identifies all local assets and uploads them to our server. The backend then remaps the paths in the JSON structure, ensuring the headless browser can find and composite every file into the final video seamlessly."},{"heading":"üèÜ Accomplishments that we're proud of","content":"A Truly Conversational Interface\n\nWe succeeded in creating an editor where complex actions can be triggered by simple, intuitive language. Watching the AI correctly interpret a command like \"add a title and make it slide in from the bottom\" and then seeing it reflected instantly on the timeline is a magical experience.\n\nThe Hybrid Editing Model\n\nWe're proud of the seamless integration between AI-driven editing and direct manual manipulation. The user is never locked into the AI's choices; they can always fine-tune the results by hand, offering the best of both worlds.\n\nThe Server-Side Rendering Pipeline\n\nBuilding a fully automated, on-demand video rendering service on the backend was a significant technical achievement. It allows for high-quality, reliable exports without freezing the user's browser, which is a common problem with client-side video rendering."},{"heading":"üéì What we learned","content":"Throughout this project, we learned that the future of creative software lies in abstracting complexity. The most powerful tool isn't necessarily the one with the most features, but the one that provides the most intuitive path from an idea to a result.\n\nRepresenting the entire video as a declarative JSON object was a revelation. It treated the video not as a monolithic file, but as a \"script\" that could be programmatically written, edited, and remixed. This approach makes it incredibly scalable and opens the door for even more powerful AI integrations in the future."},{"heading":"üó∫Ô∏è What's next for VibeMovie","content":"This hackathon project is just the beginning. We're excited about the potential to expand on this foundation:\n\nAdvanced AI Capabilities: Integrating more sophisticated AI models that can analyze video content (e.g., detecting scenes, identifying objects, generating transcripts) to perform even smarter edits. Real-Time Collaboration: Allowing multiple users to edit the same timeline, with the AI acting as a moderator and assistant. Template Library: Introducing a library of pre-built templates and effects that users can apply and customize through conversation. Expanding Media Support: Adding support for more complex media types, such as images, GIFs, and more advanced audio mixing."},{"heading":"Built With","content":"docker express.js gemini javascript node.js react remotion typescript vite zustand"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Morph","project_url":"https://devpost.com/software/morph-jcot4m","tagline":"Vibe CAD to 3D print the solutions to everyday problems in your environment, Tony Stark style, no design experience required.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/740/389/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"MLH: Best Use of MongoDB Atlas"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"snapos","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vercel","url":null}],"external_links":[{"label":"htn-frontend-seven.vercel.app","url":"https://htn-frontend-seven.vercel.app/"},{"label":"www.youtube.com","url":"https://www.youtube.com/watch?v=32oY_iV8jRo"}],"description_sections":[{"heading":"Inspiration","content":"We realized that CAD tools suck for creativity. The barrier to entry is high, the tools feel clunky and slow, and working in 2D on a screen limits how naturally people can create in 3D. We wanted to break free of those constraints and make the process as seamless as sketching on paper ‚Äî but in 3D space, grounded in the real world."},{"heading":"What it does","content":"Morph lets you design in real life, not on a screen.\n\nYou can dimension objects in context (e.g., look at your bike, gesture at a gear, and resize it with your voice). You can describe objects verbally, and our system reconstructs them in 3D. Gestures let you select, move, and edit objects naturally in space. Collaboration support means multiple people can build together. Finished models are automatically stored in a frontend STL library , where you can browse all your creations, manage versions, and prep them for printing.\n\nIt‚Äôs like having a live AI design assistant that merges physical context with digital creativity , all the way through to a print-ready workflow ."},{"heading":"How we built it","content":"LLM companion (GPT/OpenAI) to handle live prompting, pick up context, and translate natural language into design instructions. Snap 3D API for generating base objects from prompts. Voice + gesture inputs for hands-free interaction and real-world dimensioning. Trimesh + STL pipeline for mesh processing and file export. Frontend STL browser built with Vite/React to manage models and prep them for 3D printing. MongoDB backend for storing encrypted models, metadata, and embeddings. Built-in pipeline for exporting to 3D printers or integrating with CAD software."},{"heading":"Challenges we ran into","content":"Translating voice/gesture input into precise 3D edits tied to physical objects. Maintaining real-world scale across the design pipeline. Handling mesh libraries and file compatibility (STL, GLB). Building a frontend file manager that feels intuitive but still supports print-ready detail. Real-time collaboration synchronization."},{"heading":"Accomplishments that we're proud of","content":"A working pipeline that goes from voice prompt ‚Üí real-world dimensioning ‚Üí 3D model ‚Üí STL export ‚Üí print-ready library in under a minute. Built an integrated frontend STL browser that makes Morph useful beyond the hackathon demo. Lowered the barrier for 3D design, making it accessible for anyone, not just CAD experts. Proved that physical + digital hybrid design can feel natural and intuitive."},{"heading":"What we learned","content":"End-to-end usability matters ‚Äî a frontend file manager turned Morph from a cool demo into a real tool. Context matters: dimensioning in the real world makes design faster and more accurate. AI + 3D workflows can dramatically simplify prototyping when grounded in physical space. How to bridge AI generation with practical outputs that actually print."},{"heading":"What's next for Morph","content":"Expand the STL library into a shared cloud workspace with search, tagging, and version control. Add real-time multi-user editing in AR/VR spaces. Integrate physics + material simulation so models behave realistically before print. Build out a library of reusable parts for faster prototyping. Long term: make Morph the default way to dimension, design, and manage 3D models end-to-end .\n\nCheck out what people think of Morph! https://www.youtube.com/watch?v=32oY_iV8jRo"},{"heading":"Built With","content":"fastapi javascript mongodb next.js python snapos typescript vercel"},{"heading":"Try it out","content":"htn-frontend-seven.vercel.app www.youtube.com"}]},{"project_title":"CompOnion","project_url":"https://devpost.com/software/componion-7bpaxc","tagline":"Friend asking you silly questions mid-show-dialogue during the climax? No fear, CompOnion (a play on words for Companion) is here","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/743/542/datas/medium.png","prizes":[{"hackathon_name":"Hack the North 2025","hackathon_url":"https://hackthenorth2025.devpost.com/","prize_name":"MLH: Best AI Application Built with Cloudflare"}],"team_members":[],"built_with":[{"name":"cloudflare","url":"https://devpost.com/software/built-with/cloudflare"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"neon","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/taekwan-yoon/Componion-HackTheNorth2025"}],"description_sections":[{"heading":"Built With","content":"cloudflare gemini javascript neon python react"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-17T18:29:41.106293Z"}}