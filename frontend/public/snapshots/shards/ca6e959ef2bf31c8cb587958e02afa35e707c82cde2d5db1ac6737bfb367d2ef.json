{"version":"v1","hackathon_url":"https://cal-hacks-11-0.devpost.com","generated_at":"2026-02-18T16:45:02.896820Z","result":{"hackathon":{"name":"Cal Hacks 11.0","url":"https://cal-hacks-11-0.devpost.com","gallery_url":"https://cal-hacks-11-0.devpost.com/project-gallery","scanned_pages":15,"scanned_projects":352,"winner_count":62},"winners":[{"project_title":"Duet: Brainwaves -> Live Music","project_url":"https://devpost.com/software/duet-0tbkxe","tagline":"Creativity is inherent, but expressing it is hard. Duet reads live brainwaves via EEG and uses AI to generate evolving soundscapes that reflect your emotions, giving voice to your inner symphony.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/288/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: 1st Overall"}],"team_members":[],"built_with":[{"name":"emotiv","url":"https://devpost.com/software/built-with/emotiv"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"ruby","url":"https://devpost.com/software/built-with/ruby"},{"name":"singlestore","url":null},{"name":"sonicpi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/daniel04wawng/duet-frontend"},{"label":"github.com","url":"https://github.com/soapantelope/duet-backend"}],"description_sections":[{"heading":"About the project","content":"Inspiration\n\nDuet revolutionizes the way children with developmental disabilities—approximately 1 in 10 in the United States—express their creativity through music by harnessing EEG technology to translate brainwaves into personalized musical experiences.\n\nDaniel and Justin have extensive experience teaching music to children, but working with those who have developmental disabilities presents unique challenges:\n\nIdentifying and adapting resources for non-verbal and special needs students. Integrating music therapy principles into lessons to foster creativity. Encouraging improvisation to facilitate emotional expression. Navigating the complexities of individual accessibility needs.\n\nUnfortunately, many children are left without the tools they need to communicate and express themselves creatively. That's where Duet comes in. By utilizing EEG technology, we aim to transform the way these children interact with music, giving them a voice and a means to share their feelings.\n\nAt Duet, we are committed to making music an inclusive experience for all, ensuring that every child—and anyone who struggles to express themselves—has the opportunity to convey their true creative self!\n\nWhat it does:\n\nWear an EEG Experience your brain waves as music! Focus and relaxation levels will change how fast/exciting vs. slow/relaxing the music is.\n\nHow we built it:\n\nWe started off by experimenting with Emotiv’s EEGs — devices that feed a stream of brain wave activity in real time! After trying it out on ourselves, the CalHacks stuffed bear, and the Ariana Grande cutout in the movie theater, we dove into coding. We built the backend in Python, leveraging the Cortex library that allowed us to communicate with the EEGs. For our database, we decided on SingleStore for its low latency, real-time applications, since our goal was to ultimately be able to process and display the brain wave information live on our frontend.\n\nTraditional live music is done procedurally, with rules manually fixed by the developer to decide what to generate. On the other hand, existing AI music generators often generate sounds through diffusion-like models and pre-set prompts. However, we wanted to take a completely new approach — what if we could have an AI be a live “composer”, where it decided based on the previous few seconds of live emotional data, a list of available instruments it can select to “play”, and what it previously generated to compose the next few seconds of music? This way, we could have live AI music generation (which, to our knowledge, does not exist yet). Powered by Google’s Gemini LLM, we crafted a prompt that would do just that — and it turned out to be not too shabby!\n\nTo play our AI-generated scores live, we used Sonic Pi, a Ruby-based library that specializes in live music generation (think DJing in code). We fed this and our brain wave data to a frontend built in Next.js to display the brain waves from the EEG and sound spectrum from our audio that highlight the correlation between them.\n\nChallenges:\n\nOur biggest challenge was coming up with a way to generate live music with AI. We originally thought it was impossible and that the tech wasn’t “there” yet — we couldn’t find anything online about it, and even spent hours thinking about how to pivot to another idea that we could use our EEGs with.\n\nHowever, we eventually pushed through and came up with a completely new method of doing live AI music generation that, to our knowledge, doesn’t exist anywhere else! It was most of our first times working with this type of hardware, and we ran into many issues with getting it to connect properly to our computers — but in the end, we got everything to run smoothly, so it was a huge feat for us to make it all work!\n\nWhat’s next for Duet?\n\nMusic therapy is on the rise – and Duet aims to harness this momentum by integrating EEG technology to facilitate emotional expression through music. With a projected growth rate of 15.59% in the music therapy sector, our mission is to empower kids and individuals through personalized musical experiences. We plan to implement our programs in schools across the states, providing students with a unique platform to express their emotions creatively. By partnering with EEG companies, we’ll ensure access to the latest technology, enhancing the therapeutic impact of our programs. Duet gives everyone a voice to express emotions and ideas that transcend words, and we are committed to making this future a reality!\n\nBuilt with:\n\nEmotiv EEG headset SingleStore real-time database Python Google Gemini Sonic Pi (Ruby library) Next.js"},{"heading":"Built With","content":"emotiv gemini next.js python ruby singlestore sonicpi"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"SnackSnap - recycle to feed your pet","project_url":"https://devpost.com/software/snacksnap-lw4b9q","tagline":"SnackSnap is an iOS app leveraging Gemini's ML image recognition to verify recycling. Snap a picture while recycling to earn snacks to feed your digital pet. As the pet grows, carbon waste is reduced.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/469/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: 3rd Overall"}],"team_members":[],"built_with":[{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"firebasestorage","url":null},{"name":"gemini-1.5-pro","url":null},{"name":"googlegenerativeai","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"},{"name":"swiftui","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Let's be honest. Sometimes, we have a paper cup, and we look at both the trash can and the recycling bin. We might throw the paper cup away in the trash because the recycling is just a little further. I'm definitely a culprit.\n\nOur team set out to invent a fun perception of recycling by creating a digital pet that can only be cared for through recycling verified by Gemini's ML image recognition.\n\nSomething as simple as a tiny pet, backed by the complexity of Gemini, makes me take that extra step to throw away that paper cup into the recycling bin—to make sure my pet survives and keeps the world green."},{"heading":"What it does","content":"Take a photo of yourself recycling an item, and using image recognition, Gemini checks if it is a valid photo. After a successful photo, you can feed your digital pet a ton of snacks making your pet get progressively get bigger and bigger..."},{"heading":"How we built it","content":"We began sketching each page to track the user's potential dopamine flow. Usually, recycling is seen as an inconvenience, especially since they are outnumbered by regular trashcans 2-to-1. We wanted the users to associate with recycling positively, so we created a pet for them to take care of. This emotional investment changes the idea of recycling from a burden to an opportunity to care for your digital pet."},{"heading":"Challenges we ran into","content":"Integrating Gemini API was difficult at the start, but it was smooth the second time we tried. Easily viewable color scheme. Staying awake."},{"heading":"Accomplishments that we're proud of","content":"'Baby Chester' turning out as a cute pet The quick start to building SnackSnap making the rest of the days less stressful."},{"heading":"What we learned","content":"Shipping Fast!!! Pushing the limits of getting deep work done. The excitement of working with Gemini. Working with each other's strengths. Trust in each other."},{"heading":"What's next for SnackSnap","content":"Your pet can have babies. Discover new verticals. Future integration into spatial computing where there is virtually no friction for the user and we can auto-track their recycling activity."},{"heading":"Built With","content":"firebase firebasestorage gemini-1.5-pro googlegenerativeai swift swiftui"}]},{"project_title":"GhostWriter","project_url":"https://devpost.com/software/ghostwriter-491kuy","tagline":"your silent partner in progress","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/087/883/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: 2nd Overall"}],"team_members":[],"built_with":[{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"deepgram","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"llama","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/rqchao/calhacks24"}],"description_sections":[{"heading":"Inspiration","content":"Introducing Ghostwriter: Your silent partner in progress. Ever been in a class where resources are so hard to come by, you find yourself practically living at office hours? As teaching assistants on increasingly short-handed course staffs , it can be difficult to keep up with student demands while making long-lasting improvements to your favorite courses. Imagine effortlessly improving your course materials as you interact with students during office hours. Ghostwriter listens intelligently to these conversations , capturing valuable insights and automatically updating your notes and class documentation. No more tedious post-session revisions or forgotten improvement ideas. Instead, you can really focus on helping your students in the moment . Ghostwriter is your silent partner in educational excellence, turning every interaction into an opportunity for long-term improvement. It's the invisible presence that delivers visible results, making continuous refinement effortless and impactful. With Ghostwriter, you're not just tutoring or bug-bashing - you're evolving your content with every conversation ."},{"heading":"What it does","content":"Ghostwriter hosts your class resources, and supports searching across them in many ways (by metadata, semantically by content). It allows adding, deleting, and rendering markdown notes. However, Ghostwriter's core feature is in its recording capabilities. The record button starts a writing session. As you speak, Ghostwriter will transcribe and digest your speech, decide whether it's worth adding to your notes, and if so, navigate to the appropriate document and insert them at a line-by-line granularity in your notes, integrating seamlessly with your current formatting."},{"heading":"How we built it","content":"We used Reflex to build the app full-stack in Python, and support the various note-management features including addition, deleting, selecting, and rendering. As notes are added to the application database, they are also summarized and then embedded by Gemini 1.5 Flash-8B before being added to ChromaDB with a shared key. Our semantic search is also powered by Gemini-embedding and ChromaDB.\n\nThe recording feature is powered by Deepgram's threaded live-audio transcription API. The text is processed live by Gemini, and chunks are sent to ChromaDB for queries. Distance metrics are used as thresholds to not create notes, add to an existing note, or create a new note. In the latter two cases, llama3-70b-8192 is run through Groq to write on our (existing) documents. It does this through a RAG on our docs, as well as some prompt-engineering. To make insertion granular we add unique tokens to identify candidate insertion-points throughout our original text. We then structurally generate the desired markdown, as well as the desired point of insertion, and render the changes live to the user."},{"heading":"Challenges we ran into","content":"Using Deepgram and live-generation required a lot of tasks to run concurrently, without blocking UI interactivity. We had some trouble reconciling the requirements posed by Deepgram and Reflex on how these were handled, and required us redesign the backend a few times. Generation was also rather difficult, as text would come out with irrelevant vestiges and explanations. It took a lot of trial and error through prompting and other tweaks to the generation calls and structure to get our required outputs."},{"heading":"Accomplishments that we're proud of","content":"Our whole live note-generation pipeline! From audio transcription process to the granular retrieval-augmented structured generation process. Spinning up a full-stack application using Reflex (especially the frontend, as two backend engineers) We were also able to set up a few tools to push dummy data into various points of our process, which made debugging much, much easier."},{"heading":"What's next for GhostWriter","content":"Ghostwriter can work on the student-side as well, allowing a voice-interface to improving your own class notes, perhaps as a companion during lecture. We find Ghostwriter's note identification and improvement process very useful ourselves. On the teaching end, we hope GhostWriter will continue to grow into a well-rounded platform for educators on all ends. We envision that office hour questions and engagement going through our platform can be aggregated to improve course planning to better fit students' needs. Ghostwriter's potential doesn't stop at education. In the software world, where companies like AWS and Databricks struggle with complex documentation and enormous solutions teams, Ghostwriter shines. It transforms customer support calls into documentation gold, organizing and structuring information seamlessly. This means fewer repetitive calls and more self-sufficient users!"},{"heading":"Built With","content":"chroma deepgram gemini groq llama python react reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Talk Tuah","project_url":"https://devpost.com/software/talk-tuah","tagline":"Ever struggled in social settings? Not knowing what to say next? Meet Talk Tuah, AR glasses with a built in AI chatbot trained in social etiquette, as long as a mobile app to review transcripts.","preview_image_url":"https://d2dmyh35ffsxbl.cloudfront.net/assets/shared/devpost_social_icon_200_200-f56e5af715a1d95e0209bb37e899b7c18c6e7e3b933a3c1f52456a6e2ee85d09.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: Best Beginner Hack"},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Deepgram: Deepgram Voice Agent Quest"}],"team_members":[],"built_with":[{"name":"ar","url":null},{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"c","url":"https://devpost.com/software/built-with/c"},{"name":"deepgram","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"hume","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"ngrok","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"}],"external_links":[{"label":"github.com","url":"https://github.com/steven-mu12/talk-tuah/tree/main"},{"label":"www.figma.com","url":"https://www.figma.com/design/JkaK6eYw329qaWfgYo9nFC/TalkTuah?node-id=14-123&t=oDez8OD6kFvl9xpN-1"}],"description_sections":[{"heading":"Inspiration","content":"This idea came from one of our team members (who will remain unnamed) who struggles with talking to women. We wanted to build this product to help people who are introverts or socially awkward be able to find easy and positive responses to any conversation I person might face."},{"heading":"What it does","content":"The AR glasses are built with a camera and microphone to capture the facial expressions and voice of anybody speaking to the user. Using an advanced algorithm our AI chatbot is able to determine the mood of the speaker based on audio and visual cues. This response is then transmitted back through an OLED display in the glasses and provides a response to which the user can say. All these interactions, including name, hobbies, likes, dislikes etc. are also stored in a server to train the AI in future conversations. Transcripts are stored in a separate mobile app which users can access to look back on previous conversations"},{"heading":"How we built it","content":"We used an openAI API to build and train our chatbot to determine the most socially responsible and positive response based on an input. We also used the Deepgram API to convert speech to text and determine the mood behind a secondary person's input. The mobile app was built using React Native and built the glasses using self bought parts."},{"heading":"Challenges we ran into","content":"One major challenge we ran into was setting up the server to store personal data, of both the user and their chatters. We struggled to develop a secure inter-device connection between the data storage and the mobile app as speech data needed to be uploaded. We found a solution by using ngrok to allow the mobile device to connect."},{"heading":"Accomplishments that we're proud of","content":"Our team is most proud of our implementation of the mobile app. We truly believe that the app was the perfect way to tie our product in and make it more accessible for people by allowing customization and including custom prompts."},{"heading":"What we learned","content":"Many of us learned how to code in a language we didn't feel confident in or knew at all. This was a great learning experience that we all believe will carry us through our desired career paths and positions."},{"heading":"What's next for Talk Tuah","content":"Creating a customizable chatbot brings a lot of creativity and opportunities for growth. We believe there are ample opportunities to apply this outside of simple day to day conversations, whether that is teachers determining what units their students are struggling on based vocal and facial expressions, to how to craft a speech on the fly based on your audience's response to certain topics/statements."},{"heading":"Built With","content":"ar arduino c deepgram flask hume javascript ngrok python raspberry-pi react-native"},{"heading":"Try it out","content":"github.com www.figma.com"}]},{"project_title":"Unreal EngJam","project_url":"https://devpost.com/software/unreal-engjam","tagline":"A cursed programming language and game engine inside of Figma.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/730/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: Most Entertaining Hack"},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Warp: Best Developer Tool by Warp"}],"team_members":[],"built_with":[{"name":"figma","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/kognise/aldente"}],"description_sections":[{"heading":"Think of All the Benefits","content":"Why be limited to syntax highlighting when you can color-code your software? Whiteboard out math and algorithms in the same space as your business logic The visual programming appeal of Scratch, with the power and robustness of a real programming language Comments are first-class, graphical, and move around as you refactor (and refactoring is as easy as drag-and-drop)"},{"heading":"Clippy","content":"Make a mistake? Your favorite office assistant will hover above the relevant code and help you out:"},{"heading":"How We Built It","content":"We wrote the programming language entirely from scratch (no dependencies) in TypeScript. We use Figma's plugin API to traverse the document and generate an AST which we can then interpret. We provide the programmer with access to render to and manipulate a \"game window\" inside Figma.\n\nIt is important to sufficiently emphasize that this is a full, statically-typed, dynamically-bound novel programming language designed explicitly for the medium of Figma flowcharts. For example, to define a variable you draw a box. To store values, you can draw an arrow to the box, and to read values you draw an arrow from the box. With arrows, nothing needs to be referenced by name and refactoring is easy.\n\nThe assignment of function and infix operator arguments are inferred first by applied name and then internal type."},{"heading":"Challenges We Ran Into","content":"Figma runs plugins in WASM which means it is extremely difficult to debug code, and simple bugs like stack overflows often result in cryptic low-level memory leak errors.\n\nAlso, 3D models are hard to get right :)"},{"heading":"Built With","content":"figma typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Blockify","project_url":"https://devpost.com/software/blockify-x2owt7","tagline":"Empowering Artists with Blockchain: Seamless Royalty Payments and Ownership Control Through Spotify Integration.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/170/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks / Berkeley RDI: Berkeley RDI Web3 Prize"},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Sui: Reimagine the internet with Sui"}],"team_members":[],"built_with":[{"name":"blockchain","url":"https://devpost.com/software/built-with/blockchain"},{"name":"move","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react.js","url":null},{"name":"spotify","url":"https://devpost.com/software/built-with/spotify"},{"name":"sui","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ChiragSeth14/CalHacks"}],"description_sections":[{"heading":"Inspiration","content":"The project was inspired by looking at the challenges that artists face when dealing with traditional record labels and distributors. Artists often have to give up ownership of their music, lose creative control, and receive only a small fraction of the revenue generated from streams. Record labels and intermediaries take the bulk of the earnings, leaving the artists with limited financial security. Being a music producer and a DJ myself, I really wanted to make a product with potential to shake up this entire industry for the better. The music artists spend a lot of time creating high quality music and they deserve to be paid for it much more than they are right now."},{"heading":"What it does","content":"Blockify lets artists harness the power of smart contracts by attaching them to their music while uploading it and automating the process of royalty payments which is currently a very time consuming process. Our primary goal is to remove the record labels and distributors from the industry since they they take a majority of the revenue which the artists generate from their streams for the hard work which they do. By using a decentralized network to manage royalties and payments, there won't be any disputes regarding missed or delayed payments, and artists will have a clear understanding of how much money they are making from their streams since they will be dealing with the streaming services directly. This would allow artists to have full ownership over their work and receive a fair compensation from streams which is currently far from the reality."},{"heading":"How we built it","content":"BlockChain: We used the Sui blockchain for its scalability and low transaction costs. Smart contracts were written in Move, the programming language of Sui, to automate royalty distribution. Spotify API: We integrated Spotify's API to track streams in real time and trigger royalty payments. Wallet Integration: Sui wallets were integrated to enable direct payments to artists, with real-time updates on royalties as songs are streamed. Frontend: A user-friendly web interface was built using React to allow artists to connect their wallets, and track their earnings. The frontend interacts with the smart contracts via the Sui SDK."},{"heading":"Challenges we ran into","content":"The most difficult challenge we faced was the Smart Contract Development using the Move language. Unlike commonly known language like Ethereum, Move is relatively new and specifically designed to handle asset management. Another challenge was trying to connect the smart wallets in the application and transferring money to the artist whenever a song was streamed, but thankfully the mentors from the Sui team were really helpful and guided us in the right path."},{"heading":"Accomplishments that we're proud of","content":"This was our first time working with blockchain, and me and my teammate were really proud of what we were able to achieve over the two days. We worked on creating smart contracts and even though getting started was the hardest part but we were able to complete it and learnt some great stuff along the way. My teammate had previously worked with React but I had zero experience with JavaScript, since I mostly work with other languages, but we did the entire project in Node and React and I was able to learn a lot of the concepts in such a less time which I am very proud of myself for."},{"heading":"What we learned","content":"We learned a lot about Blockchain technology and how can we use it to apply it to the real-world problems. One of the most significant lessons we learned was how smart contracts can be used to automate complex processes like royalty payments. We saw how blockchain provides an immutable and auditable record of every transaction, ensuring that every stream, payment, and contract interaction is permanently recorded and visible to all parties involved. Learning more and more about this technology everyday just makes me realize how much potential it holds and is certainly one aspect of technology which would rule the future. It is already being used in so many aspects of life and we are still discovering the surface."},{"heading":"What's next for Blockify","content":"We plan to add more features, such as NFTs for exclusive content or fan engagement, allowing artists to create new revenue streams beyond streaming. There have been some real-life examples of artists selling NFT's to their fans and earning millions from it, so we would like to tap into that industry as well. Our next step would be to collaborate with other streaming services like Apple Music and eliminate record labels to the best of our abilities."},{"heading":"Built With","content":"blockchain move node.js react.js spotify sui"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Resililink","project_url":"https://devpost.com/software/resilink","tagline":"A Lifeline When It Matters Most","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/116/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: Hack for Impact"},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Skylo: Seamlessly Switch from Cellular to Satellite"}],"team_members":[],"built_with":[{"name":"lora","url":null},{"name":"modem","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"radio","url":null},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"skylo","url":null},{"name":"vite","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/12458/calhacks"}],"description_sections":[{"heading":"Built With","content":"lora modem python radio raspberry-pi react skylo vite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MafiAI","project_url":"https://devpost.com/software/mafiai","tagline":"Mafia with LLMs!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/091/254/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: Hack on AI"}],"team_members":[],"built_with":[{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/KevinL10/mafia"}],"description_sections":[{"heading":"Built With","content":"python react typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"PulseWalk","project_url":"https://devpost.com/software/pulsewalk","tagline":"Our cutting-edge shoe enhances awareness for the visually impaired by combining haptic, pressurebased feedback, and infrared tech, detecting terrain changes and obstacles for safer, smoother mobility.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/091/198/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: Best Hardware Hack"}],"team_members":[],"built_with":[{"name":"android","url":"https://devpost.com/software/built-with/android"},{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"c++","url":"https://devpost.com/software/built-with/c--3"},{"name":"dart","url":null},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"}],"external_links":[{"label":"github.com","url":"https://github.com/lucasreljic/Cal-Hacks-shoe-thing"}],"description_sections":[{"heading":"What inspired you?","content":"The inspiration behind this project came from my grandmother, who has struggled with poor vision for years. Growing up, I witnessed firsthand how her limited sight made daily tasks and walking in unfamiliar environments increasingly difficult for her. I remember one specific instance when she tripped on an uneven curb outside the grocery store. Though she wasn’t hurt, the fall shook her confidence, and she became hesitant to go on walks or run errands by herself. This incident helped spark the idea of creating something that could help people like her feel safer and more secure while navigating their surroundings. I wanted to develop a solution that would give visually impaired individuals not just mobility but confidence. The goal became clear: to create a product that could intuitively guide users through their environment, detecting obstacles like curbs, steps, and uneven terrain, and providing feedback they could easily understand. By incorporating haptic feedback, pressure-based sensors, and infrared technology, this system is designed to give users more control and awareness over their movements, helping them move through the world with greater independence and assurance. My hope is that this technology can empower people like my grandmother to reclaim their confidence and enjoy everyday activities without fear."},{"heading":"What it does","content":"This project is a smart shoe system designed to help visually impaired individuals safely navigate their surroundings by detecting obstacles and terrain changes. It uses infrared sensors located on both the front and bottom of the shoe to detect the distance to obstacles like curbs and stairs. When the user approaches an obstacle, the system provides real-time feedback through 5 servos. 3 servos are responsible for haptic feedback related to the distance from the ground, and distance in front of them, while the remaining servos are related to help guiding the user through navigation. The infrared sensors detect how far the foot is off the ground, and the servos respond accordingly. The vibrational motors, labeled 1a and 2a, are used when the distance exceeds 6 inches, delivering pulsating signals to inform the user of upcoming terrain changes. This real-time feedback ensures users can sense potential dangers and adjust their steps to prevent falls or missteps. Additionally the user would connect to the shoe based off of bluetooth. The shoe system operates using three key zones of detection: the walking range (0-6 inches), the far walking range (6-12 inches), and the danger zone (12+ inches). In the walking range, the haptic feedback is minimal but precise, giving users gentle vibrations when the shoe detects small changes, such as a flat surface or minor elevation shifts. As the shoe moves into the far walking range (6-12 inches), where curbs or stairs may appear, the intensity of the feedback increases, and the vibrational motors start to pulse more frequently. This alert serves as a warning that the user is approaching a significant elevation change. When the distance exceeds 12 inches—the danger zone—the vibrational motors deliver intense, rapid feedback to indicate a drop-off or large obstacle, ensuring the user knows to take caution and adjust their step. These zones are carefully mapped to provide a seamless understanding of the terrain without overwhelming the user. The system also integrates seamlessly with a mobile app, offering GPS-based navigation via four directional haptic feedback sensors that guide the user forward, backward, left, or right. Users can set their route through voice commands, unfortunately we had trouble integrating Deepgram AI, which would assist by understanding speech patterns, accents, and multiple languages, making it accessible to people who are impaired lingually. Additionally we had trouble integrating Skylo, which the idea would be to serve areas where Wi-Fi is unavailable, or connection unstable, the system automatically switches to Skylo via their Type1SC circuit board and antenna, a satellite backup technology, to ensure constant connectivity. Skylo sends out GPS updates every 1-2 minutes, preventing the shoe from losing its route data. If the user strays off course, Skylo triggers immediate rerouting instructions through google map’s api in the app which we did set up in our app, ensuring that they are safely guided back on track. This combination of sensor-driven feedback, haptic alerts, and robust satellite connectivity guarantees that visually impaired users can move confidently through diverse environments."},{"heading":"How we built it","content":"We built this project using a combination of hardware components and software integrations. To start, we used infrared sensors placed at the front and bottom of the shoe to detect distance and obstacles. We incorporated five servos into the design: three for haptic feedback based on distance sensing 3 and two for GPS-related feedback. Additionally, we used vibrational motors (1a and 2a) to provide intense feedback when larger drops or obstacles were detected. The app we developed integrates the Google Maps API for route setting and navigation. To ensure connectivity in areas with limited Wi-Fi, we integrated Skylo’s Type 1SC satellite hardware, allowing for constant GPS data transmission even in remote areas. For the physical prototype, we constructed a 3D model of a shoe out of cardboard. Attached to this model are two 5000 milliamp-hour batteries, providing a total of 10000 mAh to power the system. We used an ESP32 microcontroller to manage the various inputs and outputs, along with a power distribution board to efficiently allocate power to the servos, sensors, and vibrational motors. All components were securely attached to the cardboard shoe prototype to create a functional model for testing."},{"heading":"Challenges we ran into","content":"One of the main challenges we encountered was working with the Skylo Type 1SC hardware. While the technology itself was impressive, the PDF documentation and schematics were quite advanced, requiring us to dive deeper into understanding the technical details. We successfully established communication between the Arduino and the Type 1SC circuit but faced difficulties in receiving a response back from the modem, which required further troubleshooting. Additionally, distinguishing between the different components on the circuit, such as data pins and shorting components, proved challenging, as the labeling was intricate and required careful attention. These hurdles allowed us to refine our skills in circuit analysis and deepen our knowledge of satellite communication systems. On the software side, we had to address several technical challenges. Matching the correct Java version for our app development was more complex than expected, as version discrepancies affected performance. We also encountered difficulties creating a Bluetooth hotspot that could seamlessly integrate with the Android UI for smooth user interaction. On the hardware end, ensuring reliable connections was another challenge; we found that some of our solder joints for the pins weren’t as stable as needed, leading to occasional issues with connectivity. Through persistent testing and adjusting our approaches, we were able to resolve most of these challenges while gaining valuable experience in both hardware and software integration."},{"heading":"Accomplishments that we're proud of","content":"One of the accomplishments we’re most proud of is successfully setting up Skylo services and establishing satellite connectivity, allowing the system to access LTE data in areas with low or no Wi-Fi. This was a key component of the project, and getting the hardware to communicate with satellites smoothly was a significant milestone. Despite the initial challenges with understanding the complex schematics, we were able to wire the Arduino to the Type 1SC board correctly, ensuring that the system could relay GPS data and maintain consistent communication. The experience gave us a deeper appreciation for satellite technology and its role in enhancing connectivity for projects like ours.\n\nAdditionally, we’re proud of how well the array of sensors was set up and how all the hardware components functioned together. Each sensor, whether for terrain detection or obstacle awareness, worked seamlessly with the servos and haptic feedback system, resulting in better-than-expected performance. The responsiveness of the hardware components was more precise and reliable than we had originally anticipated, which demonstrated the strength of our design and implementation. This level of integration and functionality validates our approach and gives us confidence in the potential impact this project can have for the visually impaired community."},{"heading":"What we learned","content":"Throughout this project, we gained a wide range of new skills that helped bring the system to life. One of our team members learned to solder, which was essential for securing the hardware components and making reliable connections. We also expanded our knowledge in React system programming, which allowed us to create the necessary interactions and controls for the app. Additionally, learning to use Flutter enabled us to develop a smooth and responsive mobile interface that integrates with the hardware components. On the hardware side, we became much more familiar with the ESP32 microcontroller, particularly its Bluetooth connectivity functions, which were crucial for communication between the shoe and the mobile app. We also had the opportunity to dive deep into working with the Type 1SC board, becoming comfortable with its functionality and satellite communication features. These new skills not only helped us solve the challenges within the project but also gave us valuable experience for future work in both hardware and software integration."},{"heading":"What's next for PulseWalk","content":"Next for PulseWalk, we plan to enhance the system's capabilities by refining the software to provide even more precise feedback and improve user experience. We aim to integrate additional features, such as obstacle detection for more complex terrains and improved GPS accuracy with real-time rerouting. Expanding the app’s functionality to include more languages and customization options using Deepgram AI will ensure greater accessibility for a diverse range of users. Additionally, we’re looking into optimizing battery efficiency and exploring more durable materials for the shoe design, moving beyond the cardboard prototype. Ultimately, we envision PulseWalk evolving into a fully commercialized product that offers a seamless, dependable mobility aid for the visually impaired which partners with shoe brands to bring a minimalist approach to the brand and make it look less like a medical device and more like an everyday product."},{"heading":"Built With","content":"android arduino c++ dart react-native"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Refrigerator Ramsay","project_url":"https://devpost.com/software/refrigerator-ramsay","tagline":"AI-powered kitchen assistant that reduces food waste with smart recipes and real-time interaction.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/086/324/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Google: Most useful app"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"gemini-ai","url":null},{"name":"hume-evi","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"github.com","url":"https://github.com/sherwinvishesh/Refrigerator-Ramsay"}],"description_sections":[{"heading":"Inspiration","content":"The idea for Refrigerator Ramsay came from a common problem we all face: food waste. So many times, we forget about ingredients in the fridge, or we don't know how to use them, and they go bad. We wanted to create something that could help us not only use those ingredients but also inspire us to try new recipes and reduce food waste in the process."},{"heading":"What it does","content":"Refrigerator Ramsay is an AI-powered kitchen assistant that helps users make the most of their groceries. You can take a picture of your fridge or pantry, and the AI will identify the ingredients. Then, it suggests recipes based on what you have. It also has an interactive voice feature, so you can ask for cooking tips or get help with recipes in real-time."},{"heading":"How we built it","content":"We used Google Gemini AI for image recognition to identify ingredients from photos. and then, to come up with creative meal ideas. We also added Hume EVI , which is an emotionally intelligent voice AI, to make the experience more interactive. This lets users chat with the AI to get tips, learn new cooking techniques, or personalize recipes."},{"heading":"Challenges we ran into","content":"One challenge we faced was working with Hume EVI for the first time. It was really fun to explore, but there was definitely a learning curve. We had to figure out how to make the voice assistant not just smart but also engaging, making sure it could deliver cooking tips naturally and understand what users were asking. It took some trial and error to get the right balance between information and interaction.\n\nAnother challenge was training the Google Gemini AI to be as accurate as possible when recognizing ingredients. It wasn't easy to ensure the AI could reliably detect different foods, especially when items were stacked or grouped together in photos. We had to spend extra time fine-tuning the model so that it could handle a variety of lighting conditions and product packaging."},{"heading":"Accomplishments that we're proud of","content":"We're proud of creating a solution that not only helps reduce food waste but also makes cooking more fun. The AI's ability to suggest creative recipes from random ingredients is really exciting. We're also proud of how user-friendly (and funny) the voice assistant turned out to be, making it feel like you're cooking with a friend."},{"heading":"What we learned","content":"We learned a lot about how AI can be used to solve everyday problems, like reducing food waste. Working with Hume EVI taught us about building conversational AI, which was new for us. It was really fun, but there was definitely a challenge in figuring out how to make the voice interaction feel natural and helpful at the same time.\n\nWe also learned about the importance of training AI models , especially with the Gemini AI . Getting the image recognition to accurately identify different ingredients required us to experiment a lot with the data and train the model to work in a variety of environments. This taught us that accuracy is key when it comes to creating a seamless user experience."},{"heading":"What's next for Refrigerator Ramsay","content":"Next, we want to improve the AI’s ability to recognize even more ingredients and maybe even offer nutritional advice. We’re also thinking about adding features where the AI can help you plan meals for the whole week based on what you have. Eventually, we’d love to partner with grocery stores to suggest recipes based on store deals, helping users save money too!"},{"heading":"Built With","content":"fastapi gemini-ai hume-evi react"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TeachXR","project_url":"https://devpost.com/software/teachxr","tagline":"World's First AI Study Assistant in Extended Reality","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/085/927/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cal Hacks: Hackers' Choice"},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"cartesia","url":null},{"name":"deepgram","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mediapipe","url":null},{"name":"ocr","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"singlestore","url":null},{"name":"tailwind","url":null},{"name":"vapi","url":null},{"name":"xr","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/VietRocHack/TeachXR-frontend/"},{"label":"github.com","url":"https://github.com/VietRocHack/TeachXR-backend-dataflow"},{"label":"github.com","url":"https://github.com/VietRocHack/TeachXR-backend-gesture"}],"description_sections":[{"heading":"Inspiration","content":"Across the globe, a critical shortage of qualified teachers poses a significant challenge to education. The average student-to-teacher ratio in primary schools worldwide stands at an alarming 23:1! In some regions of Africa, this ratio skyrockets to an astonishing 40:1 . Research 1 and Research 2\n\nAs populations continue to explode, the demand for quality education has never been higher, yet the supply of capable teachers is dwindling . This results in students receiving neither the attention nor the personalized support they desperately need from their educators.\n\nMoreover, a staggering 20% of students experience social anxiety when seeking help from their teachers. This anxiety can severely hinder their educational performance and overall learning experience. Research 3\n\nWhile many educational platforms leverage generative AI to offer personalized support, we envision something even more revolutionary. Introducing TeachXR—a fully voiced, interactive, and hyper-personalized AI teacher that allows students to engage just like they would with a real educator, all within the immersive realm of extended reality.\n\nImagine a world where every student has access to a dedicated tutor who can cater to their unique learning styles and needs. With TeachXR, we can transform education, making personalized learning accessible to all. Join us on this journey to revolutionize education and bridge the gap in teacher shortages!"},{"heading":"What it does","content":"Introducing TeachVR: Your Interactive XR Study Assistant\n\nTeachVR is not just a simple voice-activated Q&A AI; it’s a fully interactive extended reality study assistant designed to enhance your learning experience. Here’s what it can do:\n\nIntuitive Interaction : Use natural hand gestures to circle the part of a textbook page that confuses you. Focused Questions : Ask specific questions about the selected text for summaries, explanations, or elaborations. Human-like Engagement : Interact with TeachVR just like you would with a real person, enjoying milliseconds response times and a human voice powered by Vapi.ai . Multimodal Learning : Visualize the concepts you’re asking about, aiding in deeper understanding. Personalized and Private : All interactions are tailored to your unique learning style and remain completely confidential.\n\nHow to Ask Questions:\n\nCircle the Text : Point your finger and circle the paragraph you want to inquire about. OK Gesture : Use the OK gesture to crop the image and submit your question.\n\nTeachVR's Capabilities:\n\nSummarization : Gain a clear understanding of the paragraph's meaning. TeachVR captures both book pages to provide context. Examples : Receive relevant examples related to the paragraph. Visualization : When applicable, TeachVR can present a visual representation of the concepts discussed. Unlimited Queries : Feel free to ask anything! If it’s something your teacher can answer, TeachVR can too!\n\nInteractive and Dynamic:\n\nTeachVR operates just like a human. You can even interrupt the AI if you feel it’s not addressing your needs effectively!"},{"heading":"How we built it","content":"TeachXR: A Technological Innovation in Education\n\nTeachXR is the culmination of advanced technologies, built on a microservice architecture. Each component focuses on delivering essential functionalities:\n\n1. Gesture Detection and Image Cropping\n\nWe have developed and fine-tuned a hand gesture detection system that reliably identifies gestures for cropping based on MediaPipe gesture detection . Additionally, we created a custom bounding box cropping algorithm to ensure that the desired paragraphs are accurately cropped by users for further Q&A.\n\n2. OCR (Word Detection)\n\nUtilizing Google AI OCR service , we efficiently detect words within the cropped paragraphs, ensuring speed, accuracy, and stability. Given our priority on latency—especially when simulating interactions like pointing at a book—this approach aligns perfectly with our objectives.\n\n3. Real-time Data Orchestration\n\nOur goal is to replicate the natural interaction between a student and a teacher as closely as possible. As mentioned, latency is critical. To facilitate the transfer of image and text data, as well as real-time streaming from the OCR service to the voiced assistant, we built a robust data flow system using the SingleStore database . Its powerful real-time data processing and lightning-fast queries enable us to achieve sub-1-second cropping and assistant understanding for prompt question-and-answer interactions.\n\n4. Voiced Assistant\n\nTo ensure a natural interaction between students and TeachXR, we leverage Vapi , a natural voice interaction orchestration service that enhances our feature development. By using DeepGram for transcription, Google Gemini 1.5 flash model as the AI “brain,” and Cartesia for a natural voice, we provide a unique and interactive experience with your virtual teacher—all within TeachXR."},{"heading":"Challenges we ran into","content":"Challenges in Developing TeachXR\n\nBuilding the architecture to keep the user-cropped image in sync with the chat on the frontend posed a significant challenge. Due to the limitations of the Meta Quest 3 , we had to run local gesture detection directly on the headset and stream the detected image to another microservice hosted in the cloud. This required us to carefully adjust the size and details of the images while deploying a hybrid model of microservices. Ultimately, we successfully navigated these challenges.\n\nAnother difficulty was tuning our voiced assistant. The venue we were working in was quite loud, making background noise inevitable. We had to fine-tune several settings to ensure our assistant provided a smooth and natural interaction experience."},{"heading":"Accomplishments that we're proud of","content":"Achievements\n\nWe are proud to present a complete and functional MVP! The cropped image and all related processes occur in under 1 second , significantly enhancing the natural interaction between the student and TeachVR ."},{"heading":"What we learned","content":"Developing a Great AI Application\n\nWe successfully transformed a solid idea into reality by utilizing the right tools and technologies.\n\nThere are many excellent pre-built solutions available, such as Vapi , which has been invaluable in helping us implement a voice interface. It provides a user-friendly and intuitive experience, complete with numerous settings and plug-and-play options for transcription, models, and voice solutions."},{"heading":"What's next for TeachXR","content":"We’re excited to think of the future of TeachXR holds even greater innovations! we’ll be considering* adaptive learning algorithms * that tailor content in real-time based on each student’s progress and engagement.\n\nAdditionally, we will work on integrating multi-language support to ensure that students from diverse backgrounds can benefit from personalized education. With these enhancements, TeachXR will not only bridge the teacher shortage gap but also empower every student to thrive, no matter where they are in the world!"},{"heading":"Built With","content":"cartesia deepgram gemini javascript mediapipe ocr react singlestore tailwind vapi xr"},{"heading":"Try it out","content":"github.com github.com github.com"}]},{"project_title":"Storyscape","project_url":"https://devpost.com/software/storyscape","tagline":"We brought choose your own adventure back! In the form of guided text adventures, Storyscape enables elementary and middle school students to learn the English language the way they want.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/409/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Google: Most creative app"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"hume","url":null},{"name":"nextjs","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"storyscape.courses","url":"https://storyscape.courses"},{"label":"storyscape-silk.vercel.app","url":"https://storyscape-silk.vercel.app/"},{"label":"github.com","url":"https://github.com/catfeeshing/storyscape"}],"description_sections":[{"heading":"Inspiration","content":"All our team members enjoy reading as a hobby across numerous genres. Unfortunately, reading comprehension and literacy overall have been declining, which poses a massive problem for future generations. There are countless factors at play here, and we can't solve all of them, but we can definitely work to kindle a spark that might just blossom into a love for reading. Many of these adjacent tools are made for classroom settings. On the contrary, we're built for students, by students, and by readers, for readers."},{"heading":"What it does","content":"Storyscape is an application aimed at developing elementary and middle schoolers' grasp over the English language. Turn on your mic and get ready to read some words! To be specific, we have identified three core skills to be problematic: diction, sentence structure variety, and expressiveness. You will be given an unlimited amount of examples for each of these and the opportunity to practice and replicate these examples. With the help of our cat assistant, you can learn how to read passages expressively and eloquently!"},{"heading":"How we built it","content":"Storyscape is built on five major technologies:\n\nGoogle Gemini Flash -- speedy, cost-efficient, great at storytelling Hume AI -- personable, easy to integrate TTS, and great for sentiment analysis React/Next.js -- a popular JS framework that simplifies many best practices built for vercel Vercel -- easy deployment to the web Firebase -- a handy tool for auth, functions, storage and analytics"},{"heading":"Challenges we ran into","content":"One major problem we faced during the competition was the cold weather at night. We struggled to get some quality sleep due to the lack of comfortable sleeping arrangements :("},{"heading":"Accomplishments that we're proud of","content":"We would actually use this app ourselves after some minor improvements! Created a polished and aesthetically pleasing web app demonstrating AI technologies Stayed up 24 hours in a row"},{"heading":"What we learned","content":"This hackathon was a great opportunity to learn about all the different AI technologies that are out there and how we can implement them into our lives. We learned about sentiment analysis, some handy prompt engineering, and some techniques to interface with AI in a smart way."},{"heading":"What's next for Storyscape","content":"We're still working on adding themes into the text adventures. With better prompt engineering and utilisation of the best models to foster a closer connection to the text, Storyscape can become a true companion for those who want to augment their English skills."},{"heading":"Built With","content":"css firebase gemini github html hume nextjs react tailwind typescript"},{"heading":"Try it out","content":"storyscape.courses storyscape-silk.vercel.app github.com"}]},{"project_title":"RecovARy","project_url":"https://devpost.com/software/recovary","tagline":"Welcome to RecovARy, our Augmented Reality solution solving rehab and physical therapy for patients who recently suffered from a stroke.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/106/583/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Snap: Dream it. Build it."},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"fetch.ai","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"lensstudio","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"snapincspectacles","url":null},{"name":"xano","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pranavi-ch/recovARy-agents"},{"label":"github.com","url":"https://github.com/ppilli1/RecovARy"}],"description_sections":[{"heading":"Inspiration","content":"We were inspired to build RecovARy after attending Physical Therapy for the last few weeks. One of us was diagnosed with muscle spasms in their neck. After being diagnosed with muscle knots by experienced doctors, they were told the only way to fix this challenging and painful obstacle was to increase mobility in their body. Although their story involved muscle spasms, I designed this app to take it a step further, for patients who require mobility after having a stroke. A stroke, if you aren't aware, damages brain cells due to a reduced blood flow to the brain, and thus, one of many things in the body that are affected are regular movement and mobility. Therefore, we created this app with two distinct AR games, allowing a patient to choose from either Wall Tennis or 3D Fruit Ninja."},{"heading":"What it does","content":"Our app essentially contains two video games: Wall Tennis focuses on restoring a patient's arm power, including movement in the arms, shoulders, and wrists, and we calculate a patient's hitting speed to see how much mobility they are gaining in their arms; Fruit Ninja focuses on hand-eye coordination and calculates a patient's reaction time."},{"heading":"How we built it","content":"We built this app using the following technologies: Snap Inc. Spectacles, Lens Studio, Fetch.ai, Groq, Xano, and React JS"},{"heading":"Challenges we ran into","content":"A couple of challenges that I ran into was trying to change the entire concept of Wall Tennis into Squash. However, I noticed that the game was turning more into Virtual Reality, meaning I couldn't see the real world around me anymore and rather was completely immersed in the lens' objects instead of partially. Therefore, I changed the game back to Wall Tennis."},{"heading":"Accomplishments that we're proud of","content":"We were proud of not only being able to develop these two video games but also being able to fetch two data values from Lens Studio (hitting speed from Wall Tennis and reaction time from Fruit Ninja) to calculate and analyze a patient's data to create an accurate and effective report for doctors/physical therapists to see how much mobility their patient is gaining."},{"heading":"What we learned","content":"We learned how to work together fight through obstacles, and even create a fully fleshed-out product given our limited time frame."},{"heading":"What's next for RecovARy","content":"We would love to create a wider variety of games for patients to choose from, perhaps focusing on other parts of mobility than just hitting speed from the arm, and reaction time calculating a patient's reaction time. Another game we had in mind was inspired by Valorant's shooting range, where several objects would spawn in random locations and the patient would get a limited time (a split second) to hit the object before it disappeared."},{"heading":"Built With","content":"fetch.ai groq lensstudio react snapincspectacles xano"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"Guardian Angel","project_url":"https://devpost.com/software/guardian-angel-op49t2","tagline":"Guardian Angel, always by your side.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/389/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Google: Most impactful app"}],"team_members":[],"built_with":[{"name":"deepgram","url":null},{"name":"expo.io","url":"https://devpost.com/software/built-with/expo-io"},{"name":"fastapi","url":null},{"name":"google-gemini","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react-native","url":"https://devpost.com/software/built-with/react-native"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/ArianaJohnson/cal-hacks-2024"}],"description_sections":[{"heading":"Inspiration","content":"Guardian Angel was born from the need for reliable emergency assistance in an unpredictable world. Our experiences with the elderly, such as our grandparents, who may fall when we’re not around, and the challenges we may face in vulnerable situations motivated us to create a tool that automatically reaches out for help when it’s needed most. We aimed to empower individuals to feel safe and secure, knowing that assistance is just a call away, even in their most vulnerable moments."},{"heading":"What it does","content":"Core to Guardian Angel, our life-saving Emergency Reporter AI speech app, is an LLM and text-to-speech pipeline that provides real-time, situation-critical responses to 911 dispatchers. The app automatically detects distress signals—such as falls or other emergencies—and contacts dispatch services on behalf of the user, relaying essential information like patient biometric data, medical history, current state, and location. By integrating these features, Guardian Angel enhances efficiency and improves success in time-sensitive situations where rapid, accurate responses are crucial."},{"heading":"How we built it","content":"We developed Guardian Angel using React Native with Expo, leveraging Python and TypeScript for enhanced code quality. The backend is powered by FastAPI, allowing for efficient data handling. We integrated AI technologies, including Google Gemini for voice transcription and Deepgram for audio processing, which enhances our app’s ability to communicate effectively with dispatch services."},{"heading":"Challenges we ran into","content":"Our team faced several challenges during development, including difficulties with database integration and frontend design. Many team members were new to React Native, leading to styling and compatibility issues. Additionally, figuring out how to implement functions in the API for text-to-speech and speech-to-text during phone calls required significant troubleshooting."},{"heading":"Accomplishments that we're proud of","content":"We are proud of several milestones achieved during this project. First, we successfully integrated a unique aesthetic into our UI by incorporating hand-drawn elements, which sets our app apart and creates a friendly, approachable user experience. Additionally, we reached a significant milestone in audio processing by effectively transcribing audio input using the Gemini model, allowing us to capture user commands accurately, and converting the transcribed text back to voice with Deepgram for seamless communication with dispatch. We’re also excited to share that our members have only built websites, making the experience of crafting an app and witnessing the fruits of our labor even more rewarding. It’s been exciting to acquire and apply new tools throughout this project, diving into various aspects of transforming our idea into a scalable application—from designing and learning UI/UX to implementing the React Native framework, emulating iOS and Android devices for testing compatibility, and establishing communication between the frontend and backend/database."},{"heading":"What we learned","content":"Through this hackathon, our team learned the importance of effective collaboration, utilizing a “divide and conquer” approach while keeping each other updated on our progress. We gained hands-on experience in mobile app development, transitioning from our previous focus on web development, and explored new tools and technologies essential for creating a scalable application."},{"heading":"What's next for Guardian Angel","content":"Looking ahead, we plan to enhance Guardian Angel by integrating features such as smartwatch compatibility for monitoring vital signs like heart rate and improving fall detection accuracy. We aim to refine our GPS location services for better tracking and continue optimizing our AI speech models for enhanced performance. Additionally, we’re exploring the potential for spatial awareness and microphone access to record surroundings during emergencies, further improving our response capabilities."},{"heading":"Built With","content":"deepgram expo.io fastapi google-gemini node.js python react-native typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Autonomous AI Society","project_url":"https://devpost.com/software/autonomous-ai-society","tagline":"An autonomous system of AI agents performing an intelligent disaster response from analyzing the distress calls, dispatching drones, finding humans in floods and making calls to the rescue teams.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/090/176/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"fetch.ai","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"hyperbolic","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llama","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"vapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/rajashekarcs2023/Autonomous-AI-Society"}],"description_sections":[{"heading":"Inspiration","content":"I got this idea because of the current hurricane Milton causing devastation across Florida. The inspiration behind Autonomous AI Society stems from the need for faster, more efficient, and autonomous systems that can make critical decisions during disaster situations. With multiple sponsors like Fetch.ai, Groq, Deepgram, Hyperbolic, and Vapi providing powerful tools, I envisioned an intelligent system of AI agents capable of handling a disaster response chain—from analyzing distress calls to dispatching drones and contacting rescue teams. The goal was to build an AI-driven solution that can streamline emergency responses, save lives, and minimize risks."},{"heading":"What it does","content":"Autonomous AI Society is a fully autonomous multi-agent system that performs disaster response tasks in the following workflow:\n\nDistress Call Analysis : The system first analyzes distress calls using Deepgram for speech-to-text and Hume AI to score distress levels. Based on the analysis, the agent identifies the most urgent calls and the city. Drone Dispatch : The distress analyzer agent communicates with the drone agent (built using Fetch.ai) to dispatch drones to specific locations, assisting with flood and rescue operations. Human Detection : Drones capture aerial images, which are analyzed by the human detection agent using Hyperbolic's LLaMA Vision model to detect humans in distress. The agent provides a description and coordinates. Priority-Based Action : The drone results are displayed on a dashboard, ranked based on priority using Groq. Higher priority areas receive faster dispatches, and this is determined dynamically. Rescue Call : The final agent, built using Vapi, places an emergency call to the rescue team. It uses instructions generated by Hyperbolic’s text model to give precise directions based on the detected individuals and their location."},{"heading":"How I built it","content":"The system consists of five agents, all built using Fetch.ai ’s framework, allowing them to interact autonomously and make real-time decisions:\n\nRequest-sender agent sends the initial requests. Distress analyzer agent uses Hume AI to analyze calls and Groq to generate dramatic messages. Drone agent dispatches drones to designated areas based on the distress score. Human detection agent uses Hyperbolic’s LLaMA Vision to process images and detect humans in danger. Call rescue agent sends audio instructions using Deepgram ’s TTS and Vapi for automated phone calls."},{"heading":"Challenges I ran into","content":"Simulating a drone movement on florida map : The lat_lon_to_pixel function converts latitude and longitude coordinates to pixel positions on the screen. The drone starts at the center of Florida. Its movement is calculated using trigonometry. The angle to the target city is calculated using math.atan2. The drone moves towards the target using sin and cos functions.This allows placing cities and the drone accurately on the map. Callibrating the map to right coordinates : I had manually experiment with increasing and decreasing the coordinates to fit them at right spots on the florida map. Coordinating AI agents : Getting agents to communicate effectively while working autonomously was a challenge. Handling dynamic priorities : Ensuring real-time analysis and updating the priority of drone dispatch based on Groq's risk assessment was tricky. Integration of multiple APIs : Each sponsor's tools had specific nuances, and integrating all of them smoothly, especially with Fetch.ai, required careful handling."},{"heading":"Accomplishments that I am proud of","content":"Successfully built an end-to-end autonomous system where AI agents can make intelligent decisions during a disaster, from distress call analysis to rescue actions. Integrated cutting-edge technologies like Fetch.ai , Groq , Hyperbolic , Deepgram , and Vapi in a single project to create a highly functional and real-time response system."},{"heading":"What I learned","content":"AI for disaster response : Building systems that leverage multimodal AI agents can significantly improve response times and decision-making in life-critical scenarios. Cross-platform integration : We learned how to seamlessly integrate various tools, from vision AI to TTS to drone dispatch, using Fetch.ai and sponsor technologies. Working with real-time data : Developing an autonomous system that processes data in real-time provided insights into handling complex workflows."},{"heading":"What's next for Autonomous AI Society","content":"Scaling to more disasters : Expanding the system to handle other types of natural disasters like wildfires or earthquakes. Edge deployment : Enabling drones and agents to run on the edge to reduce response times further. Improved human detection : Enhancing human detection with more precise models to handle low-light or difficult visual conditions. Expanded rescue communication : Integrating real-time communication with the victims themselves using Deepgram’s speech technology."},{"heading":"Built With","content":"express.js fetch.ai groq hyperbolic javascript llama node.js python react vapi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"DayGenie","project_url":"https://devpost.com/software/daygeine","tagline":"An AI tool for automatically planning trips and make travel suggestions.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/807/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"google-calendar","url":"https://devpost.com/software/built-with/google-calendar"},{"name":"google-maps","url":"https://devpost.com/software/built-with/google-maps"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"large-language-model","url":null},{"name":"natural-language-processing","url":"https://devpost.com/software/built-with/natural-language-processing"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reddit","url":"https://devpost.com/software/built-with/reddit"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"The inspiration for our auto-scheduling project emerged from the increased need for automated and personalized recommendations in travel planning tools. We recognized that multi-agent LLMs hold immense potential to tackle complex tasks such as personalized scheduling . This potential drove our team to want to explore how these models could offer solutions that can integrate into daily life problems. We were also inspired to create this app because of the tediousness of scheduling, and we recongized that our tool could save lots of time."},{"heading":"Our Goal","content":"We aim to simplify the process of planning trips and transport for important trips. We also want to introduce users to popular and fun locations along the way. DayGenie automatically generates recommendations based on user preferences and user schedule."},{"heading":"How we built it (Structure of our model)","content":"Agent Workflow\n\nDayGenie is constructed as a decentralized multi-agent large language model . We defined 5 LLM Agents (InfoAgent, MapAgent, RedditAgent, SummaryAgent, Feedback Agent), each with specific prompting for their purpose.\n\nUser Input : user preference(str), google calendar schedule(google calendar API call)\n\nInfoAgent : Fetches User Input, generate prompts for MapAgent and RedditAgent. MapAgent : Find the location of events happening in the calendar. RedditAgent : Call Reddit API, fetch and analyze reviews based on the important keyword input. SummaryAgent : Calculates the transportation(time, cost)/ recommendations based on locatison and preferences. FeedbackAgent : Get physical feedback from the user, if negative run model again/ else output is provided to the user.\n\nModel Output : A list of personalized recommendations of Transportation, Restaurant/Cafe/Event places.\n\nConversation construction\n\nWe used Fetch AI to facilitate conversations and communication between our agents. We designed each agent to solve their own specific tasks and then pass results onto other in our workflow.\n\nWebsite construction\n\nOur website is constructed using Next.js and Tailwind CSS. We built an intuitive and attractive user interface using these tools. While we have not deployed the website yet, we plan on doing so in the future."},{"heading":"Challenges we ran into","content":"Number of Agents: How many Agents should we define? Reducing Hallucinations of Agents: How do we handle hallucinations generated by large language models..? Understanding Fetch AI We had some issues understanding the structure of Fetch AI and how to use it most effectively."},{"heading":"Accomplishments","content":"During this Hackathon, the major accomplishment was successfully generating conversations between the agents we designed. By calling Google Calendar API, user input, Google Map API, and reddit API, we could successfully generate output recommendations for users.\n\nTowards the end of the project, we were also able to add a feedback agent , which plays a crucial role in ensuring that users receive the exact information they are seeking from our application.\n\nFinally, our team cooperation made all of this possible, which we could learn a lot from each other and have fun participating on this project."},{"heading":"What's next for DayGenie","content":"Feedback on website : We did not have enough time to add the feedback sign on the frontend website. We will work further to generate getting the feedback from the users so that we can call the feedbackAgent in the website. Speech to text generation : DayGenie can only get input by text till now. The next step is to recognize speech from the user and input it as user preference. Addition of ScoringAgent : To ensure quality recommendations, we make a ScoringAgent to evaluate the generated output. Fine-tune the DayGenie model to foster better abilities."},{"heading":"Built With","content":"css google-calendar google-maps javascript large-language-model natural-language-processing python reddit"}]},{"project_title":"Databae.","project_url":"https://devpost.com/software/databae","tagline":"Simplifying databases for everyone!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/635/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"gcp","url":null},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"llm","url":null},{"name":"next.js","url":null},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"uagents","url":null},{"name":"visualizations","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/trungtran1234/databae"}],"description_sections":[{"heading":"Inspiration","content":"As the list of use cases grow for artificial intelligence (AI for short), we thought that it is important to use AI in order to bridge the gap between those who are tech-savvy and those who are non tech-savvy. We were inspired to create this project because we know people that are not knowledgeable in SQL; we know people who are not that knowledgeable in technology that is more on the complicated side. Not everyone is a data scientist. However, a database is an entity that most people often (but do not realize) interact with. In fact, people close to us such as our parents, often are not knowledgeable in SQL. That's where we thought of Databae. Why not just use a language that most people know (in the US) such as English and use that to provide insightful analysis and visualization to the user?"},{"heading":"What it does","content":"Let's introduce you to your new friend, Databae. It is your personal AI assistant when it comes to making queries to any database and providing insightful analysis on the dataset. Let's be honest. Not everyone wants to write SQL queries. Not everyone knows SQL. Not everyone knows how to use complicated technologies. That's where Databae comes in. We utilize AI in order to use human language to provide insights on the data in your database.\n\nDatabae generates a SQL query, double checks the SQL query against the database schema and prompt to make sure that it is a valid query, execute the query, and analyze the result set in order to provide you visualizations (only table form is available for now) about data that exists in your database.\n\nWe do not just generate SQL queries and execute them. That is boring!!! We provide analysis on your data. Your question does not have to have anything corresponding with an SQL statement. You can perhaps ask, \"Which employee in the company is best suited for frontend development?,\" and Databae will make sure to provide you analysis on that rather than just generating a SQL query and getting the result set."},{"heading":"How we built it","content":"Frontend:\n\nNext.js - Basically, the core of our frontend. It displays the front page, connection page, and dashboard. Shadcn/ui - We used certain components from Shadcn/ui such as the buttons. Framer Motion - Do you see the cool animations on our website? Well, Framer Motion allows us to display those cool animations to you.\n\nBackend:\n\nPython - This is the programming language that powers our backend. FastAPI - FastAPI powers our API endpoints and allows the frontend to request data to display on the frontend. Groq: We utilized Groq's LLaMA models to be able to generate SQL queries, check the SQL queries, and analyze the result set and provide insightful analysis and visualizations. uAgents: We utilized uAgents to create AI agents that are able to communicate with one another. uAgents powers our query generator, query checker, query executor, and query analyzer.\n\nAI Agents (part of our backend):\n\nQuery Generator: The query generator receive the user's request, utilizing the LLaMA model to process the user's prompt, understand and determining if a tool is need or not. If an SQL query and AI tool is needed, it will send the query to the Query Checker. If not, the workflow stops there and it will provide the user with general knowledge related to the database or its schemas. Query Checker: The query checker checks whether the SQL query generated abides by the database schema and user's prompt. If the query checker approves of the SQL query, it will forward the query to the query executor. Otherwise, it will notify the user that the query generated cannot be executed due to the query being too vague or unrelated. Query Executor: The query executor executes the query, gets the dataset from the query, and forwards the dataset to the query analyzer along with the database schema and prompt. Query Analyzer: The query analyzer analyzes the result set, gives insightful results and visualizes the results in a clear and concise manner accordingly to the user's request."},{"heading":"Challenges we ran into","content":"As our frontend has a lot of animations and other cool elements to it, it took a long time to be able to perfect and optimize the appearance of our frontend. Being able to finetune and prompt engineer the LLM models in order to provide the appropriate responses. We had to make lots of adjustments in the prompts in order for it to provide good analysis of the data. We had to figure out how to properly translate the LLM's response into a proper Pandas DataFrame (basically the table). Handling multi-agent communication in uAgents was also a struggle as we had to figure out how to make one agent talk to another that talks to another and vice versa."},{"heading":"Accomplishments that we're proud of","content":"We are proud of being able to utilize Fetch.AI's AI agents in order to satisfy our workflow of generating a SQL query => checking the SQL query => executing the SQL query => analyzing the SQL query and providing proper visualizations. We are also proud of being able to use Groq's extremely fast LLMs in order to quickly provide quality but fast analysis of the data in the database. Our frontend was also an important aspect. We used many libraries in order to optimize and \"perfect\" the appearance of our website. It took us a long time to design the website in the first place."},{"heading":"What we learned","content":"We learned about new amazing frameworks such as Fetch.AI's AI agents. With the assistance of many mentors in CalHacks, we were able to learn a lot about new technologies and techniques We also learned about Groq's LLMs; those were vital in providing those ultra quick responses to the user, and we see many other possible use cases for it."},{"heading":"What's next for Databae.","content":"More tools: We intend to create more tools for Databae. Our application was designed to have many more tools such as a pie chart generator, prediction models, and other tools. For our use case, we decided to only generate our visualizations in table form, but it is able to scale up in order to be able to display it in many more forms such as pie charts, bar charts, etc. Cross database analysis: What if you want to analyze multiple databases and make correlations between multiple databases? We want to add this functionality. Integration with more databases: We want to be able to have Databae be able to be integrated into more databases such as PostgreSQL, MongoDB, and other DBs. Finetuning the prompts: We want to finetune the prompts in order to provide more insightful responses and visualizations."},{"heading":"Built With","content":"fastapi gcp google groq llm next.js pandas python react uagents visualizations"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"MD FactFarm","project_url":"https://devpost.com/software/md-factfarm","tagline":"Truth in the Digital Age: Mastering Information Literacy","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/087/336/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"ai-agent","url":null},{"name":"ai-engine","url":null},{"name":"chrome","url":"https://devpost.com/software/built-with/chrome"},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"fetch.ai","url":null},{"name":"figma","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"front-end","url":null},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llm","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"social-media","url":null},{"name":"web","url":"https://devpost.com/software/built-with/web"}],"external_links":[{"label":"github.com","url":"https://github.com/courtofdreams/md-fact-farm-api"},{"label":"github.com","url":"https://github.com/courtofdreams/md-fact-farm-extenstion"}],"description_sections":[{"heading":"Inspiration","content":"Our mission is rooted in the fight against fake news, misinformation, and disinformation, which are increasingly pervasive threats in today’s digital world. As the saying goes, \"the pen is mightier than the sword,\" which underscores the power of words and information. We aim to ensure that no one falls victim to digital deception.\n\nWhile technology has contributed to the spread of misinformation, we believe it can also be a powerful ally in promoting the truth. By leveraging AI for good, we aim to combat falsehoods and uphold the integrity of information.\n\nFun fact: Moodeng is a pygmy hippopotamus born on July 10, 2024, living in Khao Kheow Open Zoo, Thailand. She became a viral internet sensation during a busy political season in the US. Amid the flood of true and half-true information, Moodeng, symbolizing purity and honesty, stood as a beacon of clarity. Like Moodeng, our tool is here to cut through the noise and keep things transparent. So, Vote for Moodeng!"},{"heading":"What it does","content":"Social media platforms are now major sources of rapidly shared information. Our Chrome extension, MD FactFarm, simplifies fact-checking through AI-driven content analysis and verification. Initially focused on YouTube, our tool offers real-time fact-checking by scanning video content to identify and flag misinformation while providing reliable sources for users to verify accuracy."},{"heading":"How we built it","content":"At the core of our system is a Large Language Model (LLM) that we trained and optimized to accurately understand and interpret various forms of misinformation, powering our fact-checking capabilities. We integrated an AI agent using Fetch.ai and built services and APIs to enable seamless communication with the agent. Our front-end, built with HTML, CSS, and JavaScript, was designed and deployed as a Chrome extension."},{"heading":"Challenges we ran into","content":"One of the major challenges we encountered was ensuring that the AI could accurately differentiate between fact, opinion, and misleading content. Early on, the outputs were inconsistent, making it difficult to trust the results. To achieve this, we had to rethink our approach to prompt engineering. We provided the AI with more detailed context and built a structured framework to clearly separate different types of content. Additionally, we implemented a formula for the AI to use to determine a confidence score for each output. These changes helped us generate more consistent and reliable results, enabling the AI to better recognize the subtle distinctions between fact, opinion, and misleading content. Another challenge was integrating multiple agent frameworks into a unified system that could operate seamlessly. Managing the intricacies of coordinating tasks and data flow between these diverse components contributed to a complex integration process."},{"heading":"Accomplishments that we're proud of","content":"We successfully developed a Chrome extension that that provides real-time fact-checking for YouTube, empowering users to make informed decisions. We crafted prompts that effectively leverage the LLM's ability to detect misinformation. We successfully integrated Fetch.ai, utilizing agents that lay the foundation for scalability."},{"heading":"What we learned","content":"We learned the importance of defining the problem clearly and deciding on a minimum viable product (MVP) within a limited timeframe. Additionally, we focused on framing our work to align with the AI agent framework, which has been crucial in improving our approach to misinformation detection."},{"heading":"What's next for MD FactFarm","content":"Moving forward, we plan to expand our platform to include other social networks, such as Twitter and Facebook, where misinformation spreads rapidly. We aim to gather a wider range of information sources to ensure more comprehensive fact-checking and cover more diverse content. Moreover, we are working on enhancing our AI's fact-checking mechanics, utilizing more advanced techniques to improve accuracy."},{"heading":"Built With","content":"ai-agent ai-engine chrome css fetch.ai figma flask front-end html javascript llm python social-media web"},{"heading":"Try it out","content":"github.com github.com"}]},{"project_title":"BrowseBlind","project_url":"https://devpost.com/software/browseblind","tagline":"World's First AI Browser. Allowing blind people to interact for the first time with the internet. Also helping visually impaired. Also, automated processes help average people save time.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/087/970/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"agenticframework","url":null},{"name":"browsers","url":null},{"name":"deepgram","url":null},{"name":"fetch","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"gemini-bounding-box","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"pyqt5","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"}],"external_links":[{"label":"github.com","url":"https://github.com/RoyceAroc/browseblind"},{"label":"browseblind.co","url":"https://browseblind.co/"}],"description_sections":[{"heading":"Inspiration","content":"I was doing an application last week when one of the questions about diversity prompted me into thinking about how we take things for more than just granted. It struck me that blind people aren't able to interact with the internet (shop on e-commerce websites, research to learn, play video games, social media - the list is quite endless). I then asked my dad, who works in the computer-science industry, about whether the visually impaired and blind are able to contribute to their likeness in the workforce when I realized how far off the scales were tipped.\n\nAssil Eye Institute\n\nIf I were in their shoes, then I would immediately build such a software to be connected like and with everyone else, but in their case I would be blind so I wouldn't be able to, luckily I'm not so I know the right thing to do would be to build a software for them. Also, I realized that for people like me who are quite lazy or would like parts of their browser processes being automated (like telling it to reserve my tennis court from 5-6 PM rather than doing it myself), an AI browser might also sound quite valuable. Rather more of a business market \"want\" than a \"need\", I think regardless it's novelty in the market and it's usefulness for a customer segment (did a bit of customer discovery as well reaching out to centers and found out directly from people that they are very much look forward to it) makes this idea worth venturing about this hackathon in CalHacks."},{"heading":"What it does","content":"It's the same as your regular web browser application (Chrome, Safari, Brave, etc) in the way you surf the web, but in addition, you can do so freely while being blind-folded. Well, how does that work? Also, controlled by text and/or speech input (pressing the space bar for over a second starts a recording and leaving it ends the recording), any user's instructions are followed by the browser. When a user lands on a page, a quick summary of the page is read including the nav bar components, etc so the user can navigate and explore the web just like any other person. A user can ask the browser to click on any element/part of the page and also fill in information on the page via true NLP. They can also open/close tabs, search using the search engine, ask questions (variant of RAG approach taken) over the current page, save the page locally/print the page, and so many other tasks that the current set of agents have to offer. Every time something new is displayed on the browser (ex. going to a different page) they are notified so they can take judgements."},{"heading":"How we built it","content":"Using the PyQt5 Browser Development framework, I started my code by building a browser. I organized my prompt engineering agentic framework using Fetch AI's agentic system. For prompting questions with images/text I used Gemini's models. I also used Gemini's bounding box model for detecting where to proceed next on the page. This was not too accurate, so I coupled it with my algorithm I wrote where I take the html code of a website and parsed it down to the important segments (removing PII and unnecessary contents) in order to save tokens and decide based on html where to move next as a backup. After much testing, I settled on Groq for the decision-making segments of the LLM chain for its speed. I also used it for the STT part where the user has the option to speak in our application. The TTS part was handled by DeepGram and other voice agent integrations. Building requires testing and I tested this by imitating a blind person by being blind. One such successful testing included making accounts on websites I have never visited before."},{"heading":"Challenges we ran into","content":"Parsing the HTML took one of the longest parts due to its implicit complexity. Event listeners attached to elements across the DOM tree. Event listeners on images like a hamburger icon which has no text so a mapping is needed. These multiple edge cases had to be considered before reaching the threshold where it was doing perfect on every website as it is doing now. Another challenge I ran into was fine-tuning. The accuracy of this model was at around 60% and it was a hard and enduring work to get it to around 95% where it is currently at right now."},{"heading":"Accomplishments that we're proud of","content":"Controlling the browser by \"having a 2-street conversation\" with it simply blows my mind and really changes the way one surfs the web. It's really fun and really useful at the same time."},{"heading":"What we learned","content":"How to build your own browser. How to fine tune the boundary box model with Gemini through prompt engineering to extract high accurate insights. Learned documentation for multiple voice agent companies that were integrated."},{"heading":"What's next for BrowseBlind","content":"Perfecting the software model and finding ways to even further reduce token consumption in order to push this into the market as soon as possible. Also, building \"background tabs\" a feature I didn't have time to finish but essentially you can give a tab a task (ex. find the part on the wikipedia page that talks about fourier transform or find the contact page for X, Y, Z company) and the tab does the task in the background and comes back up when finished."},{"heading":"Built With","content":"agenticframework browsers deepgram fetch gemini gemini-bounding-box groq pyqt5 python"},{"heading":"Try it out","content":"github.com browseblind.co"}]},{"project_title":"Healora","project_url":"https://devpost.com/software/helora","tagline":"\"Healora, powered by Nurse Joy, uses empathetic AI for symptom tracking and predictive analysis, helping patients receive faster care while easing the workload for healthcare professionals.\"","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/199/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Hume: Best use of emotionally intelligent voice AI"}],"team_members":[],"built_with":[{"name":"fastapi","url":null},{"name":"fetch.ai","url":null},{"name":"figma","url":null},{"name":"framermotion","url":null},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"hume.ai","url":null},{"name":"next.js","url":null},{"name":"shadcn","url":null},{"name":"supabase","url":null},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Zalinto/med-ai"}],"description_sections":[{"heading":"Background","content":"Why should nurses and doctors have to rely solely on manual inputs when modern healthcare technology has so much more potential?\n\nIn hospitals and healthcare settings, medical professionals are often overwhelmed , managing complex workflows, multiple patients, and endless data inputs, often missing key details that could improve patient care. But what if technology could alleviate this burden , helping healthcare workers make quicker, more accurate decisions?\n\nIn today’s world, we have empathetic AI, predictive models , and vast amounts of data at our disposal. Yet, many hospital systems continue to rely on outdated methods for managing symptoms, diagnoses, and patient interactions. Healthcare workers lose valuable time manually entering and interpreting data when AI could be working alongside them .\n\nHealora was created to bridge that gap . Empowering medical staff with AI-driven tools , Helora leverages Nurse Joy , a virtual assistant, to intelligently track symptoms , perform predictive analysis , and provide emotional understanding in patient interactions. With Helora, hospitals can streamline workflows , allowing nurses and doctors to focus more on what truly matters— saving lives and improving patient outcomes ."},{"heading":"What is Healora?","content":"Healora is an AI-powered healthcare platform designed to streamline patient care by integrating real-time health monitoring, virtual assistance, and efficient data management for healthcare providers.\n\nThe pre-screening page allows patients to enter their initial health data and symptoms, which helps healthcare providers assess their condition quickly before moving forward with more detailed examinations. The treatment page is where Nurse Joy, Healora's AI assistant, interacts with patients. This page also displays vital signs, such as respiratory rate, blood pressure, and heart rate, allowing medical staff to monitor patient health in real-time. The patient list provides healthcare professionals with an overview of all patients currently under care, including their basic health details and treatment status. The staff table offers a detailed view of the healthcare team, enabling easy management of roles and responsibilities within the medical staff."},{"heading":"Features","content":"Symptom Tracking : Nurse Joy, Helora's virtual assistant, allows patients to log symptoms efficiently. These symptoms are recorded and analyzed to help healthcare workers make data-driven decisions. Predictive Analysis Helora uses AI models to perform predictive analysis on the collected data, offering potential diagnoses and next steps for treatment based on the symptoms entered. Agentic Backend Helora uses AI agents for majority of its backend tasks, powered by Fetch.AI Empathetic AI : Helora incorporates emotional intelligence to provide compassionate, context-aware responses, ensuring that patients feel understood and supported during their care, powered by Hume AI AI-Generated Feedback : Nurse Joy provides real-time feedbackafter each interaction, helping healthcare professionals review patient information, track symptoms, and make informed decisions quickly. Real-Time Data Analysis Helora processes patient data in real-time, providing medical staff with up-to-date information on patient vitals and symptoms. Voice Interaction Helora supports voice input, giving patients flexible communication options based on their preferences."},{"heading":"Planning","content":"We began by thoroughly researching the current state of hospital workflows, particularly focusing on how AI could alleviate the pressures faced by healthcare workers. From understanding existing tools to studying user needs, our research laid the foundation for building Helora.\n\nOnce we had a clear understanding of the problem space, we created user personas and developed user flows to guide our design. Using Figma, we designed prototypes to rapidly iterate and refine the user experience. Our focus was on creating a system that is intuitive for both healthcare workers and patients, with a clean, user-friendly interface that reflects the needs of medical professionals."},{"heading":"System Architecture","content":"Frontend\n\nHelora is a responsive desktop and mobile-friendly web application built using Next.js, with Tailwind CSS for streamlined and highly customizable styling. We use shadcn components to provide a cohesive design system and Framer Motion to add smooth, engaging animations for improved user experience.\n\nOur frontend efficiently manages real-time communication between healthcare professionals and Nurse Joy, ensuring seamless interaction. Data flows between users and the backend are handled securely, providing healthcare workers with immediate feedback from the AI and allowing for effortless tracking of symptoms, analysis, and patient interactions.\n\nBackend\n\nOur project's backend is a robust and scalable architecture designed to deliver advanced AI capabilities. Our backend employs Fetch AI agents orchestrated through FastAPI to manage workflows and interactions efficiently. We use Supabase with PostgreSQL for real-time data storage and management. For language-based analysis, we have integrated Llama and Mixtral models via Groq , while Hume AI powers our conversational tasks with advanced sentiment and emotional analysis.\n\nThe technologies that we used to power Healora."},{"heading":"To run the project","content":"Clone the repository at Healora Create a .env file, place it in the root directory and fill in with api keys of your configuration:\n\nNEXT_PUBLIC_SUPABASE_URL= NEXT_PUBLIC_SUPABASE_KEY= SESSION_SECRET= SESSION_EXPIRES_IN= NEXT_PUBLIC_DEVELOPMENT= NEXT_PUBLIC_BASE_LOCAL_URL= NEXT_PUBLIC_BASE_PROD_URL= HUME_API_KEY= HUME_API_SECRET= HUME_CONFIG_ID= NEXT_PUBLIC_HUME_CONFIG_ID_SUK= HUME_API_KEY_SUK= HUME_SECRET_KEY_SUK=\n\nRun the following commands(terminal) in the root directory:\n\nnpm install npm run dev"},{"heading":"Takeaways","content":"What we learned\n\nThrough building Healora, we discovered the complexities of integrating AI into healthcare workflows. We learned how crucial it is to blend advanced technology with empathy to enhance patient care. We also deepened our understanding of AI agents and language models, realizing their potential to make a real difference in medical workflow management and Nurse Assisstant settings\n\nAccomplishments\n\nWe’re incredibly proud of what we achieved during this hackathon. Not only did we integrate a wide range of features like symptom tracking, AI-powered predictive analysis, and real-time empathetic feedback, but we also managed to design a clean, user-friendly interface tailored for healthcare professionals. This project also gave us the opportunity to apply research in meaningful ways by reading research papers and using user personas to ensure Helora meets the specific needs of both nurses and patients.\n\nOn the design side, we’re proud of the consistent and intuitive UI built using Tailwind CSS and Framer Motion for smooth interactions. Helora’s design language prioritizes clarity and accessibility, catering to both healthcare workers and patients alike.\n\nWhat's Next\n\nMoving forward, we plan to focus on scaling Helora’s AI capabilities. While we’ve implemented empathetic AI and predictive analysis, we see room for improvement in terms of diagnosis accuracy and real-time feedback. Additionally, we aim to expand Helora’s capabilities by incorporating more comprehensive datasets to improve symptom recognition and predictive healthcare outcomes.\n\nWe also want to refine Helora’s security and privacy features, ensuring patient data is handled with the utmost care and confidentiality. Our next steps include conducting more user testing, enhancing multilingual support, and refining Helora's user experience for different healthcare roles (nurses, doctors, HR personnel).\n\nHelora’s journey is just beginning, and we believe it has the potential to revolutionize how healthcare professionals interact with patients, improving both workflow efficiency and patient care."},{"heading":"Built With","content":"fastapi fetch.ai figma framermotion github groq hume.ai next.js shadcn supabase tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"INFERmary","project_url":"https://devpost.com/software/infermary","tagline":"Simulating virtual cities to outsmart viral outbreaks","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/082/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"fast","url":null},{"name":"fetch","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"mapbox","url":"https://devpost.com/software/built-with/mapbox"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[],"description_sections":[{"heading":"💥 How it all started","content":"I'm sure we're all well aware that having an informed response to an epidemic is crucial. The viral outbreak simulations that currently inform these responses are largely rules and statistics-based. These may work well at approximating human behavior, but they fail to account for how personal choices, life circumstances, and unpredictable interactions influence the spread of a virus.\n\nWith the shadow of COVID-19 receding into the past, we thought it important to stay vigilant and look towards the future to safeguard our communities against the next outbreak, whenever it may be. We realized that having a more powerful simulation tool to test response plans before they happen would be one of the best ways to stay prepared, and we set our sights on finding a better way to model the spread of epidemics.\n\nBy leveraging fetch.ai's ability to create AI agents that could act intelligently by themselves and interact organically with each other, we aimed to simulate a living, breathing city that could better model the nuanced nature of humanity that statistical models can only approximate. This allows for us to better capture the complex emergent behaviors of a community, and therefore better simulate viral outbreaks and the efficacy of potential responses."},{"heading":"📖 What it does","content":"Enter INFERmary , a simulation tool designed to realistically simulate an organic city and test out potential disaster responses.\n\nINFERmary starts out by getting context information about the city in question, like locations of interest. Using the city's demographic data, it then generates independent agents according to represent a scaled-down version of the city's population. The user can then tweak the virus spread parameters like its infectiousness and the number of initially infected patients, as well as add custom directions for the simulation.\n\nHere’s how INFERmary leverages advanced technologies to better achieve its goal:\n\nIntelligent AI Agents: Thanks to fetch.ai, our simulated city is populated by AI agents, each with their own real life. Every agent has a personalized profile, encompassing their age, socioeconomic status, habits, and more. They don’t just follow scripted paths—they generate their own daily routines, engaging in the kind of organic interactions you’d expect to see in any real-world city. Central Direction System: A central LLM system powered by Groq allows the user to act as a regulatory body and issue directives to all agents (such as enforcing a lockdown, setting mask mandates, or establishing vaccination campaigns), allowing for deep simulation of not only viral spread but also the societal response to those interventions."},{"heading":"🔧 How we built it","content":"To build INFERmary, we leveraged the power of generative AI to generate various personalities. These personalities are turned into their own individual Fetch.ai agents who control their own lives and communicate with each other.\n\nTo power the generative abilities of our solution, we used Groq’s various hosted models. To visualize our agents, we display them on our React front end.\n\nTechnologies:\n\nfetch.ai: Used for creating multiple AI agents for simulating Groq: Used for initializing all the AI agents as well as general LLM tasks Mapbox: Used to visualize the simulated agents in an intuitive way React: Used to create user friendly frontend"},{"heading":"🚩 Challenges we ran into","content":"Alex: We were pretty tired of working with the same old LLMs and functions, so we wanted to experiment with something new this hackathon! I was responsible for generating people and implementing their actions with agents, and wow was it hard. Figuring out how to take advantage of agents fully was a tall task. I loved it though, it was something fresh and unique from what we usually do!\n\nNicholas: The biggest challenge for me was wrapping my head around the paradigm shift from one or very few backend services to many agents that all communicate with each other. Most of my time was spent reading documentation and asking questions instead of coding which was interesting."},{"heading":"🏆 Accomplishments that we're proud of","content":"The ability to generate multiple AI agents with unique personalities and interests Implementing"},{"heading":"📝 What we learned","content":"Alex: This is probably the last ever collegiate hackathon I'll ever be at, so I want this to be more of a reflection on everything I've done. I think the greatest lesson I learned here and everywhere else is my potential. I never guessed I would be able to do this much in such little time with other people. A lot can be done in an hour, and a lot more can be done in 36. Hackathons have given me so much confidence in my abilities and allowed me to have so much fun with my friends.\n\nSpecifically here though, I also learned a lot about agentic AI and how versatile they are. I never expected how good they would be at checking each other and how much the agentic factor improves LLM and application performance. It was really fun using fetch.ai's technology!\n\nEthan: I specifically wanted to approach Cal Hacks 11.0 with a focus on presentation and marketing, so I was the team's UX designer as well as marketing lead. It allowed me to see the project from a different angle than I usually tackle them from, and emphasized to me the importance of a good pitch. I think I've gained some valuable experience with selling our product and representing it in a way that not only plays to its strengths, but my and my teammates' strengths as well.\n\nNicholas: I learned a lot about the use of AI agents. From what I’ve seen, utilizing multiple AI agents is the next big thing in tech. Being able to work with agent technology and see how others use it was very fun.\n\nWesley: This was my first hackathon and keeping up with contributing to the project with other experienced members was definitely a challenge that many people overlook. I worked on and learned about frontend development using Reactjs and integrated Mapbox to render the map simulation environment."},{"heading":"✈️ What's next for INFERmary","content":"With a simulation system as advanced as INFERmary, simulating basic situations and outbreak reponses is just the tip of the iceberg! We've got a ton of ideas for INFERmary going forward to turn it into the best possible version of what it can be.\n\nAI Agent-Powered Outbreak Response Team: In addition to just dictating blanket commands, this will allow the user to act as the head of an outbreak response team, managing AI agents that fulfill the role of epidemiologists, onsite doctors, and more. This will allow for an even more in-depth simulation of a health agency's possible response to an outbreak. Mutating Viruses and Different Strains: Currently, the virus has set parameters at the beginning of the simulation. However, viruses could be simulated to mutate and develop into different strains with the passage of time, developing different transmission rates, mortality rates, or resistance to treatments. This would allow for a more comprehensive look at long-term epidemic management. Public Sentiment and Media Influence: We all have first-hand experience with how media and public influence shape the response to an epidemic. Simulating the influence of media, social networks, and public sentiment on behavior—such as how misinformation or social pressure might impact mask-wearing, social distancing, or vaccine uptake—could offer a unique insight into outbreak response in today's media-driven world."},{"heading":"📋 Evaluator's Guide to INFERmary","content":"Intended for judges, however the viewing public is welcome to take a look. Hey! We wanted to make this guide in order to help provide you further information on our implementations of certain programs and provide a more in-depth look to cater to both the viewing audience and evaluators like yourself.\n\nSPONSOR SERVICES WE HAVE USED THIS HACKATHON\n\nfetch.ai Groq"},{"heading":"Built With","content":"fast fetch groq mapbox python react"}]},{"project_title":"MedKnight","project_url":"https://devpost.com/software/medknight","tagline":"A one-of-a-kind, AR assistant medical system is made to guide first emergency first responders through critical, uncommon medical procedures with real-time support.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/087/389/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Fetch.ai: Agentic Track Prize"}],"team_members":[],"built_with":[{"name":"agents","url":null},{"name":"ar","url":null},{"name":"c#","url":"https://devpost.com/software/built-with/c--2"},{"name":"deepgram","url":null},{"name":"fetch.ai","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"large-language-models","url":null},{"name":"meta","url":"https://devpost.com/software/built-with/meta"},{"name":"oculus","url":"https://devpost.com/software/built-with/oculus"},{"name":"openai","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"quest","url":null},{"name":"singlestore","url":null},{"name":"unity","url":"https://devpost.com/software/built-with/unity"}],"external_links":[{"label":"github.com","url":"https://github.com/amanpdesai/calhacks"},{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1rvTs7ApXq069VlVl9Zh_iDK0p_Ww1zAFDaY8q-f5wIw/edit?usp=sharing"},{"label":"drive.google.com","url":"https://drive.google.com/file/d/1byB_JDotZYgAWMJIKd7aITAxeUIrhcHG/view?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Natural disasters often put emergency medical responders (EMTs, paramedics, combat medics, etc.) in positions where they must assume responsibilities beyond the scope of their day-to-day job. Inspired by this reality, we created MedKnight, an AR solution designed to empower first responders. By leveraging cutting-edge computer vision and AR technology, MedKnight bridges the gap in medical expertise, providing first responders with life-saving guidance when every second counts."},{"heading":"What it does","content":"MedKnight helps first responders perform critical, time-sensitive medical procedures on the scene by offering personalized, step-by-step assistance. The system ensures that even \"out-of-scope\" operations can be executed with greater confidence. MedKnight also integrates safety protocols to warn users if they deviate from the correct procedure and includes a streamlined dashboard that streams the responder’s field of view (FOV) to offsite medical professionals for additional support and oversight."},{"heading":"How we built it","content":"We built MedKnight using a combination of AR and AI technologies to create a seamless, real-time assistant:\n\nMeta Quest 3 : Provides live video feed from the first responder’s FOV using a Meta SDK within Unity for an integrated environment. OpenAI (GPT models) : Handles real-time response generation, offering dynamic, contextual assistance throughout procedures. Dall-E : Generates visual references and instructions to guide first responders through complex tasks. Deepgram : Enables speech-to-text and text-to-speech conversion, creating an emotional and human-like interaction with the user during critical moments. Fetch.ai : Manages our system with LLM-based agents, facilitating task automation and improving system performance through iterative feedback. Flask (Python) : Manages the backend, connecting all systems with a custom-built API. SingleStore : Powers our database for efficient and scalable data storage."},{"heading":"SingleStore","content":"We used SingleStore as our database solution for efficient storage and retrieval of critical information. It allowed us to store chat logs between the user and the assistant, as well as performance logs that analyzed the user’s actions and determined whether they were about to deviate from the medical procedure. This data was then used to render the medical dashboard, providing real-time insights, and for internal API logic to ensure smooth interactions within our system."},{"heading":"Fetch.ai","content":"Fetch.ai provided the framework that powered the agents driving our entire system design. With Fetch.ai, we developed an agent capable of dynamically responding to any situation the user presented. Their technology allowed us to easily integrate robust endpoints and REST APIs for seamless server interaction. One of the most valuable aspects of Fetch.ai was its ability to let us create and test performance-driven agents. We built two types of agents: one that automatically followed the entire procedure and another that responded based on manual input from the user. The flexibility of Fetch.ai’s framework enabled us to continuously refine and improve our agents with ease."},{"heading":"Deepgram","content":"Deepgram gave us powerful, easy-to-use functionality for both text-to-speech and speech-to-text conversion. Their API was extremely user-friendly, and we were even able to integrate the speech-to-text feature directly into our Unity application. It was a smooth and efficient experience, allowing us to incorporate new, cutting-edge speech technologies that enhanced user interaction and made the process more intuitive."},{"heading":"Challenges we ran into","content":"One major challenge was the limitation on accessing AR video streams from Meta devices due to privacy restrictions. To work around this, we used an external phone camera attached to the headset to capture the field of view. We also encountered microphone rendering issues, where data could be picked up in sandbox modes but not in the actual Virtual Development Environment, leading us to scale back our Meta integration. Additionally, managing REST API endpoints within Fetch.ai posed difficulties that we overcame through testing, and configuring SingleStore's firewall settings was tricky but eventually resolved. Despite these obstacles, we showcased our solutions as proof of concept."},{"heading":"Accomplishments that we're proud of","content":"We’re proud of integrating multiple technologies into a cohesive solution that can genuinely assist first responders in life-or-death situations. Our use of cutting-edge AR, AI, and speech technologies allows MedKnight to provide real-time support while maintaining accuracy and safety. Successfully creating a prototype despite the hardware and API challenges was a significant achievement for the team, and was a grind till the last minute. We are also proud of developing an AR product as our team has never worked with AR/VR."},{"heading":"What we learned","content":"Throughout this project, we learned how to efficiently combine multiple AI and AR technologies into a single, scalable solution. We also gained valuable insights into handling privacy restrictions and hardware limitations. Additionally, we learned about the importance of testing and refining agent-based systems using Fetch.ai to create robust and responsive automation. Our greatest learning take away however was how to manage such a robust backend with a lot of internal API calls."},{"heading":"What's next for MedKnight","content":"Our next step is to expand MedKnight’s VR environment to include detailed 3D renderings of procedures, allowing users to actively visualize each step. We also plan to extend MedKnight’s capabilities to cover more medical applications and eventually explore other domains, such as cooking or automotive repair, where real-time procedural guidance can be similarly impactful."},{"heading":"Built With","content":"agents ar c# deepgram fetch.ai flask large-language-models meta oculus openai opencv python quest singlestore unity"},{"heading":"Try it out","content":"github.com docs.google.com drive.google.com"}]},{"project_title":"DeeR","project_url":"https://devpost.com/software/deer","tagline":"Deep Retention of Concepts using research-backed techniques!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/269/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Agora: Best Practice of TEN"}],"team_members":[],"built_with":[{"name":"cartesia","url":null},{"name":"chromadb","url":null},{"name":"deepgram","url":null},{"name":"fastapi","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"hume","url":null},{"name":"hyperbolic","url":null},{"name":"llama","url":null},{"name":"nextjs","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"tailwindcss","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"}],"external_links":[{"label":"github.com","url":"https://github.com/Priyansh4444/DeeR"}],"description_sections":[{"heading":"Inspiration","content":"Jumping from study habit to study habit is a loop hole, and I had been stuck in it for a long time! I wanted to build something which had intersection in the domains of studying, neuroscience, and computer science, so I built DeeR. I can finally scrap all the useless apps that I have and be proud to use my own :D\n\nHeavily reccommend watching this video: https://youtube.com/shorts/icfI_iVLeRs?si=1AhrOj9470H4WgDW since its the basis of this entire project"},{"heading":"What it does","content":"DeeR is an AI-powered study companion that:\n\nUses AI to detect emotional conditions which I have a hard time recognizing about myself Implements the Feynman technique, one of the best approaches in learning techniques that has served me well for the past few weeks, which actively makes me understand rather than brute force remembrance Adds Study Cycles Creates workflows Generates nice summaries from PDFs, PPTs, and other document formats Emotional Analysis to see when you are distressed and reccommending you to take a break Hume Model to talk about what you learned about to improve your recalling and retention"},{"heading":"Challenges we ran into","content":"Cartesia API credits reaching 662% of the free usage limit 🤭 Hume API requiring manual screenshots for each inference rather than connecting through WebSockets Managing time and presenting solo"},{"heading":"Accomplishments that we're proud of","content":"Integrating multiple technologies and participating in various tracks, especially as a solo developer Overcoming anxiety by participating in competitions like these Successfully building a functional prototype of DeeR"},{"heading":"What we learned","content":"Solo hackathons are long but fun No merge conflicts when working alone! Discovered interesting aspects of Hume and Deepgram's model training approaches Improved skills in working with WebSockets and fetch requests Enhanced ability to read documentation and manage time effectively"},{"heading":"What's next for DeeR","content":"Adding authentication for user accounts Adding advice on what to do when the person is distressed Implementing multi-user support with individual LLM preferences Adding a feature to skip voice lines by Cartesia Fine-tuning models for better outcomes"},{"heading":"Built With","content":"cartesia chromadb deepgram fastapi gemini groq hume hyperbolic llama nextjs python tailwindcss typescript websockets"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Notefly - AI Meeting Notes on the Fly","project_url":"https://devpost.com/software/notefly-ai-meeting-notes-on-the-fly","tagline":"Notefly is your real time online meeting note taker. See meeting transcript and AI summary while on the call. Always stay on point with short notes from your personal typist for meetings.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/296/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Agora: Best Innovation of TEN"}],"team_members":[],"built_with":[{"name":"agora","url":null},{"name":"fetch.ai","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"next.js","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwid","url":null},{"name":"zustand","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Almasx/calhack"}],"description_sections":[{"heading":"Inspiration","content":"We are students at Minerva University which uses innovative curriculum and learning methods. Online forum classes is how we learn, they require active participation. While on the classes, we might doze off or forget about something discussed, which diminishes our grades. We always wanted a live, automatic AI note taker so we won't need to take notes all the time."},{"heading":"What it does","content":"Our product, Notefly, turns the talks in the meetings into live transcript text. Then it turns the transcripts into short note blocks, a 40 words speech becomes 10 words. The short note blocks turn into bullet points and bullet points turn into main idea summary notes. All these summaries can be seen while in the meeting so everyone knows what we are discussing and important details, like a customers mail address."},{"heading":"How we built it","content":"Started by using agora's SDK to create a basic web conferencing software using next.js for the frontend. Then used agoras transcription features and figured out how we retrieve, store and process that data using fetch.ai agents. Used gemini for processing and mongo.db for storing data"},{"heading":"Challenges we ran into","content":"Our tech stack didn't exactly matched with the project. Storing transcripts was a BIG bugger, we thought agora had place to store data but they didn't. we tried to send to singlestore but that didn't worked properly as well. Making fetch.ai agents work and make them retrieve and send data, and communicate with each other was also a problem. Making the app live update was also a challenge."},{"heading":"Accomplishments that we're proud of","content":"IT FUC'IN WORRKKS!!!"},{"heading":"What we learned","content":"How to create a video calling app, something that we have 0 knowledge about, using tools that we have 0 knowledge about. Also deciding on the idea took much longer than it should, we will focus on not the perfect but the most applicable and team friendly idea next time."},{"heading":"What's next for Notefly - AI Meeting Notes on the Fly","content":"After an all-nighter, the app is far from done, there could be more improvements in the summary logic. We can turn this into a more scalable product that allows people to host their own meetings in the cloud, its on a local host that lives in agoras serves now, we sadly can't send try us links rn."},{"heading":"Built With","content":"agora fetch.ai gemini mongodb next.js react tailwid zustand"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Watchdog","project_url":"https://devpost.com/software/watchdog-2vwqlf","tagline":"Help us make our schools safer.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/340/datas/medium.webp","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Hyperbolic: AI Inference Application Bounty"}],"team_members":[],"built_with":[{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"convex","url":null},{"name":"deepgram","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"fetch.ai","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"hyperbolic","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"tailwindcss","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/oviozz/WatchDog-2024-Hackathon-Berkeley"}],"description_sections":[{"heading":"What inspired us to build it","content":"Guns are now the leading cause of death among American children and teens, with 1 in every 10 gun deaths occurring in individuals aged 19 or younger. School shootings, in particular, have become a tragic epidemic in the U.S., underscoring the urgent need for enhanced safety measures. Our team united with a shared vision to leverage AI technology to improve security in American schools, helping to protect children and ensure their safety."},{"heading":"What it does","content":"Our product leverages advanced AI technology to enhance school safety by detecting potential threats in real-time. By streaming surveillance footage, our AI system can identify weapons, providing instant alerts to security personnel and administrators. In addition to visual monitoring, we integrate audio streaming to analyze changes in sentiment, such as raised voices or signs of distress. This dual approach—combining visual and auditory cues—enables rapid response to emerging threats."},{"heading":"How we built it","content":"We partnered with incredible sponsors—Deepgram, Hyperbolic, Groq, and Fetch.AI—to develop a comprehensive security solution that uses cutting-edge AI technologies. With their support, we were able to conduct fast AI inference, deploy an emergency contact agent, and create intelligent systems capable of tracking potential threats and key variables, all to ensure the safety of our communities.\n\nFor real-time data processing, we utilized Firebase and Convex to enable rapid write-back and retrieval of critical information. Additionally, we trained our weapon detection agent using Ultralytics YOLO v8 on the Roboflow platform, achieving an impressive ~90% accuracy. This high-performance detection system, combined with AI-driven analytics, provides a robust safety infrastructure capable of identifying and responding to threats in real time."},{"heading":"Challenges we ran into","content":"Streaming a real-time AI object detection model with both low latency and high accuracy was a significant challenge. Initially, we experimented with Flask and FastAPI for serving our model, followed by trying AWS and Docker to improve performance. However, after further optimization efforts, we ultimately integrated Roboflow.js directly in the browser using a Native SDK. This approach gave us a substantial advantage, allowing us to run the model efficiently within the client environment. As a result, we achieved the ability to track weapons quickly and accurately in real time, meeting the critical demands of our security solution."},{"heading":"Accomplishments that we're proud of","content":"We are incredibly proud of the features our product offers, providing a comprehensive and fully integrated security experience. Beyond detecting weapons and issuing instant alerts to law enforcement, faculty, and students through AI-powered agents, we also implemented extensive sentiment analysis. This enables us to detect emotional escalations that may signal potential threats. All of this is supported by real-time security data displays, ensuring that key decision-makers are always informed with up-to-the-minute information. Our system seamlessly brings together cutting-edge AI and real-time data processing to deliver a robust, proactive security solution."},{"heading":"What we learned","content":"We learned that the night is darkest right before the dawn... and that we need to persevere and be steadfast as a team to see our vision come to fruition."},{"heading":"What's next for Watchdog","content":"We want to get incorporated in the American school system!"},{"heading":"Built With","content":"chroma convex deepgram express.js fetch.ai groq hyperbolic javascript next.js python tailwindcss"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Teddy.AI","project_url":"https://devpost.com/software/teddy-ai-uq04z1","tagline":"Talk to your toys with Teddy.AI; the worlds first AI powered teddy bear that can tell children stories and tutor them through its fun journeys riddled with interactive academic puzzles.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/740/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Hyperbolic: AI Inference Application Bounty"}],"team_members":[],"built_with":[{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"generativeai","url":null},{"name":"hume","url":null},{"name":"hyperbolic","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"large-language-models","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"}],"external_links":[{"label":"empathic-voice-interface-starter-iil2si3j9.vercel.app","url":"https://empathic-voice-interface-starter-iil2si3j9.vercel.app/"}],"description_sections":[{"heading":"Inspiration","content":"My cousin recently had children, and as they enter their toddler years, I saw how they use their toys. They buy them, use them for a couple weeks, but then get bored. To be honest, however, I can't blame them. Currently, toys are used in one way, with no real interaction coming from both the child and the toy. What I really wanted to do was bring Toy Story to real life, and allow children to talk and learn from their toys, maximizing their happiness and education."},{"heading":"What it does","content":"We use Hume's API to allow you to talk with your toys and have full blown conversations with them. We utilize prompt engineering and allow you to have math lessons embedded within their choose your own adventure stories."},{"heading":"How we built it","content":"We embedded a raspberry pi, speaker, and microphone in the animal, which hosts a web app through which it can speak."},{"heading":"Challenges we ran into","content":"The Hume API was down sometimes which was tough to navigate, which halted our ability to make a self improving prompt (track progress of kids lessons). We also had a broken raspberry pi for the first 12 hours of the Hackathon. Hyperbolic randomly stopped working for us so our interactive story pictures stopped working."},{"heading":"Accomplishments that we're proud of","content":"Learning about how to prompt image generation (feeding the text transcription into Hyperbolic)"},{"heading":"What we learned","content":"Image generation, prompt engineering, function calling"},{"heading":"What's next for Teddy.AI","content":"Memory and lesson progress reports"},{"heading":"Built With","content":"gemini generativeai hume hyperbolic javascript large-language-models node.js raspberry-pi"},{"heading":"Try it out","content":"empathic-voice-interface-starter-iil2si3j9.vercel.app"}]},{"project_title":"EYEdentity","project_url":"https://devpost.com/software/eyedentity","tagline":"Welcome to the JARVIS for first responders!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/767/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Snap: Dream it. Build it."}],"team_members":[],"built_with":[{"name":"face-recognition","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"lens-studio","url":null},{"name":"pil","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"snapchat","url":"https://devpost.com/software/built-with/snapchat"},{"name":"spectacles","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1K41ArhGy6HgdhWuWSoGtBkhscxycKVTnzTSsnapsv9o/edit?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for EYEdentity came from the need to enhance patient care through technology. Observing the challenges healthcare professionals face in quickly accessing patient information, we envisioned a solution that combines facial recognition and augmented reality to streamline interactions and improve efficiency."},{"heading":"What it does","content":"EYEdentity is an innovative AR interface that scans patient faces to display their names and critical medical data in real-time. This technology allows healthcare providers to access essential information instantly, enhancing the patient experience and enabling informed decision-making on the spot."},{"heading":"How we built it","content":"We built EYEdentity using a combination of advanced facial recognition and facial tracking algorithms and the new Snap Spectacles. The facial recognition component was developed using machine learning techniques to ensure high accuracy, while the AR interface was created using cutting-edge software tools that allow for seamless integration of data visualization in a spatial format. Building on the Snap Spectacles provided us with a unique opportunity to leverage their advanced AR capabilities, resulting in a truly immersive user experience."},{"heading":"Challenges we ran into","content":"One of the main challenges we faced was ensuring the accuracy and speed of the facial recognition system in various lighting conditions and angles. Additionally, integrating real-time data updates into the AR interface required overcoming technical hurdles related to data synchronization and display."},{"heading":"Accomplishments that we're proud of","content":"We are proud of successfully developing a prototype that demonstrates the potential of our technology in a real-world healthcare setting. The experience of building on the Snap Spectacles allowed us to create a user experience that feels natural and intuitive, making it easier for healthcare professionals to engage with patient data."},{"heading":"What we learned","content":"Throughout the development process, we learned the importance of user-centered design in healthcare technology. Communicating with healthcare professionals helped us understand their needs and refine our solution to better serve them. We also gained valuable insights into the technical challenges of integrating AR with real-time data."},{"heading":"What's next for EYEdentity","content":"Moving forward, we plan to start testing in clinical environments to gather more feedback and refine our technology. Our goal is to enhance the system's capabilities, expand its features, and ultimately deploy EYEdentity in healthcare facilities to revolutionize patient care."},{"heading":"Built With","content":"face-recognition flask javascript lens-studio pil python snapchat spectacles typescript"},{"heading":"Try it out","content":"docs.google.com"}]},{"project_title":"CLaiM","project_url":"https://devpost.com/software/autoclaim-q8who1","tagline":"Automatically file home insurance claims after natural disasters.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/055/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Hyperbolic: AI Inference Application Bounty"}],"team_members":[],"built_with":[{"name":"amazon-ec2","url":"https://devpost.com/software/built-with/amazon-ec2"},{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"chromadb","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"nextjs","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"s3","url":null},{"name":"segment-anything","url":null},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"},{"name":"yolo","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/SonavAgarwal/calhacks-24"}],"description_sections":[{"heading":"Inspiration","content":"After the recent hurricanes in Florida, one of our team members who is from Florida had damage to their house. Normally, the process for filing an insurance claim after disaster is extremely cumbersome, requiring proof of all items and their value that were in the house before the disaster. This is very hard for disaster victims to prove in such a tough situation, which dissuades many people from filing insurance claims that cover the full value of all the items they lost."},{"heading":"What it does","content":"Our app allows homeowners to record a video of their house, while our innovative AI automatically catalogues all items in your house and their approximate value. After a disaster, you an rescan your house and we automatically suggest items that might be lost to add to your insurance claim, with image proof of the before and after."},{"heading":"How we built it","content":"Our frontend is built with next.js. Our backend uses Flask and sqllite. We also use xrpl in order to put items on the blockchain for proof of their existence before the disaster. Here are the steps our AI uses to process videos and pictures into items:\n\nThe video is stitched together into a large panorama with OpenCV We use YOLOv8 and Meta's segment anything model to generate areas of interest on the image. We use Hyperbolic's inference (Qwen2 VL7B) to analyze each area and decide if it is an object to record and find its estimated monetary value We place all images in ChromaDB to group/count items of the same type and match images before and after disaster. We originally used ResNet for our embedding for the vector database in order to compare items, but later found all-MiniLM-L6-v2 to work better for vector encoding. The ResNet embeddings were too high dimensional. We generate a pdf claim to submit to the insurance company based on items that didn't appear after the disaster and were added by the user."},{"heading":"Challenges we ran into","content":"Segmentation problem: We tried a lot of different things in order to segment images well. We tried the segment everything model, SAM+yolo, and tried asking gemini to bounding boxes (it's the only vision language model that advertises this capability). We found SAM+yolo worked the best by far but only after lots of iteration and tuning. System Design: lots of iteration required and thinking about the customer. For simplicity chose to host a single AWS EC2 instance although seriously considered hosting multiple servers for each general task like segmentation. Video Panorama: How do you do segmentation on a video without getting duplicate items across frames? Our solution was to turn the video into a large image and run our machine learning on this image. CORS struck us once again."},{"heading":"Accomplishments that we're proud of","content":"Super fast Segment Everything + Yolo + recognition + grouping pipeline that robustly recognized and catalogued items from video. Well designed and aesthetic front-end."},{"heading":"What we learned","content":"There are no power outlets in SF. We learned how to deploy an EC2 instance, and also how to use vector databases."},{"heading":"What's next for CLaiM","content":"We would like to"},{"heading":"Built With","content":"amazon-ec2 amazon-web-services chromadb flask nextjs python s3 segment-anything sqlite yolo"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Noq","project_url":"https://devpost.com/software/noq-92iysh","tagline":"AI-Powered Real-Time Note Taker","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/891/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Groq: The Best on Groq"}],"team_members":[],"built_with":[{"name":"chromadb","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"next.js","url":null},{"name":"tailwind","url":null},{"name":"whisper","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pnavab/calhacks11"}],"description_sections":[{"heading":"Inspiration","content":"Our team has consistently tried to improve our personal school experiences slowly but surely by optimizing every small process we can. This tool takes it to the next level and completely streamlines the learning process up to the point where you can start studying!"},{"heading":"What it does","content":"Noq listens into your lecture, compiling and categorizing all of the lecture’s most important points for you. They are automatically expanded and revised, separating all the different ideas to be ready for vectorization. Once entered into the vectorstore, the RAG search engine allows for context-driven searching through your notes to easily find what you need. You can also generate diagrams for specific lines within a note, easily visualizing what has been summarized for the user."},{"heading":"How we built it","content":"Using Groq as the main infrastructure behind our application, we leveraged its highly fast speeds to run multiple inference calls in our backend AI agent network almost instantly. ChromaDB also served as the vectorstore to store the notes and run our semantic search engine for easy querying. Our actual website was also built in Next.js and Tailwind."},{"heading":"Challenges we ran into","content":"It was hard creating a complex real-time app without streaming with minimal delay and having AI agents acting in the background"},{"heading":"Accomplishments that we're proud of","content":"Being able to integrate external tools and technologies such as embedding models with Groq, as well as maintaining multiple servers interacting with each other to create a seamless and extremely fast interaction"},{"heading":"What we learned","content":"Frontend is hard"},{"heading":"What's next for Noq","content":"Completely integrate it with personal calendars and email, using tool calling for extracting upcoming events and deadlines and automatically creating reminders for those. This is a product we could easily see becoming a staple in our daily lives, and will definitely continue improving upon it!"},{"heading":"Built With","content":"chromadb groq next.js tailwind whisper"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"SnapChest","project_url":"https://devpost.com/software/spectacles-treasure-hunt","tagline":"Find treasure using the Spectacles + haptic smartphone feedback!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/832/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Snap: Dream it. Build it."}],"team_members":[],"built_with":[{"name":"figma","url":null},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"lens-studio","url":null},{"name":"spectacles","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/dawoon19/treasure-hunt-spectacles"}],"description_sections":[{"heading":"Inspiration","content":"Open-world AR applications like Pokemon Go that bring AR into everyday life and the outdoors were major inspirations for this project. Additionally, in thinking about how to integrate smartphones with Spectacles, we found inspiration in video games like Phasmophobia's EMP sensors that react more strongly in the presence of ghosts or The Legend of Zelda: Skyward Sword that contained an in-game tracking functionality that pulsates more strongly when facing the direction of and walking closer to a target."},{"heading":"What it does","content":"This game integrates the Spectacles gear and smartphones together by allowing users to leverage the gyroscopic, haptic, and tactile functionalities of phones to control or receive input about their AR environment. In the game, users have to track down randomly placed treasure chests in their surrounding environment by using their phone as a sensor that begins vibrating when the user is facing a treasure and enters stronger modes of haptic feedback as users get closer to the treasure spots.\n\nThese chests come in three types: monetary, puzzle, and challenge. Monetary chests immediately give users in-game rewards. Puzzle chests engage users in a single-player mini-game that may require cognitive or physical activity. Finally, challenge chests similarly engage users in activities not necessarily games, and a stretch goal for multiplayers was that if multiple users were near a spot that another user found a treasure in, the other n users could challenge the treasure finder in a n vs. 1 duel, with the winner(s) taking the rewards."},{"heading":"How we built it","content":"Once we figured out our direction for the project, we built a user flow architecture in Figma to brainstorm the game design for our application ( link ), and we also visualized how to implement the system for integrating phone haptic feedback with the spectacles depending on distance and directional conditions.\n\nFrom there, we each took on specific aspects of the user flow architecture to primarily work on: (1) the treasure detection mechanism, (2) spawning the treasure once the user entered within a short distance from the target, and (3) the content of the treasure chests (i.e. rewards or activities). Nearly everything was done using in-house libraries, assets, and the GenAI suite within Snap's Lens Studio."},{"heading":"Challenges we ran into","content":"As we were working with Spectacles for the first time (compounded with internet problems), we initially encountered technical issues with setting up our development environment and linking the Spectacles for debugging. Due to limited documentation and forums since it is limited-access technology, we had to do a lot of trial-and-error and guessing to figure out how to get our code to work, but luckily, Snap's documentation provided templates to work off of and the Snap staff was able to provide technical assistance to guide us in the right direction. Additionally, given one Spectacle to work with, parallelizing our development work was quite challenging as we had to integrate everything onto one computer while dealing with merge conflicts between our code."},{"heading":"Accomplishments that we're proud of","content":"In a short span of time, we were able to successfully build a game that provides a unique immersive experience! We've come across and solved errors that didn't have solutions on the internet. For a couple of members of our team, this sparks a newfound interest in the AR space."},{"heading":"What we learned","content":"This was our first time working with Lens Studio and it's unanimously been a smooth and great software to work with. For the experienced members on our team, it's been a rewarding experience to make an AR application using JS/TS instead of C# which is the standard language used in Unity."},{"heading":"What's next for SnapChest","content":"We're excited to push this app forward by adding more locations for treasures, implementing a point system, and also a voice agent integration that provides feedback based on where you're going so you won't get bored on your journey! If Spectacles would be made available to the general public, a multiplayer functionality would definitely gain a lot of traction and we're looking forward to the future!"},{"heading":"Built With","content":"figma github javascript lens-studio spectacles typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"FRED","project_url":"https://devpost.com/software/fred-bpszwn","tagline":"FRED is a satellite-powered emergency response tool that allows users in remote areas to send GPS coordinates, images, and voice data to responders, ensuring help can be dispatched from anywhere.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/409/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Skylo: Seamlessly Switch from Cellular to Satellite"}],"team_members":[],"built_with":[{"name":"cellular","url":null},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"ios","url":"https://devpost.com/software/built-with/ios"},{"name":"llama","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"raspberry-pi","url":"https://devpost.com/software/built-with/raspberry-pi"},{"name":"satellite","url":null},{"name":"skylo","url":null},{"name":"swift","url":"https://devpost.com/software/built-with/swift"}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"The need for faster and more reliable emergency communication in remote areas inspired the creation of FRED (Fire & Rescue Emergency Dispatch). Whether due to natural disasters, accidents in isolated locations, or a lack of cellular network coverage, emergencies in remote areas often result in delayed response times and first-responders rarely getting the full picture of the emergency at hand. We wanted to bridge this gap by leveraging cutting-edge satellite communication technology to create a reliable, individualized, and automated emergency dispatch system. Our goal was to create a tool that could enhance the quality of information transmitted between users and emergency responders, ensuring swift, better informed rescue operations on a case-by-case basis."},{"heading":"What it does","content":"FRED is an innovative emergency response system designed for remote areas with limited or no cellular coverage. Using satellite capabilities, an agentic system, and a basic chain of thought FRED allows users to call for help from virtually any location. What sets FRED apart is its ability to transmit critical data to emergency responders, including GPS coordinates, detailed captions of the images taken at the site of the emergency, and voice recordings of the situation. Once this information is collected, the system processes it to help responders assess the situation quickly. FRED streamlines emergency communication in situations where every second matters, offering precise, real-time data that can save lives."},{"heading":"How we built it","content":"FRED is composed of three main components: a mobile application, a transmitter, and a backend data processing system.\n\n1. Mobile Application: The mobile app is designed to be lightweight and user-friendly. It collects critical data from the user, including their GPS location, images of the scene, and voice recordings. 2. Transmitter: The app sends this data to the transmitter, which consists of a Raspberry Pi integrated with Skylo’s Satellite/Cellular combo board. The Raspberry Pi performs some local data processing, such as image transcription, to optimize the data size before sending it to the backend. This minimizes the amount of data transmitted via satellite, allowing for faster communication. 3. Backend: The backend receives the data, performs further processing using a multi-agent system, and routes it to the appropriate emergency responders. The backend system is designed to handle multiple inputs and prioritize critical situations, ensuring responders get the information they need without delay. 4. Frontend: We built a simple front-end to display the dispatch notifications as well as the source of the SOS message on a live-map feed."},{"heading":"Challenges we ran into","content":"One major challenge was managing image data transmission via satellite. Initially, we underestimated the limitations on data size, which led to our satellite server rejecting the images. Since transmitting images was essential to our product, we needed a quick and efficient solution. To overcome this, we implemented a lightweight machine learning model on the Raspberry Pi that transcribes the images into text descriptions. This drastically reduced the data size while still conveying critical visual information to emergency responders. This solution enabled us to meet satellite data constraints and ensure the smooth transmission of essential data."},{"heading":"Accomplishments that we’re proud of","content":"We are proud of how our team successfully integrated several complex components—mobile application, hardware, and AI powered backend—into a functional product. Seeing the workflow from data collection to emergency dispatch in action was a gratifying moment for all of us. Each part of the project could stand alone, showcasing the rapid pace and scalability of our development process. Most importantly, we are proud to have built a tool that has the potential to save lives in real-world emergency scenarios, fulfilling our goal of using technology to make a positive impact."},{"heading":"What we learned","content":"Throughout the development of FRED, we gained valuable experience working with the Raspberry Pi and integrating hardware with the power of Large Language Models to build advanced IOT system. We also learned about the importance of optimizing data transmission in systems with hardware and bandwidth constraints, especially in critical applications like emergency services. Moreover, this project highlighted the power of building modular systems that function independently, akin to a microservice architecture. This approach allowed us to test each component separately and ensure that the system as a whole worked seamlessly."},{"heading":"What’s next for FRED","content":"Looking ahead, we plan to refine the image transmission process and improve the accuracy and efficiency of our data processing. Our immediate goal is to ensure that image data is captioned with more technical details and that transmission is seamless and reliable, overcoming the constraints we faced during development. In the long term, we aim to connect FRED directly to local emergency departments, allowing us to test the system in real-world scenarios. By establishing communication channels between FRED and official emergency dispatch systems, we can ensure that our product delivers its intended value—saving lives in critical situations."},{"heading":"Built With","content":"cellular firebase ios llama python raspberry-pi satellite skylo swift"}]},{"project_title":"SpeakEasy: AI Language Companion","project_url":"https://devpost.com/software/speakeasy-ai-language-companion","tagline":"Visiting another country but don't want to sound like a robot? Want to learn a new language but can't get your intonation to sound like other people's? SpeakEasy can make you sound like, well, you!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/317/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Cartesia: Best AI Voice Project"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"cartesia","url":null},{"name":"ffmpeg","url":"https://devpost.com/software/built-with/ffmpeg"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sqlalchemy","url":"https://devpost.com/software/built-with/sqlalchemy"},{"name":"sqlite","url":"https://devpost.com/software/built-with/sqlite"}],"external_links":[{"label":"github.com","url":"https://github.com/Boomaa23/speak-easy"}],"description_sections":[{"heading":"Overview","content":"SpeakEasy: AI Language Companion\n\nVisiting another country but don't want to sound like a robot? Want to learn a new language but can't get your intonation to sound like other people's? SpeakEasy can make you sound like, well, you!"},{"heading":"Features","content":"SpeakEasy is an AI language companion which centers around localizing your own voice into other languages.\n\nIf, for example, you wanted to visit another country but didn't want to sound like a robot or Google Translate, you could still talk in your native language. SpeakEasy can then automatically repeat each statement in the target language in exactly the intonation you would have if you spoke that language.\n\nSay you wanted to learn a new language but couldn't quite get your intonation to sound like the source material you were learning from. SpeakEasy is able to provide you phrases in your own voice so you know exactly how your intonation should sound."},{"heading":"Background","content":"SpeakEasy is the product of a group of four UC Berkeley students. For all of us, this is our first submission to a hackathon and the result of several years of wanting to get together to create something cool together. We are excited to present every part of SpeakEasy; from the remarkably accurate AI speech to just how much we've all learned about rapidly developed software projects.\n\nInspiration\n\nOur group started by thinking of ways we could make an impact. We then expanded our search to include using and demonstrating technologies developed by CalHacks' generous sponsors, as we felt this would be a good way to demonstrate how modern technology can be used to help everyday people.\n\nIn the end, we decided on SpeakEasy and used Cartesia to realize many of the AI-powered functions of the application. This enabled us to make something which addresses a specific real-world problem (robotic-sounding translations) many of us have either encountered or are attempting to avoid.\n\nChallenges\n\nOur group has varying levels of software development experience, and especially given our limited hackathon experience (read: none), there were many challenging steps. For example: deciding on project scope, designing high-level architecture, implementing major features, and especially debugging.\n\nWhat was never a challenge, however, was collaboration. We worked quite well as a team and had a good time doing it.\n\nAccomplishments / Learning\n\nWe are proud to say that despite the many challenges we accomplished a great deal with this project. We have a fully functional Flask backend with React frontend (see \"Technical Details\") which uses multiple different APIs. This project successfully ties together audio processing, asynchonrous communication, artificial intelligence, UI/UX design, database management, and so much more. What's more is that many of our group members learned this from base fundamentals."},{"heading":"Technical Details","content":"As mentioned in an earlier section, SpeakEasy is designed with a Flask (Python) backend and React (JavaScript) frontent. This is a very standard setup that is used often at hackathons due to its easy implementation and relatively limited required setup. Flask only requires two lines of code to make an entirely new endpoint, while React can make a full audio-playing page with callbacks that looks absolutely beautiful in less than an hour. For storing data, we use SQLAlchemy (backed by SQLite).\n\nWhen a user opens SpeakEasy, they are first sent to a landing page. After pressing any key, they are taken to a training screen. Here they will record a 15-20 second message (ideally the one shown on screen) which will be used to create an embedding. This is accomplished with the Cartesia \"Clone Voice from Clip\" endpoint. A Cartesia Voice (abbreviated as \"Voice\") is created from the returned embedding (using the \"Create Voice\" endpoint) which contains a Voice ID. This Voice ID is used to uniquely identify each voice, which itself is in a specific language. The database then stores this voice and creates a new user which this voice is associated with. When the recording is complete and the user clicks \"Next\", they will be taken to a split screen where they can choose between the two main program functions of SpeakEasy. If the user clicks on the vocal translation route, they will be brought to another recording screen. Here, they record a sound in English which is then sent to the backend. The backend encodes this MP3 data into PCM, sends it to a speech-to-text API, and then transfers it into a text translation API. Separately, the backend trains a new Voice (using the Cartesia Localize Voice endpoint, wrapped by get/create Voice since Localize requires an embedding instead of a Voice ID) with the intended target language and uses the Voice ID it returns. The backend then sends the translated text to the Cartesia \"Text to Speech (Bytes)\" endpoint using this new Voice ID. This is then played back to the user as a response to the original backend request. All created Voices are stored in the database and associated with the current user. This is done so returning users do not have to retrain their voices in any language. If the user clicks on the language learning route, they will be brought to a page which displays a randomly selected phrase in a certain language. It will then query the Cartesia API to pronounce that phrase in that language, using the preexisting Voice ID if available (or prompting to record a new phrase if not). A request is made to the backend to input some microphone input, which is then compared to Cartesia's estimation of your speech in a target language. The backend then returns a set of feedback using the difference between the two pronounciations, and displays that to the user on the frontend. After each route is selected, the user may choose to go back and select either route (the same route again or the other route)."},{"heading":"Cartesia Issues","content":"We were very impressed with Cartesia and its abilities, but noted a few issues which would improve the development experience.\n\nClone Voice From Clip endpoint documentation The documentation for the endpoint in question details a Response which includes a variety of fields: id , name , language , and more. However, the endpoint only returns the embedding in a dictonary. It is then required to send the embedding into the \"Create Voice\" endpoint to create an id (and other fields), which are required for some further endpoints. Clone Voice From Clip endpoint length requirements The clip supplied to the endpoint in question appears to require a duration of greater than a second or two. Se \"Error reporting\" for further details. Text to Speech (Bytes) endpoint output format The TTS endpoint requires an output format be specified. This JSON object notably lacks an encoding field in the MP3 configuration which is present for the other formats (raw and WAV). The solution to this is to send an encoding field with the value for one of the other two formats, despite this functionally doing nothing. Embedding format The embedding is specified as a list of 192 numbers, some of which may be negative. Python's JSON parser does not like the dash symbol and frequently encounters issues with this. If possible, it would be good to either allow this encoding to be base64 encoded, hashed, or something else to prevent negatives. Optimally embeddings do not have negatives, though this seems difficult to realize. Response code mismatches Some response codes returned from endpoints do not match their listed function. For example, a response code of 405 should not be returned when there is a formatting error in the request. Similarly, 400 is returned before 404 when using invalid endpoints, making it difficult to debug. There are several other instances of this but we did not collate a list. Error reporting If (most) endpoints return in JSON format, errors should also be turned in JSON format. This prevents many parsing issues and would simplify design. In addition, error messages are too vague to glean any useful information. For example, 500 is always \"Bad request\" regardless of the underlying error cause. This is the same thing as the error name."},{"heading":"Future Improvements","content":"In the future, it would be interesting to investigate the following:\n\nProper authentication Cloud-based database storage (with redundancy) Increased error checking Unit and integration test coverage, with CI/CD Automatic recording quality analysis Audio streaming (instead of buffering) using WebSockets Mobile device compatibility Reducing audio processing overhead"},{"heading":"Built With","content":"ai cartesia ffmpeg flask javascript python react sqlalchemy sqlite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"CryptoWise: Intelligent Trading and Insights Platform","project_url":"https://devpost.com/software/cryptowise-intelligent-trading-and-insights-platform","tagline":"Revolutionize your cryptocurrency trading experience with our platform that combines real-time data visualization, intelligent query handling, and automated trading strategies.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/921/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Singlestore: Best Real-Time App"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"fast","url":null},{"name":"fetch","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"kraken","url":"https://devpost.com/software/built-with/kraken"},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"metamask","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"single-store","url":null},{"name":"smart-contract","url":null},{"name":"websockets","url":"https://devpost.com/software/built-with/websockets"}],"external_links":[{"label":"docs.google.com","url":"https://docs.google.com/presentation/d/1TDGFxihhECPf01kBEuqnaqiSKVZF6DWpon8X1Au-7BY/edit#slide=id.gc6f90357f_0_47"},{"label":"github.com","url":"https://github.com/sohail-coder/kraken-analytics-backend"},{"label":"github.com","url":"https://github.com/Mahendra-Chittupolu/kraken-frontend"},{"label":"github.com","url":"https://github.com/KrishnaVPabbisetty/crypto-analysis-ml"},{"label":"146.190.123.50","url":"http://146.190.123.50:3000"},{"label":"146.190.123.50","url":"http://146.190.123.50:4000"},{"label":"146.190.123.50","url":"http://146.190.123.50:5000"},{"label":"drive.google.com","url":"https://drive.google.com/file/d/1UaHgjAHs3uYtlir3zmUPV_K5svMfR6xr/view?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for CryptoWise stemmed from the dynamic and often unpredictable nature of the cryptocurrency market. We recognized the need for a comprehensive platform that not only provides real-time data but also offers intelligent insights and automated trading capabilities. Our goal was to empower traders, researchers, and enthusiasts with tools that enhance decision-making and streamline trading processes."},{"heading":"What it does","content":"CryptoWise is an all-in-one platform designed to revolutionize the cryptocurrency trading experience. It offers:\n\nReal-Time Data Visualization: Users can view up-to-the-minute cryptocurrency values and trends through interactive charts. Intelligent Query Handling: A chatbot integrated with advanced LLM technology answers queries related to real-time, historical, and generic cryptocurrency data. Automated Trading: The platform includes a predictive model that suggests Buy, Sell, or Hold actions based on real-time data analysis, with the capability to execute trades automatically when conditions are met."},{"heading":"How we built it","content":"We built CryptoWise using a combination of cutting-edge technologies:\n\nData Acquisition: Integrated WebSocket APIs to fetch real-time cryptocurrency data, which is stored in SingleStore Database for fast retrieval and analytics. Visualization: Used SingleStore's analytics API to create dynamic charts that display changing market trends. Chatbot Integration: Leveraged the GROQ API and OpenAI 3.5 Turbo model to develop a chatbot capable of handling complex queries about cryptocurrency. Predictive Modeling: Developed a model trained on real-time and historical data from Kraken's WebSocket and OHLC APIs to make trading decisions. Automated Trading with Smart Contracts: Utilized smart contracts for executing trades securely. MetaMask authentication ensures secure access to users' wallets using private keys. Initially planned integration with Fetch.ai's agent-based tools faced challenges due to technical integration complexity and resource allocation constraints."},{"heading":"Challenges we ran into","content":"Throughout the development of CryptoWise, we faced several challenges:\n\nData Integration: Ensuring seamless integration of real-time data from multiple sources was complex and required robust solutions. Model Training: Continuously training the predictive model on diverse datasets to maintain accuracy was demanding. User Interface Design: Creating an intuitive and user-friendly interface that effectively displays complex data was challenging but crucial for user engagement. Agent-Based Trading Integration: The complexity of integrating Fetch.ai's agents with our existing infrastructure proved challenging, along with resource limitations."},{"heading":"Accomplishments that we're proud of","content":"We are proud of several key accomplishments:\n\nSeamless Data Flow: Successfully integrated various data sources into a cohesive platform that delivers real-time insights. Advanced AI Integration: Implemented a sophisticated chatbot that enhances user interaction with accurate and relevant responses. Automated Trading Feature: Developed an automated trading system using smart contracts that executes trades based on predictive analytics, offering users a competitive edge."},{"heading":"What we learned","content":"The development of CryptoWise taught us invaluable lessons:\n\nImportance of Data Accuracy: Real-time accuracy is critical in financial applications, necessitating rigorous testing and validation. AI Capabilities: Leveraging AI for query handling and decision-making can significantly enhance user experience and platform functionality. User-Centric Design: Designing with the user in mind is essential for creating an engaging and effective interface."},{"heading":"What's next for CryptoWise: Intelligent Trading and Insights Platform","content":"Looking ahead, we plan to expand CryptoWise's capabilities by:\n\nEnhanced Predictive Analytics: Further refining our models to improve prediction accuracy and broaden the range of supported cryptocurrencies. User Personalization: Introducing features that allow users to customize their experience based on individual preferences and trading strategies. Community Engagement: Building a community around CryptoWise where users can share insights, strategies, and feedback to continuously improve the platform.\n\nCryptoWise aims to set a new standard in cryptocurrency trading by combining powerful analytics with intuitive design, making it an indispensable tool for anyone involved in the crypto market.\n\nAPI Documentation"},{"heading":"Overview","content":"This section provides details about the various API endpoints used in the CryptoWise platform, categorized according to their role in the architecture."},{"heading":"Automated Trading","content":"Automated Purchase Endpoint URL: http://146.190.123.50:4000/api/simulate-purchase Purpose: Simulates the purchase of cryptocurrency based on model predictions."},{"heading":"Query Handling","content":"Query to LLM URL: http://146.190.123.50:4000/query Purpose: Sends user queries to the LLM for categorization and response generation."},{"heading":"Data Acquisition","content":"Kraken WebSocket API URL: wss://ws.kraken.com/ Purpose: Provides real-time cryptocurrency data for live updates. Historical Data URL: https://api.kraken.com/0/public/OHLC Purpose: Fetches historical price data for cryptocurrencies."},{"heading":"Machine Learning","content":"ML Prediction API URL: http://146.190.123.50:5000/prediction Purpose: Provides predictions on Buy, Sell, or Hold actions based on real-time data analysis."},{"heading":"Analytics","content":"Bollinger Bands URL: http://146.190.123.50:4000/api/analytics/bollinger-bands?coin=XBT&period=20&stdDev=2 Purpose: Calculates Bollinger Bands for specified cryptocurrency and period. Historical Volatility URL: http://146.190.123.50:4000/api/analytics/historical_volatility?coin=XBT Purpose: Computes historical volatility for the specified cryptocurrency. Stats URL: http://146.190.123.50:4000/api/analytics/stats Purpose: Provides statistical analysis of cryptocurrency data. Candle Data URL: http://146.190.123.50:4000/api/analytics/get_candle_data?coin=XBT Purpose: Retrieves candlestick data for detailed market analysis."},{"heading":"Built With","content":"express.js fast fetch groq javascript kraken machine-learning metamask node.js python react single-store smart-contract websockets"},{"heading":"Try it out","content":"docs.google.com github.com github.com github.com 146.190.123.50 146.190.123.50 146.190.123.50 drive.google.com"}]},{"project_title":"Canary","project_url":"https://devpost.com/software/canary-lrg3ep","tagline":"Satellite-enabled wireless sensor network for early wildfire detection.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/364/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Skylo: Seamlessly Switch from Cellular to Satellite"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"esp8266","url":null},{"name":"gcp","url":null},{"name":"iot","url":null},{"name":"monogoto","url":null},{"name":"murata","url":null},{"name":"pandas","url":"https://devpost.com/software/built-with/pandas"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"skylo","url":null},{"name":"streamlit","url":null},{"name":"udp","url":null},{"name":"wifi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/tsai-henry/canary"}],"description_sections":[{"heading":"Inspiration 🔥","content":"While on the way to CalHacks, we drove past a fire in Oakland Hills that had started just a few hours prior, meters away from I-580. Over the weekend, the fire quickly spread and ended up burning an area of 15 acres, damaging 2 homes and prompting 500 households to evacuate. This served as a harsh reminder that wildfires can and will start anywhere as long as few environmental conditions are met, and can have devastating effects on lives, property, and the environment.\n\nThe following statistics are from the year 2020[1].\n\nPeople: Wildfires killed over 30 people in our home state of California. The pollution is set to shave off a year of life expectancy of CA residents in our most polluted counties if the trend continues.\n\nProperty: We sustained $19b in economic losses due to property damage.\n\nEnvironment: Wildfires have made a significant impact on climate change. It was estimated that the smoke from CA wildfires made up 30% of the state’s greenhouse gas emissions. UChicago also found that “a single year of wildfire emissions is close to double emissions reductions achieved over 16 years.”\n\nRight now (as of 10/20, 9:00AM): According to Cal Fire, there are 7 active wildfires that have scorched a total of approx. 120,000 acres.\n\n[1] - news.chicago.edu"},{"heading":"Our Solution: Canary 🐦🚨","content":"Canary is an early wildfire detection system powered by an extensible, low-power, low-cost, low-maintenance sensor network solution. Each sensor in the network is placed in strategic locations in remote forest areas and records environmental data such as temperature and air quality, both of which can be used to detect fires. This data is forwarded through a WiFi link to a centrally-located satellite gateway computer. The gateway computer leverages a Monogoto Satellite NTN (graciously provided by Skylo) and receives all of the incoming sensor data from its local network, which is then relayed to a geostationary satellite. Back on Earth, we have a ground station dashboard that would be used by forest rangers and fire departments that receives the real-time sensor feed. Based on the locations and density of the sensors, we can effectively detect and localize a fire before it gets out of control."},{"heading":"What Sets Canary Apart 💡","content":"Current satellite-based solutions include Google’s FireSat and NASA’s GOES satellite network. These systems rely on high-quality imagery to localize the fires, quite literally a ‘top-down’ approach. Google claims it can detect a fire the size of a classroom and notify emergency services in 20 minutes on average, while GOES reports a latency of 3 hours or more. We believe these existing solutions are not effective enough to prevent the disasters that constantly disrupt the lives of California residents as the fires get too big or the latency is too high before we are able to do anything about it. To address these concerns, we propose our ‘bottom-up’ approach, where we can deploy sensor networks on a single forest or area level and then extend them with more sensors and gateway computers as needed."},{"heading":"Technology Details 🖥️","content":"Each node in the network is equipped with an Arduino 101 that reads from a Grove temperature sensor. This is wired to an ESP8266 that has a WiFi module to forward the sensor data to the central gateway computer wirelessly. The gateway computer, using the Monogoto board, relays all of the sensor data to the geostationary satellite. On the ground, we have a UDP server running in Google Cloud that receives packets from the satellite and is hooked up to a Streamlit dashboard for data visualization."},{"heading":"Challenges and Lessons 🗻","content":"There were two main challenges to this project.\n\nHardware limitations: Our team as a whole is not very experienced with hardware, and setting everything up and getting the different components to talk to each other was difficult. We went through 3 Raspberry Pis, a couple Arduinos, different types of sensors, and even had to fashion our own voltage divider before arriving at the final product. Although it was disheartening at times to deal with these constant failures, knowing that we persevered and stepped out of our comfort zones is fulfilling.\n\nSatellite communications: The communication proved to be tricky due to inconsistent timing between sending and receiving the packages. We went through various socket ids and ports to see if there were any patterns to the delays. Through our thorough documentation of steps taken, we were eventually able to recognize a pattern in when the packages were being sent and modify our code accordingly."},{"heading":"What’s Next for Canary 🛰️","content":"As we get access to better sensors and gain more experience working with hardware components (especially PCB design), the reliability of our systems will improve. We ran into a fair amount of obstacles with the Monogoto board in particular, but as it was announced as a development kit only a week ago, we have full faith that it will only get better in the future. Our vision is to see Canary used by park services and fire departments in the most remote areas of our beautiful forest landscapes in which our satellite-powered sensor network can overcome the limitations of cellular communication and existing fire detection solutions."},{"heading":"Built With","content":"arduino esp8266 gcp iot monogoto murata pandas python skylo streamlit udp wifi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Notic3.","project_url":"https://devpost.com/software/notic3","tagline":"The blockchain solution for content creators offering a decentralized platform for creators and users, powered by Sui.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/112/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Sui: Reimagine the internet with Sui"}],"team_members":[],"built_with":[{"name":"move","url":null},{"name":"next.js","url":null},{"name":"sui","url":null},{"name":"tailwindcss","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"walrus","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/kevinru2023/notic3"},{"label":"notic3.vercel.app","url":"https://notic3.vercel.app/"}],"description_sections":[{"heading":"Overview","content":"Notic3 is a decentralized platform designed to empower content creators, offering a blockchain-based alternative to platforms like Patreon (and that one unholy platform). Our mission is to eliminate intermediaries, giving creators direct access to their supporters while ensuring secure, transparent, and tamper-proof data. Built fully on-chain, Notic3 exemplifies the spirit of decentralization by providing trustless interactions and immutable records.\n\nWhile many blockchain-based projects use off-chain solutions to sidestep technical challenges, our team committed to going all-in on decentralization. This approach presented unique obstacles, but it also set us apart, reinforcing our belief in the transformative potential of blockchain for creative industries."},{"heading":"Inspiration","content":"Blockchain technology offers more than just financial innovation—it provides a new way to manage ownership, access, and rewards. Inspired by these capabilities, we wanted to build something that could give content creators control over their work and revenue streams without relying on centralized platforms that charge high fees or control the distribution of content.\n\nWhile the Web3 space has already seen some early experiments in creator tools, many platforms are still hybrids—leveraging blockchain partially but retaining centralized components. We wanted to explore what would happen if we stayed true to the core ethos of decentralization. Notic3 was born from that idea: a fully on-chain platform that doesn't compromise on its principles."},{"heading":"Our Approach","content":"We chose to build Notic3 entirely on-chain to guarantee transparency, immutability, and censorship resistance. This meant every interaction—from subscription payments to content access—would be recorded directly on the blockchain (available to the general public). This decision came with trade-offs:\n\nSecure data storage: Managing private files like videos or audio on-chain is insecure if unencrypted, so we had to creatively manage metadata to ensure users are truly accessing content they own. Novelty of Move and Sui: The Move programming language, native to blockchains like Aptos and Sui, was completely new to our team. Learning it on the fly was one of the biggest challenges we faced, in addition to learning about the Sui SDK. Smart Contract Design: Writing complex smart contracts to handle subscription models, creator payouts, and access permissions directly on-chain was difficult."},{"heading":"Key Features","content":"Subscription-based Payments: Creators can set up recurring subscriptions that allow supporters to access premium content. Payments are processed seamlessly on-chain, ensuring transparency and immediate distribution of funds. Web3 Storage: Everything about our app is in the chain, including the file storage. We utilized Sui's newest Data Storage solution, Walrus, to store encrypted files on the chain using blobs allowing creators to leverage the chain for distribution of their content."},{"heading":"Conclusion","content":"Working on Notic3 has been an incredible experience. We took on ambitious challenges, and despite the difficulties, we believe we succeeded in delivering a powerful product. Along the way, we learned new technologies, embraced decentralized principles, and grew both as developers and as a team.\n\nNotic3 is proof of what can be achieved when you stay true to your mission, even when easier paths are available. We are excited to continue developing the platform beyond the hackathon and see how it can empower creators around the world.\n\nThank you for the opportunity to present Notic3, and we look forward to feedback and collaboration as we continue this journey!"},{"heading":"Built With","content":"move next.js sui tailwindcss typescript walrus"},{"heading":"Try it out","content":"github.com notic3.vercel.app"}]},{"project_title":"FLEX","project_url":"https://devpost.com/software/flex-se8mtf","tagline":"AI. Blockchain. Freelancing. Find your BEST candidates!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/373/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Sui: Reimagine the internet with Sui"}],"team_members":[],"built_with":[{"name":"deepgram","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"fetch.ai","url":null},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"move","url":null},{"name":"next.js","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"singlestore","url":null},{"name":"sui","url":null},{"name":"terraform","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/DPulavarthy/CalHacks"}],"description_sections":[{"heading":"Inspiration","content":"Freelancers deserve a platform where they can fully showcase their skills, without worrying about high fees or delayed payments. Companies need fast, reliable access to talent with specific expertise to complete jobs efficiently. \"FLEX\" bridges the gap, enabling recruiters to instantly find top candidates through AI-powered conversations, ensuring the right fit, right away."},{"heading":"What it does","content":"Clients talk to our AI, explaining the type of candidate they need and any specific skills they're looking for. As they speak, the AI highlights important keywords and asks any more factors that they would need with the candidate. This data is then analyzed and parsed through our vast database of Freelancers or the best matching candidates. The AI then talks back to the recruiter, showing the top candidates based on the recruiter’s requirements. Once the recruiter picks the right candidate, they can create a smart contract that’s securely stored and managed on the blockchain for transparent payments and agreements."},{"heading":"How we built it","content":"We built starting with the Frontend using Next.JS , and deployed the entire application on Terraform for seamless scalability. For voice interaction, we integrated Deepgram to generate human-like voice and process recruiter inputs, which are then handled by Fetch.ai 's agents. These agents work in tandem: one agent interacts with Flask to analyze keywords from the recruiter's speech, another queries the SingleStore database, and the third handles communication with Deepgram .\n\nUsing SingleStore's real-time data analysis and Full-Text Search, we find the best candidates based on factors provided by the client. For secure transactions, we utilized SUI blockchain, creating an agreement object once the recruiter posts a job. When a freelancer is selected and both parties reach an agreement, the object gets updated, and escrowed funds are released upon task completion—all through Smart Contracts developed in Move . We also used Flask and Express.js to manage backend and routing efficiently."},{"heading":"Challenges we ran into","content":"We faced challenges integrating Fetch.ai agents for the first time, particularly with getting smooth communication between them. Learning Move for SUI and connecting smart contracts with the frontend also proved tricky. Setting up reliable Speech to Text was tough, as we struggled to control when voice input should stop. Despite these hurdles, we persevered and successfully developed this full stack application."},{"heading":"Accomplishments that we're proud of","content":"We’re proud to have built a fully finished application while learning and implementing new technologies here at CalHacks. Successfully integrating blockchain and AI into a cohesive solution was a major achievement, especially given how cutting-edge both are. It’s exciting to create something that leverages the potential of these rapidly emerging technologies."},{"heading":"What we learned","content":"We learned how to work with a range of new technologies, including SUI for blockchain transactions, Fetch.ai for agent communication, and SingleStore for real-time data analysis. We also gained experience with Deepgram for voice AI integration."},{"heading":"What's next for FLEX","content":"Next, we plan to implement DAOs for conflict resolution, allowing decentralized governance to handle disputes between freelancers and clients. We also aim to launch on the SUI mainnet and conduct thorough testing to ensure scalability and performance."},{"heading":"Built With","content":"deepgram express.js fetch.ai flask move next.js python singlestore sui terraform"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"GifTr","project_url":"https://devpost.com/software/giftr-uo159g","tagline":"The Decentralized Gift Card Marketplace","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/090/867/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Sui: Reimagine the internet with Sui"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sui","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/TejasRaghuram/CalHacks"}],"description_sections":[{"heading":"Inspiration","content":"As college students who recently graduated high school in the last year or two, we know first-hand the sinking feeling that you experience when you open an envelope after your graduation, and see a gift card to a clothing store you'll never set foot into in your life. Instead, you can't stop thinking about the latest generation of AirPods that you wanted to buy. Well, imagine a platform where you could trade your unwanted gift card for something you would actually use... you would actually be able to get those AirPods, without spending money out of your own pocket. That's where the idea of GifTr began."},{"heading":"What it does","content":"Our website serves as a decentralized gift card trading marketplace . A user who wants to trade their own gift card for a different one can log in and connect to their Sui wallet . Following that, they will be prompted to select their gift card company and cash value. Once they have confirmed that they would like to trade the gift card, they can browse through options of other gift cards \"on the market\", and if they find one they like, send a request to swap. If the other person accepts the request, a trustless swap is initiated without the use of a intermediary escrow, and the swap is completed."},{"heading":"How we built it","content":"In simple terms, the first party locks the card they want to trade, at which point a lock and a key are created for the card. They can request a card held by a second party, and if the second party accepts the offers, both parties swap gift cards and corresponding keys to complete the swap. If a party wants to tamper with their object, they must use their key to do so. The single-use key would then be consumed by the smart contract, and the trade would not be possible.\n\nOur website was built in three stages: the smart contract, the backend, and the frontend.\n\nThe smart contract hosts all the code responsible for automating a trustless swap between the sender and the recipient. It specifies conditions under which the trade will occur, such as the assets being exchanged and their values. It also has escrow functionality , responsible for holding the cards deposited by both parties until swap conditions have been satisfied. Once both parties have undergone verification , the swap will occur if all conditions are met, and if not, the process will terminate.\n\nThe backend* acts as a bridge between the smart contract and the front end, allowing for **communication between the code and the user interface. The main way it does this is by managing all data , which includes all the user accounts, their gift card inventories, and more. Anything that the user does on the website is communicated to the Sui blockchain. This blockchain integration is crucial so that users can initiate trades without having to deal with the complexities of blockchain.\n\nThe frontend is essentially everything the user sees and does, or the UI. It begins with user authentication such as the login process and connection to Sui wallet. It allows the user to manage transactions by initiating trades, entering in attributes of the asset they want to trade, and viewing trade offers. This is all done through React to ensure real-time interaction so that new offers are seen and updated without refreshing the page."},{"heading":"Challenges we ran into","content":"This was our first step into the field of Sui blockchain and web 3 entirely, so we found it to be really informative, but also really challenging. The first step we had to take to address this challenge was to begin learning Move through some basic tutorials and set up a development environment. Another challenge was the many aspects of escrow functionality , which we addressed through embedding many tests within our code. For instance, we had to test that that once an object was created, it would actually lock and unlock, and also that if the second shared party stopped responding or an object was tampered with, the trade would be terminated."},{"heading":"Accomplishments that we're proud of","content":"We're most proud of the look and functionality of our user interface , as user experience is one of our most important focuses. We wanted to create a platform that was clean, easy to use and navigate, which we did by maintaining a sense of consistency throughout our website and keep basic visual hierarchy elements in mind when designing the website. Beyond this, we are also proud of pulling off a project that relies so heavily on Sui blockchain , when we entered this hackathon with absolutely no knowledge about it."},{"heading":"What we learned","content":"Though we've designed a very simple trading project implementing Sui blockchain, we've learnt a lot about the implications of blockchain and the role it can play in daily life and cryptocurrency. The two most important aspects to us are decentralization and user empowerment. On such a simple level, we're able to now understand how a dApp can reduce reliance on third party escrows and automate these processes through a smart contract, increasing transparency and security. Through this, the user also gains more ownership over their own financial activities and decisions. We're interested in further exploring DeFi principles and web 3 in our future as software engineers, and perhaps even implementing it in our own life when we day trade."},{"heading":"What's next for GifTr","content":"Currently, GifTr only facilitates the exchange of gift cards, but we are intent on expanding this to allow users to trade their gift cards for Sui tokens in particular. This would encourage our users to shift from traditional banking systems to a decentralized system, and give them access to programmable money that can be stored more securely, integrated into smart contracts, and used in instant transactions."},{"heading":"Built With","content":"express.js node.js react sui"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"LiveStory","project_url":"https://devpost.com/software/livestory","tagline":"Talk to the characters in your favourite story!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/088/046/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"11labs","url":null},{"name":"deepgram","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"lottie","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"reflex","url":null},{"name":"vapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/pranavbedi/CalHacks2024"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration comes from one of our team member's bedtime story sessions with his little cousin. Whenever he reads him a picture book, his cousin always has questions like, \"Why did Goldilocks walk into the house?\"—and most of the time, we don’t have the answers. We wished there was a way for him to ask the characters directly, to hear their side of the story. That’s where the idea for LiveStory came from. We wanted to bring storybook characters to life and let kids interact with them directly to get answers to their questions in real time."},{"heading":"What it does","content":"LiveStory is a children's storybook that comes alive! At ANY point in the story, interact with a character on the page and chat with them; how are they feeling about what just happened? With industry-leading AI voice-to-voice pipelines, readers can feel the emotion of their favourite characters."},{"heading":"How we built it","content":"Our web application was built primarily using Reflex :\n\nReflex for both frontend and backend React.js for managing the database of each page and character Custom assistants powered by Vapi, Groq, Deepgram, and 11Labs to simulate character interactions Reflex for Deploy"},{"heading":"Challenges we ran into","content":"We initially struggled with Reflex, but over time, it became our go-to tool for building the project. We had to prevent characters from spoiling the story by restricting their responses to what the reader had already seen. To solve this, we fed the accumulative story log into the voice API, ensuring characters only referenced the relevant parts of the story."},{"heading":"Accomplishments that we're proud of","content":"Completing the project on time and getting it fully functional!!! Learning Reflex and Lottie from scratch and successfully implementing them over the weekend. Collaborating with amazing Reflex engineers to create a solid product based on their platform. Committing 20+ hours and $1,000 on travel from Waterloo, Canada, to make this hackathon happen!"},{"heading":"What we learned","content":"Making a full-stack app in Reflex Implementing beautiful vector animations with Lottie Implementing voice-to-voice models in web apps"},{"heading":"What's next for LiveStory","content":"As time is the biggest limitation during a hackathon, we would have loved to pour more time into the art to make a more beautiful experience.\n\nMore stories! More animations! Characters could, based on the emotions of their speech, have reactive animations Character Interaction Interface: a more advanced UI can note your emotions Choose your own adventures! With supported stories, the conversations with the characters could influence the story! Customization for reader! We can also try feeding reader's information such as name, hobby and academic interest to serve better user experience."},{"heading":"Built With","content":"11labs deepgram groq javascript lottie python react reflex vapi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"AngelShot","project_url":"https://devpost.com/software/angelshot","tagline":"Natural conversation, safer situation.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/086/806/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"deepgram","url":null},{"name":"elevenlabs","url":null},{"name":"nextjs","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"vapi","url":null}],"external_links":[{"label":"AngelShot.co","url":"https://AngelShot.co/"},{"label":"github.com","url":"https://github.com/mikieyx/SafeTalk"},{"label":"angelshot-yoydc2b.gamma.site","url":"https://angelshot-yoydc2b.gamma.site/"}],"description_sections":[{"heading":"Inspiration","content":"Whether it's at a college party, a public outing, or during a simple stroll downtown, individuals often find themselves in uncomfortable, unwanted situations due to unwanted attention. In these moments, a discreet way to seek help is crucial.\n\nIn the bartending world, this concept is known as an \" angel shot \"—a code word or drink order that discreetly signals to staff that a customer needs assistance.\n\nPhone calls are a powerful tool in these uncomfortable situations, as they not only deter unwanted attention by creating an external conversation but also offer a lifeline to contact emergency services or trusted individuals. Thus, they server as angel shots outside of a bartending context.\n\nBut what if no one is available to answer the call? How can individuals ensure they'll have someone to talk to when they need help the most?"},{"heading":"What it does","content":"AngelShot simulates a realistic phone call a variety of user-customizable AI-based assistants.\n\nUsers will pre-define assistants that they can request a call from whenever they're placed in an unwanted situation. These assistants can take on roles; for instance, user's can create an assistant meant to be \"an uncle that they haven't seen in awhile\". Additionally, assistants can be given a conversation starter. This could range from topics like sports, gardening, etc... anything that the user will feel comfortable talking about in an uncomfortable situation.\n\nWhen an individual is in an uncomfortable situation, they can request a call from any of their created assistants to start a normal conversation. However, with each response, the assistant provides the user two discreet, context-based code words .\n\nThese code words trigger pre-configured safety actions of two levels. For example, in a gardening-themed conversation, the assistant may provide the words \"monstera\" and \"weeding\".\n\nIf the user says the first keyword \"monstera\" in their response, the assistant will know to share the user’s live conversation with emergency contacts. If the user says the second keyword \"weeding\" in their response, the assistant will know to forward you to emergency services instantly."},{"heading":"How we built it","content":"We deployed a Next.js application on Vercel, written in Typescript and styled using Tailwind + Shadcn and a variety of frontend libraries. For authentication, we used Clerk to allow users to quickly signup using their phone numbers.\n\nAs a means of handling phone communication, we utilized VAPI 's API to efficiently create customizable AI assistants. VAPI streamlined the integration of voice communication in our application, allowing us to simulate realistic phone calls with AI-based assistants.\n\nFor speech-to-text functionality, we used a Deepgram's Nova 2 Phonecall Model, specifically tailored for low-bit phone calls. This ensures accurate transcription, even if users are calling from remote areas or in noisy environments, guaranteeing that conversations and safety triggers are captured correctly.\n\nThen, to simulate natural and context-aware dialogue, we used OpenAI's GPT-4 model. Using VAPI's API, we passed a system prompt to ensure the AI assistant can generate relevant conversations, generate two context-specific code words, and react appropriately via function calls if any of the code words are spoken.\n\nLastly, for text-to-speech conversion, we chose ElevenLabs' models to create high-quality, natural-sounding voices for the AI assistants, enhancing the realism and comfort of the simulated calls."},{"heading":"Challenges we ran into","content":"Our entire team didn't have WiFi for essentially half the event, so we spent the first half of the event ideating. The last half of the event was when our application truly came to life. Another issue that we ran into was correctly prompt engineering the virtual assistant. Once we found the right prompts, it was smooth sailing."},{"heading":"Accomplishments that we're proud of","content":"We're proud of developing a discreet safety tool that could potentially save lives. Integrating customizable AI assistants and creating a reliable emergency response system were key milestones that we were able to accomplish."},{"heading":"What's next for AngelShot","content":"We plan to enhance AngelShot with more customization options, additional safety features such as sharing location and real time stress level analysis. We also aim to improve accessibility, perhaps making the application into a mobile app using technologies like React Native."},{"heading":"Built With","content":"deepgram elevenlabs nextjs react tailwind vapi"},{"heading":"Try it out","content":"AngelShot.co github.com angelshot-yoydc2b.gamma.site"}]},{"project_title":"soundchain","project_url":"https://devpost.com/software/soundchain","tagline":"decentralizing music by empowering artists to create & monetize their music independently, with backers that care about even the smallest ideas.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/305/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Sui: Reimagine the internet with Sui"}],"team_members":[],"built_with":[{"name":"enoki","url":null},{"name":"google-auth","url":null},{"name":"materialui","url":null},{"name":"next.js","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"sui","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/m20arcusk/soundchain"}],"description_sections":[{"heading":"Inspiration","content":"Growing up, many of us dreamed of exploring the world of music. However, we soon discovered that pursuing this passion often requires significant financial investment in production, mixing, and additional instruments. 🎛️ Well-meaning adults frequently warned us about the instability and security concerns associated with a music career. The reality is that music can be challenging, and we want to change that narrative.\n\nThere are barriers to entry at every turn:\n\n87.6% of all artists remain undiscovered. Out of 1.3 million registered artists on Chartmetric over the past year, 710,000 have yet to achieve any career milestones. Only 11% of independent artists can sustain a living through music.\n\nEven signing with a record label doesn’t guarantee success:\n\nLabels typically take 50-90% of the revenue generated by an artist, often with fine print that includes hidden distributor fees and overhead costs. Remarkably, even with a major label, only 1 in 2,149 artists achieves commercial success.\n\nWith soundchain, we aim to empower aspiring artists to pursue their musical dreams independently while connecting them with a supportive community that believes in their potential. This platform is also perfect for hobbyists who have a polished piece sitting in their files, just waiting for the right moment to be released. Whether you’re looking to build a career in music or simply share your passion, we’re here to help you take that next step. 🔊"},{"heading":"What it does","content":"soundchain is a decentralized platform that connects aspiring artists with music enthusiasts, enabling fans to discover and support emerging talent through unbiased music playback and NFT purchases in exchange for rights. Artists and \"angel music investors\" are matched through our music-forward playback system.\n\nIt's actually quite simple:\n\nArtists upload snippets of their creative process (verses, beats, melody lines, anything in between). They set SUI-based crowdfunding goals to produce their song, and issuance terms for partners. Potential supporters browse uploaded projects. They can view goals and decide to support projects based on the provided snippets. They buy-in to the potential of the project. The Sui blockchain records the smart contract and creates an enforceable legal parameter for investors. If the project takes off, initial investors can now resell their rights on secondary web3 markets. 🚀"},{"heading":"How we built it","content":"We utilized the Sui blockchain for its scalability and performance, implementing smart contracts to handle transactions. The application is integrated/connected with google-auth and the Enoki developer portal. The platform was developed with a user-friendly interface mocked on Figma to ensure seamless interactions between artists and listeners. Our front-end is made using Next.js, typescript, and assets from libraries such as material UI."},{"heading":"Challenges we ran into","content":"This project was our team's first time ever working with any web3 products. We went from not understanding what blockchain even was to maybe understanding a little bit of how blockchain works. We faced challenges in the integration of blockchain elements into our front-end application since we weren't familiar with the technology. Additionally, navigating copyright issues and establishing fair revenue-sharing models for artists required careful consideration."},{"heading":"Accomplishments that we're proud of","content":"We think that we came up with a bangin' 🎸 idea that can really change the way funding in the music industry works. We also ensured that this product was realistic and viable for integration in the current state of the music industry."},{"heading":"What we learned","content":"SO MUCH !!!! All new ways of designing, methodologies of ideation, new technologies, and first-time experiences. We also learned the importance of community engagement in building a platform that truly serves its users. Understanding the pain points and publication process from both artists and listeners helped us refine our approach and identify key features that enhance the overall experience."},{"heading":"What's next for soundchain","content":"Mint NFTs that provide a visual badge/proof of support for supporters. Implement tags for different types of projects including Songwriting, Lyrics, Rap, and Beats. Integrate tracking systems to measure expected revenue shares through traditional music streaming applications, allowing the smart contract to take a more active role in revenue sharing."},{"heading":"Built With","content":"enoki google-auth materialui next.js react sui typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Companion","project_url":"https://devpost.com/software/companion-43xfsi","tagline":"Companion is a voice-powered web AI agent that can autonomously execute web tasks based on user requests, made for seniors/people with disabilities who are unfamiliar with technology.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/090/369/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gpt","url":null},{"name":"openai","url":null},{"name":"opencv","url":"https://devpost.com/software/built-with/opencv"},{"name":"pinecone","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"selenium","url":"https://devpost.com/software/built-with/selenium"},{"name":"vapi","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"A study recently done in the UK learned that 69% of people above the age of 65 lack the IT skills needed to use the internet. Our world's largest resource for information, communication, and so much more is shut off to such a large population. We realized that we can leverage artificial intelligence to simplify completing online tasks for senior citizens or people with disabilities. Thus, we decided to build a voice-powered web agent that can execute user requests (such as booking a flight or ordering an iPad)."},{"heading":"What it does","content":"The first part of Companion is a conversation between the user and a voice AI agent in which the agent understands the user's request and asks follow up questions for specific details. After this call, the web agent generates a plan of attack and executes the task by navigating the to the appropriate website and typing in relevant search details/clicking buttons. While the agent is navigating the web, we stream the agent's actions to the user in real time, allowing the user to monitor how it is browsing/using the web. In addition, each user request is stored in a Pinecone database, to the agent has context about similar past user requests/preferences. The user can also see the live state of the web agent navigation on the app."},{"heading":"How we built it","content":"We developed Companion using a combination of modern web technologies and tools to create an accessible and user-friendly experience: For the frontend, we used React, providing a responsive and interactive user interface. We utilized components for input fields, buttons, and real-time feedback to enhance usability as well as integrated VAPI, a voice recognition API, to enable voice commands, making it easier for users with accessibility needs. For the Backend we used Flask to handle API requests and manage the server-side logic. For web automation tasks we leveraged Selenium, allowing the agent to navigate websites and perform actions like filling forms and clicking buttons. We stored user interactions in a Pinecone database to maintain context and improve future interactions by learning user preferences over time, and the user can also view past flows. We hosted the application on a local server during development, with plans for cloud deployment to ensure scalability and accessibility. Thus, Companion can effectively assist users in navigating the web, particularly benefiting seniors and individuals with disabilities."},{"heading":"Challenges we ran into","content":"We ran into difficulties getting the agent to accurately complete each task. Getting it to take the right steps and always execute the task efficiently was a hard but fun problem. It was also challenging to prompt the voice agent such to effectively communicate with the user and understand their request."},{"heading":"Accomplishments that we're proud of","content":"Building a complete, end-to-end agentic flow that is able to navigate the web in real time. We think that this project is socially impactful and can make a difference for those with accessibility needs."},{"heading":"What we learned","content":"The small things that can make or break an AI agent such as the way we display memory, how we ask it to reflect, and what supplemental info we give it (images, annotations, etc.)"},{"heading":"What's next for Companion","content":"Making it work without CSS selectors; training a model to highlight all the places the computer can click because certain buttons can be unreachable for Companion."},{"heading":"Built With","content":"flask gpt openai opencv pinecone python react selenium vapi"}]},{"project_title":"Echocare","project_url":"https://devpost.com/software/echocare-i6csrv","tagline":"3.4 million individuals are food insecure in SF. 150,000 tonnes of food are wasted every year in SF. Echocare leverages A.I. and advanced tools to kill these 2 problems with 1 stone.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/849/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"aceternity-ui","url":null},{"name":"ant-design","url":null},{"name":"axios","url":null},{"name":"cartesia","url":null},{"name":"clerk","url":"https://devpost.com/software/built-with/clerk"},{"name":"framer-motion","url":null},{"name":"google-gemini-flash","url":null},{"name":"google-maps","url":"https://devpost.com/software/built-with/google-maps"},{"name":"google-places","url":"https://devpost.com/software/built-with/google-places"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"gsap","url":null},{"name":"lucide-react","url":null},{"name":"neondb","url":null},{"name":"next.js","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwindcss","url":null},{"name":"three.js","url":"https://devpost.com/software/built-with/three-js"},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"vapi-ai","url":null},{"name":"vercel-postgres","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/ppinkfreudd/echocare"}],"description_sections":[{"heading":"Echocare","content":"TABLE NUMBER 100"},{"heading":"Inspiration","content":"In the U.S. and S.F., homelessness continues to be a persistent issue, affecting more than 650,000 individuals in 2023 alone. Despite the lack of stable housing, many people experiencing homelessness own mobile phones. Research indicates that as many as 94% of homeless individuals have access to a cellphone, with around 70% having smartphones. This is crucial because phones act as a lifeline for communication, health services, and access to support networks. Phones help users connect with essential services like medical care, job opportunities, and safety alerts. Unfortunately, barriers such as maintaining a charge or affording phone plans remain common issues for this population.\n\nIn addition, U.S. faces a huge problem of food wastage. 150,000 tonnes of food are wasted each year JUST in SF.\n\nWith Echocare, we aim to kill these 2 problems with 1 stone. We leverage S.O.T.A Artificial Intelligence to provide essential services to homeless and people suffering from food insecurity in a more accessible and intuitive manner. Additionally, we allow restaurants to donate and keep track of leftover food using an inventory tracker which can be used to feed the homeless people."},{"heading":"What it does","content":"Echocare is an intuitive platform designed to assist users in locating and connecting with nearby care services, such as medical centers, pharmacies, or home care providers. The application uses voice input, interactive maps, and real-time search functionalities to help users find the services they need quickly and seamlessly. With user-friendly navigation and smart recommendations, Echocare empowers people to get help with minimal effort. In addition, it offers a separate platform for restaurants to keep track of leftover food and donate them at the end of each business day to homeless people."},{"heading":"How we built it","content":"Next.js for server-side rendering and static generation. React to build interactive and modular UI components for the front end. TypeScript for type safety and a robust backend. Tailwind CSS for rapid, utility-first styling of the application. Framer Motion for smooth and declarative animations. GSAP (GreenSock Animation Platform) for high-performance animations. (Used for Echo) Three.js for creating 3D graphics in the browser, adding depth and interactivity. (Echo has 3d graphics built in it) Vercel Postgres to communicate with Neon DB (Serverless Postgres) Neon Database a serverless PostgreSQL option, for robust backend storage to store the donated food items of the restaurants. Uses minimal compute and is good for developing low-latency applications Clerk to implement secure and seamless user authentication for managers of the restaurants who want to donate food. Google Maps API to power the mapping functionality and location services (This was embedded with Echo to provide precise directions). Google Places API for autocomplete suggestions and retrieving detailed place information. Ant Design and Aceternity UI for building the forms in the food donation page and having a clean look for the landing page (inspired by the multicolored and vibrant lights of SF in the night) Axios for making API requests to external services easily (Google Maps and Places API) Lucide React for all of the icons used in the application. Vapi.ai For creating the one and only assistant Google Gemini Flash 1.5 to potentially assist with generating user-facing responses. Groq 3.1 70b versatile (fine-tuned) to assist Vapi.ai with insights. Cartesia to provide hyper-realistic service for users. Deepgram for encoding"},{"heading":"Challenges we ran into","content":"We found it very difficult to transcribe the conversations between the user and Vapi.ai echo agent. Rishit had to code for 16 hours straight to get it working. We also found the Google Gemini Integration to be hard because the multimodal functionality wasn't easy to implement. Especially with Typescript which isn't well documented as compared to Python. Finally, stitching the backend and frontend together in the food donation page also took a lot of time to carry out."},{"heading":"Accomplishments that we're proud of","content":"Getting 4 sponsored tools to be seamlessly integrated in the application Using a grand total of 18 tools to build Echocare from the ground up Finishing the entire product 10 hours before the deadline Creating a product which can truly be used to help burdened communities thrive Having a lot of fun and enjoying the process of building Echocare!"},{"heading":"What we learned","content":"Typescript - We have never used it to build a project before and through this hackathon, we gained understanding of how we can use it to build cohesive applications Vapi.AI - Using the dashboard and integrating custom APIs into Vapi was a tricky operation, but we toughed it out and made it work. NeonDB - We used this DB and learned basic SQL queries to insert and get data from the DB which we used to setup the donation page. Google Maps/Places API - Although they were relatively easy to implement, some time had to spent to initialize them. Groq, Cartesia - They were definitely tricky to implement with Vapi's dashboard. Gemini Integration with TS - Although it was < 100 lines of code, we kept running into errors. Turned out that Gemini Pro Vision was deprecated and we had to use Gemini 1.5 Flash instead ;("},{"heading":"What's next for Echocare","content":"We would like to train our VAPI RAG with more datasets from other cities in the United States. Also, we would like to improve the responsive dimensions of our website."},{"heading":"Built With","content":"aceternity-ui ant-design axios cartesia clerk framer-motion google-gemini-flash google-maps google-places groq gsap lucide-react neondb next.js react tailwindcss three.js typescript vapi-ai vercel-postgres"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"EduGap","project_url":"https://devpost.com/software/edugap","tagline":"EduGap enhances the AI models Students can use by automating the integration of class-specific materials in Google Classroom while Teachers gain insight on student problem-areas.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/570/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"The House Fund: \"Cal\" Track"}],"team_members":[],"built_with":[{"name":"api","url":null},{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"chromadb","url":null},{"name":"chrome","url":"https://devpost.com/software/built-with/chrome"},{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"dbscan","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"huggingface","url":null},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"langchain","url":null},{"name":"machine-learning","url":"https://devpost.com/software/built-with/machine-learning"},{"name":"pca","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sklearn","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/sanjanataware/calhacks11.0"}],"description_sections":[{"heading":"Inspiration","content":"Students will use AI for their school work anyway, so why not bridge the gap between students and teachers and make it beneficial for both parties?\n\n1 All of us experienced going through middle school, high school, and now college surrounded by AI-powered tools that were strongly antagonizedantagonized in the classroom by teachers by teachers. As the prevalence of AI and technology increases in today’s world, we believe that classrooms should embrace AI to enhance the classroom which acts very parallel to when calculators were introduced to the classroom. Mathematicians around the world believed that calculators would stop math education all-together, but instead it enhanced student education allowing higher level math such as calculus to be taught earlier. Similarly, we believe that with the proper tools and approach AI can enhance education and teaching for both teachers and students. .\n\n2 In strained public school systems where the student-to-teacher ratio is low, such educational models can make a significant difference in a young student’s educational journey by providing individualized support when a teacher can’t with information specific to their classroom. One of our members who attends a Title 1 high school particularly inspired this project.\n\n3 Teachers are constantly seeking feedback on how their students are performing and where they can improve their instruction. What better way to receive this direct feedback than machine learning analysis of the questions students are asking specifically about their class, assignments, and content?\n\nWe wanted to create a way for AI model education support to be easily and more effectively integrated into classrooms especially for early education, providing a controlled alternative to using already existing chat models as the teacher can ensure accurate information about their class is integrated into the model."},{"heading":"What it does","content":"Students will use AI for their school work anyway, so why not bridge the gap between students and teachers? EduGap, a Chrome Extension for Google Classroom, enhances the AI models students can use by automating the integration of class-specific materials into the model. Teachers benefit from gaining machine learning analytics on what areas students struggle with the most through the questions they ask the model."},{"heading":"How we built it","content":"Front End: Used HTML/CSS to create deploy a 2-page chrome extension 1 page features an AI chatbot that the user can interact with The second page is exclusively for teacher users who can review trends from their most asked prompts\n\nBack End: Built on Javascript and python scripts Created custom api endpoints for retrieving information from the Google Classroom API, Google User Authentication, prompting Gemini via Gemini API, Conducting Prompt Analysis Storage and vector embeddings were created using Chroma DB for the Student Experience\n\nAI/ML LLM: Google Gemini 1.5-flash ChromaDB for vector embeddings and semantic search as it relates to google classroom documents/information Langchain for vector embeddings as it relates to prompts; DBSCAN algorithm to develop clusters for the embeddings via Sklearn using PCA to downsize dimensionality via sklearn General themes of largest cluster are shared with teacher summarized by Gemini"},{"heading":"Challenges we ran into","content":"We spent a significant portion of our time trying to integrate sponsor technologies with our application as resources on the web are sparse and some of the functionalities are buggy. It was a frustrating process but we eventually overcame it by improvising.\n\nWe also spent some time to choose the best clustering method for our project, and hyperparameter tuning in the constrained time period was also highly challenging as we had to create multiple scripts to cater for different types of models to choose the best ones for our use case"},{"heading":"Accomplishments that we're proud of","content":"Creating a fully functioning Chrome Extension linked to Google Classroom while integrating multiple APIs, machine learning, and database usage. Working with a team we formed right at the Hackathon!"},{"heading":"What we learned","content":"We learned how to work together to create a user-friendly application while integrating a complex backend. For most of us, this was our first hackathon so we learned how to learn fast and productively for the techniques, technology, and even languages we were implementing."},{"heading":"What's next for EduGap","content":"1 Functionality for identifying and switching between different classes. 2 Handling separate user profiles from a database perspective 3 A more comprehensive analytic dashboard and classroom content suggestion for teachers + more personalized education support tutoring according to the class content for students. 4 Pilot programs at schools to implement! 5 Chrome Extension Deployment 6 Finalize Google Classroom Integration and increase file compatibility"},{"heading":"Built With","content":"api chroma chromadb chrome css dbscan gemini google-cloud html huggingface javascript json langchain machine-learning pca python sklearn"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Ducky.AI","project_url":"https://devpost.com/software/ducky-ai","tagline":"An LLM Rubber Ducky. Never worry about being unprepared for a presentation ever again.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/087/871/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"The House Fund: \"Cal\" Track"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"deno","url":null},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"nginx","url":"https://devpost.com/software/built-with/nginx"},{"name":"pulumi","url":null},{"name":"rabbitmq","url":"https://devpost.com/software/built-with/rabbitmq"},{"name":"redis","url":"https://devpost.com/software/built-with/redis"},{"name":"vite","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/adit-bala/ducky.ai"}],"description_sections":[{"heading":"Inspiration","content":"Sometimes it's hard to find the right person to practice your presentation with. Maybe you're presenting to a grad class about distributed systems, or you're presenting in your anthropology class about a presentation you made at 4am and you have no idea of what you're saying, or maybe you're a startup presenting to an AI accelerator.\n\nIn all of these situations -- it's quite hard to find someone who's an accurate representation of your audience member, and sometimes the feedback that people can give us doesn't really match what we need for preparation. After all, how can your dog ask you if Paxos is deployed in an asynchronous, partially synchronous, or synchronous model"},{"heading":"What it does","content":"Ducky.AI is a platform for you to upload, present, and persist your presentations. It's very simple. 1) Upload a presentation 2) Upload configurations like what you're presenting about, who you're presenting to, and what kind of tone you want the presentation to be -- is it casual? or is it as serious as talking to a C-level executive"},{"heading":"How we built it","content":"Front-End: Vite, Nginx Back-End: Deno, AWS S3/Lambda, RabbitMQ Databases: Redis, MongoDB Infra, Pulumi (IaC), Docker"},{"heading":"Challenges we ran into","content":"Integration + those pesky bugs that you can only test in a full run through of the pipeline"},{"heading":"Accomplishments that we're proud of","content":"A completely working prototype."},{"heading":"What we learned","content":"too much to write here"},{"heading":"What's next for Ducky.AI","content":"series A, real-time AI questions, posture and expression analysis"},{"heading":"Built With","content":"amazon-web-services deno docker mongodb nginx pulumi rabbitmq redis vite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Letmetalktoahuman AI","project_url":"https://devpost.com/software/letmetalktohuman-ai","tagline":"Our project's voice agent interacts with redundant customer service bots, quickly navigating through automated systems to connect users with real human agents through call forwarding.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/159/389/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"react-router-dom","url":null},{"name":"vapi","url":null}],"external_links":[{"label":"letmeetalktoahuman.netlify.app","url":"https://letmeetalktoahuman.netlify.app/"},{"label":"github.com","url":"https://github.com/ConnectAI-CalHacks/ConnectAI"}],"description_sections":[{"heading":"Inspiration","content":"We often encountered challenges navigating automated call systems, which left us spending excessive time on hold and feeling frustrated. These experiences made us realize how much valuable time was being wasted when we could have been focusing on more productive tasks and we were left angry and wondering if there was a better way to optimize telecommunication systems. This frustration inspired us to develop a solution that streamlines the process, minimizing wait times and improving the overall customer experience."},{"heading":"What it does","content":"The system includes a form where clients can enter their name, phone number, and a brief description of their issue, such as requesting a refund or returning an item. Once submitted, the VAPI system automatically places a call to the provided number. A virtual assistant then guides the client through a series of questions to better understand their problem with the VAPI system answering the questions based off the description given. The VAPI setup even handles the wait time on the client's behalf, ensuring they’re connected directly to the appropriate support agent without unnecessary delays."},{"heading":"How we built it","content":"We implemented the solution using React.js for the front-end interface and VAPI for handling the automated calls. The form submission triggers the VAPI system, which initiates and manages the call flow. For version control and collaboration, we hosted the project on a GitHub repository, utilizing GitHub Actions for continuous integration and automated testing to ensure a smooth deployment process. We used llama within Groq as the LLM as we saw significant difference in response time when using groq vs openai. This setup allowed us to efficiently manage code updates and track changes while leveraging VAPI’s capabilities to handle real-time interactions with clients."},{"heading":"Challenges we ran into","content":"We encountered challenges managing different branches, as the primary branch frequently stalled during the process."},{"heading":"Accomplishments that we're proud of","content":"We were able to integrate the front end with the VAPI connector after clicking the submit button which took time, but we were persistent in solving the problem."},{"heading":"What we learned","content":"We explored various functionalities within the React ecosystem, gaining a deeper understanding of tools and techniques available to enhance our applications. For instance, we learned about the @media query, which allows us to create responsive designs by applying different styles based on screen size and device characteristics. Additionally, we became proficient in utilizing VAPI to manage automated calls, including how to implement its features for efficient interaction with clients. This knowledge has equipped us to build more dynamic and user-friendly applications."},{"heading":"What's next for Letmetalktohuman AI","content":"We aim to implement a feature that recognizes and performs specific dial tones, as these are a common part of phone interactions. This feature will enhance the user experience by allowing the system to respond to different inputs appropriately."},{"heading":"Built With","content":"html node.js react react-router-dom vapi"},{"heading":"Try it out","content":"letmeetalktoahuman.netlify.app github.com"}]},{"project_title":"LanguaLine","project_url":"https://devpost.com/software/langualine","tagline":"Are you anxious about speaking a foreign language or don't know where to start? Practice your conversational language skills and get real-time feedback with LanguaLine!","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/651/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Vapi: Show us your Voice AI"}],"team_members":[],"built_with":[{"name":"deepgram","url":null},{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"openai","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null},{"name":"vapi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/SrimanCode/CalHacks-11.0"}],"description_sections":[{"heading":"Inspiration","content":"With a variety of language learning resources out there, we set out to create a tool that can help us practice a part of language that keeps things flowing -- conversation! LanguaLine aims to empower users to speak their foreign language by helping them develop their conversational skills."},{"heading":"What it does","content":"We wanted to create an interface that can help users practice speaking a foreign language. Through LanguaLine, users can:\n\nSelect a language they wish to practice speaking in Select how \"motivating\" they want their Mentor to be (Basic is normal, Extra Motivation is a tough love approach). Enter their phone number and receive a phone call from our AI Mentor Answer questions posed by the Mentor Receive real-time feedback about their performance View a transcript and summary of the call after the conversation is completed View a generalized report on user's language strengths and weaknesses across all conversations"},{"heading":"How we built it","content":"We used React.js for our frontend, and Firebase for our database. To style our components, we utilized TailwindCSS and React MaterialUI .\n\nOur backend system is comprised of Node.js and Express.js , which we use to make calls to Google's Gemini model.\n\nTo create, tune, and prompt engineer our AI Mentor, we used the VAPI.ai API. Our transcriber model is Deepgram's nova-2 multi and our model is gpt-4o-mini provided by Open.AI .\n\nWe are also using Gemini to implement Retrieval-Augmented Generation (RAG). We use past call transcripts and summaries to train and optimize our model. This training data is maintained in our Firebase database. In addition, this feature analyzes users' call transcripts and generates a report identifying strengths and weaknesses in their speaking skills."},{"heading":"Challenges we ran into","content":"Our biggest challenge was understanding the VAPI documentation, as it was our first time working with a voice AI API. We had to make a few changes to our project stack to accommodate for VAPI, as we could only make client-side API calls.\n\nSince the majority of our team has limited experience working with LLMs and voice AI Agents, we faced some difficulties prompt engineering our Mentor, requiring us to tweak various model parameters and experiment through VAPI's dashboard."},{"heading":"Accomplishments that we're proud of","content":"The turning point in our development process was when we were able to start conversing with our Mentor. After this was solidified, our project trajectory only went upwards. We're proud of the fact we were able to turn this idea into an operational and functional application."},{"heading":"What we learned","content":"The team behind LanguaLine had a variety of skill levels; for some, this was their first project using this tech stack, while for others, this was familiar. Some of us mastered the ability to send API calls and parse JSON data. Some of us also learned how to prompt engineer for a particular language choice. There were lessons being learned all throughout the 36 hours of development, which helped us feel connected to the project and motivated to keep creating."},{"heading":"What's next for LanguaLine","content":"Our biggest goal is to deploy and market this project. Being language learners ourselves, having a service like LanguaLine is invaluable to making progress toward achieving fluency. In addition, this increases accessibility for language learners by encompassing a wide range of supported languages and providing customizable support.\n\nOur project aims to support all languages. Due to our lack of control regarding the accuracy across various languages within LLMs, this feature needs more testing and tuning to be perfected.\n\nWe plan to offer more variation in the Mentors we offer. Right now, we only offer Mentors based off of a language choice and motivation level. In the future, we plan to include language difficulties, personalities, a wider variety of supported languages, custom prompts, and scheduled calling.\n\nWe also plan to offer improvement plans for grammar, pronunciation, and vocabulary, as well as a scoring system for users' performances during Mentor sessions."},{"heading":"Built With","content":"deepgram express.js firebase gemini javascript node.js openai react tailwind vapi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Pawn Up","project_url":"https://devpost.com/software/gambit-y0j7mu","tagline":"Pawn Up: Your Personal AI Chess Coach—Analyze, Train, and Master Chess on Your Own Time.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/090/701/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Chroma: Build your project with Chroma in an AI application"}],"team_members":[],"built_with":[{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"css3","url":"https://devpost.com/software/built-with/css3"},{"name":"docker","url":"https://devpost.com/software/built-with/docker"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"git","url":"https://devpost.com/software/built-with/git"},{"name":"github","url":"https://devpost.com/software/built-with/github"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"html5","url":"https://devpost.com/software/built-with/html5"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"llama","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"oauth","url":"https://devpost.com/software/built-with/oauth"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rapidapi","url":"https://devpost.com/software/built-with/rapidapi"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwindcss","url":null},{"name":"vite","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/satwikug25/calhacks"}],"description_sections":[{"heading":"About the Project","content":"We are a bunch of amateur players who love playing chess, but over time we noticed that our improvement has become stagnant. Like many college students, we neither have the time nor the financial means to invest in professional coaching to take our game to the next level. This frustration sparked the idea behind Pawn Up —a project built to help players like us break through the plateau and improve their chess skills in their own time, without expensive coaches or overwhelming resources.\n\nWhat Inspired Us\n\nAs passionate chess players, we struggled with finding affordable and effective ways to improve. Chess can be an expensive hobby if you want to seek professional help or guidance. The available tools often lacked the depth we needed or came with hefty price tags. We wanted something that would provide personalized feedback, targeted training, and insights that anyone could access—regardless of their financial situation.\n\nHow We Built It\n\nWe started by integrating Lichess authentication to fetch a user's game history, allowing them to directly analyze their own performance. With Groq and Llama3.1 , we leveraged AI to categorize mistakes, generate feedback, and suggest relevant puzzles to help users train and improve. We also levergae ChromaDB for vector search features and Gemini pro and Gemini embedding\n\nOur project features four key components:\n\nAnalyze : Fetches the user's last 10 games, provides analysis on each move, and visualizes a heatmap showing the performance of legal moves for each piece. Users can also interact with the game for deeper analysis. Train : Using AI, the system analyzes the user's past games and suggests categorized puzzles that target areas of improvement. Search : We created a vector database storing thousands of grandmaster games. Users can search for specific games and replay them with detailed analysis, just like with their own games. Upload : Users can upload their own chess games and perform the same analyses and training as with the Search feature.\n\nWhat We Learned\n\nThroughout the development of Pawn Up , we gained a deeper understanding of AI-powered analysis and how to work with complex game datasets. We learned how to integrate chess engines, handle large amounts of data, and create user-friendly interfaces. Additionally, we explored how LLMs (large language models) can provide meaningful feedback and how vector databases can be used to store and retrieve massive datasets efficiently.\n\nChallenges We Faced\n\nOne of the main challenges we encountered was making the AI feedback meaningful for players across various skill levels. It was crucial that the system didn’t just provide generic advice but rather tailored suggestions that were both practical and actionable. Handling large amounts of chess data efficiently, without compromising on speed and usability, also posed a challenge. Building the vector database to store and search through grandmaster games was a particularly challenging but rewarding experience.\n\nDespite these hurdles, we’re proud of what we’ve built. Pawn Up is the solution we wish we had when we first started hitting that plateau in our chess journeys, and we hope it can help others as well."},{"heading":"Built With","content":"chroma css3 docker flask gemini git github groq html5 javascript llama node.js oauth python rapidapi react tailwindcss vite"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TrashToTreasure - Sustainable Showcase","project_url":"https://devpost.com/software/trashtotreasure-sustainable-showcase","tagline":"Improvements in Pepsi's Global Value chain end with consumers but so does waste. Imagine if customers were inspired by AI to make beautiful projects w/ trash, promoting brand value and sustainability.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/700/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"SCET/Pepsico: PEP+ Sustainability Prize"}],"team_members":[],"built_with":[{"name":"express.js","url":"https://devpost.com/software/built-with/express-js"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"nextjs","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"tailwind","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/garvcodes/sustainableshowcase"}],"description_sections":[{"heading":"Built With","content":"express.js mongodb nextjs node.js react tailwind"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Ground Truth","project_url":"https://devpost.com/software/ground-truth-97nqws","tagline":"Remove the need to manually update your docs. When you make a PR, Doc's are auto updated based on the changes you committed.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/093/992/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Chroma: Build your project with Chroma in an AI application"}],"team_members":[],"built_with":[{"name":"chromadb","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"rag","url":null},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Hassanmushtaq524/groundtruth"}],"description_sections":[{"heading":"Inspiration","content":"When I was interviewing at a ~20 person startup and reading through their introduction doc, I read that the co-founders would often spend lots of nights just catching up on docs from the prior week. To me, the last thing any employee, much less a founder, should be spending their time on is writing docs. Ground Truth lets you simply review and accept changes to your docs based on what you have been coding."},{"heading":"What it does","content":"Each time you make a commit to a specific repo, a new entry is made"},{"heading":"How we built it","content":"Chroma\n\nDocs can be pretty large. Far too large to pass multiple pages into a context window. To account for this, we embedded each page of the docs into a chromaDB using their built in functions, and we query the Vector DB we created to find the most similar doc so we can pass that as context.\n\nGroq\n\nWe used Groq for basically all of our small context text completion. This was especially helpful when passing a code diff into the llama model and getting a description of what was being updated. This allowed us to consistently retrieve the right documentation that we would then update\n\nReflex.dev\n\nSince most of our initialization and ChromaDB stuff was already being done in Python, it made sense to continue using it and go ahead with Reflex for the full frontend and backend."},{"heading":"Challenges we ran into","content":"The biggest issue for us was finding good contenders for projects. Firstly, they had to be open source, non negotiable for us to test with. Secondly, they had to have a developer program or some need for docs that we could reasonably update."},{"heading":"Accomplishments that we're proud of","content":"This was all of our first times working with any of these technologies (RAG anything, really) and we're proud to have put it all together in a somewhat attractive way over the 36 hours. The approach we took probably was a good 10 hours of ideation so it's nice to have it built and working"},{"heading":"What we learned","content":"Pretty much everything about RAG and Reflex. We all knew very little about the domain coming into this."},{"heading":"What's next for Ground Truth","content":"This project has real potential as a company but is an incredibly hard engineering problem. If we stay excited about building it, we could make it widespread and very modular."},{"heading":"Built With","content":"chromadb groq python rag reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"AI Traffic Control (AITC)","project_url":"https://devpost.com/software/ai-traffic-control-aitc","tagline":"AiTC is transforming the aviation industry by automating ATC radio calls and providing analytics for faster, more accurate responses, allowing air traffic controllers to focus on critical situations.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/408/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"The House Fund: \"Cal\" Track"}],"team_members":[],"built_with":[{"name":"ai","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null},{"name":"vapi","url":null}],"external_links":[{"label":"drive.google.com","url":"https://drive.google.com/file/d/1VElIhy33DyWRrJnw8xPdQApaap4K_d_e/view?usp=sharing"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration for AiTC came from our use of various forms of transportation and our close connections with friends in the aviation industry. Seeing firsthand the complexities and challenges that air traffic controllers face, we wanted to create a solution that could alleviate some of the burdens on agents in control towers. AiTC is designed to streamline communication, reduce errors, and enhance the efficiency of air traffic management."},{"heading":"What it does","content":"AiTC is an AI-driven platform designed to assist air traffic controllers by automating routine radio communication with pilots and providing real-time flight data insights. It leverages advanced speech recognition and natural language processing to analyze ATC-pilot communications, flag potential issues, and ensure that critical information is delivered accurately and on time. The system works in tandem with controllers, acting as a digital assistant to help manage complex airspace efficiently. The long-term goal is to fully automate the ATC communications."},{"heading":"How we built it","content":"We built AiTC using several powerful tools and technologies. We used Vapi to train the models with out datasets and for real-time flight data integration, providing up-to-the-minute information about flights. We used Deepgram for speech-to-text capabilities, converting real-time ATC communications into actionable data. We used OpenAI to to interpret and assist with communication, as well as to improve decision-making processes within the control tower. We used hugging face datasets of ATC call transcripts and guides to train the AI models ensuring accurate communication processing."},{"heading":"Accomplishments that we're proud of","content":"We’re super proud of developing a working prototype that integrates real-time flight data with AI-driven communication tools. The ability of AiTC to accurately process and respond to ATC communications is a major milestone, as is its potential to enhance safety and efficiency in one of the most critical sectors of transportation. We’re also proud of how we were able to incorporate machine learning models into a real-time system without sacrificing performance."},{"heading":"What we learned","content":"Through this project, we learned the importance of handling real-time data effectively. We also gained valuable experience in the integration of various APIs and the unique challenges of real-time communication systems."},{"heading":"What's next for AI Traffic Control (AITC)","content":"The next step for AiTC is to improve its scalability and robustness. We plan to expand its ability to handle more complex airspaces, integrate additional datasets for more nuanced decision-making, and further reduce latency in communication. The long-term goal is to fully automate this communication system. We also aim to pilot the system with actual air traffic control teams to gather real-world feedback and refine the tool for broader adoption."},{"heading":"Built With","content":"ai python reflex vapi"},{"heading":"Try it out","content":"drive.google.com"}]},{"project_title":"Lucy","project_url":"https://devpost.com/software/lucy-x0wd86","tagline":"Military personnel will utilize Lucy to identify potential threats during field operations.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/252/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Chroma: Build your project with Chroma in an AI application"}],"team_members":[],"built_with":[{"name":"arduino","url":"https://devpost.com/software/built-with/arduino"},{"name":"chromadb","url":null},{"name":"deepgram","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"lmnt","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"spotrobot","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Jinash-Rouniyar/SpotDog"}],"description_sections":[{"heading":"Inspiration","content":"We are tinkerers and builders who love getting our hands on new technologies. When we discovered that the Spot Robot Dog from Boston Dynamics was available to build our project upon, we devised different ideas about the real-world benefits of robot dogs. From a conversational companion to a navigational assistant, we bounced off different ideas and ultimately decided to use the Spot robot to detect explosives in the surrounding environment as we realized the immense amount of time and resources that are put into training real dogs to perform these dangerous yet important tasks."},{"heading":"What it does","content":"Lucy uses the capabilities of Spot Robot Dog to help identify potentially threatening elements in a surrounding through computer vision and advanced wave sensing capabilities. A user can command the dog to inspect a certain geographic area and the dog autonomously walks around the entire area and flags objects that could be a potential threat. It captures both raw and thermal images of the given object in multiple frames, which are then stored on a vector database and can be searched through semantic search.\n\nThis project is a simplified approach inspired by the research \"Atomic Magnetometer Multisensor Array for rf Interference Mitigation and Unshielded Detection of Nuclear Quadrupole Resonance\" ( https://link.aps.org/accepted/10.1103/PhysRevApplied.6.064014 )."},{"heading":"How we built it","content":"We've combined the capabilities of OpenCV with a thermal sensing camera to allow Spot Robot to identify and flag potentially threatening elements in a given surrounding. To simulate these elements in the surroundings, we built a simple Arduino application that emits light waves in irregular patterns. The robot dog operates independently through speech instructions, which are powered by DeepGram's Speech to Text and Llama-3-8b model hosted on the Groq platform. Furthermore, we've leveraged ChromDB's vector database to tokenize images that allow people to easily search through images, which are captured in the range of 20-40fps."},{"heading":"Challenges we ran into","content":"The biggest challenge we encountered was executing and testing our code on Spot due to the unreliable internet connection. We also faced configuration issues, as some parts of modules were not supported and used an older version, leading to multiple errors during testing. Additionally, the limited space made it difficult to effectively run and test the code."},{"heading":"Accomplishments that we're proud of","content":"We are proud that we took on the challenge of working with something that we had never worked with before and even after many hiccups and obstacles we were able to convert our idea in our brains into a physical reality."},{"heading":"What we learned","content":"We learned how to integrate and deploy our program onto Spot. We also learned that to work around the limitations of the technology and our experience working with them."},{"heading":"What's next for Lucy","content":"We want to integrate LiDar in our approach, providing more accurate results then cameras. We plan to experiment beyond light to include different wave forms, thus helping improve the reliability of the results."},{"heading":"Built With","content":"arduino chromadb deepgram groq lmnt python spotrobot"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Peps Towards Sustainability","project_url":"https://devpost.com/software/peps-towards-sustainability","tagline":"PepsiCo has many sustainability practices, but there’s no centralized metric to assess progress across goals. 'Peps Towards Sustainability' provides a way to measure the journey to a green future.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/036/datas/medium.jpg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"SCET/Pepsico: PEP+ Sustainability Prize"}],"team_members":[],"built_with":[{"name":"css","url":"https://devpost.com/software/built-with/css"},{"name":"html","url":"https://devpost.com/software/built-with/html"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"}],"external_links":[{"label":"github.com","url":"https://github.com/bmcal2005/CalHacksPepsico.io"}],"description_sections":[{"heading":"Built With","content":"css html javascript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Grocify","project_url":"https://devpost.com/software/grocify","tagline":"Track your pantry, ensure dietary restrictions, craft personalized recipes—ensuring zero waste, smart shopping, and efficient cooking.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/088/303/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"[MLH] Best Domain Name from GoDaddy Registry"}],"team_members":[],"built_with":[{"name":"barcodelookup","url":null},{"name":"digitalocean","url":"https://devpost.com/software/built-with/digitalocean"},{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"mongodb","url":"https://devpost.com/software/built-with/mongodb"},{"name":"nginx","url":"https://devpost.com/software/built-with/nginx"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"react","url":"https://devpost.com/software/built-with/react"}],"external_links":[{"label":"grocify.co","url":"https://grocify.co/"},{"label":"github.com","url":"https://github.com/Sachin-dot-py/Grocify"}],"description_sections":[{"heading":"Inspiration","content":"The idea for Grocify came from a small but frustrating moment in our busy college lives. We bought a perfect avocado, full of possibilities. But in the loop of classes and assignments, it sat there, forgotten. By the time we remembered, it was too late—a sad, mushy reminder of how hard it can be to keep track of fresh groceries and how much food we waste.\n\nThat moment made us wonder—how many others face the same problem? Wasting food isn’t just annoying; it’s wasteful and bad for the environment. We wanted a way to avoid these moments, save money, and make our busy lives a bit more organized. That’s how Grocify was born: a simple, intuitive tool to help manage groceries, track expiry dates, and reduce food waste, all while making it easier to use what we buy."},{"heading":"What It Does","content":"Grocify is like having a personal grocery assistant in your pocket, making food management easy. From keeping track of what’s in your pantry to ensuring nothing goes to waste, Grocify helps you stay on top of your groceries without the hassle.\n\nUsing barcode and image scanning, adding items is quick and painless—just scan the barcode or click the photo. Once items are in the inventory, Grocify takes care of tracking expiry dates and helping you avoid the mess.\n\nGrocify can suggest recipes based on what you have, prioritizing ingredients nearing expiration. It even considers your dietary preferences, so the recipes are not just convenient but fit your needs. And if you get stuck while cooking, our helpful assistant is just a tap away to guide you through the steps."},{"heading":"How We Built It","content":"Building Grocify was an iterative process that began with understanding how to simplify grocery management for users. We started by prototyping a user journey that made adding items effortless—leading us to integrate barcode scanning using an API and advanced image recognition powered by Llama. Once core features were established, we added an inventory page to track product statuses in real time. Next, we introduced a Recipes page that curates personalized recipes tailored to the ingredients users already have, utilizing the mixtral and gemma models through Groq to deliver optimal suggestions.\n\nTo enhance the experience, we integrated a chatbot that offers step-by-step cooking guidance, understanding that many college students may need extra support in the kitchen. We also included options for specifying dietary preferences and special requests, allowing the app to generate customized recipes that fit users' needs. Overall, Grocify aims to take the hassle out of grocery management and make cooking approachable and enjoyable."},{"heading":"Challenges We Ran Into","content":"As with any ambitious project, building Grocify came with its fair share of challenges—some of which were quite unexpected! One of the first major hurdles was getting the barcode scanner to correctly identify individual products, especially when they were originally part of a larger package. We had a particularly amusing incident where we scanned a single granola bar, and it read as a 24-pack—likely because it was originally part of a larger box.\n\nAnother challenge was integrating the expiry date feature. We thought it would be simple enough to estimate how long items would stay fresh—until we realized our initial model displayed expiry dates for ripe bananas as being three weeks later. After plenty of trial and error, we fine-tuned the system to provide realistic expiry estimates, even for tricky produce.\n\nThe chatbot also posed an interesting problem. Initially, it answered simple cooking questions, but we quickly found users asking wild inquiries—like substituting pancake mix for flour in every recipe. The chatbot ended up giving overly confident (and wrong) advice, so we had to add extra checks to prevent it from turning into a rogue chef.\n\nThrough all these challenges, we learned to stay adaptable. Every hurdle helped us refine Grocify, ensuring users would have an intuitive and reliable experience."},{"heading":"Accomplishments That We're Proud Of","content":"We are especially proud of how Grocify makes sustainable living effortless. It’s not just about managing groceries—it’s about reducing food waste, saving money, and making it all incredibly easy. The convenience is unmatched: you can check what’s in your pantry on your way home, plan a recipe, and know exactly what to cook before stepping through the door.\n\nOne of our biggest wins is making sustainability accessible. As a vegetarian, I love how easy it is to see if a product fits my dietary restrictions just by scanning it. Grocify takes the guesswork out of managing groceries, ensuring everyone can make choices that are good for them and for the planet. Although working with personalized dietary restrictions added complexity, there were moments where we were surprised by how aware the system was about common allergens in products. For example, we learned through the app's warning that Cheetos contain milk and aren't vegan—a detail we would never have guessed. Seeing the system catch these details made us incredibly proud.\n\nAnother highlight was realizing our image recognition feature could distinguish even the most visually similar produce items. During testing, we saw the app correctly identify subtle differences between items like mandarins and oranges, which felt like a major leap forward. We're also proud of how Grocify estimates expiry dates. Though we initially had some hiccups, refining that system into something accurate and reliable took a lot of work, and the payoff was worth it."},{"heading":"What We Learned","content":"Building Grocify taught us the importance of empathy and thoughtful design. Small features like barcode scanning and personalized recipes can make managing groceries easier and reduce waste. The vision of integrating with stores for automatic syncing showed us how to reach a broader audience, making things effortless for users. Sustainability doesn't have to be complicated with the right tools, and these insights will guide us as we continue improving Grocify.\n\nWe also learned how to effectively leverage generative AI and LLM (Large Language Model) APIs. Integrating these technologies allowed us to build features far more sophisticated than we imagined. The chatbot, powered by an LLM, evolved into an intelligent assistant capable of answering complex cooking questions, generating custom recipes, and offering helpful substitutions. Working with generative AI taught us about prompt engineering—how to structure inputs to get useful outputs, and how to refine responses for user-friendliness."},{"heading":"What's Next for Grocify","content":"Moving forward, we want Grocify to empower users even more when it comes to choosing what they bring into their homes. By expanding our barcode scanning feature, we aim to give users the ability to quickly evaluate products based on their ingredient lists, helping them avoid items with excessive chemicals or preservatives and make healthier choices.\n\nWe also plan to enhance our recipe generator by incorporating even more personalization, so users can discover meals that align perfectly with their health goals and preferences. Our mission is to make grocery management smarter and healthier for everyone—while keeping it simple, intuitive, and focused on sustainability.\n\nAdditionally, we want to integrate Grocify with popular stores like Trader Joe's, allowing users to sync purchases directly with the app. This means items bought in-store can be automatically added to their inventory, making the process seamless. By leveraging our barcode scanning feature, users will also be able to access detailed nutritional facts, enabling more informed dietary decisions."},{"heading":"Built With","content":"barcodelookup digitalocean flask groq javascript mongodb nginx python react"},{"heading":"Try it out","content":"grocify.co github.com"}]},{"project_title":"BuildBlocks x XRPL","project_url":"https://devpost.com/software/buildblocks-x-xrpl","tagline":"Create and deploy audited smart contracts on the XRPL EVM side chain using easy to understand smart contract blocks.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/087/526/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Ripple: Best App by Ripple"}],"team_members":[],"built_with":[{"name":"evm","url":null},{"name":"nextjs","url":null},{"name":"rainbowkit","url":null},{"name":"ripple","url":"https://devpost.com/software/built-with/ripple"},{"name":"sidechain","url":null},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"},{"name":"wagmi","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/sanjayamirthraj/blockbuild/tree/sanjay-xrpl"}],"description_sections":[{"heading":"Inspiration","content":"There is a huge barrier of entry when it comes to entering the web3 space. A minor mistake in your code can cost you thousands of dollars! Firms and companies spend hundreds of thousands of dollars auditing their smart contracts to ensure that there are no exploits in their code, but with BuildBlocks, we have created drag and drop dynamic smart contract components and a platform that allows anyone to develop, compile, and deploy audited smart contracts on the XRPL EVM side chain! Along the way, we found ways to integrate classic EVM hooks and injectable wallets onto the chain."},{"heading":"What it does","content":"Using a Retrieval pipeline on audited DeFi contracts from resources such as Uniswap, OpenZeppelin, etc., BuildBlocks allows for users to use easy-to-understand blocks to create extremely intensive smart contracts in the blink of an eye."},{"heading":"How we built it","content":"We built BuildingBlocks using the standard NextJS, Tailwind stack, but on top of that, we used injectable wallets, the XRPL EVM side chain RPC provider for wallet injection, the SolC compiler for automated Solidity contract compilation, and SIDAI's RAG pipeline generation service which we created buckets of audited Solidity smart contracts."},{"heading":"Challenges we ran into","content":"The XRPL chain is not fully EVM compatible, so our first challenge was deciding which chain of XRPL to build on. After deciding on using the EVM-compatible chain due to its potential for widespread adoption. Secondly, as the EVM side-chain is very new, there were no integrations with popular providers such as MetaMask or Wagmi. Because of this, we had to work around this by manually injecting the XRPL RPC and chain ID into both of these providers. Additionally, working with developing RAGs was a new learning experience for the entire team!"},{"heading":"Accomplishments that we're proud of","content":"Created high quality smart contracts with drag and drop composables! Full end-to-end suite of developing, compiling, and deploying a smart contract!"},{"heading":"What we learned","content":"The life cycle of a smart contract! Our team learned a lot about full stack and on-chain development. Working with a new chain is always cool, especially a Ripple side chain!"},{"heading":"What's next for BuildBlocks x XRPL","content":"Cross chain compatability and developing smart contracts for cross-chain compatability!"},{"heading":"Built With","content":"evm nextjs rainbowkit ripple sidechain tailwind typescript wagmi"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Memento","project_url":"https://devpost.com/software/memento-1p0jel","tagline":"Memento helps elderly people connect with their families by reflecting on and sharing cherished memories, sparking meaningful conversations that strengthen bonds across generations.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/824/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Ripple: Best App by Ripple"},{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Reflex: Best app built using reflex.dev"}],"team_members":[],"built_with":[{"name":"chroma","url":"https://devpost.com/software/built-with/chroma"},{"name":"deepgram","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"reflex","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/AlexLuu1/Memento"}],"description_sections":[{"heading":"Inspiration","content":"Our inspiration came from the importance of connecting with family and cherishing proud personal stories. We recognized that many elderly people in nursing homes feel isolated from their families, despite the wealth of memories they carry. These memories hold so much value about family history, wisdom, and identity. By creating a platform that enables them to reflect on and share these moments, we aimed to bridge generational gaps and strengthen family bonds. Through storytelling, we want to foster a tight family bond, ensuring that cherished memories are passed down and that the elderly feel heard, valued, and connected.\n\nWe wanted to emphasize the story aspect of these memories. When people want to share their memories with their family, especially virtually, they aren't able to fully relive or cherish that memory- a simple text message can't fully do justice to a fond memory. Thus, we wanted to bring life into these memories that shared within families online and especially provide elderly people who might not meet their families often to have an immersive experience with their family's memories."},{"heading":"What it does","content":"Memento allows families to document fond memories that they have, and share them to the user. Families can upload memories that contain a date, description, and image. We target this product to the elderly in nursing homes who are usually alone and can benefit from having someone like family to talk to. The elderly user can then speak to the application and can have a conversation about the details of any memory. The application will also display the most relevant image to the conversation to help improve the experience. This enables the elderly user to feel like they are talking to a family member or someone they know well. It allows them to stay connected with their loved ones without the continuous presence of them."},{"heading":"How we built it","content":"We designed Memento to be simple and accessible for both elderly users and their families. For this reason, we used Reflex to implement an elegant UI, and implemented a Chroma database to store the memories and their embeddings for search. We also integrated Whisper, a speech-to-text model through Groq’s fast inference API to decode what the elderly person is saying. Using this input, we query our database, and feed this information through Gemini , an LLM developed by Google, to give a coherent response that incorporates information from the families’ inputs. Finally, we used Deepgram’s text-to-speech model to convert the LLM’s outputs back to an audio format that we could speak back to the elderly user."},{"heading":"Challenges we ran into","content":"Integration : It was difficult to integrate all of the sponsor’s softwares into the final application; we had to pore over documentation while becoming familiar with each API, which led to many hours of debugging. Non-determinism : Our models were non-deterministic; errors caused by specific outputs from the LLM were hard to replicate. Due to the background noise, we also could not efficiently test our speech-to-text model’s accuracy. Inference speed : Throughout this application, we make many API calls to large models, such as Whisper, Gemini, and the Aura TTS model. Because of this, we had to find clever optimizations to speed up the inference time to quickly speak back to the elderly user, especially since the WiFi was unusable most of the time."},{"heading":"Accomplishments that we're proud of","content":"Design and User Experience : We are proud of our design since it encompasses the mood we were aiming for – a warm, welcoming environment, focusing on the good things that happen in life. Large Language Model and Vector Search : We are especially proud of how the LLM turned out and how well the RAG model worked. We spent lots of time prompting the different components to create the warm, empathetic, and welcoming environment the LLM provides. TTS and STT : Although we struggled a bit with this part, we are really proud about how it turned out. We feel we did a great job encompassing the ideals of the product by allowing users to reflect on past memories and connect closer with family."},{"heading":"What we learned","content":"Working with STT and TTS models: many members of our group had never worked with speech-to-text or text-to-speech models, so this was a learning experience for all of us. We learned about the impressive accuracy that the state-of-the-art models are able to achieve but also encountered some of the drawbacks of these models, since many of them don’t work as well with moderate levels of background noise. How to make a great UI:"},{"heading":"What's next for Memento","content":"Because of time constraints, there were many features and improvements we wanted to implement but could not.\n\nContinuous LLM Conversation : We wanted to be able to talk to the LLM continuously without having to press a microphone button. Due to time constraints, we were not able to implement this feature User Personalization and Customization : We aimed to personalize the website to users by adding custom themes, colors, and fonts, but we ran out of time to do so."},{"heading":"Built With","content":"chroma deepgram gemini groq python reflex"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"TAShopping","project_url":"https://devpost.com/software/tashopping","tagline":"Tool Assisted Shopping; Accelerate your shopping to robot-like efficiency","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/608/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Reflex: Best app built using reflex.dev"}],"team_members":[],"built_with":[{"name":"amazon-web-services","url":"https://devpost.com/software/built-with/amazon-web-services"},{"name":"firebase","url":"https://devpost.com/software/built-with/firebase"},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"lambda","url":null},{"name":"reflex","url":null},{"name":"s3","url":null},{"name":"selenium","url":"https://devpost.com/software/built-with/selenium"}],"external_links":[{"label":"tas-frontend-backup.reflex.run","url":"https://tas-frontend-backup.reflex.run/"},{"label":"github.com","url":"https://github.com/Turtdle/TAS-Shopping-2"},{"label":"github.com","url":"https://github.com/Turtdle/TAS-Frontend"},{"label":"github.com","url":"https://github.com/s-akhtar-dev/TAS-Shopping-App"}],"description_sections":[{"heading":"Inspiration","content":"Food is a basic human need. As someone who often finds themselves wandering the aisles of Target, I know firsthand how easy it is to get lost among the countless products and displays. The experience can quickly become overwhelming, leading to forgotten items and a less-than-efficient shopping trip. This project was born from the desire to transform that chaos into a seamless shopping experience. We aim to create a tool that not only helps users stay organized with their grocery lists but also guides them through the store in a way that makes shopping enjoyable and stress-free."},{"heading":"What it does","content":"TAShopping is a smart grocery list app that records your grocery list in an intuitive user interface and generates a personalized route in (almost) any Target location across the United States. Users can easily add items to their lists, and the app will optimize their shopping journey by mapping out the most efficient path through the store."},{"heading":"How we built it","content":"Data Aggregation: We utilized Selenium for web scraping, gathering product information and store layouts from Target's website. Object Storage: Amazon S3 was used for storing images and other static files related to the products. User Data Storage: User preferences and grocery lists are securely stored using Google Firebase . Backend Compute: The backend is powered by AWS Lambda , allowing for serverless computing that scales with demand. Data Categorization: User items are classified with Google Gemini API: AWS API Endpoint provides a reliable way to interact with the backend services and handle requests from the front end. Webapp: The web application is developed using Reflex , providing a responsive and modern interface for users. iPhone App: The iPhone application is built with Swift , ensuring a seamless experience for iOS users."},{"heading":"Challenges we ran into","content":"Data Aggregation: Encountered challenges with the rigidity of Selenium for scraping dynamic content and navigating web page structures. Object Storage: N/A (No significant issues reported) User Data Storage: N/A (No significant issues reported) Backend Compute: Faced long compute times; resolved this by breaking the Lambda function into smaller, more manageable pieces for quicker processing. Backend Compute: Dockerized various builds to ensure compatibility with the AWS Linux environment and streamline deployment. API: Managed the complexities of dealing with and securing credentials to ensure safe API access. Webapp: Struggled with a lack of documentation for Reflex , along with complicated Python dependencies that slowed development. iPhone App: N/A (No significant issues reported)"},{"heading":"Accomplishments that we're proud of","content":"Successfully delivered a finished product with a relatively good user experience that has received positive feedback. Achieved support for hundreds of Target stores across the United States, enabling a wide range of users to benefit from the app."},{"heading":"What we learned","content":"We learned a lot about: Gemini: Gained insights into effective data aggregation and user interface design. AWS: Improved our understanding of cloud computing and serverless architecture with AWS Lambda. Docker: Mastered the process of containerization for development and deployment, ensuring consistency across environments. Reflex: Overcame challenges related to the framework, gaining hands-on experience with Python web development. Firebase: Understood user authentication and real-time database capabilities through Google Firebase. User Experience (UX) Design: Emphasized the importance of intuitive navigation and clear presentation of information in app design. Version Control: Enhanced our collaboration skills and code management practices using Git."},{"heading":"What's next for TAShopping","content":"There are many exciting features on the horizon, including: Google SSO for web app user data: Implementing Single Sign-On functionality to simplify user authentication. Better UX for grocery list manipulation: Improving the user interface for adding, removing, and organizing items on grocery lists. More stores: Expanding support to additional retailers, including Walmart and Home Depot, to broaden our user base and shopping capabilities."},{"heading":"Built With","content":"amazon-web-services firebase gemini google-cloud lambda reflex s3 selenium"},{"heading":"Try it out","content":"tas-frontend-backup.reflex.run github.com github.com github.com"}]},{"project_title":"Elevyn","project_url":"https://devpost.com/software/elevyn-unlock-premium-content-with-decentralized-funding","tagline":"Elevyn empowers creators to monetize exclusive content and fundraisers through blockchain-powered payments, allowing supporters to unlock premium content and contribute transparently.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/089/474/datas/medium.PNG","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"[MLH] Best Use of Midnight"}],"team_members":[],"built_with":[{"name":"eluv.io","url":null},{"name":"midnight","url":null},{"name":"node.js","url":"https://devpost.com/software/built-with/node-js"},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"ripple","url":"https://devpost.com/software/built-with/ripple"},{"name":"sui","url":null},{"name":"web3","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/Yash-kumar7/Elevyn"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Elevyn came from the need to empower creators and fundraisers by offering a decentralized platform where they can monetize their content securely and transparently. We wanted to bridge the gap between content creators and their supporters while integrating the benefits of blockchain technology for seamless transactions and ownership."},{"heading":"What it does","content":"Elevyn allows creators to sell exclusive content and raise funds for causes through a decentralized platform. Contributors can make payments in cryptocurrency, and upon successful payment, they gain access to premium content such as videos, tutorials, or unique digital assets hosted on Eluv.io . It also offers a transparent way to support social causes or fund creative projects. With the integration of Midnight, the platform enhances privacy for both creators and supporters by enabling secure, confidential transactions."},{"heading":"How I built it","content":"Elevyn is built using a combination of React.js for the front-end and Node.js/Express for the backend. The platform integrates with the Ripple (XRP) network for cryptocurrency payments and Sui blockchain for logging donations. The content is hosted and accessed through Eluv.io , ensuring secure and decentralized delivery of premium content. Additionally, Midnight is integrated to ensure privacy and anonymity for users, allowing for private transactions without compromising on blockchain transparency."},{"heading":"Challenges I ran into","content":"One of the main challenges was integrating the different blockchain systems (Ripple and Sui) with the content delivery platform. Ensuring that access to the content was granted only after successful payments and that the content could be delivered securely through Eluv.io also posed technical hurdles. Balancing decentralization with ease of use for non-technical users was another challenge."},{"heading":"Accomplishments that I am proud of","content":"I am proud to have successfully built a decentralized platform that integrates blockchain payments and secure content delivery. Elevyn provides a seamless user experience where supporters can unlock premium content in exchange for cryptocurrency, supporting creators transparently. The successful integration of multiple blockchain systems and content delivery was a significant accomplishment."},{"heading":"What I learned","content":"Throughout the project, we learned a lot about decentralized technologies, blockchain integrations, and managing cryptocurrency transactions. The project pushed us to deepen our understanding of how blockchain can be applied to creative industries and how to handle secure content distribution in a decentralized manner."},{"heading":"What's next for Elevyn:","content":"The next steps for Elevyn involve expanding the types of content creators can sell and adding more flexible payment methods. We plan to introduce more blockchain networks for payments and further enhance the platform's security and scalability. Additionally, we aim to add more community features for creators and supporters to interact and engage on the platform."},{"heading":"Built With","content":"eluv.io midnight node.js react ripple sui web3"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Nova AI","project_url":"https://devpost.com/software/nova-your-automated-pa","tagline":"Your Automated Personal Assistant.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/340/datas/medium.jpeg","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Interaction Company: Build AI Agents that Proactively Handle Your Email!"}],"team_members":[],"built_with":[{"name":"flask","url":"https://devpost.com/software/built-with/flask"},{"name":"gmail","url":"https://devpost.com/software/built-with/gmail"},{"name":"google","url":"https://devpost.com/software/built-with/google--2"},{"name":"google-calendar","url":"https://devpost.com/software/built-with/google-calendar"},{"name":"gpt","url":null},{"name":"json","url":"https://devpost.com/software/built-with/json"},{"name":"openai","url":null},{"name":"pickle","url":null},{"name":"python","url":"https://devpost.com/software/built-with/python"},{"name":"sinch","url":"https://devpost.com/software/built-with/sinch"}],"external_links":[{"label":"github.com","url":"https://github.com/jp-singaraju/nova-ai"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Nova came from the overwhelming volume of emails and tasks that professionals face daily. We aimed to create a solution that simplifies task management and reduces cognitive load, allowing users to focus on what truly matters."},{"heading":"What it does","content":"Nova is an automated email assistant that intelligently processes incoming emails, identifies actionable items, and seamlessly adds them to your calendar. It also sends timely text reminders, ensuring you stay organized and on top of your commitments without the hassle of manual tracking."},{"heading":"How we built it","content":"We built Nova using natural language processing algorithms to analyze email content and extract relevant tasks. By integrating with calendar APIs and SMS services, we created a smooth workflow that automates task management and communication, making it easy for users to manage their schedules."},{"heading":"Challenges we ran into","content":"One of the main challenges was accurately interpreting the context of emails to distinguish between urgent tasks and general information. Additionally, ensuring seamless integration with various calendar platforms and messaging services required extensive testing and refinement."},{"heading":"Accomplishments that we're proud of","content":"We are proud of developing a fully functional prototype of Nova that effectively reduces users' daily load by automating task management. Initial user feedback has been overwhelmingly positive, highlighting the assistant's ability to streamline workflows and enhance productivity."},{"heading":"What we learned","content":"Throughout the development process, we learned the importance of user feedback in refining our algorithms and improving the overall user experience. We also gained insights into the complexities of integrating multiple services to create a cohesive solution."},{"heading":"What's next for Nova","content":"Moving forward, we plan to enhance Nova's capabilities by incorporating machine learning to improve task recognition and prioritization. Our goal is to expand its features and ultimately launch it as a comprehensive productivity tool that transforms how users manage their daily tasks."},{"heading":"Built With","content":"flask gmail google google-calendar gpt json openai pickle python sinch"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Leyline","project_url":"https://devpost.com/software/leyline","tagline":"An AI email assistant that automates form-filling, summarizes messages, and streamlines inbox tasks","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/940/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Interaction Company: Build AI Agents that Proactively Handle Your Email!"}],"team_members":[],"built_with":[{"name":"drizzle","url":null},{"name":"google-cloud","url":"https://devpost.com/software/built-with/google-cloud"},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"nextjs","url":null},{"name":"shadcn","url":null},{"name":"supabase","url":null},{"name":"tailwind","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/gursheyss/leyline-calhacks"}],"description_sections":[{"heading":"Inspiration","content":"The inspiration for Leyline came from The Interaction Company of California's challenge to design an AI agent that proactively handles emails. While Google's AI summarization features in their email app were a good start, we felt they lacked the comprehensive functionality users need. We envisioned an agent that could handle more complex tasks like filling out forms and booking flights, truly taking care of the busy work in your inbox."},{"heading":"What it does","content":"Leyline connects to your Gmail account and parses incoming emails. It uses AI models to analyze the subject, body text, and attachments to determine the best course of action for each email. For example, if there's a form attached with instructions to fill it out, Leyline completes the form for you using information from our database."},{"heading":"How we built it","content":"We developed Leyline as a Next.js app with Supabase as the backend. We use Google Cloud Pub/Sub to connect to Gmail accounts. A custom webhook receives emails from the Pub/Sub topic. The webhook first summarizes the email, then decides if there's a possible tool it could use. We implemented Groq's tool-calling models to determine appropriate actions, such as filling out attached forms using user data from our database. The frontend was built with Tailwind CSS and shadcn UI components."},{"heading":"Challenges we ran into","content":"Processing Gmail messages correctly was challenging due to conflicting documentation and inconsistent data formats. We had to implement workarounds and additional API calls to fetch all the necessary information. Getting the tool-calling workflow to function properly was difficult, as it sometimes failed to call tools or missed steps in the process."},{"heading":"Accomplishments that we're proud of","content":"We created an intuitive and visually appealing user interface. We implemented real-time email support, allowing emails to appear instantly and their processing status to update live. We developed a functional tool workflow for handling various email tasks."},{"heading":"What we learned","content":"This was our first time using Google Cloud Pub/Sub, which taught us about the unique characteristics of cloud provider APIs compared to consumer APIs. We gained experience with tool-calling in AI models and learned about its intricacies."},{"heading":"What's next for Leyline","content":"We plan to expand Leyline's capabilities by:\n\nAdding more tools to handle a wider variety of email-related tasks. Implementing a containerized browser to perform tasks like flight check-ins. Supporting additional email providers beyond Gmail. Developing more specialized tools for processing different types of emails and requests."},{"heading":"Built With","content":"drizzle google-cloud groq nextjs shadcn supabase tailwind typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"AutoLock","project_url":"https://devpost.com/software/autolock","tagline":"Have you ever written insecure code? Probably, but how do you know? How do you fix it? Finding and fixing insecure code can be hard, so AutoLock makes simple.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_photos/003/089/098/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"[MLH] Best Use of Terraform"}],"team_members":[],"built_with":[{"name":"gin","url":null},{"name":"go","url":"https://devpost.com/software/built-with/go"},{"name":"javascript","url":"https://devpost.com/software/built-with/javascript"},{"name":"postgresql","url":"https://devpost.com/software/built-with/postgresql"},{"name":"svelte","url":null},{"name":"typescript","url":"https://devpost.com/software/built-with/typescript"}],"external_links":[{"label":"github.com","url":"https://github.com/tbwrigh/CalHacks"}],"description_sections":[{"heading":"Inspiration","content":"During my last internship, I worked on an aging product with numerous security vulnerabilities, but identifying and fixing these issues was a major challenge. One of my key projects was to implement CodeQL scanning to better locate vulnerabilities. While setting up CodeQL wasn't overly complex, it became repetitive as I had to manually configure it for every repository, identifying languages and creating YAML files. Fixing the issues proved even more difficult as many of the vulnerabilities were obscure, requiring extensive research and troubleshooting. With that experience in mind, I wanted to create a tool that could automate this process, making code security more accessible and ultimately improving internet safety"},{"heading":"What it does","content":"AutoLock automates the security of your GitHub repositories. First, you select a repository and hit install, which triggers a pull request with a GitHub Actions configuration to scan for vulnerabilities and perform AI-driven analysis. Next, you select which vulnerabilities to fix, and AutoLock opens another pull request with the necessary code modifications to address the issues."},{"heading":"How I built it","content":"I built AutoLock using Svelte for the frontend and Go for the backend. The backend leverages the Gin framework and Gorm ORM for smooth API interactions, while the frontend is powered by Svelte and styled using Flowbite."},{"heading":"Challenges we ran into","content":"One of the biggest challenges was navigating GitHub's app permissions. Understanding which permissions were needed and ensuring the app was correctly installed for both the user and their repositories took some time. Initially, I struggled to figure out why I couldn't access the repos even with the right permissions."},{"heading":"Accomplishments that we're proud of","content":"I'm incredibly proud of the scope of this project, especially since I developed it solo. The user interface is one of the best I've ever created—responsive, modern, and dynamic—all of which were challenges for me in the past. I'm also proud of the growth I experienced working with Go, as I had very little experience with it when I started."},{"heading":"What we learned","content":"While the unstable CalHacks WiFi made deployment tricky (basically impossible, terraform kept failing due to network issues 😅), I gained valuable knowledge about working with frontend component libraries, Go's Gin framework, and Gorm ORM. I also learned a lot about integrating with third-party services and navigating the complexities of their APIs."},{"heading":"What's next for AutoLock","content":"I see huge potential for AutoLock as a startup. There's a growing need for automated code security tools, and I believe AutoLock's ability to simplify the process could make it highly successful and beneficial for developers across the web."},{"heading":"Built With","content":"gin go javascript postgresql svelte typescript"},{"heading":"Try it out","content":"github.com"}]},{"project_title":"Hello","project_url":"https://devpost.com/software/hello-gfsy3l","tagline":"Video chat web app powered with real-time ASL transcription.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/087/558/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Deepgram: Deepgram Voice Agent Quest"}],"team_members":[],"built_with":[{"name":"agora","url":null},{"name":"deepgram","url":null},{"name":"gemini","url":"https://devpost.com/software/built-with/gemini"},{"name":"heroku","url":"https://devpost.com/software/built-with/heroku"},{"name":"hume","url":null},{"name":"hyperbolic","url":null},{"name":"react","url":"https://devpost.com/software/built-with/react"},{"name":"vite","url":null}],"external_links":[],"description_sections":[{"heading":"Inspiration","content":"Only a small percentage of Americans use ASL as their main form of daily communication. Hence, no one notices when ASL-first speakers are left out of using FaceTime, Zoom, or even iMessage voice memos. This is a terrible inconvenience for ASL-first speakers attempting to communicate with their loved ones, colleagues, and friends.\n\nThere is a clear barrier to communication between those who are deaf or hard of hearing and those who are fully-abled.\n\nWe created Hello as a solution to this problem for those experiencing similar situations and to lay the ground work for future seamless communication.\n\nOn a personal level, Brandon's grandma is hard of hearing, which makes it very difficult to communicate. In the future this tool may be their only chance at clear communication."},{"heading":"What it does","content":"Expectedly, there are two sides to the video call: a fully-abled person and a deaf or hard of hearing person.\n\nFor the fully-abled person:\n\nTheir speech gets automatically transcribed in real-time and displayed to the end user Their facial expressions and speech get analyzed for sentiment detection\n\nFor the deaf/hard of hearing person:\n\nTheir hand signs are detected and translated into English in real-time The translations are then cleaned up by an LLM and displayed to the end user in text and audio Their facial expressions are analyzed for emotion detection"},{"heading":"How we built it","content":"Our frontend is a simple React and Vite project. On the backend, websockets are used for real-time inferencing. For the fully-abled person, their speech is first transcribed via Deepgram, then their emotion is detected using HumeAI. For the deaf/hard of hearing person, their hand signs are first translated using a custom ML model powered via Hyperbolic, then these translations are cleaned using both Google Gemini and Hyperbolic. Hume AI is used similarly on this end as well. Additionally, the translations are communicated back via text-to-speech using Cartesia/Deepgram."},{"heading":"Challenges we ran into","content":"Custom ML models are very hard to deploy (Credits to https://github.com/hoyso48/Google---American-Sign-Language-Fingerspelling-Recognition-2nd-place-solution ) Websockets are easier said than done Spotty wifi"},{"heading":"Accomplishments that we're proud of","content":"Learned websockets from scratch Implemented custom ML model inferencing and workflows More experience in systems design"},{"heading":"What's next for Hello","content":"Faster, more accurate ASL model. More scalability and maintainability for the codebase."},{"heading":"Built With","content":"agora deepgram gemini heroku hume hyperbolic react vite"}]},{"project_title":"Text to Dot","project_url":"https://devpost.com/software/text-to-dot","tagline":"Real time translation from text to braille.","preview_image_url":"https://d112y698adiu2z.cloudfront.net/photos/production/software_thumbnail_photos/003/091/667/datas/medium.png","prizes":[{"hackathon_name":"Cal Hacks 11.0","hackathon_url":"https://cal-hacks-11-0.devpost.com/","prize_name":"Deepgram: Deepgram Voice Agent Quest"}],"team_members":[],"built_with":[{"name":"cartesia","url":null},{"name":"deepgram","url":null},{"name":"groq","url":"https://devpost.com/software/built-with/groq"},{"name":"hyperbolic","url":null}],"external_links":[{"label":"github.com","url":"https://github.com/psycho-baller/text2dot"}],"description_sections":[{"heading":"Inspiration","content":"Our team focuses primarily on implementing innovative technologies to spark positive change. In line with these ideals, we decided to explore the potential impact of a text reading software that not only provided an additional layer of accessibility for blind and deafblind individuals. Through these efforts, we discovered that despite previous attempts to solve similar problems, solutions were often extremely expensive and incongruous. According to many visually challenged advocates these technologies often lacked real-world application and were specific to online texts or readings which limited their opportunities in everyday tasks like reading books or scanning over menus. Upon further research into afflicted groups, we discovered that there was additionally a large population of people who were both deaf and blind which stopped them from utilizing any forms of auditory input as an alternative, significantly obstructing their means of communication. Employing a very human centered design rooted in various personal accounts and professional testimony, we were able to develop a universal design that provides the visually and dual sensory impaired to experience the world from a new lens. By creating a handheld text to braille and speech generator, we are able to revolutionize the prospects of interpersonal communication for these individuals."},{"heading":"What it does","content":"This solution utilizes a singular piece with two modules, a video camera to decipher text, and a set of solenoids that imitates a standard Grade 2 Braille Grid. This portable accessory is intended to be utilized by a visually impaired or blind deaf individual when they’re attempting to analyze a physical text. This three finger supplement, equipped with a live action camera and sensitive solenoid components, is capable of utilizing a live camera feed to discern the diction of a physical text. Moving forward, the scanned text is moved to an A.I application to clean up the text for either auditory or sensory output in the form of TTS and braille. The text is then adapted into an audio format through a web application or to the classic 6 cells present in the Braille dictionary. Users are given a brief moment to make sense of each braille letter before the system automatically iterates through the remainder of the text. This technology effectively provides users with an alternative method to receive information that isn’t ordinarily accessible to them, granting a more authentic and amplified understanding of the world around them. In this unique application of these technologies, those who are hard of seeing and/or dual sensory impaired receive a more genuine appreciation for texts."},{"heading":"How we built it","content":"As our project required two extremely different pieces, we decided to split up our resources in hopes of tackling both problems at the same time. Regardless, we needed to set firm goals and plan out our required resources or timeline which helped us stay on schedule and formulate a final product that fully utilized our expertise. In terms of hardware, we were somewhat limited for the first half of the hackathon as we had to purchase many of our materials and were unable to complete much of this work till later. We started by identifying a potential circuit design and creating a rigid structure to house our components. From there we simply spent a large amount of time actually implementing our theoretical circuit and applying it to our housing model in addition to cleaning the whole product up. For software, we mostly had problems with connecting each of the pieces after building them out. We first created an algorithm that could take a camera feed and produce a coherent string of text. This would then be run through an AI text to speech generator that could decipher any gibberish. Finally, these texts would be sent through to either be read out loud or be compared against a dictionary to create a binary code that would dictate the on/off states off our solenoids. Finally, we prototyped our product and tested it to see what we could improve in our final implementation to both increase efficiency and decrease latency."},{"heading":"Challenges we ran into","content":"This project was extremely technical and ambitious which meant that it was plagued with difficulties. As a large portion of the project relied on its hardware and implementing complementary materials to formulate a cohesive product, there were countless problems throughout the building phase. We often had incompatible parts whether it be cables, Voltage output/input, or even sizing and scaling issues, we were constantly scrambling to alter, scavenge, and adapt materials for uncommon use cases. Even our main board didn’t produce enough power, leading to an unusual usage of a dehumidifier charger and balled up aluminum foil as a makeshift power bank. All of these mechanical complexities followed by a difficult software end of the project led to an innovative and reworked solution that maintained applicative efficiency. These modifications even continued just hours before the submission deadline when we revamped the entire physical end of our project to make use of newly acquired materials using a more efficient modeling technique. These last second improvements gave our product a more polished and adept edge, making a more impactful and satisfying design. Software wise we also strove to uncover the underappreciated features from our various APIs and tools which often didn’t coincide with our team’s strengths. As we had to simultaneously build out an effective product while troubleshooting our software side, we often ran into incompetencies and struggles. Regardless, we were able to overcome these adversities and produce an impressive result."},{"heading":"Accomplishments that we're proud of","content":"We are proud that we were able to overcome the various difficulties that arose throughout our process and to still showcase the level of success that we did even given such a short timeframe. Our team came in with some members having never done a hackathon before and we made extremely ambitious goals that we were unsure we could uphold. However, we were able to effectively work as a team to develop a final product that clearly represents our initial intentions for the project."},{"heading":"What we learned","content":"As a result of the many cutting-edge sponsors and new technological constraints, our whole team was able to draw from new more effective tools to increase efficiency and quality of our product. Through our careful planning and consistent collaboration, we experienced the future of software and progressed in our intricate technical knowledge within our fields and across specializations. and Because of the cross discipline nature of this project. Additionally, we became more flexible with what materials we needed to build out our hardware applications and especially utilized new TTS technologies to amplify the impact of our projects. In the future, we intend to continue to develop these crucial skills that we obtained at Cal Hacks 11.0, working towards a more accessible future."},{"heading":"What's next for Text to Dot","content":"We would like to work on integrating a more refined design to the hardware component of our project. Unforeseen circumstances with the solenoid led to our final design needing to be adjusted beyond the design of the original model, which could be rectified in future iterations."},{"heading":"Built With","content":"cartesia deepgram groq hyperbolic"},{"heading":"Try it out","content":"github.com"}]}],"generated_at":"2026-02-18T16:45:02.896820Z"}}